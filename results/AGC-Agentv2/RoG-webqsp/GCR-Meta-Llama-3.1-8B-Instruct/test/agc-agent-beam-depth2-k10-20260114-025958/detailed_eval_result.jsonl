{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2gx3\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> base.schemastaging.context_name.pronunciation -> g.125_kkdgj\n# Answer:\nJamaican English"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.district_represented -> Tennessee's 9th congressional district\n# Answer:\nTennessee's 9th congressional district", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician"], "ground_truth": ["Governor of Tennessee", "Speaker of the United States House of Representatives", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4800000000000001, "ans_precission": 0.375, "ans_recall": 0.6666666666666666, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.6451612903225806, "path_ans_precision": 0.625, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Martinique\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Martinique\n# Answer:\nSemi-presidential system"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1000", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.containedby -> Pacific Ocean\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.languages_spoken -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Tuvalu -> location.location.containedby -> Polynesia\n# Answer:\nTuvalu", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Tuvalu -> location.location.contains -> Funafuti\n# Answer:\nTuvalu"], "ground_truth": ["Australia", "Kiribati", "Tonga", "Papua New Guinea", "Indonesia", "Solomon Islands", "New Zealand", "Fiji", "Northern Mariana Islands", "Samoa", "Vanuatu", "New Caledonia", "Marshall Islands", "Guam", "Cook Islands"], "ans_acc": 0.13333333333333333, "ans_hit": 1, "ans_f1": 0.22641509433962267, "ans_precission": 0.75, "ans_recall": 0.13333333333333333, "path_f1": 0.22641509433962267, "path_precision": 0.75, "path_recall": 0.13333333333333333, "path_ans_f1": 0.22641509433962267, "path_ans_precision": 0.75, "path_ans_recall": 0.13333333333333333}
{"id": "WebQTest-1001", "prediction": ["# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> location.location.people_born_here -> Siobhan Cropper\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.image -> Music Truck In Trinidad Carnival\n# Answer:\nSaint James, Trinidad and Tobago", "# Reasoning Path:\nNicki Minaj -> people.person.nationality -> United States of America -> common.topic.notable_types -> Country\n# Answer:\nUnited States of America", "# Reasoning Path:\nNicki Minaj -> music.artist.origin -> Jamaica -> location.location.containedby -> New York City\n# Answer:\nJamaica", "# Reasoning Path:\nNicki Minaj -> people.person.place_of_birth -> Saint James, Trinidad and Tobago -> common.topic.notable_for -> g.125532q8s\n# Answer:\nSaint James, Trinidad and Tobago"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1002", "prediction": ["# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.actor -> Richard Harris\n# Answer:\nRichard Harris", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.actor -> Richard Harris\n# Answer:\nRichard Harris", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.02tb7yy -> film.performance.actor -> Michael Gambon\n# Answer:\nMichael Gambon", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03bvlq0 -> film.performance.film -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Philosopher's Stone", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.02vcbhh -> film.performance.actor -> Michael Gambon\n# Answer:\nMichael Gambon", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.03lwx73 -> film.performance.actor -> Michael Gambon\n# Answer:\nMichael Gambon", "# Reasoning Path:\nProfessor Albus Dumbledore -> film.film_character.portrayed_in_films -> m.0jz6c8 -> film.performance.film -> Harry Potter and the Chamber of Secrets\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nProfessor Albus Dumbledore -> cvg.game_character.games -> m.0hhjwzp -> cvg.game_performance.performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Michael Gambon", "Richard Harris", "Toby Regbo", "Dane Farwell"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5555555555555556, "ans_precission": 0.625, "ans_recall": 0.5, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.5555555555555556, "path_ans_precision": 0.625, "path_ans_recall": 0.5}
{"id": "WebQTest-1003", "prediction": ["# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0vbg9b5 -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0k_kpqx -> government.government_position_held.office_holder -> Nawaz Sharif\n# Answer:\nNawaz Sharif", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.0vsc6pz -> government.government_position_held.office_holder -> Malik Meraj Khalid\n# Answer:\nMalik Meraj Khalid", "# Reasoning Path:\nPrime Minister of Pakistan -> common.topic.notable_for -> g.1255j1742\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7q -> government.government_position_held.jurisdiction_of_office -> Pakistan\n# Answer:\nPakistan", "# Reasoning Path:\nPrime Minister of Pakistan -> government.government_office_or_title.office_holders -> m.088yy7z -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister"], "ground_truth": ["Nawaz Sharif", "Moeenuddin Ahmad Qureshi"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.4444444444444444, "ans_recall": 0.5, "path_f1": 0.5714285714285714, "path_precision": 0.4444444444444444, "path_recall": 0.8, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.5}
{"id": "WebQTest-1004", "prediction": ["# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> award.award_nomination.award -> Primetime Emmy Award for Best Actor in a Single Performance\n# Answer:\nPrimetime Emmy Award for Best Actor in a Single Performance", "# Reasoning Path:\nChristopher Plummer -> common.resource.annotations -> m.0944d05 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07cyt_y -> award.award_nomination.nominated_for -> Our Fathers\n# Answer:\nOur Fathers", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07shq4d -> award.award_nomination.nominated_for -> The Big Event\n# Answer:\nThe Big Event", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07t6yf8 -> award.award_nomination.nominated_for -> Our Fathers\n# Answer:\nOur Fathers", "# Reasoning Path:\nChristopher Plummer -> award.award_winner.awards_won -> m.07t6t0l -> award.award_honor.honored_for -> The Big Event\n# Answer:\nThe Big Event", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.07t6z0n -> award.award_nomination.nominated_for -> The Thorn Birds\n# Answer:\nThe Thorn Birds", "# Reasoning Path:\nChristopher Plummer -> award.award_nominee.award_nominations -> m.010nysm3 -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Heidi", "Lily in Love", "An American Tail", "Battle of Britain", "The Day That Shook the World", "Highpoint", "American Tragedy", "Harrison Bergeron", "The Man Who Would Be King", "Woman Wanted", "Money", "The Boy in Blue", "Wind Across the Everglades", "A Beautiful Mind", "The Imaginarium of Doctor Parnassus", "Silver Blaze", "Nobody Runs Forever", "Starcrash", "Dracula 2000", "Hector and the Search for Happiness", "Somewhere in Time", "The First Circle", "Souvenir", "Ararat", "Elsa & Fred", "On Golden Pond", "Inside Man", "Madeline: Lost in Paris", "The Conspiracy of Fear", "Man in the Chair", "The Silent Partner", "Must Love Dogs", "Impolite", "12 Monkeys", "Triple Cross", "The Clown at Midnight", "Remember", "The Dinosaur Hunter", "The Return of the Pink Panther", "Four Minutes", "Wolf", "Full Disclosure", "Winchell", "9", "Priest", "The Assignment", "The Boss' Wife", "Riel", "Babes in Toyland", "Dreamscape", "Lucky Break", "Caesar and Cleopatra", "Night Flight", "The Legend of Sarila", "Ordeal by Innocence", "Nicholas Nickleby", "The Royal Hunt of the Sun", "Muhammad Ali's Greatest Fight", "A Hazard of Hearts", "The Lake House", "The Tempest", "The Insider", "Already Dead", "Dolores Claiborne", "The Man Who Planted Trees", "Possessed", "Emotional Arithmetic", "Dragnet", "Shadow Dancing", "The Last Station", "Vampire in Venice", "Dark Descent of the Forgotten Empress", "The Forger", "Up", "Rumpelstiltskin", "Blizzard", "Where the Heart Is", "Little Gloria... Happy at Last", "My Dog Tulip", "Blackheart", "A Marriage: Georgia O'Keeffe and Alfred Stieglitz", "Five Good Years", "Lock Up Your Daughters", "I Love N.Y.", "International Velvet", "Syriana", "The Girl with the Dragon Tattoo", "Barrymore", "Aces High", "Hanover Street", "The Fall of the Roman Empire", "Hamlet at Elsinore", "Eyewitness", "When the Circus Came to Town", "Skeletons", "The Pyx", "Firehead", "The New World", "Young Catherine", "National Treasure", "Beginners", "Waterloo", "Secrets", "Malcolm X", "Kali the Little Vampire", "Hidden Agenda", "Imagine", "Liar's Edge", "Oedipus the King", "The Gnomes' Great Adventure", "The Gospel of John", "The Sound of Music", "Prototype", "A Doll's House", "Cold Creek Manor", "Inside Daisy Clover", "Gandahar", "Our Fathers", "Murder by Decree", "Star Trek VI: The Undiscovered Country", "Rock-a-Doodle", "Closing the Ring", "Alexander", "The Spiral Staircase", "Stage Struck", "The Scarlet and the Black", "The Night of the Generals", "The Amateur", "Conduct Unbecoming"], "ans_acc": 0.015503875968992248, "ans_hit": 1, "ans_f1": 0.015037593984962405, "ans_precission": 0.25, "ans_recall": 0.007751937984496124, "path_f1": 0.07547169811320754, "path_precision": 0.25, "path_recall": 0.044444444444444446, "path_ans_f1": 0.029776674937965257, "path_ans_precision": 0.375, "path_ans_recall": 0.015503875968992248}
{"id": "WebQTest-1005", "prediction": ["# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> location.location.containedby -> Rabat-Sal\u00e9-Zemmour-Zaer\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> travel.travel_destination.tourist_attractions -> Mausoleum of Mohammed V\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.capital -> Rabat -> common.topic.notable_types -> City/Town/Village\n# Answer:\nRabat", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Western Sahara -> location.country.capital -> Laayoune\n# Answer:\nWestern Sahara", "# Reasoning Path:\nMorocco -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nMorocco -> location.country.administrative_divisions -> Western Sahara -> location.location.containedby -> Greater Maghreb\n# Answer:\nWestern Sahara", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> common.topic.article -> m.0dgr09\n# Answer:\nTaza-Al Hoceima-Taounate", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Chaouia-Ouardigha -> location.location.time_zones -> Western European Time Zone\n# Answer:\nChaouia-Ouardigha"], "ground_truth": ["Rabat"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1006", "prediction": ["# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.film -> The Program\n# Answer:\nThe Program", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.076_c29 -> film.performance.film -> Breakfast of Champions\n# Answer:\nBreakfast of Champions", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k0669 -> film.performance.film -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0jvwcx -> film.performance.film -> Brother\n# Answer:\nBrother", "# Reasoning Path:\nOmar Epps -> film.actor.film -> m.0k66h_ -> film.performance.character -> Darnell Jefferson\n# Answer:\nDarnell Jefferson", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0b3v06c -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0x0_l37 -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nOmar Epps -> award.award_nominee.award_nominations -> m.0_sxmgs -> award.award_nomination.nominated_for -> Love & Basketball\n# Answer:\nLove & Basketball"], "ground_truth": ["Deadly Voyage", "The Program", "Juice", "Against the Ropes", "Daybreak", "Big Trouble", "Scream 2", "Perfume", "The Wood", "Alfie", "In Too Deep", "Love & Basketball", "Major League II", "Higher Learning", "Dracula 2000", "MTV 20: Jams", "Don't Be a Menace to South Central While Drinking Your Juice in the Hood", "Conviction", "First Time Felon", "The Mod Squad", "Breakfast of Champions", "Brother", "A Day in the Life"], "ans_acc": 0.17391304347826086, "ans_hit": 1, "ans_f1": 0.2901554404145078, "ans_precission": 0.875, "ans_recall": 0.17391304347826086, "path_f1": 0.3684210526315789, "path_precision": 0.875, "path_recall": 0.23333333333333334, "path_ans_f1": 0.2901554404145078, "path_ans_precision": 0.875, "path_ans_recall": 0.17391304347826086}
{"id": "WebQTest-1007", "prediction": ["# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> base.wikipedia_infobox.settlement.area_code -> Area codes 410, 443, and 667\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> government.politician.government_positions_held -> m.0239jrx -> government.government_position_held.district_represented -> California\u2019s 5th congressional district\n# Answer:\nCalifornia\u2019s 5th congressional district", "# Reasoning Path:\nNancy Pelosi -> people.person.place_of_birth -> Baltimore -> location.location.containedby -> Area codes 410, 443, and 667\n# Answer:\nBaltimore", "# Reasoning Path:\nNancy Pelosi -> people.person.nationality -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUnited States of America"], "ground_truth": ["California\u2019s 12th congressional district", "California\u2019s 5th congressional district", "California\u2019s 8th congressional district"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.125, "ans_recall": 0.3333333333333333, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.125, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1008", "prediction": ["# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> United States, with Territories\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> United States of America\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Cass County -> location.location.containedby -> Indiana\n# Answer:\nCass County", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> location.location.containedby -> Contiguous United States\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> Indiana -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nIndiana", "# Reasoning Path:\nWalton -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Indiana\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton -> location.statistical_region.population -> g.11b66g7n0c\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Tipton Township", "United States of America", "Cass County", "Indiana"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.6086956521739131, "path_precision": 0.7777777777777778, "path_recall": 0.5, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-1009", "prediction": ["# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.0sw3d4k -> education.education.institution -> Davenport College\n# Answer:\nDavenport College", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.03k_sf2 -> education.education.institution -> University Yale\n# Answer:\nUniversity Yale", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.education -> m.04hdfwr -> education.education.institution -> Phillips Academy\n# Answer:\nPhillips Academy", "# Reasoning Path:\nGeorge H. W. Bush -> common.topic.notable_for -> g.1255hslmk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.people_with_this_profession -> Bruce Evans\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.specializations -> United States Naval Aviator\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> fictional_universe.character_occupation.characters_with_this_occupation -> Chuck Yeager\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.people_with_this_profession -> Chuck Yeager\n# Answer:\nMilitary aviator", "# Reasoning Path:\nGeorge H. W. Bush -> people.person.profession -> Military aviator -> people.profession.part_of_professional_field -> Military aviation\n# Answer:\nMilitary aviator"], "ground_truth": ["University Yale"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1010", "prediction": ["# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.event -> Athletics at the 1976 Summer Olympics - Men's Decathlon\n# Answer:\nAthletics at the 1976 Summer Olympics - Men's Decathlon", "# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.olympics -> 1976 Summer Olympics\n# Answer:\n1976 Summer Olympics", "# Reasoning Path:\nCaitlyn Jenner -> common.topic.notable_for -> g.12582g99v\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCaitlyn Jenner -> olympics.olympic_athlete.medals_won -> m.07ygz5n -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.01z02gc -> film.performance.film -> Can't Stop the Music\n# Answer:\nCan't Stop the Music", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0_vcp39 -> film.performance.film -> The Hungover Games\n# Answer:\nThe Hungover Games", "# Reasoning Path:\nCaitlyn Jenner -> film.actor.film -> m.0cgnzpc -> film.performance.film -> The Big Tease\n# Answer:\nThe Big Tease", "# Reasoning Path:\nCaitlyn Jenner -> common.topic.webpage -> m.0bnsb5c -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic"], "ground_truth": ["Athletics at the 1976 Summer Olympics - Men's Decathlon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1011", "prediction": ["# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Tibet\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Zhuang languages\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> base.locations.countries.continent -> Asia\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> base.schemastaging.context_name.pronunciation -> m.013160gt\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Zhejiang\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> base.aareas.schema.administrative_area.administrative_children -> Zhejiang\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.administrative_divisions -> Anhui\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Chinese language\n# Answer:\nChina", "# Reasoning Path:\nStandard Tibetan -> language.human_language.main_country -> China -> location.country.administrative_divisions -> Tibet\n# Answer:\nChina"], "ground_truth": ["Tibet", "China"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.6086956521739131, "path_precision": 0.7777777777777778, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1012", "prediction": ["# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> Universal City\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> amusement_parks.park.rides -> Curious George Goes to Town\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.events -> 16th People's Choice Awards\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> book.book_subject.works -> American Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century -> book.written_work.subjects -> James McNamara\n# Answer:\nAmerican Lightning: Terror, Mystery, the Birth of Hollywood, and the Crime of the Century", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> travel.tourist_attraction.near_travel_destination -> Universal City\n# Answer:\nUniversal Studios Hollywood", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal CityWalk -> location.location.containedby -> Los Angeles County\n# Answer:\nUniversal CityWalk", "# Reasoning Path:\nHollywood -> travel.travel_destination.tourist_attractions -> Universal Studios Hollywood -> location.location.containedby -> California\n# Answer:\nUniversal Studios Hollywood"], "ground_truth": ["Universal CityWalk", "Grauman's Egyptian Theatre", "Hollywood Walk of Fame", "Hollywood Heritage Museum", "Hollywood Wax Museum", "Universal Studios Hollywood", "Hollywood Sign", "Griffith Observatory", "TCL Chinese Theatre", "Dolby Theatre"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.32558139534883723, "ans_precission": 0.875, "ans_recall": 0.2, "path_f1": 0.32558139534883723, "path_precision": 0.875, "path_recall": 0.2, "path_ans_f1": 0.32558139534883723, "path_ans_precision": 0.875, "path_ans_recall": 0.2}
{"id": "WebQTest-1014", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.popstra.location.arrestee -> m.064jg0c\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.place_of_birth -> Los Angeles\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> base.ndbcd.buoy.region -> California\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.parents -> Helen Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKim Kardashian -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Kris Jenner -> people.person.place_of_birth -> San Diego\n# Answer:\nKris Jenner", "# Reasoning Path:\nKim Kardashian -> people.person.parents -> Robert Kardashian -> people.person.gender -> Male\n# Answer:\nRobert Kardashian"], "ground_truth": ["Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1015", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon"], "ground_truth": ["Canada", "Stratford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1017", "prediction": ["# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.actor -> Eric Lloyd\n# Answer:\nEric Lloyd", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0k6ckh -> film.performance.character -> Charlie Calvin\n# Answer:\nCharlie Calvin", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0w4bf87 -> film.performance.actor -> Marc Pichette\n# Answer:\nMarc Pichette", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.02tbbmx -> film.performance.actor -> Peter Boyle\n# Answer:\nPeter Boyle", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_0 -> film.performance.actor -> Jesse Collins\n# Answer:\nJesse Collins", "# Reasoning Path:\nThe Santa Clause -> film.film.starring -> m.0dltq_6 -> film.performance.actor -> Joshua Satok\n# Answer:\nJoshua Satok", "# Reasoning Path:\nThe Santa Clause -> media_common.netflix_title.netflix_genres -> Ages 11-12\n# Answer:\nAges 11-12", "# Reasoning Path:\nThe Santa Clause -> common.topic.webpage -> m.09syg4y -> common.webpage.resource -> Review of Santa Clause, The by Hollis Chacona at Austin Chronicle\n# Answer:\nReview of Santa Clause, The by Hollis Chacona at Austin Chronicle", "# Reasoning Path:\nThe Santa Clause -> film.film.produced_by -> Robert Newmyer -> film.producer.film -> The Santa Clause 2\n# Answer:\nRobert Newmyer"], "ground_truth": ["Eric Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.actor -> Danielle Judovits\n# Answer:\nDanielle Judovits", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nYvonne Craig", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.seasons -> The Batman - Season 4\n# Answer:\nThe Batman - Season 4", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.series -> The Batman\n# Answer:\nThe Batman", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nAngie Harmon", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0cgn08z -> tv.regular_tv_appearance.actor -> Dina Meyer\n# Answer:\nDina Meyer"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1020", "prediction": ["# Reasoning Path:\nR. Kelly -> people.person.education -> m.0h3355s -> education.education.institution -> Kenwood Academy\n# Answer:\nKenwood Academy", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> g.11bypnyn2g\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nR. Kelly -> music.group_member.instruments_played -> Keyboard -> music.instrument.variation -> Piano\n# Answer:\nKeyboard", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nR. Kelly -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: V101\n# Answer:\nContemporary R&B", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.releases -> Age Ain't Nothing but a Number\n# Answer:\nAge Ain't Nothing But a Number", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.01100cnr -> music.track_contribution.track -> Hotel\n# Answer:\nHotel", "# Reasoning Path:\nR. Kelly -> music.artist.track_contributions -> m.010_sxwg -> music.track_contribution.track -> I'm Your Angel\n# Answer:\nI'm Your Angel", "# Reasoning Path:\nR. Kelly -> music.producer.tracks_produced -> Age Ain't Nothing But a Number -> music.recording.contributions -> m.0rqjwzt\n# Answer:\nAge Ain't Nothing But a Number"], "ground_truth": ["Kenwood Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1021", "prediction": ["# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> common.topic.article -> m.0w1f1w0\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> 2013 Benghazi conflict -> time.event.included_in_event -> Factional violence in Libya\n# Answer:\n2013 Benghazi conflict", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.locations -> Tunis\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> Second Battle of Benghazi -> base.culturalevent.event.entity_involved -> Anti-Gaddafi forces\n# Answer:\nSecond Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> common.topic.article -> m.0m2t_23\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> time.event.included_in_event -> 2012 Benghazi attack\n# Answer:\n2012 U.S. diplomatic missions attacks", "# Reasoning Path:\nBenghazi -> location.location.events -> Second Battle of Benghazi -> common.topic.notable_types -> Military Conflict\n# Answer:\nSecond Battle of Benghazi", "# Reasoning Path:\nBenghazi -> location.location.events -> 2012 U.S. diplomatic missions attacks -> common.topic.notable_for -> g.1255jv6pr\n# Answer:\n2012 U.S. diplomatic missions attacks"], "ground_truth": ["2012 U.S. diplomatic missions attacks", "Battle of Benina Airport", "Second Battle of Benghazi", "2012 Benghazi attack", "2013 Benghazi conflict", "Operation Dignity Battle", "First Battle of Benghazi"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1022", "prediction": ["# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.citytown -> Beaverton\n# Answer:\nBeaverton", "# Reasoning Path:\nNike, Inc. -> organization.organization.phone_number -> m.0my29ql -> common.phone_number.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> location.mailing_address.postal_code -> 97005\n# Answer:\n97005", "# Reasoning Path:\nNike, Inc. -> organization.organization.headquarters -> m.09rzrss -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nNike, Inc. -> organization.organization.phone_number -> m.0my29ql -> common.phone_number.category -> Customer Service\n# Answer:\nCustomer Service", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m8k -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1024", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71yj1cp\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1025", "prediction": ["# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.team -> Tampa Bay Buccaneers\n# Answer:\nTampa Bay Buccaneers", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcn0z -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcpht -> sports.sports_team_roster.position -> Punter\n# Answer:\nPunter", "# Reasoning Path:\nSteve Spurrier -> people.person.education -> m.03p7x9l -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nSteve Spurrier -> sports.pro_athlete.teams -> m.0hpcphg -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback"], "ground_truth": ["San Francisco 49ers", "Tampa Bay Buccaneers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1027", "prediction": ["# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> 60612\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> sports.sports_facility.teams -> Chicago Sting\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> location.location.containedby -> Cook County\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> location.location.containedby -> Chicago\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> sports.sports_facility.teams -> Chicago Cougars\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> Chicago Stadium -> base.politicalconventions.convention_venue.presidential_nomination_conventions_hosted -> 1932 Democratic National Convention\n# Answer:\nChicago Stadium", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.arena_stadium -> United Center -> sports.sports_facility.teams -> Chicago Bulls\n# Answer:\nUnited Center", "# Reasoning Path:\nChicago Blackhawks -> sports.sports_team.venue -> m.0wz229c -> sports.team_venue_relationship.venue -> Chicago Stadium\n# Answer:\nChicago Stadium"], "ground_truth": ["United Center"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1028", "prediction": ["# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.actor -> Christian Bale\n# Answer:\nChristian Bale", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.actor -> Edward Furlong\n# Answer:\nEdward Furlong", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.04m5b3q -> film.performance.film -> Terminator Salvation\n# Answer:\nTerminator Salvation", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.actor -> Nick Stahl\n# Answer:\nNick Stahl", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.0k45ck -> film.performance.actor -> Edward Furlong\n# Answer:\nEdward Furlong", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.0z43pz8 -> film.performance.actor -> Michael Edwards\n# Answer:\nMichael Edwards", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.02vbxnf -> film.performance.film -> T2 3-D:Battle Across Time\n# Answer:\nT2 3-D:Battle Across Time", "# Reasoning Path:\nJohn Connor -> film.film_character.portrayed_in_films -> m.03n9_pb -> film.performance.film -> Terminator 3: Rise of the Machines\n# Answer:\nTerminator 3: Rise of the Machines"], "ground_truth": ["Dalton Abbott", "Christian Bale", "Michael Edwards", "Edward Furlong", "Nick Stahl"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7017543859649122, "ans_precission": 0.625, "ans_recall": 0.8, "path_f1": 0.6666666666666666, "path_precision": 0.625, "path_recall": 0.7142857142857143, "path_ans_f1": 0.7017543859649122, "path_ans_precision": 0.625, "path_ans_recall": 0.8}
{"id": "WebQTest-1029", "prediction": ["# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.main_country -> Guatemala\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> common.topic.notable_types -> Human Language\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.language_family -> Mayan languages\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Awakateko Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nAwakateko Language", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Maya, Yucat\u00e1n Language -> language.human_language.region -> Americas\n# Answer:\nMaya, Yucat\u00e1n Language", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language"], "ground_truth": ["Tojolabal Language", "Awakateko Language", "Tepehuan, Southeastern Language", "Huastec, Veracruz Language", "O'odham language", "Pima Bajo Language", "Totonac, Highland Language", "Tzotzil language", "Mixtecan languages", "Kickapoo Language", "Tepehua languages", "Chichimeca Jonaz language", "Mazatecan languages", "Lacandon Language", "Yaqui Language", "Mazahua language", "K'iche' language", "Ixcatec Language", "Spanish Language", "Tarahumara language", "Huamelula Language", "Kumiai Language", "Cocopah Language", "Cora, El Nayar Language", "Huarijio Language", "Mixe languages", "Huichol Language", "Chicomuceltec Language", "Chinantecan languages", "Guerrero Amuzgo language", "Chuj language", "Mayo Language", "Tzeltal language", "Cochimi Language", "Pame language", "Chochotec Language", "Chontal, Tabasco Language", "Tlapanec, Azoy\u00fa Language", "Seri Language", "Nahuatl languages", "Huave language", "Tataltepec Chatino Language", "Italian Language", "Pur\u00e9pecha language", "Otomi language", "Q\u2019anjob\u2019al language", "Cuicatec, Teutila Language", "Ixil, San Juan Cotzal Language", "Kiliwa Language", "Paipai Language", "Texistepec language", "Mocho Language", "Maya, Yucat\u00e1n Language", "Matlatzinca language", "Ch\u2019ol language", "Trique language", "Jakaltek language", "Q'eqchi' Language"], "ans_acc": 0.05172413793103448, "ans_hit": 1, "ans_f1": 0.09716599190283401, "ans_precission": 0.8, "ans_recall": 0.05172413793103448, "path_f1": 0.09716599190283401, "path_precision": 0.8, "path_recall": 0.05172413793103448, "path_ans_f1": 0.09716599190283401, "path_ans_precision": 0.8, "path_ans_recall": 0.05172413793103448}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\n1992 NBA draft", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season"], "ground_truth": ["Miami Heat", "Orlando Magic", "LSU Tigers men's basketball", "Phoenix Suns", "Boston Celtics", "Cleveland Cavaliers", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.5, "ans_recall": 0.2857142857142857, "path_f1": 0.2105263157894737, "path_precision": 0.5, "path_recall": 0.13333333333333333, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.5, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1030", "prediction": ["# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.included_in_group -> White people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> Irish people\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.geographic_distribution -> Europe\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Lorenzo Tramaglino -> fictional_universe.fictional_character.gender -> Male\n# Answer:\nLorenzo Tramaglino", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> common.topic.notable_types -> Ethnicity\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.languages_spoken -> Italian Language\n# Answer:\nEuropeans", "# Reasoning Path:\nItalian people -> people.ethnicity.languages_spoken -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nItalian people -> people.ethnicity.included_in_group -> Europeans -> people.ethnicity.includes_groups -> French people\n# Answer:\nEuropeans"], "ground_truth": ["Latin European peoples", "Europeans"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-1031", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nAngelina Jolie -> award.award_nominee.award_nominations -> m.0z9t1ch -> award.award_nomination.nominated_for -> Mr. & Mrs. Smith\n# Answer:\nMr. & Mrs. Smith", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Mervin Praison\n# Answer:\nFilm director"], "ground_truth": ["Film Producer", "Actor", "Screenwriter", "Voice Actor", "Author", "Writer", "Model", "Film director"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.7291666666666666, "path_ans_precision": 0.875, "path_ans_recall": 0.625}
{"id": "WebQTest-1032", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0tln5xz -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> tv.tv_character.appeared_in_tv_program -> m.0v_cdgd -> tv.regular_tv_appearance.actor -> Jim Carrey\n# Answer:\nJim Carrey", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.012vszbf -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.film -> Star Trek VI: The Undiscovered Country\n# Answer:\nStar Trek VI: The Undiscovered Country"], "ground_truth": ["William Shatner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.3157894736842105, "path_precision": 0.375, "path_recall": 0.2727272727272727, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1033", "prediction": ["# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.04f8_rv -> film.performance.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.0k6zn6 -> film.performance.actor -> Gillian Anderson\n# Answer:\nGillian Anderson", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.04f8_rv -> film.performance.film -> The X-Files: I Want to Believe\n# Answer:\nThe X-Files: I Want to Believe", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.series -> The X-Files\n# Answer:\nThe X-Files", "# Reasoning Path:\nDana Scully -> tv.tv_character.appeared_in_tv_program -> m.02nv3nm -> tv.regular_tv_appearance.seasons -> The X-Files season 9\n# Answer:\nThe X-Files season 9", "# Reasoning Path:\nDana Scully -> film.film_character.portrayed_in_films -> m.0k6zn6 -> film.performance.film -> The X-Files\n# Answer:\nThe X-Files", "# Reasoning Path:\nDana Scully -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The X-Files Universe -> common.topic.notable_types -> Fictional Universe\n# Answer:\nThe X-Files Universe"], "ground_truth": ["Gillian Anderson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.11111111111111109, "path_precision": 0.375, "path_recall": 0.06521739130434782, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1035", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.corresponding_type -> Author\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> common.topic.notable_types -> Profession\n# Answer:\nPoet"], "ground_truth": ["Novelist", "Writer", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1036", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> base.schemastaging.context_name.pronunciation -> g.125_kkdgj\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2gx3\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment"], "ground_truth": ["Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1037", "prediction": ["# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.coach -> Mike Woodson\n# Answer:\nMike Woodson", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> sports.sports_team_coach.sports_coached -> Basketball\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qhk3g\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04qkqr7\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> tv.tv_actor.guest_roles -> m.09nch_2\n# Answer:\nDerek Fisher", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0qt2p64 -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.coaches -> m.0112zxyh -> sports.sports_team_coach_tenure.coach -> Kurt Rambis\n# Answer:\nKurt Rambis", "# Reasoning Path:\nNew York Knicks -> basketball.basketball_team.head_coach -> Derek Fisher -> basketball.basketball_player.player_statistics -> m.04q9tdk\n# Answer:\nDerek Fisher"], "ground_truth": ["Derek Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1038", "prediction": ["# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> people.profession.specialization_of -> Athlete\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nDancer", "# Reasoning Path:\nWesley Snipes -> common.topic.notable_for -> g.1254x6509\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> base.descriptive_names.names.descriptive_name -> m.0105yc8_\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> common.topic.notable_for -> g.12565kr7j\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Martial Artist -> base.descriptive_names.names.descriptive_name -> m.0108mspk\n# Answer:\nMartial Artist", "# Reasoning Path:\nWesley Snipes -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer"], "ground_truth": ["Film Producer", "Actor", "Singer", "Martial Artist", "Businessperson", "Dancer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.64, "path_precision": 0.8888888888888888, "path_recall": 0.5, "path_ans_f1": 0.64, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5}
{"id": "WebQTest-1039", "prediction": ["# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.0wk6dss -> people.place_lived.location -> Avon\n# Answer:\nAvon", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.containedby -> Ohio\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11b66g3p9g\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.hud_foreclosure_area.total_residential_addresses -> m.07dcp5y\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.places_lived -> m.04hcytr -> people.place_lived.location -> Mansfield\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.people_born_here -> Amy Douglass\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.statistical_region.population -> g.11bc8563lx\n# Answer:\nMansfield", "# Reasoning Path:\nSherrod Brown -> people.person.place_of_birth -> Mansfield -> location.location.containedby -> Richland County\n# Answer:\nMansfield"], "ground_truth": ["Avon", "Mansfield"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> travel.travel_destination.tourist_attractions -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.contains -> Thomas Paine Cottage\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.containedby -> Westchester County\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> organization.organization_scope.organizations_with_this_scope -> iFix iBuy\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle -> location.location.containedby -> New York\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> film.actor.film -> m.02t9_1z -> film.performance.film -> Americathon\n# Answer:\nAmericathon"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1040", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language"], "ground_truth": ["Wayuu Language", "Macuna Language", "P\u00e1ez language", "Piapoco Language", "Playero language", "Siona Language", "Inga, Jungle Language", "Piaroa Language", "Tunebo, Angosturas Language", "Achawa language", "Tucano Language", "Providencia Sign Language", "Omejes Language", "Waimaj\u00e3 Language", "Koreguaje Language", "Cof\u00e1n Language", "Macagu\u00e1n Language", "Cumeral Language", "Guahibo language", "Guambiano Language", "Cams\u00e1 Language", "Minica Huitoto", "Hupd\u00eb Language", "Carijona Language", "Baudo language", "Arhuaco Language", "Cuiba language", "Nukak language", "Piratapuyo Language", "Cabiyar\u00ed Language", "Puinave Language", "Spanish Language", "Awa-Cuaiquer Language", "Totoro Language", "Cagua Language", "Andoque Language", "Uwa language", "Romani, Vlax Language", "Siriano Language", "Tunebo, Barro Negro Language", "Murui Huitoto language", "Coxima Language", "Bora Language", "Curripaco Language", "Ponares Language", "Tanimuca-Retuar\u00e3 Language", "Ticuna language", "Runa Language", "Islander Creole English", "Kuna, Border Language", "Tunebo, Central Language", "Tuyuca language", "Palenquero Language", "Carabayo Language", "Bar\u00ed Language", "Natagaimas Language", "Ember\u00e1, Northern Language", "Tunebo, Western Language", "Tomedes Language", "Tinigua language", "Yucuna Language", "Macaguaje Language", "Inga Language", "Yukpa Language", "Andaqui Language", "Tama Language", "Barasana Language", "Ocaina Language", "Quechua, Napo Lowland Language", "Kogi Language", "Catio language", "Anserma Language", "Cocama language", "Cubeo Language", "Nonuya language", "Malayo Language", "Guanano Language", "Desano Language", "Nheengatu language", "Coyaima Language", "S\u00e1liba Language", "Chipiajes Language", "Pijao Language", "Muinane Language", "Guayabero Language", "Colombian Sign Language"], "ans_acc": 0.011627906976744186, "ans_hit": 1, "ans_f1": 0.022988505747126436, "ans_precission": 1.0, "ans_recall": 0.011627906976744186, "path_f1": 0.02316602316602317, "path_precision": 0.75, "path_recall": 0.011764705882352941, "path_ans_f1": 0.022988505747126436, "path_ans_precision": 1.0, "path_ans_recall": 0.011627906976744186}
{"id": "WebQTest-1041", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> England\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> United Kingdom\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.currency_used -> Pound sterling\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nUnited Kingdom -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Wales\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Wales\n# Answer:\nPound sterling"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1042", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_episodes -> m.0jzvxtw -> tv.tv_guest_role.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_episodes -> g.11byb39pmc\n# Answer:\ntv.tv_character.appeared_in_tv_episodes", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_episodes -> m.0jzvxtw -> tv.tv_guest_role.episodes_appeared_in -> Peter's Progress\n# Answer:\nPeter's Progress", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nMila Kunis"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1043", "prediction": ["# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> United Arab Republic\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Arab Republic\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nMali -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6flnr\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nMali -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Burkina Faso\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6ntbzfs\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Semi-presidential system", "Unitary state", "Republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.6829268292682926, "path_precision": 0.7, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1044", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> base.schemastaging.athlete_extra.salary -> m.0j5172y -> base.schemastaging.athlete_salary.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0g5h3bt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F."], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.75, "path_recall": 0.3, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1045", "prediction": ["# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> location.postal_code.country -> United States of America\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> common.topic.notable_for -> g.125dzdth5\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26507 -> common.topic.notable_types -> Postal Code\n# Answer:\n26507", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26504 -> common.topic.notable_types -> Postal Code\n# Answer:\n26504", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> location.postal_code.country -> United States of America\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> location.location.geometry -> m.058x11n\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26508 -> common.topic.notable_types -> Postal Code\n# Answer:\n26508", "# Reasoning Path:\nMorgantown -> location.citytown.postal_codes -> 26504 -> location.postal_code.country -> United States of America\n# Answer:\n26504"], "ground_truth": ["26505", "26501", "26506", "26502", "26508", "26504", "26507"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.6, "path_precision": 1.0, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1046", "prediction": ["# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rxy8r\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.statistical_region.population -> g.11b674hjtb\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nJacksonville -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.0j44rkj\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nJacksonville -> location.location.adjoin_s -> m.02xjbkj -> location.adjoining_relationship.adjoins -> St. Johns County\n# Answer:\nSt. Johns County"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1047", "prediction": ["# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.character -> Steve the Pirate\n# Answer:\nSteve the Pirate", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jp9kl -> film.performance.character -> Dwight\n# Answer:\nDwight", "# Reasoning Path:\nAmy Stiller -> people.person.sibling_s -> m.04dq69h -> people.sibling_relationship.sibling -> Ben Stiller\n# Answer:\nBen Stiller", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jqjf8 -> film.performance.character -> Blazer\n# Answer:\nBlazer", "# Reasoning Path:\nAmy Stiller -> film.actor.film -> m.063wc2g -> film.performance.character -> Flower Girl/Carol\n# Answer:\nFlower Girl/Carol", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.02kdl29 -> film.performance.actor -> Alan Tudyk\n# Answer:\nAlan Tudyk", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.starring -> m.03jqjt0 -> film.performance.character -> Fran\n# Answer:\nFran", "# Reasoning Path:\nDodgeBall: A True Underdog Story -> film.film.subjects -> Dodgeball -> common.topic.article -> m.01ft7_\n# Answer:\nDodgeball"], "ground_truth": ["Keno Waitress"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1048", "prediction": ["# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.place_of_birth -> Winchester Cathedral Priory\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.parents -> Elizabeth of York\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> common.topic.notable_for -> g.125c04cnp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.sibling_s -> m.04jt6xb\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Elizabeth I of England\n# Answer:\nHenry VIII of England", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Arthur, Prince of Wales -> people.person.gender -> Male\n# Answer:\nArthur, Prince of Wales", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Edmund Tudor, Duke of Somerset -> common.topic.notable_for -> g.125fhsz99\n# Answer:\nEdmund Tudor, Duke of Somerset", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Elizabeth Tudor -> people.person.place_of_birth -> Richmond Palace\n# Answer:\nElizabeth Tudor", "# Reasoning Path:\nHenry VII of England -> people.person.children -> Henry VIII of England -> people.person.children -> Mary I of England\n# Answer:\nHenry VIII of England"], "ground_truth": ["Arthur, Prince of Wales", "Henry VIII of England", "Elizabeth Tudor", "Roland de Velville", "Mary Tudor, Queen of France", "Edmund Tudor, Duke of Somerset", "Edward Tudor", "Margaret Tudor", "Katherine Tudor"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.5925925925925926, "ans_precission": 0.8888888888888888, "ans_recall": 0.4444444444444444, "path_f1": 0.5517241379310346, "path_precision": 0.8888888888888888, "path_recall": 0.4, "path_ans_f1": 0.5925925925925926, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> language.human_language.region -> Asia\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language -> common.topic.notable_types -> Human Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language -> media_common.netflix_genre.titles -> Shutter\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language -> language.human_language.countries_spoken_in -> Laos\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language -> language.human_language.main_country -> Myanmar\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.11b60tqlwz\n# Answer:\nlocation.statistical_region.gdp_growth_rate"], "ground_truth": ["Mlabri Language", "Khmer language", "Nyaw Language", "Akha Language", "Hmong language", "Malay, Pattani Language", "Mon Language", "Vietnamese Language", "Lao Language", "Thai Language", "Saek language", "Cham language", "Phu Thai language"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.4444444444444444, "ans_precission": 0.8, "ans_recall": 0.3076923076923077, "path_f1": 0.4274809160305344, "path_precision": 0.7, "path_recall": 0.3076923076923077, "path_ans_f1": 0.4444444444444444, "path_ans_precision": 0.8, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-1050", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Mickey McFinnegan\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Meg Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Peter Griffin\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.locations -> Quahog\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin"], "ground_truth": ["Meg Griffin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1051", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> United States of America\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Yerba Buena Center for the Arts\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> Area codes 415 and 628\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1906\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> film.film_subject.films -> San Francisco -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> California\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor\n# Answer:\nSan Francisco"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1052", "prediction": ["# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.actor -> Seamus Davey-Fitzpatrick\n# Answer:\nSeamus Davey-Fitzpatrick", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.actor -> Tomas Wooler\n# Answer:\nTomas Wooler", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610c -> film.performance.actor -> Liev Schreiber\n# Answer:\nLiev Schreiber", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610p -> film.performance.character -> Damien Thorn\n# Answer:\nDamien Thorn", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.05h9v78 -> film.performance.character -> Damien Thorn\n# Answer:\nDamien Thorn", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.0k610c -> film.performance.character -> Robert Thorn\n# Answer:\nRobert Thorn", "# Reasoning Path:\nThe Omen -> film.film.starring -> m.02tb9t2 -> film.performance.character -> Keith Jennings\n# Answer:\nKeith Jennings", "# Reasoning Path:\nThe Omen -> film.film.sequel -> Damien: Omen II -> film.film.sequel -> Omen III: The Final Conflict\n# Answer:\nDamien: Omen II"], "ground_truth": ["Tomas Wooler", "Seamus Davey-Fitzpatrick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1054", "prediction": ["# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.actor -> Jon Favreau filmography\n# Answer:\nJon Favreau filmography", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483k3 -> film.performance.character -> Denver\n# Answer:\nDenver", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.actor -> Dwight Yoakam\n# Answer:\nDwight Yoakam", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.0h2z5vr -> film.performance.actor -> Zachary Gordon\n# Answer:\nZachary Gordon", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.04ddq2f -> film.performance.actor -> Reese Witherspoon\n# Answer:\nReese Witherspoon", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.04ddq2l -> film.performance.actor -> Vince Vaughn\n# Answer:\nVince Vaughn", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.06483jz -> film.performance.character -> Pastor Phil\n# Answer:\nPastor Phil", "# Reasoning Path:\nFour Christmases -> film.film.starring -> m.0h2z5vr -> film.performance.character -> Kid in Jump-Jump\n# Answer:\nKid in Jump-Jump"], "ground_truth": ["Jon Favreau filmography"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1055", "prediction": ["# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> travel.tourist_attraction.near_travel_destination -> Colchester\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.article -> m.05zjnk\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> base.aareas.schema.administrative_area.capital -> Chelmsford -> travel.travel_destination.tourist_attractions -> Riverside Ice & Leisure Centre\n# Answer:\nChelmsford", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.image -> A Wolf at Colchester Zoo\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Beth Chatto Gardens -> travel.tourist_attraction.near_travel_destination -> Colchester\n# Answer:\nBeth Chatto Gardens", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> location.location.containedby -> Colchester\n# Answer:\nColchester Zoo", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Waltham Abbey Royal Gunpowder Mills -> location.location.containedby -> United Kingdom\n# Answer:\nWaltham Abbey Royal Gunpowder Mills", "# Reasoning Path:\nEssex -> travel.travel_destination.tourist_attractions -> Colchester Zoo -> common.topic.webpage -> m.03l72ys\n# Answer:\nColchester Zoo"], "ground_truth": ["Colchester Zoo", "Cudmore Grove Country Park", "Mistley Place Park", "RHS Garden, Hyde Hall", "Beth Chatto Gardens", "Thorndon Country Park", "Waltham Abbey Royal Gunpowder Mills", "Marsh Farm Country Park", "Stansted Mountfitchet Castle", "Green Island Gardens"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.44680851063829785, "ans_precission": 0.875, "ans_recall": 0.3, "path_f1": 0.44680851063829785, "path_precision": 0.875, "path_recall": 0.3, "path_ans_f1": 0.44680851063829785, "path_ans_precision": 0.875, "path_ans_recall": 0.3}
{"id": "WebQTest-1056", "prediction": ["# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> sports.sports_team_roster.player -> LeBron James\n# Answer:\nLeBron James", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0csn6xn -> sports.sports_team_roster.player -> LeBron James\n# Answer:\nLeBron James", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0102thh3 -> sports.sports_team_roster.player -> Scotty Hopson\n# Answer:\nScotty Hopson", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.0114813g -> sports.sports_team_roster.player -> Brendan Haywood\n# Answer:\nBrendan Haywood", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.011461z_ -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nCleveland Cavaliers -> sports.sports_team.roster -> m.01148phj -> sports.sports_team_roster.player -> Edin Bav\u010di\u0107\n# Answer:\nEdin Bav\u010di\u0107"], "ground_truth": ["Shannon Brown", "Anthony Bennett", "LeBron James", "Dion Waiters", "Andrew Wiggins", "Luke Jackson", "JJ Hickson"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.25, "ans_recall": 0.14285714285714285, "path_f1": 0.1904761904761905, "path_precision": 0.25, "path_recall": 0.15384615384615385, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.25, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1057", "prediction": ["# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abel Herzberg\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.article -> m.0139dt\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Abraham Asscher\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> common.topic.image -> Mass graves at Bergen Belsen\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> base.schemastaging.context_name.pronunciation -> g.125_q_7ms\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> book.book_subject.works -> The Diary of a Young Girl\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Adrien de Noailles\n# Answer:\nBergen-Belsen concentration camp", "# Reasoning Path:\nAnne Frank -> base.fictionaluniverse.deceased_fictional_character.place_of_death -> Bergen-Belsen concentration camp -> people.place_of_interment.interred_here -> Alfred Wiener\n# Answer:\nBergen-Belsen concentration camp"], "ground_truth": ["Bergen-Belsen concentration camp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1059", "prediction": ["# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Fataluku Language -> common.topic.notable_for -> g.1258tz_b0\n# Answer:\nFataluku Language", "# Reasoning Path:\nTimor-Leste -> location.country.official_language -> Tetun Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTetun Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Fataluku Language -> common.topic.article -> m.063xm2\n# Answer:\nFataluku Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Fataluku Language -> base.rosetta.languoid.local_name -> Fataluku\n# Answer:\nFataluku Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Bekais -> common.topic.notable_types -> Human Language\n# Answer:\nBekais", "# Reasoning Path:\nTimor-Leste -> location.country.official_language -> Tetun Language -> base.rosetta.languoid.parent -> East Nuclear Timor Group\n# Answer:\nTetun Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Fataluku Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nFataluku Language", "# Reasoning Path:\nTimor-Leste -> location.country.languages_spoken -> Indonesian Language -> language.human_language.countries_spoken_in -> Indonesia\n# Answer:\nIndonesian Language", "# Reasoning Path:\nTimor-Leste -> location.statistical_region.internet_users_percent_population -> g.11b60w2nsc\n# Answer:\nlocation.statistical_region.internet_users_percent_population"], "ground_truth": ["Kemak Language", "Wetarese", "Mambai Language", "Makuv'a Language", "Makasae Language", "Galoli Language", "Kawaimina languages", "Portuguese Language", "English Language", "Dawan", "Indonesian Language", "Habu Language", "Bunak Language", "Idalaka", "Tetun-Terik", "Tukudede Language", "Makalero dialect", "Fataluku Language", "Bekais", "Tetun Language"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.326530612244898, "ans_precission": 0.8888888888888888, "ans_recall": 0.2, "path_f1": 0.326530612244898, "path_precision": 0.8888888888888888, "path_recall": 0.2, "path_ans_f1": 0.326530612244898, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.2}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Programmer\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Entrepreneur\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> film.film_subject.films -> Mark Zuckerberg: Inside Facebook\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.profession -> Businessperson\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.owner -> Mark Zuckerberg\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.parents -> Edward Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.directed_by -> David Fincher -> film.director.film -> 20,000 Leagues Under the Sea: Captain Nemo\n# Answer:\nDavid Fincher", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Catfish\n# Answer:\nSocial networking service"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1060", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1061", "prediction": ["# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> common.topic.article -> m.03v0m\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.parent -> Greek Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> base.schemastaging.context_name.pronunciation -> g.125_r38g5\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Hellenic languages -> common.topic.article -> m.05pd8n2\n# Answer:\nHellenic languages", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Goidelic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.languoid_class -> Group\n# Answer:\nAttic Group", "# Reasoning Path:\nGreek Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_for -> g.1yg57hd5_\n# Answer:\nIndo-European languages", "# Reasoning Path:\nGreek Language -> base.rosetta.languoid.parent -> Attic Group -> base.rosetta.languoid.child -> Yevanic language\n# Answer:\nAttic Group"], "ground_truth": ["Attic Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1062", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> common.topic.notable_types -> Country -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> common.topic.notable_types -> Country -> freebase.type_hints.included_types -> Location\n# Answer:\nCountry"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1063", "prediction": ["# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.containedby -> Brussels\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Ad\u00e8le Christiaens\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.contains -> La Cambre Abbey\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.notable_types -> City/Town/Village\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> common.topic.image -> La Cambre Abbey in Ixelles.\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.people_born_here -> Agn\u00e8s Varda\n# Answer:\nIxelles", "# Reasoning Path:\nAudrey Hepburn -> award.ranked_item.appears_in_ranked_lists -> m.09p5yb2 -> award.ranking.list -> AFI's 100 Years...100 Stars\n# Answer:\nAFI's 100 Years...100 Stars", "# Reasoning Path:\nAudrey Hepburn -> people.person.place_of_birth -> Ixelles -> location.location.containedby -> Belgium\n# Answer:\nIxelles"], "ground_truth": ["Ixelles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1065", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars"], "ground_truth": ["James Earl Jones", "Dr. Smoov", "Abraham Benrubi", "Matt Lanter", "Zac Efron"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.30303030303030304, "ans_precission": 0.625, "ans_recall": 0.2, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.30303030303030304, "path_ans_precision": 0.625, "path_ans_recall": 0.2}
{"id": "WebQTest-1067", "prediction": ["# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.04hdcyk -> education.education.institution -> United States Military Academy\n# Answer:\nUnited States Military Academy", "# Reasoning Path:\nDouglas MacArthur -> people.person.education -> m.0n0mx8r -> education.education.institution -> TMI \u2014 The Episcopal School of Texas\n# Answer:\nTMI \u2014 The Episcopal School of Texas", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> education.educational_institution.school_type -> High school\n# Answer:\nDouglas MacArthur High School", "# Reasoning Path:\nDouglas MacArthur -> award.award_winner.awards_won -> m.01145h_f -> freebase.valuenotation.has_value -> Year\n# Answer:\nYear", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> A general is just as good or just as bad as the troops under his command make him. -> common.topic.notable_types -> Quotation\n# Answer:\nA general is just as good or just as bad as the troops under his command make him.", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> MacArthur Causeway -> common.topic.article -> m.09tmvf\n# Answer:\nMacArthur Causeway", "# Reasoning Path:\nDouglas MacArthur -> people.person.quotations -> Age wrinkles the body. Quitting wrinkles the soul. -> common.topic.notable_types -> Quotation\n# Answer:\nAge wrinkles the body. Quitting wrinkles the soul.", "# Reasoning Path:\nDouglas MacArthur -> symbols.name_source.namesakes -> Douglas MacArthur High School -> organization.organization.headquarters -> m.0dhdwr7\n# Answer:\nDouglas MacArthur High School"], "ground_truth": ["TMI \u2014 The Episcopal School of Texas", "United States Military Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1068", "prediction": ["# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Rihanna\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Anderson Sealy\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.webpage -> m.0b47dqc\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Barto Bartlett\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> common.topic.image -> Map of Barbados showing the Saint Michael parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.people_born_here -> Carlisle Best\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Saint Michael Parish -> location.location.containedby -> Barbados Island\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nBarbados -> location.location.contains -> Barbados Island -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados Island", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Hugh Gordon Cummins", "Shai Hope", "Malcolm Marshall", "Sheridan Grosvenor", "Diquan Adamson", "Gregory Goodridge", "Kandy Fenty", "Anthony Kellman", "Douglas Dummett", "Dennis Archer", "Earl Maynard", "Micky Welch", "Ronald Fenty", "Nita Barrow", "Kycia Knight", "Frank Collymore", "Richard Pile", "James Waithe", "Dwayne Stanford", "Denys Williams", "Barry Skeete", "John Parris", "Tony Cordle", "Ella Jane New", "Shakera Reece", "William Maynard Gomm", "Frank Leslie Walcott", "Murr", "Romario Harewood", "Colin Young", "Andy Straughn", "Pamela Lavine", "Ryan Hinds", "Lloyd Erskine Sandiford", "Jon Rubin", "Christopher Codrington", "Robert Bailey", "Wilfred Wood", "Jamal Chandler", "Martin Nurse", "Stephen Griffith", "Mia Mottley", "James Wedderburn", "Peter Lashley", "Chris Braithwaite", "Kyle Mayers", "Emmerson Trotman", "Edwin Lascelles, 1st Baron Harewood", "Omar Archer", "Jonathan Straker", "Jason Holder", "Jomo Brathwaite", "Alvin Rouse", "John Richard Farre", "Kyle Gibson", "William Perkins", "Riviere Williams", "Lionel Paul", "Sylvester Braithwaite", "Tristan Parris", "Kyle Hope", "Rashida Williams", "Denzil H. Hurley", "Clyde Mascoll", "Marita Payne", "Raheim Sargeant", "Charlie Griffith", "Campbell Foster", "George Blackman", "Arnold Josiah Ford", "Clennell Wickham", "Tony White", "Ron King", "Tom Adams", "Adriel Brathwaite", "Hadan Holligan", "Roger Blades", "Seibert Straughn", "Glenville Lovell", "Sam Seale", "Carl Joseph", "Lloyd A. Thompson", "Deandra Dottin", "Alison Sealy-Smith", "Richard B. Moore", "Lene Hall", "Wayne Sobers", "Magnet Man", "Don Kinch", "Jackie Roberts", "Anne Cools", "Frank L. White", "William Cleeve", "Edward Evelyn Greaves", "Richard Lavine", "Renaldo Fenty", "Renaldo Gilkes", "Bryan Neblett", "Nick Nanton", "Roy Callender", "Robert Callender", "Albert Beckles", "Jason Carter", "Agymah Kamau", "Winston Reid", "Shakera Selman", "Chris Jordan", "Jennifer Gibbons", "Jaicko", "Zeeteah Massiah", "Kirk Edwards", "Ivor Germain", "George Alleyne", "Ryan Wiggins", "Rickey A. Walcott", "Eyre Sealy", "Samuel Jackman Prescod", "Rashidi Boucher", "Billie Miller", "Seymour Nurse", "Louis Eugene King", "Goodridge Roberts", "David Comissiong", "Kyshona Knight", "Keith Griffith", "Alana Shipp", "Greg Armstrong", "Jabarry Chandler", "Monica Braithwaite", "Carlos Brathwaite", "John Holder", "June Gibbons", "Andre Bourne", "John Lucas", "Arturo Tappin", "Sir William Randolph Douglas", "Mabel Keaton Staupers", "Karen Lord", "Neville Goddard", "Richard Moody", "Philo Wallace", "Wyndham Gittens", "Hal Linton", "Cynthia Rosalina La Touche Andersen", "Colin Forde", "Austin Clarke", "Bentley Springer", "Ricky Hoyte", "Norman Forde", "Hartley Alleyne", "David Holford", "Hugh Springer", "Float Woods", "Renn Dickson Hampden", "Redd Pepper", "Lunden De'Leon", "Tony Cozier", "Prof Edwards", "Robin Bynoe", "Ashley Bickerton", "Javon Searles", "Chai Lloyd", "Hugh Laing", "Branford Taitt", "Wendell White", "Mario Harte", "Kerry Holder", "Shawn Terry Cox", "William T. Shorey", "Sylvesta Stuart", "Henry Honychurch Gorringe", "Ricardo Ellcock", "Ramuel Miller", "Trevor W. Payne", "Arthur Hendy", "Romell Brathwaite", "Annalee Davis", "Dadrian Forde", "Samuel Hinds", "Owen Arthur", "Conrad Hunte", "Dwight James", "Dwayne Griffith", "Henry Doorly", "Omari Eastmond", "Hal Padmore", "Armando Lashley", "Gordon Greenidge", "Kensley Reece", "Horace Stoute", "Tony Reid", "G. Aubrey Goodman", "Ricardo Morris", "Richard Roett", "Priya Balachandran", "Anthony Forde", "George Moe", "Xavier Marcus", "Hilary Beckles", "Kadeem Latham", "Thomas Jordan", "Jim Hendy", "John Goddard", "Curtis Odle", "Tim Thorogood", "Cecil Foster", "Anthony Alleyne", "Rayvon", "Crystal Fenty", "Shane Dowrich", "Odimumba Kwamdela", "Dave Wilkins", "Alan Emtage", "Avinash Persaud", "Menelik Shabazz", "Anderson Cummins", "Kemar Headley", "John Shepherd", "George Codrington", "Nevada Phillips", "Percy Tarilton", "Shaquana Quintyne", "Craig Worrell", "Kirk Corbin", "Woodie Blackman", "Jason Boxhill", "Kat Flint", "Michael Stoute", "Adrian Chase", "Kyle Corbin", "Brandy Fenty"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1069", "prediction": ["# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.statistical_region.official_development_assistance -> g.12cp_kgkf\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.breed_of -> Dog\n# Answer:\nTibetan Terrier", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> India\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> base.petbreeds.dog_breed.color -> Black\n# Answer:\nTibetan Terrier"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Teacher -> common.topic.notable_types -> Profession\n# Answer:\nTeacher", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.occupation -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman"], "ground_truth": ["Philosopher", "Teacher", "Author", "Writer", "Architect", "Lawyer", "Statesman", "Archaeologist", "Farmer", "Inventor"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.6666666666666666, "ans_recall": 0.5, "path_f1": 0.46511627906976744, "path_precision": 0.5555555555555556, "path_recall": 0.4, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6}
{"id": "WebQTest-1070", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> Actor\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> people.person.parents -> Debbie Rowe\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.subject_of -> Diamond Ranch Academy\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.sibling_s -> m.098j68b\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> TV Personality\n# Answer:\nMichael Joseph Jackson, Jr."], "ground_truth": ["Michael Joseph Jackson, Jr.", "Paris-Michael Katherine Jackson", "Prince Michael Jackson II"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1072", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nPaul Fannin", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nJan Brewer", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0108gfng -> government.government_position_held.office_holder -> Keith Brown\n# Answer:\nKeith Brown", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.010dylrt -> government.government_position_held.office_holder -> Terry Goddard\n# Answer:\nTerry Goddard"], "ground_truth": ["Jan Brewer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1074", "prediction": ["# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.team -> Dallas Mavericks\n# Answer:\nDallas Mavericks", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.team -> Los Angeles Clippers\n# Answer:\nLos Angeles Clippers", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0_w1nkg -> base.schemastaging.athlete_salary.team -> Los Angeles Clippers\n# Answer:\nLos Angeles Clippers", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j2gtf_ -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0j2l84k -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nLamar Odom -> sports.pro_athlete.teams -> m.0j7rs8_ -> sports.sports_team_roster.team -> Dallas Mavericks\n# Answer:\nDallas Mavericks", "# Reasoning Path:\nLamar Odom -> basketball.basketball_player.player_statistics -> m.04qrr58 -> basketball.basketball_player_stats.season -> 1999\u20132000 NBA season\n# Answer:\n1999\u20132000 NBA season", "# Reasoning Path:\nLamar Odom -> base.schemastaging.athlete_extra.salary -> m.0_w1nkg -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["New York Knicks", "Dallas Mavericks", "Los Angeles Clippers", "Saski Baskonia", "Los Angeles Lakers"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6122448979591836, "ans_precission": 0.625, "ans_recall": 0.6, "path_f1": 0.4166666666666667, "path_precision": 0.625, "path_recall": 0.3125, "path_ans_f1": 0.6122448979591836, "path_ans_precision": 0.625, "path_ans_recall": 0.6}
{"id": "WebQTest-1075", "prediction": ["# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Giza Governorate\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> location.location.containedby -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> travel.tourist_attraction.near_travel_destination -> Giza -> location.location.containedby -> Egypt\n# Answer:\nGiza", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> base.militaryinfiction.location_in_fiction.contains -> Ancient Egypt\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nEgypt", "# Reasoning Path:\nGreat Pyramid of Giza -> location.location.containedby -> Giza Plateau -> travel.tourist_attraction.near_travel_destination -> Giza\n# Answer:\nGiza Plateau", "# Reasoning Path:\nGreat Pyramid of Giza -> architecture.building.building_complex -> Giza Plateau -> travel.tourist_attraction.near_travel_destination -> Giza\n# Answer:\nGiza Plateau"], "ground_truth": ["Giza Plateau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1077", "prediction": ["# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Altaic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> common.topic.notable_types -> Human Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Turkic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Tsakhur Language", "Ukrainian Language", "Adyghe Language", "Karachay-Balkar Language", "Khakas Language", "Abaza Language", "Tabassaran Language", "Ingush Language", "Dargwa Language", "Yakut Language", "Buryat language", "Udmurt Language", "Mari language", "Altai language", "Russian Language", "Lezgi Language", "Kabardian Language", "Lak Language", "Tatar Language", "Kalmyk-Oirat Language", "Crimean Turkish Language", "Rutul language", "Chechen Language", "Nogai Language", "Komi language", "Azerbaijani language", "Erzya Language", "Avar Language", "Moksha Language", "Yiddish Language", "Bashkir Language", "Aghul language", "Kumyk Language", "Tuvin Language", "Osetin Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15483870967741936, "ans_precission": 0.8, "ans_recall": 0.08571428571428572, "path_f1": 0.15483870967741936, "path_precision": 0.8, "path_recall": 0.08571428571428572, "path_ans_f1": 0.15483870967741936, "path_ans_precision": 0.8, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1078", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "Lojban", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1079", "prediction": ["# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.containedby -> Theban Necropolis\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.containedby -> Egypt\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> travel.tourist_attraction.near_travel_destination -> Luxor\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.languages_spoken -> Modern Standard Arabic\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> common.topic.article -> m.01qg6v\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> common.topic.notable_types -> Tourist attraction\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Valley of the Kings -> location.location.contains -> KV6\n# Answer:\nValley of the Kings", "# Reasoning Path:\nKV62 -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Cairo\n# Answer:\nEgypt", "# Reasoning Path:\nKV62 -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Tutankhamun and the Golden Age of the Pharaohs -> exhibitions.exhibition.subjects -> Tutankhamun\n# Answer:\nTutankhamun and the Golden Age of the Pharaohs"], "ground_truth": ["Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_types -> Literary Series\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin"], "ground_truth": ["The Autobiography of Charles Darwin [EasyRead Edition]", "The Origin of Species (Variorum Reprint)", "The structure and distribution of coral reefs", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Voyage of the Beagle (Dover Value Editions)", "Reise eines Naturforschers um die Welt", "The expression of the emotions in man and animals", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "On Natural Selection", "Voyage d'un naturaliste autour du monde", "On the tendency of species to form varieties", "The Origin of Species (Great Minds Series)", "The Structure And Distribution of Coral Reefs", "Wu zhong qi yuan", "The descent of man and selection in relation to sex.", "Del Plata a Tierra del Fuego", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Correspondence of Charles Darwin, Volume 15", "Darwin Darwin", "The Correspondence of Charles Darwin, Volume 6", "Darwin's notebooks on transmutation of species", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Fertilisation of Orchids", "The Autobiography of Charles Darwin (Large Print)", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 8", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Correspondence of Charles Darwin, Volume 10", "La facult\u00e9 motrice dans les plantes", "Evolution by natural selection", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The voyage of Charles Darwin", "The descent of man, and selection in relation to sex", "Insectivorous Plants", "The foundations of the Origin of species", "ontstaan der soorten door natuurlijke teeltkeus", "Evolution", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The autobiography of Charles Darwin, 1809-1882", "Leben und Briefe von Charles Darwin", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Notebooks on transmutation of species", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Works", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 11: 1863", "Reise um die Welt 1831 - 36", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 13", "The Life of Erasmus Darwin", "Darwin's journal", "Les mouvements et les habitudes des plantes grimpantes", "La vie et la correspondance de Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "The education of Darwin", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Formation of Vegetable Mould through the Action of Worms", "El Origin De Las Especies", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 4", "Darwin en Patagonia", "The Descent of Man and Selection in Relation to Sex", "Human nature, Darwin's view", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Opsht\u0323amung fun menshen", "The Origin of Species (Mentor)", "The Correspondence of Charles Darwin, Volume 18: 1870", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 9", "Origin of Species", "Autobiography of Charles Darwin", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The principal works", "The Darwin Reader Second Edition", "The Origin of Species (Collector's Library)", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Voyage of the Beagle (NG Adventure Classics)", "The Variation of Animals and Plants under Domestication", "Darwin-Wallace", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 11", "The Voyage of the Beagle (Mentor)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Cartas de Darwin 18251859", "Charles Darwin's marginalia", "Origin of Species (Everyman's University Paperbacks)", "Proiskhozhdenie vidov", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin and Henslow", "The Essential Darwin", "On evolution", "Darwin for Today", "The origin of species : complete and fully illustrated", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Voyage of the Beagle (Everyman Paperbacks)", "Voyage Of The Beagle", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Diario del Viaje de Un Naturalista Alrededor", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 16: 1868", "From so simple a beginning", "Kleinere geologische Abhandlungen", "Diary of the voyage of H.M.S. Beagle", "The Orgin of Species", "Rejse om jorden", "To the members of the Down Friendly Club", "The Autobiography of Charles Darwin (Great Minds Series)", "Motsa ha-minim", "The structure and distribution of coral reefs.", "The Darwin Reader First Edition", "The Autobiography of Charles Darwin (Dodo Press)", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Part I: Contributions to the Theory of Natural Selection / Part II", "Voyage of the Beagle", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Darwin's Ornithological notes", "Origins", "The Origin of Species (World's Classics)", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 14", "The Autobiography Of Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Origin of Species (Harvard Classics, Part 11)", "Volcanic Islands", "From Darwin's unpublished notebooks", "Charles Darwin's letters", "On a remarkable bar of sandstone off Pernambuco", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 1", "The Voyage of the Beagle (Unabridged Classics)", "Resa kring jorden", "Voyage of the Beagle (Harvard Classics, Part 29)", "The descent of man, and selection in relation to sex.", "Die geschlechtliche Zuchtwahl", "The Expression of the Emotions in Man and Animals", "H.M.S. Beagle in South America", "Beagle letters", "The Expression Of The Emotions In Man And Animals", "Metaphysics, Materialism, & the evolution of mind", "red notebook of Charles Darwin", "The\u0301orie de l'e\u0301volution", "Questions about the breeding of animals", "Evolution and natural selection", "Charles Darwin's natural selection", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The autobiography of Charles Darwin", "The expression of the emotions in man and animals.", "Darwin on humus and the earthworm", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Voyage of the Beagle", "The Expression of the Emotions in Man And Animals", "On the Movements and Habits of Climbing Plants", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 5", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 12", "vari\u00eberen der huisdieren en cultuurplanten", "Die fundamente zur entstehung der arten", "The Origin of Species (Great Books : Learning Channel)", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 17: 1869", "Notes on the fertilization of orchids", "The origin of species", "Charles Darwin on the routes of male humble bees", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Autobiography of Charles Darwin, and selected letters", "A student's introduction to Charles Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 10: 1862", "Memorias y epistolario i\u0301ntimo", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "monograph on the sub-class Cirripedia", "Darwin", "Darwinism stated by Darwin himself", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 3", "A Darwin Selection", "The voyage of the Beagle.", "The living thoughts of Darwin", "The Voyage of the Beagle (Adventure Classics)", "From So Simple a Beginning", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Correspondence of Charles Darwin, Volume 15: 1867", "On the origin of species by means of natural selection", "The Origin Of Species", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Autobiography of Charles Darwin", "The Origin of Species", "Gesammelte kleinere Schriften", "The Origin of Species (Oxford World's Classics)", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Het uitdrukken van emoties bij mens en dier", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 7", "The Correspondence of Charles Darwin, Volume 2"], "ans_acc": 0.06074766355140187, "ans_hit": 1, "ans_f1": 0.018494055482166444, "ans_precission": 0.875, "ans_recall": 0.009345794392523364, "path_f1": 0.34285714285714286, "path_precision": 1.0, "path_recall": 0.20689655172413793, "path_ans_f1": 0.11453744493392069, "path_ans_precision": 1.0, "path_ans_recall": 0.06074766355140187}
{"id": "WebQTest-1080", "prediction": ["# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Ziryab\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.containedby -> Middle East\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Enheduanna\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.deceased_person.place_of_death -> Canaan -> common.topic.article -> m.0d9fc\n# Answer:\nCanaan", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.contains -> Ur Ka\u015bdim\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Noah\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> book.book_subject.works -> The Sacred Band\n# Answer:\nMesopotamia", "# Reasoning Path:\nAbraham -> people.person.place_of_birth -> Mesopotamia -> location.location.people_born_here -> Thoma Darmo\n# Answer:\nMesopotamia"], "ground_truth": ["Canaan", "Mesopotamia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1081", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> education.education.institution -> Simpson College\n# Answer:\nSimpson College", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.institution -> Iowa State University\n# Answer:\nIowa State University", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.institution -> Iowa State University\n# Answer:\nIowa State University", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0_swr6_ -> education.education.institution -> Minneapolis High School\n# Answer:\nMinneapolis High School", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.degree -> Master's Degree\n# Answer:\nMaster's Degree", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0n1916z -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.04hdfv4 -> education.education.degree -> Bachelor of Science\n# Answer:\nBachelor of Science", "# Reasoning Path:\nGeorge Washington Carver -> people.person.education -> m.0ztr7gt -> education.education.major_field_of_study -> Botany\n# Answer:\nBotany"], "ground_truth": ["Simpson College", "Iowa State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1082", "prediction": ["# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> sports.sports_team.sport -> Basketball\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Joventut Badalona -> sports.sports_team.sport -> Basketball\n# Answer:\nJoventut Badalona", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> sports.sports_team.location -> M\u00e1laga\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Baloncesto M\u00e1laga -> common.topic.notable_types -> Basketball Team\n# Answer:\nBaloncesto M\u00e1laga", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Obradoiro CAB -> sports.sports_team.sport -> Basketball\n# Answer:\nObradoiro CAB", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Valencia BC -> sports.sports_team.sport -> Basketball\n# Answer:\nValencia BC", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> CB Sevilla -> sports.sports_team.sport -> Basketball\n# Answer:\nCB Sevilla", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> sports.sports_team_location.teams -> Joventut Badalona -> common.topic.notable_types -> Basketball Team\n# Answer:\nJoventut Badalona"], "ground_truth": ["Spain national handball team", "Real Madrid Baloncesto", "CB Murcia", "CB Avenida", "Spain national beach handball team", "B\u00e0squet Manresa", "CB Breog\u00e1n", "San Sebasti\u00e1n Gipuzkoa BC", "Saski Baskonia", "Valencia BC", "CB 1939 Canarias", "CB Sant Josep", "Basket Zaragoza 2002", "Spain men's national volleyball team", "CB Sevilla", "Baloncesto M\u00e1laga", "Spain women's national handball team", "Joventut Badalona", "Ros Casares Valencia", "Spain women's national volleyball team", "Spain national football team", "CB Estudiantes", "Obradoiro CAB"], "ans_acc": 0.21739130434782608, "ans_hit": 1, "ans_f1": 0.34188034188034183, "ans_precission": 0.8, "ans_recall": 0.21739130434782608, "path_f1": 0.34188034188034183, "path_precision": 0.8, "path_recall": 0.21739130434782608, "path_ans_f1": 0.34188034188034183, "path_ans_precision": 0.8, "path_ans_recall": 0.21739130434782608}
{"id": "WebQTest-1083", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> broadcast.producer.location -> Washington, D.C. -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> broadcast.producer.location -> Washington, D.C. -> travel.travel_destination.tourist_attractions -> White House\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> fictional_universe.fictional_setting.fictional_characters_born_here -> Barack Obama in comics\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> broadcast.producer.location -> Washington, D.C. -> travel.travel_destination.tourist_attractions -> United States Capitol\n# Answer:\nWashington, D.C."], "ground_truth": ["Honolulu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1084", "prediction": ["# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.sport -> Basketball\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> sports.sports_team.colors -> White\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Basket Zielona G\u00f3ra -> common.topic.notable_types -> Basketball Team\n# Answer:\nBasket Zielona G\u00f3ra", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> GT\u017b Grudzi\u0105dz -> sports.sports_team.sport -> Motorcycle speedway\n# Answer:\nGT\u017b Grudzi\u0105dz", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> common.topic.notable_types -> Basketball Team\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> sports.sports_team.sport -> Basketball\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Asseco Gdynia -> sports.sports_team.colors -> Blue\n# Answer:\nAsseco Gdynia", "# Reasoning Path:\nPoland -> sports.sports_team_location.teams -> Stal Gorz\u00f3w Wielkopolski -> common.topic.notable_types -> Sports Team\n# Answer:\nStal Gorz\u00f3w Wielkopolski"], "ground_truth": ["Poland women's national volleyball team", "Poland women's national handball team", "Asseco Gdynia", "Stal Gorz\u00f3w Wielkopolski", "KK W\u0142oc\u0142awek", "GT\u017b Grudzi\u0105dz", "Basket Zielona G\u00f3ra", "\u015al\u0105sk Wroc\u0142aw", "Poland national football team", "Tur\u00f3w Zgorzelec", "Poland men's national volleyball team", "Poland national handball team"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.47058823529411764, "ans_precission": 0.8, "ans_recall": 0.3333333333333333, "path_f1": 0.47058823529411764, "path_precision": 0.8, "path_recall": 0.3333333333333333, "path_ans_f1": 0.47058823529411764, "path_ans_precision": 0.8, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1085", "prediction": ["# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Joe Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.065q3fy -> base.popstra.dated.participant -> Joe Jonas\n# Answer:\nJoe Jonas", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Don't Forget -> music.composition.composer -> Joe Jonas\n# Answer:\nDon't Forget", "# Reasoning Path:\nDemi Lovato -> music.artist.album -> Demi -> common.topic.article -> m.0sghhtd\n# Answer:\nDemi", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> La La Land -> music.composition.composer -> Joe Jonas\n# Answer:\nLa La Land", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.composer -> Kevin Jonas\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> music.composer.compositions -> Behind Enemy Lines -> music.composition.language -> English Language\n# Answer:\nBehind Enemy Lines", "# Reasoning Path:\nDemi Lovato -> base.popstra.celebrity.dated -> m.063gd6q -> base.popstra.dated.participant -> Cody Linley\n# Answer:\nCody Linley"], "ground_truth": ["Joe Jonas", "Cody Linley", "Wilmer Valderrama", "Nicholas Braun"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.24, "path_precision": 0.375, "path_recall": 0.17647058823529413, "path_ans_f1": 0.5555555555555556, "path_ans_precision": 0.625, "path_ans_recall": 0.5}
{"id": "WebQTest-1087", "prediction": ["# Reasoning Path:\nScarlett Johansson -> award.award_winner.awards_won -> m.0gm4_vf -> award.award_honor.honored_for -> Lost in Translation\n# Answer:\nLost in Translation", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z83g54 -> award.award_nomination.nominated_for -> We Bought a Zoo\n# Answer:\nWe Bought a Zoo", "# Reasoning Path:\nScarlett Johansson -> award.award_winner.awards_won -> m.07zmzyw -> award.award_honor.honored_for -> Lost in Translation\n# Answer:\nLost in Translation", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0z9m97f -> award.award_nomination.nominated_for -> The Black Dahlia\n# Answer:\nThe Black Dahlia", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0gm4_rg -> award.award_nomination.nominated_for -> The Perfect Score\n# Answer:\nThe Perfect Score", "# Reasoning Path:\nScarlett Johansson -> award.award_winner.awards_won -> m.0gm4_vf -> award.award_honor.award -> Venice Film Festival Upstream Prize for Best Actress\n# Answer:\nVenice Film Festival Upstream Prize for Best Actress", "# Reasoning Path:\nScarlett Johansson -> award.award_winner.awards_won -> m.0gm4rly -> award.award_honor.honored_for -> Lost in Translation\n# Answer:\nLost in Translation", "# Reasoning Path:\nScarlett Johansson -> award.award_nominee.award_nominations -> m.0ndt3vc -> award.award_nomination.nominated_for -> The Avengers\n# Answer:\nThe Avengers"], "ground_truth": ["The Horse Whisperer", "The Black Dahlia", "The Avengers: Age of Ultron", "Lucy", "The Island", "In Good Company", "Lost in Translation", "The Man Who Wasn't There", "If Lucy Fell", "Vicky Cristina Barcelona", "Manny & Lo", "Girl with a Pearl Earring", "A Love Song for Bobby Long", "Match Point", "Hitchcock", "Captain America: Civil War", "Eight Legged Freaks", "Under the Skin", "The Spirit", "Just Cause", "The Other Boleyn Girl", "The SpongeBob SquarePants Movie", "Don Jon", "An American Rhapsody", "Buck", "The Perfect Score", "Ghost World", "The Prestige", "North", "We Bought a Zoo", "Chef", "Fall", "Home Alone 3", "The Jungle Book", "The Avengers", "Iron Man 2", "Her", "Captain America: The Winter Soldier", "He's Just Not That Into You", "A Good Woman", "Scoop", "The Nanny Diaries", "My Brother the Pig"], "ans_acc": 0.11627906976744186, "ans_hit": 1, "ans_f1": 0.20527859237536658, "ans_precission": 0.875, "ans_recall": 0.11627906976744186, "path_f1": 0.13333333333333333, "path_precision": 0.875, "path_recall": 0.07216494845360824, "path_ans_f1": 0.20527859237536658, "path_ans_precision": 0.875, "path_ans_recall": 0.11627906976744186}
{"id": "WebQTest-1088", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.010btxt8 -> tv.regular_tv_appearance.actor -> Brandon Richardson\n# Answer:\nBrandon Richardson", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.0k6pxpv -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02kk65p -> tv.regular_tv_appearance.actor -> Seth MacFarlane\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntq_4 -> tv.regular_tv_appearance.character -> Meg Griffin\n# Answer:\nMeg Griffin", "# Reasoning Path:\nFamily Guy -> tv.tv_program.regular_cast -> m.02ntqzx -> tv.regular_tv_appearance.actor -> Seth Green\n# Answer:\nSeth Green"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1089", "prediction": ["# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.actor -> Lucy Punch\n# Answer:\nLucy Punch", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.actor -> Jason Segel\n# Answer:\nJason Segel", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jt -> film.performance.character -> Amy Squirrel\n# Answer:\nAmy Squirrel", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j5 -> film.performance.actor -> Cameron Diaz\n# Answer:\nCameron Diaz", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jb -> film.performance.actor -> Eric Stonestreet\n# Answer:\nEric Stonestreet", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798jh -> film.performance.actor -> Justin Timberlake\n# Answer:\nJustin Timberlake", "# Reasoning Path:\nBad Teacher -> film.film.starring -> m.0b798j0 -> film.performance.character -> Russell Gettis\n# Answer:\nRussell Gettis", "# Reasoning Path:\nBad Teacher -> award.award_nominated_work.award_nominations -> m.0z87zxd -> freebase.valuenotation.has_value -> Award Nominee\n# Answer:\nAward Nominee"], "ground_truth": ["Lucy Punch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nTim Tebow -> common.topic.notable_for -> g.1255t_dt0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1090", "prediction": ["# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.citytown -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.state_province_region -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> location.location.containedby -> United States of America\n# Answer:\nSeattle", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> location.mailing_address.postal_code -> 60606\n# Answer:\n60606", "# Reasoning Path:\nBoeing -> organization.organization.headquarters -> m.02_6pm_ -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nBoeing -> business.business_operation.current_liabilities -> m.090887v -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nBoeing -> organization.organization.place_founded -> Seattle -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSeattle"], "ground_truth": ["Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1091", "prediction": ["# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvq\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvh\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Upper Midwest American English -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nUpper Midwest American English", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lvy\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lw4\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> location.statistical_region.religions -> m.03q1lwc\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> United States Minor Outlying Islands\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nAbenaki language", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> common.topic.notable_types -> Country\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Abenaki language -> language.human_language.region -> Americas\n# Answer:\nAbenaki language"], "ground_truth": ["Christianity", "Hinduism", "Unitarian Universalism", "Islam", "Judaism", "Buddhism", "Atheism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1092", "prediction": ["# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.character -> Aragorn\n# Answer:\nAragorn", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9q -> film.performance.character -> Arwen\n# Answer:\nArwen", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6bnm19y\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nViggo Mortensen -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9k -> film.performance.character -> Aragorn\n# Answer:\nAragorn", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> g.11b6ntbrrh\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nViggo Mortensen -> film.actor.film -> m.0k5s9k -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc6p4 -> film.performance.character -> Meriadoc Brandybuck\n# Answer:\nMeriadoc Brandybuck", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.0k5s9q -> film.performance.actor -> Liv Tyler\n# Answer:\nLiv Tyler", "# Reasoning Path:\nViggo Mortensen -> tv.tv_actor.starring_roles -> m.0gy9zr2 -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Aragorn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1094", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> common.topic.notable_types -> Profession\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.0106_ymb\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Heraclitus\n# Answer:\nPhilosopher"], "ground_truth": ["Philosopher", "Astronomer", "Physicist", "Scientist", "Mathematician", "Chemist"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 1.0, "ans_recall": 0.8333333333333334, "path_f1": 0.9090909090909091, "path_precision": 1.0, "path_recall": 0.8333333333333334, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 1.0, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1095", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.nationality -> United Kingdom\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nAnne Hathaway", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Susanna Hall -> people.person.place_of_birth -> Stratford-upon-Avon\n# Answer:\nSusanna Hall", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.deceased_person.place_of_burial -> Church of the Holy Trinity, Stratford-upon-Avon\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.place_of_birth -> Stratford-upon-Avon\n# Answer:\nJudith Quiney"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1096", "prediction": ["# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.nominated_for -> Last Holiday\n# Answer:\nLast Holiday", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.09k3ppj -> award.award_nomination.nominated_for -> Hairspray\n# Answer:\nHairspray", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b3t_h2 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.ceremony -> 2006 Teen Choice Awards\n# Answer:\n2006 Teen Choice Awards", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0b4d600 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0x0zxs3 -> award.award_nomination.nominated_for -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.award_nominee -> LL Cool J\n# Answer:\nLL Cool J", "# Reasoning Path:\nQueen Latifah -> award.award_nominee.award_nominations -> m.0z9rhy8 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Liplock\n# Answer:\nTeen Choice Award for Choice Movie: Liplock"], "ground_truth": ["Hairspray 2: White Lipstick", "The Dilemma", "The Making of What's Going On: An All-Star Tribute", "Bringing Out the Dead", "Living Out Loud", "The Bone Collector", "Kung Faux: Vol. 3", "House Party 2", "Taxi", "Bringing Down the House", "Brown Sugar", "Jungle Fever", "One Love: The Bob Marley All-Star Tribute", "Ice Age: Continental Drift", "Mad Money", "Juice", "Zac Efron's Pool Party", "Scary Movie 3", "Beauty Shop", "Ice Age: The Meltdown", "Breaking Out: The Alcatraz Concert", "Kung Faux: Vol. 5", "Bessie", "Steel Magnolias", "The Muppets' Wizard of Oz", "The Secret Life of Bees", "Chicago", "Kung Faux: Vol. 1", "Set It Off", "Stranger than Fiction", "Ice Age: A Mammoth Christmas", "Hoodlum", "Barbershop 2: Back in Business", "What Happens in Vegas", "Last Holiday", "The Country Bears", "Kung Faux: Vol. 2", "Sphere", "The Perfect Holiday", "Through the Years of Hip-Hop: Vol. 1: Graffiti", "Just Wright", "Mama Flora's Family", "Life Support", "Queen Latifah - Unauthorized", "Life in the 310: Vol. 3", "g.11bc95kg2q", "My Life", "The Cookout", "Hairspray", "Barbershop 3", "Kung Faux: Vol. 4", "Living with the Dead", "22 Jump Street", "Joyful Noise", "We Are Family", "We Are One: The Obama Inaugural Celebration at the Lincoln Memorial", "Ice Age: Dawn of the Dinosaurs", "Arctic Tale", "Valentine's Day"], "ans_acc": 0.05084745762711865, "ans_hit": 1, "ans_f1": 0.09404388714733543, "ans_precission": 0.625, "ans_recall": 0.05084745762711865, "path_f1": 0.09174311926605504, "path_precision": 0.625, "path_recall": 0.04950495049504951, "path_ans_f1": 0.09404388714733543, "path_ans_precision": 0.625, "path_ans_recall": 0.05084745762711865}
{"id": "WebQTest-1097", "prediction": ["# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> education.education.institution -> Whitney M. Young Magnet High School\n# Answer:\nWhitney M. Young Magnet High School", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.major_field_of_study -> Sociology\n# Answer:\nSociology", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wpt7p -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Specialization\n# Answer:\nSpecialization", "# Reasoning Path:\nMichelle Obama -> common.topic.notable_for -> g.12582g7kh\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.02wn0g2 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMichelle Obama -> people.person.education -> m.0n1db15 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree"], "ground_truth": ["Princeton University", "Harvard Law School", "Whitney M. Young Magnet High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1098", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Romantic music -> music.genre.parent_genre -> Classical music\n# Answer:\nRomantic music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> music.genre.subgenre -> Chamber music\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> The Collector\u2019s Shelf\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> film.music_contributor.film -> The Loves of Liszt -> film.film.music -> Ferenc Farkas\n# Answer:\nThe Loves of Liszt", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> common.topic.notable_types -> Musical genre\n# Answer:\nClassical music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nClassical music"], "ground_truth": ["Chamber music", "Art song", "Romantic music", "Opera", "Classical music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8358208955223881, "ans_precission": 0.875, "ans_recall": 0.8, "path_f1": 0.8358208955223881, "path_precision": 0.875, "path_recall": 0.8, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1099", "prediction": ["# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> sports.pro_athlete.teams -> m.010p28b1 -> sports.sports_team_roster.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Luiz -> sports.pro_athlete.teams -> m.04mg4fm -> sports.sports_team_roster.team -> S.L. Benfica\n# Answer:\nS.L. Benfica", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w9h40k -> soccer.football_player_stats.team -> Esporte Clube Vit\u00f3ria\n# Answer:\nEsporte Clube Vit\u00f3ria", "# Reasoning Path:\nDavid Luiz -> sports.pro_athlete.teams -> m.0j3_hmz -> sports.sports_team_roster.team -> Brazil national football team\n# Answer:\nBrazil national football team", "# Reasoning Path:\nDavid Luiz -> soccer.football_player.statistics -> m.0w8zrhb -> soccer.football_player_stats.team -> S.L. Benfica\n# Answer:\nS.L. Benfica", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Acre\n# Answer:\nBrazil", "# Reasoning Path:\nDavid Luiz -> people.person.nationality -> Brazil -> base.aareas.schema.administrative_area.administrative_children -> Amazonas\n# Answer:\nBrazil"], "ground_truth": ["Brazil national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.rushing -> m.09tcfsk -> american_football.player_rushing_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1100", "prediction": ["# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> book.book_edition.book -> Jefferson Bible\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A Manual of Parliamentary Practice Composed for the Use of the Senate of the United States -> book.book_edition.book -> A Manual of Parliamentary Practice for the Use of the Senate of the United States\n# Answer:\nA Manual of Parliamentary Practice Composed for the Use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> common.topic.notable_types -> Book Edition\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A manual of parliamentary practice for the use of the Senate of the United States -> book.book_edition.book -> A Manual of Parliamentary Practice for the Use of the Senate of the United States\n# Answer:\nA manual of parliamentary practice for the use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> book.book_edition.publisher -> Beacon Press\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Polly Jefferson\n# Answer:\nPolly Jefferson", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> book.book_edition.isbn -> 9780448381008\n# Answer:\nThe Jefferson Bible", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> A Manual of Parliamentary Practice Composed for the Use of the Senate of the United States -> book.book_edition.isbn -> 9781417905218\n# Answer:\nA Manual of Parliamentary Practice Composed for the Use of the Senate of the United States", "# Reasoning Path:\nThomas Jefferson -> book.author.book_editions_published -> The Jefferson Bible -> book.book_edition.isbn -> 9780486449210\n# Answer:\nThe Jefferson Bible"], "ground_truth": ["Responsibility Skills", "The Papers of Thomas Jefferson, Retirement Series: Volume 3: 12 August 1810 to 17 June 1811", "The wisdom of Thomas Jefferson", "Speech of Thomas Jefferson, president of the United States, delivered at his instalment, March 4, 1801, at the city of Washington", "Basic writings of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 21: Index, Vols. 1-20", "The Papers of Thomas Jefferson, Volume 15: March 1789 to November 1789", "Jefferson's ideas on a university library", "The Papers of Thomas Jefferson, Volume 6: May 1781 to March 1784", "A supplementary note on the mould board described in a letter to Sir John Sinclair, of March 23, 1798", "Crusade against ignorance", "Jeffersonian principles", "The Papers of Thomas Jefferson, Retirement Series: Volume 1: 4 March 1809 to 15 November 1809", "Foundations of Freedom", "The quotable Jefferson", "Calendar of the correspondence of Thomas Jefferson", "The republic of letters", "The Papers of Thomas Jefferson, Volume 23: 1 January to 31 May 1792", "Public and private papers", "The speech of Logan", "The Papers of Thomas Jefferson, Volume 4: October 1780 to February 1781", "The Papers of Thomas Jefferson, Volume 33: 17 February to 30 April 1801", "The Papers of Thomas Jefferson, Retirement Series: Volume 5: 1 May 1812 to 10 March 1813", "The Papers of Thomas Jefferson, Volume 14: October 1788 to March 1789", "Light and liberty", "The Literary Bible of Thomas Jefferson", "The papers of Thomas Jefferson. Index", "Jefferson on Jefferson", "Correspondence between His Excellency Thomas Jefferson, President of the United States and James Monroe, Esq., late American ambassador to the Court of St. James", "Jefferson's extracts from the Gospels", "The Papers of Thomas Jefferson, Volume 34: 1 May to 31 July 1801", "Unpublished correspondence between Thomas Jefferson and some American Jews", "A Manual of Parliamentary Practice for the Use of the Senate of the United States", "The best letters of Thomas Jefferson", "An essay towards facilitating instruction in the Anglo-Saxon and modern dialects of the English language. For the use of the University of Virginia", "Jefferson's literary commonplace book", "Thomas Jefferson correspondence", "The Papers of Thomas Jefferson, Volume 30: 1 January 1798 to 31 January 1799", "Thomas Jefferson Travels", "Jefferson abroad", "The life and selected writings of Thomas Jefferson", "An American Christian Bible", "Speech of Thomas Jefferson, president of the United States, delivered in the Senate chamber, March 4th, 1801", "The Papers of Thomas Jefferson, Volume 17: July 1790 to November 1790", "The political writings of Thomas Jefferson", "Jefferson Bible", "The Papers of Thomas Jefferson, Volume 8: February 1785 to October 1785", "State of the Union Addresses of Thomas Jefferson", "Catalogue", "Junior Fact Summer 2007 Bundle", "Thomas Jefferson", "The living thoughts of Thomas Jefferson", "Manual de pra\u0301ctica parlamentaria", "The portable Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 7: March 1784 to February 1785", "The Papers of Thomas Jefferson, Volume 18: 4 November 1790 to 24 January 1791", "Citizen Jefferson", "The Papers of Thomas Jefferson, Volume 27: 1 September to 31 December 1793", "Memoir, correspondence, and miscellanies", "Memorandums taken on a journey from Paris into the southern parts of France and Northern Italy, in the year 1787", "Thomas Jefferson's architectural drawings", "Minor Vocabularies of Nanticoke-Conoy", "The Papers of Thomas Jefferson, Volume 26: 11 May to 31 August 1793", "Jefferson in love", "The Papers of Thomas Jefferson, Retirement Series: Volume 6: 11 March to 27 November 1813", "An appendix to the Notes on Virginia relative to the murder of Logan's family", "The Papers of Thomas Jefferson, Volume 29: 1 March 1796 to 31 December 1797", "The Papers of Thomas Jefferson, Volume 25: 1 January to 10 May 1793", "Master thoughts of Thomas Jefferson", "Revolutionary Philosopher", "The Papers of Thomas Jefferson, Volume 36: 1 December 1801 to 3 March 1802", "The Papers of Thomas Jefferson, Volume 31: 1 February 1799 to 31 May 1800", "Thomas Jefferson, political writings", "The Papers of Thomas Jefferson, Volume 35: 1 August to 30 November 1801", "The President's speech", "Letters and addresses of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 2: 16 November 1809 to 11 August 1810", "The Papers of Thomas Jefferson, Volume 13: March 1788 to October 1788", "The inaugural addresses of President Thomas Jefferson, 1801 and 1805", "The correspondence of Jefferson and Du Pont de Nemours", "Jefferson himself", "A Jefferson profile as revealed in his letters", "Elementary School Support Kit/Bulletin Boards", "The Papers of Thomas Jefferson, Volume 2: January 1777 to 18 June 1779", "Notes on the State of Virginia", "Autobiography of Thomas Jefferson", "Thomas Jefferson's Farm book", "The Papers of Thomas Jefferson, Volume 10: June 1786 to December 1786", "The Papers of Thomas Jefferson, Volume 22: 6 August to 31 December 1791", "The Statute of Virginia for Religious Freedom", "The Papers of Thomas Jefferson, Volume 3: June 1779 to September 1780", "The Commonplace book of Thomas Jefferson", "The essential Jefferson", "Papers", "Letters", "The four versions of Jefferson's letter to Mazzei", "Jefferson and Madison on the Separation of Church and State", "The Papers of Thomas Jefferson, Volume 1: 14 January 1760 to 6 December 1776", "Il pensiero politico e sociale di Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 19: January 1791 to March 1791", "Speech of Thomas Jefferson, president of the United States", "The Jefferson-Dunglison letters", "Jefferson the man", "Letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 12: August 1787 to March 1788", "The inaugural speeches and messages of Thomas Jefferson, Esq", "The writings of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 5: February 1781 to May 1781", "\\\"Ye will say I am no Christian\\\"", "Jefferson's Germantown letters", "Account of Louisiana", "To the girls and boys", "The life & morals of Jesus Christ of Nazareth", "The Works of Thomas Jefferson", "The Papers of Thomas Jefferson, Retirement Series: Volume 4: 18 June 1811 to 30 April 1812", "The address of Thomas Jefferson", "Thomas Jefferson, his words and vision", "The Papers of Thomas Jefferson, Volume 28: 1 January 1794 to 29 February 1796", "Documents Relating To The Purchase And Exploration Of Louisiana", "Jefferson's proposed instructions to the Virginia delegates, 1744", "John Dewey presents the living thoughts of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 9: November 1785 to June 1786", "A Summary View of the Rights of British America", "The religious and moral wisdom of Thomas Jefferson", "The life and letters of Thomas Jefferson", "The family letters of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 24: 1 June to 31 December 1792", "The complete Jefferson", "The anas of Thomas Jefferson", "The Papers of Thomas Jefferson, Volume 11: January 1787 to August 1787", "The Papers of Thomas Jefferson, Volume 20: April 1791 to August 1791", "Speech of Thomas Jefferson, president of the United States, delivered at his inauguration, March 4, 1801", "Republican notes on religion ; and, An act establishing religious freedom, passed in the assembly of Virginia, in the year 1786", "The Papers of Thomas Jefferson, Volume 32: 1 June 1800 to 16 February 1801", "United States Declaration of Independence", "The essence of Jefferson", "The Papers of Thomas Jefferson, Volume 16: November 1789 to July 1790", "Jefferson's parliamentary writings", "The proceedings of the government of the United States, in maintaining the public right to the beach of the Missisipi"], "ans_acc": 0.02158273381294964, "ans_hit": 1, "ans_f1": 0.02810304449648712, "ans_precission": 0.6, "ans_recall": 0.014388489208633094, "path_f1": 0.08080808080808081, "path_precision": 1.0, "path_recall": 0.042105263157894736, "path_ans_f1": 0.04225352112676056, "path_ans_precision": 1.0, "path_ans_recall": 0.02158273381294964}
{"id": "WebQTest-1101", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Thomas Nkobi\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.nobelprizes.nobel_subject_area.nobel_awards -> m.063k0hx\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.profession -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Albert Lutuli\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Coretta Scott King\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> common.topic.notable_types -> Literature Subject\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAfrican Americans' rights"], "ground_truth": ["Pastor", "Writer", "Minister", "Clergy", "Humanitarian", "Civil rights activist"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.14285714285714288, "ans_precission": 0.125, "ans_recall": 0.16666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.125, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1102", "prediction": ["# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> location.location.containedby -> Canada\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> base.aareas.schema.administrative_area.administrative_area_type -> Canadian province\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> common.image.size -> m.02dtr85\n# Answer:\ncommon.image.size", "# Reasoning Path:\nVictoria -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> British Columbia\n# Answer:\nCanada", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> location.statistical_region.population -> m.010rqww2\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.contains -> Alix Goolden Performance Hall -> location.location.containedby -> Victoria Conservatory of Music\n# Answer:\nAlix Goolden Performance Hall", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> Woodsmen of the West\n# Answer:\nBritish Columbia", "# Reasoning Path:\nVictoria -> location.location.containedby -> British Columbia -> book.book_subject.works -> A Ghost in the Water\n# Answer:\nBritish Columbia"], "ground_truth": ["British Columbia", "Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1103", "prediction": ["# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Warryn Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> common.topic.notable_for -> g.1258xk87v\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> music.album.release_type -> Album\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.composition.composer -> Erica Campbell\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> Go Get It -> music.album.album_content_type -> Compilation album\n# Answer:\nGo Get It", "# Reasoning Path:\nMary Mary -> music.artist.album -> A Mary Mary Christmas -> common.topic.notable_types -> Musical Album\n# Answer:\nA Mary Mary Christmas", "# Reasoning Path:\nMary Mary -> award.award_winner.awards_won -> m.0_r9jdp -> freebase.valuenotation.has_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nMary Mary -> award.award_nominee.award_nominations -> m.05bw3kw -> award.award_nomination.award -> Grammy Award for Best Gospel/Contemporary Christian Music Performance\n# Answer:\nGrammy Award for Best Gospel/Contemporary Christian Music Performance"], "ground_truth": ["Tina Campbell", "Erica Campbell"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714288, "path_precision": 0.125, "path_recall": 0.16666666666666666, "path_ans_f1": 0.2, "path_ans_precision": 0.125, "path_ans_recall": 0.5}
{"id": "WebQTest-1104", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc"], "ground_truth": ["Euro", "CFP franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1106", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hj3 -> soccer.football_player_stats.team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hjh -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1107", "prediction": ["# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nLove Don't Cost a Thing", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.09k3pqj -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nBall Don't Lie", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\n2004 Teen Choice Awards", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nMen in Black II", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award -> Teen Choice Award for Choice Movie: Chemistry\n# Answer:\nTeen Choice Award for Choice Movie: Chemistry", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.award_nominee -> Christina Milian\n# Answer:\nChristina Milian"], "ground_truth": ["Shall We Dance?", "Whatever It Takes", "Monster House", "American Son", "Drumline", "Even Money", "Chiraq", "Drumline: A New Beat", "Men in Black II", "Underclassman", "Roll Bounce", "The Adventures of Brer Rabbit", "Ball Don't Lie", "A Very School Gyrls Holla-Day", "Garfield: The Movie", "Love Don't Cost a Thing", "Weapons", "Day of the Dead", "Bobby", "The Killing Room", "Goal II: Living the Dream"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.291970802919708, "ans_precission": 0.625, "ans_recall": 0.19047619047619047, "path_f1": 0.2564102564102564, "path_precision": 0.625, "path_recall": 0.16129032258064516, "path_ans_f1": 0.291970802919708, "path_ans_precision": 0.625, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-1108", "prediction": ["# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> common.topic.notable_for -> g.1258r1rck\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.location.geolocation -> m.04nm5hc\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.mailing_address.postal_code -> 20002\n# Answer:\n20002", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> location.mailing_address.state_province_region -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nNPR -> organization.organization.place_founded -> Washington, D.C. -> location.citytown.postal_codes -> 20002\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nNPR -> organization.organization.headquarters -> m.02hr995 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nNPR -> organization.organization.place_founded -> Washington, D.C. -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nNPR -> broadcast.producer.produces -> Car Talk -> broadcast.content.genre -> Hobbies\n# Answer:\nCar Talk"], "ground_truth": ["WNYC"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1109", "prediction": ["# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nNew York Giants", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> sports.sports_team_roster.team -> Texas Longhorns football\n# Answer:\nTexas Longhorns football", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgc -> sports.sports_team_roster.position -> Cornerback\n# Answer:\nCornerback", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hpglgp -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nTom Landry -> sports.pro_athlete.teams -> m.0hph6vx -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0h_j38w\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> sports.pro_athlete.teams -> m.0vyl_2v\n# Answer:\nLandry Jones", "# Reasoning Path:\nTom Landry -> symbols.name_source.namesakes -> Landry Jones -> common.topic.image -> Jones winding up to pass in a game againt Tulsa in 2009.\n# Answer:\nLandry Jones"], "ground_truth": ["Texas Longhorns football", "New York Giants", "New York Yankees"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Spain\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> organization.organization_scope.organizations_with_this_scope -> Amasyn\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary -> location.location.containedby -> Eurasia\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1110", "prediction": ["# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> media_common.quotation_subject.quotations_about_this_subject -> \\\"History dressed up in the glow of love\u2019s kiss turned grief into beauty.\\\"\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.person.place_of_birth -> New York City -> periodicals.newspaper_circulation_area.newspapers -> The New York Times\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Jay -> people.deceased_person.place_of_death -> Bedford -> location.location.containedby -> United States of America\n# Answer:\nBedford", "# Reasoning Path:\nJohn Jay -> people.deceased_person.place_of_death -> Bedford -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nBedford", "# Reasoning Path:\nJohn Jay -> government.politician.party -> m.0lr1zhl -> government.political_party_tenure.party -> Federalist Party\n# Answer:\nFederalist Party", "# Reasoning Path:\nJohn Jay -> common.topic.image -> John Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts. -> common.image.appears_in_topic_gallery -> Federalist No. 64\n# Answer:\nJohn Jay, einer der Gr\u00fcnderv\u00e4ter der Vereinigten Staaten, war direkter Vorfahre Trotts."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1111", "prediction": ["# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.containedby -> Japan\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Fuji-Hakone-Izu National Park -> location.location.containedby -> Kant\u014d region\n# Answer:\nFuji-Hakone-Izu National Park", "# Reasoning Path:\nMount Fuji -> base.schemastaging.context_name.pronunciation -> g.125_ptxg0\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.geolocation -> m.0dbffb_\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Asia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nAsia", "# Reasoning Path:\nMount Fuji -> common.topic.notable_for -> g.1254zjntj\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> location.location.contains -> Abu\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> base.schemastaging.context_name.pronunciation -> g.125_mp9wt\n# Answer:\nHonshu", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Ch\u016bbu region -> location.location.containedby -> Japan\n# Answer:\nCh\u016bbu region", "# Reasoning Path:\nMount Fuji -> location.location.containedby -> Honshu -> common.topic.notable_for -> g.125ctnjc8\n# Answer:\nHonshu"], "ground_truth": ["Japan", "Ch\u016bbu region", "Asia", "Fuji-Hakone-Izu National Park", "Honshu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8000000000000002, "ans_precission": 0.8, "ans_recall": 0.8, "path_f1": 0.8000000000000002, "path_precision": 0.8, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1113", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Vincent Og\u00e9\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.notable_types -> Location\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Dominique You\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.article -> m.01f5dl\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> people.person.gender -> Male\n# Answer:\nSaint-Jean Louverture", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.notable_for -> g.125fkm225\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.people_born_here -> Jean Lafitte\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> location.location.events -> Haitian Revolution\n# Answer:\nSaint-Domingue"], "ground_truth": ["Saint-Domingue"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1114", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Line Digital Ltd\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1115", "prediction": ["# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.containedby -> Comanche County\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> common.topic.notable_types -> Postal Code\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.location.geolocation -> m.03dw8k7\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.geolocation -> m.03dw8kf\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73502 -> common.topic.notable_types -> Postal Code\n# Answer:\n73502", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73507 -> location.location.geometry -> m.058yndn\n# Answer:\n73507", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73505 -> location.postal_code.country -> United States of America\n# Answer:\n73505", "# Reasoning Path:\nLawton -> location.citytown.postal_codes -> 73506 -> common.topic.notable_for -> g.1257cwm4k\n# Answer:\n73506"], "ground_truth": ["73501", "73507", "73505", "73506", "73502"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.888888888888889, "path_precision": 1.0, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1116", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Qu\u00e9!\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_parent -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Iberian Peninsula\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_area_type -> City/Town/Village\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> ADN\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_children -> Chafarinas Islands -> location.location.contains_major_portion_of -> Morocco\n# Answer:\nChafarinas Islands", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Diario AS\n# Answer:\nMadrid"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1117", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland -> location.country.official_language -> Italian Language\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard"], "ground_truth": ["Canada", "Switzerland", "Belgium", "Austria", "South Africa", "Cyprus", "Luxembourg", "Liechtenstein", "Vatican City", "Germany", "Denmark", "East Germany", "West Germany", "Czech Republic", "Second Polish Republic"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5490196078431373, "ans_precission": 0.875, "ans_recall": 0.4, "path_f1": 0.48275862068965514, "path_precision": 0.875, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-1118", "prediction": ["# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkgnq -> baseball.batting_statistics.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\n1988 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfx7 -> baseball.batting_statistics.season -> 1986 Major League Baseball Season\n# Answer:\n1986 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkfyq -> baseball.batting_statistics.season -> 1987 Major League Baseball Season\n# Answer:\n1987 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkg0w -> baseball.batting_statistics.season -> 1989 Major League Baseball Season\n# Answer:\n1989 Major League Baseball Season", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkgnq -> baseball.batting_statistics.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.batting_stats -> m.06rkf_h -> baseball.batting_statistics.team -> Oakland Athletics\n# Answer:\nOakland Athletics", "# Reasoning Path:\nMark McGwire -> baseball.baseball_player.position_s -> First baseman -> sports.sports_position.players -> g.11bxv84dh5\n# Answer:\nFirst baseman"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1119", "prediction": ["# Reasoning Path:\nColorado Technical University -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado Technical University -> common.topic.notable_types -> College/University -> type.type.expected_by -> Universities\n# Answer:\nCollege/University", "# Reasoning Path:\nColorado Technical University -> education.educational_institution.phone_number -> m.0nbc0rl\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nColorado Technical University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado Technical University -> common.topic.notable_types -> College/University -> type.type.expected_by -> University\n# Answer:\nCollege/University", "# Reasoning Path:\nColorado Technical University -> location.location.containedby -> Colorado -> location.location.nearby_airports -> Denver International Airport\n# Answer:\nColorado", "# Reasoning Path:\nColorado Technical University -> education.university.number_of_undergraduates -> m.0hpxh03\n# Answer:\neducation.university.number_of_undergraduates", "# Reasoning Path:\nColorado Technical University -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado Technical University -> location.location.containedby -> Colorado Springs -> location.location.nearby_airports -> Colorado Springs Airport\n# Answer:\nColorado Springs", "# Reasoning Path:\nColorado Technical University -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America"], "ground_truth": ["College/University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard"], "ground_truth": ["Bard", "Writer", "Poet", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1120", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Wallis and Futuna\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> New Caledonia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["French franc", "Assignat"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1121", "prediction": ["# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nKentucky -> symbols.flag_referent.flag -> m.0jsrj2m -> symbols.flag_use.flag -> Flag of Kentucky\n# Answer:\nFlag of Kentucky", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nBrachiopod", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04st742 -> location.location_symbol_relationship.symbol -> Milk\n# Answer:\nMilk", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04st93t -> location.location_symbol_relationship.symbol -> Viceroy\n# Answer:\nViceroy", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nLiriodendron", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil"], "ground_truth": ["United we stand, divided we fall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1124", "prediction": ["# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.0kdhbl1 -> tv.regular_tv_appearance.actor -> Maurice Evans\n# Answer:\nMaurice Evans", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.actor -> Dick York\n# Answer:\nDick York", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.actor -> Agnes Moorehead\n# Answer:\nAgnes Moorehead", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqtj -> tv.regular_tv_appearance.actor -> David White\n# Answer:\nDavid White", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqv8 -> tv.regular_tv_appearance.actor -> Elizabeth Montgomery\n# Answer:\nElizabeth Montgomery", "# Reasoning Path:\nBewitched -> tv.tv_program.seasons -> Bewitched - Season 8 -> tv.tv_series_season.regular_cast -> m.02nrqt1\n# Answer:\nBewitched - Season 8", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqs4 -> tv.regular_tv_appearance.character -> Darrin\n# Answer:\nDarrin", "# Reasoning Path:\nBewitched -> tv.tv_program.regular_cast -> m.02nrqt1 -> tv.regular_tv_appearance.character -> Endora\n# Answer:\nEndora"], "ground_truth": ["Elizabeth Montgomery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.08695652173913045, "path_precision": 0.125, "path_recall": 0.06666666666666667, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1125", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.01066smw\n# Answer:\nPhysicist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nIsaac Newton -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Heraclitus\n# Answer:\nPhilosopher"], "ground_truth": ["Philosopher", "Astronomer", "Physicist", "Scientist", "Mathematician", "Chemist"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 1.0, "ans_recall": 0.8333333333333334, "path_f1": 0.9090909090909091, "path_precision": 1.0, "path_recall": 0.8333333333333334, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 1.0, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1126", "prediction": ["# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> aviation.airport.serves -> San Diego -> location.location.containedby -> San Diego County\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego International Airport -> location.location.containedby -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1127", "prediction": ["# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.country.first_level_divisions -> Alaska\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Nebraska\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth America -> base.locations.continents.countries_within -> Antigua and Barbuda -> location.country.languages_spoken -> English Language\n# Answer:\nAntigua and Barbuda"], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1130", "prediction": ["# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.actor -> Googy Gress\n# Answer:\nGoogy Gress", "# Reasoning Path:\nSanta Claus -> film.film.genre -> Cult film\n# Answer:\nCult film", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.0111f_4n -> film.performance.actor -> Jim Broadbent\n# Answer:\nJim Broadbent", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.010rvdmk -> film.performance.film -> Naughty or Nice\n# Answer:\nNaughty or Nice", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w47wj -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nSanta Claus -> common.topic.webpage -> m.09w85m2 -> common.webpage.resource -> Uma Thurman, Jesse L. Martin to appear in NBC's Muppets holiday special\n# Answer:\nUma Thurman, Jesse L. Martin to appear in NBC's Muppets holiday special", "# Reasoning Path:\nSanta Claus -> film.film.written_by -> Adolfo Torres Portillo -> people.person.gender -> Male\n# Answer:\nAdolfo Torres Portillo", "# Reasoning Path:\nSanta Claus -> film.film_character.portrayed_in_films -> m.01143_m2 -> film.performance.actor -> Boris Martemyanov\n# Answer:\nBoris Martemyanov", "# Reasoning Path:\nSanta Claus -> fictional_universe.fictional_character.based_on -> Saint Nicholas -> symbols.name_source.namesakes -> Agios Nikolaos\n# Answer:\nSaint Nicholas"], "ground_truth": ["Saint Nicholas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1131", "prediction": ["# Reasoning Path:\nKelly Slater -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nKelly Slater -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Actor -> common.topic.subject_of -> Michael Palance\n# Answer:\nActor", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> base.descriptive_names.names.descriptive_name -> m.0106b5xy\n# Answer:\nFilm Producer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Surfer -> fictional_universe.character_occupation.characters_with_this_occupation -> Merliah Summers\n# Answer:\nSurfer", "# Reasoning Path:\nKelly Slater -> people.person.profession -> Film Producer -> award.award_discipline.awards_in_this_discipline -> David di Donatello for Best Producer\n# Answer:\nFilm Producer"], "ground_truth": ["1990"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1132", "prediction": ["# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.notable_types -> Symptom\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Erotic asphyxiation\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.article -> m.019dmk\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> common.image.size -> m.02cscjf\n# Answer:\ncommon.image.size", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.notable_for -> g.125fqhmdk\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Strangling\n# Answer:\nRespiratory failure", "# Reasoning Path:\nCoretta Scott King -> people.person.ethnicity -> African American -> medicine.risk_factor.diseases -> Sickle cell disease\n# Answer:\nAfrican American"], "ground_truth": ["Respiratory failure"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1133", "prediction": ["# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_for -> g.1258jb2m3\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.spouse -> Emily Smith, 1st Viscountess Hambleden\n# Answer:\nEmily Smith, 1st Viscountess Hambleden", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> common.topic.article -> m.02xlj10\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> location.location.partially_contains -> River Thames\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> symbols.name_source.namesakes -> 8837 London\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Henry Smith -> people.person.spouse_s -> m.0j4mrzc -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam Henry Smith -> common.topic.notable_types -> Politician -> owl#inverseOf -> Party\n# Answer:\nPolitician", "# Reasoning Path:\nWilliam Henry Smith -> people.person.place_of_birth -> London -> periodicals.newspaper_circulation_area.newspapers -> i\n# Answer:\ni"], "ground_truth": ["WHSmith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1135", "prediction": ["# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> A spirit of national masochism prevails, encouraged by an effete corps of impudent snobs who characterize themselves as intellectuals.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> Freedom of speech is useless without freedom of thought. And I fear that the politics of protest is shutting out the process of thought, so necessary to rational discussion. We are faced with the Ten Commandments of Protest:Thou Shalt Not Allow Thy Opponent to Speak. Thou Shalt Not Set Forth a Program of Thine Own. Thou Shalt Not Trust Anybody Over Thirty. Thou Shalt Not Honor Thy Father or Thy Mother. Thou Shalt Not Heed the Lessons of History. Thou Shalt Not Write Anything Longer than a Slogan. Thou Shalt Not Present a Negotiable Demand. Thou Shalt Not Accept Any Establishment Idea. Thou Shalt Not Revere Any but Totalitarian Heroes. Thou Shalt Not Ask Forgiveness for Thy Transgressions, Rather Thou Shalt Demand Amnesty for Them.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> film.person_or_entity_appearing_in_film.films -> m.0114dk_n\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> In the United States today, we have more than our share of the nattering nabobs of negativism. They have formed their own 4H Clubthe hopeless, hysterical hypochondriacs of history.\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.education -> m.02wpd1f\n# Answer:\nSpiro Agnew", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Gerald Ford -> people.person.profession -> Politician\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> government.us_president.vice_president -> Spiro Agnew -> people.person.quotations -> Some newspapers are fit only to line the bottom of bird cages.\n# Answer:\nSpiro Agnew"], "ground_truth": ["Spiro Agnew", "Gerald Ford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1136", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.is_part_of -> Christianity\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Catholicism -> religion.religion.branched_from -> Judaism\n# Answer:\nCatholicism", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.branched_from -> Judaism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> people.person.religion -> Christianity -> religion.religion.includes -> Catholicism\n# Answer:\nChristianity", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Christianity", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1137", "prediction": ["# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> United States of America\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.location.containedby -> East North Central States\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> location.location.containedby -> Illinois -> location.statistical_region.religions -> m.04403jk\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois River -> geography.body_of_water.bridges -> Abraham Lincoln Memorial Bridge -> location.location.containedby -> LaSalle County\n# Answer:\nAbraham Lincoln Memorial Bridge", "# Reasoning Path:\nIllinois River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> United States of America\n# Answer:\nMississippi River"], "ground_truth": ["United States of America", "North America", "Illinois"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.7058823529411765, "path_precision": 0.75, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1138", "prediction": ["# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.containedby -> Americas\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> base.locations.continents.planet -> Earth\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> base.locations.countries.continent -> North America -> location.location.time_zones -> Central Time Zone\n# Answer:\nNorth America", "# Reasoning Path:\nHonduras -> location.location.containedby -> Latin America -> location.location.time_zones -> Central Time Zone\n# Answer:\nLatin America", "# Reasoning Path:\nHonduras -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71rqx_6\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament"], "ground_truth": ["Central America", "Americas", "North America", "Latin America"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.6086956521739131, "path_precision": 0.7777777777777778, "path_recall": 0.5, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1142", "prediction": ["# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9hk0b -> soccer.football_player_stats.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j_7v43 -> sports.sports_team_roster.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nRonaldinho -> sports.pro_athlete.teams -> m.0j_7v43 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9db76 -> soccer.football_player_stats.team -> Brazil national football team\n# Answer:\nBrazil national football team", "# Reasoning Path:\nRonaldinho -> base.schemastaging.athlete_extra.salary -> m.0r9mccp -> base.schemastaging.athlete_salary.currency -> Brazilian real\n# Answer:\nBrazilian real", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> people.person.nationality -> Brazil -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldinho -> soccer.football_player.statistics -> m.0w9b586 -> soccer.football_player_stats.team -> Clube de Regatas do Flamengo\n# Answer:\nClube de Regatas do Flamengo"], "ground_truth": ["Clube de Regatas do Flamengo", "Brazil national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1143", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Werner Heisenberg\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.schemastaging.context_name.pronunciation -> g.125_nmg0m\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> common.topic.image -> Johannes Hevelius\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist"], "ground_truth": ["Solar System", "Kinematics", "Heliocentrism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1144", "prediction": ["# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> time.event.locations -> Berlin\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGermany -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.host_city -> Berlin\n# Answer:\n1936 Summer Olympics"], "ground_truth": ["Tanzania", "Scotland", "Nepal", "Tuvalu", "Afghanistan", "Albania", "Antigua and Barbuda", "Algeria", "United States of America", "Madagascar"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1145", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkfc -> education.education.institution -> Brigham Young University\n# Answer:\nBrigham Young University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0125cyb9 -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["Brigham Young University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1146", "prediction": ["# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nVictor Van Dort", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.character -> Victor Van Dort\n# Answer:\nVictor Van Dort", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03jsyd7 -> film.performance.character -> Elder Gutknecht\n# Answer:\nElder Gutknecht", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.actor -> Johnny Depp\n# Answer:\nfilm.performance.actor", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.03l4b_9 -> film.performance.character -> Nell Van Dort\n# Answer:\nNell Van Dort", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> g.11b6tp52w8\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.0jwq1q -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nJohnny Depp -> film.actor.film -> m.0jwq1q -> film.performance.film -> Corpse Bride\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nCorpse Bride -> film.film.starring -> m.06644pd -> film.performance.character -> William Van Dort\n# Answer:\nWilliam Van Dort"], "ground_truth": ["Victor Van Dort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1147", "prediction": ["# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1211 Stone\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.postal_code.country -> United States of America\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.containedby -> Craighead County\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72403 -> common.topic.notable_for -> g.1259zdl3v\n# Answer:\n72403", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.contains -> 1222 W Monroe @ West Elementary\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72401 -> location.location.geolocation -> m.03dn5cm\n# Answer:\n72401", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72404 -> location.location.contains -> Stadium Place Apartments on Stadium\n# Answer:\n72404", "# Reasoning Path:\nJonesboro -> location.citytown.postal_codes -> 72404 -> location.location.geolocation -> m.03dn5ct\n# Answer:\n72404", "# Reasoning Path:\nJonesboro -> location.statistical_region.population -> g.11b66bywlp\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["72401", "72402", "72403", "72404"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.8135593220338982, "path_precision": 0.8888888888888888, "path_recall": 0.75, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-1148", "prediction": ["# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.04hd46b -> education.education.institution -> University of Tulsa\n# Answer:\nUniversity of Tulsa", "# Reasoning Path:\nS. E. Hinton -> people.person.education -> m.0n1jntg -> education.education.institution -> Will Rogers High School\n# Answer:\nWill Rogers High School", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.book -> The Outsiders\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.number_of_pages -> m.0b3snbp\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.isbn -> 9780143039853\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> film.actor.film -> m.0t_lwd8 -> film.performance.film -> Rumble Fish\n# Answer:\nRumble Fish", "# Reasoning Path:\nS. E. Hinton -> book.author.book_editions_published -> The Outsiders (Penguin Classics) -> book.book_edition.publisher -> Penguin Classics\n# Answer:\nThe Outsiders (Penguin Classics)", "# Reasoning Path:\nS. E. Hinton -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Oklahoma\n# Answer:\nUnited States of America"], "ground_truth": ["University of Tulsa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1149", "prediction": ["# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> Taylor Swift Fearless 2009 Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> The 1989 World Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.album -> Speak Now\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.album_or_release_supporting -> Fearless -> music.album.artist -> Taylor Swift\n# Answer:\nFearless", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.concert_tours -> The Red Tour\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.lyricist.lyrics_written -> You're Not Sorry\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.artist.album -> Fearless\n# Answer:\nTaylor Swift", "# Reasoning Path:\nFearless Tour -> music.concert_tour.artist -> Taylor Swift -> music.lyricist.lyrics_written -> Ours\n# Answer:\nTaylor Swift"], "ground_truth": ["2009-04-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9xs -> sports.sports_award.season -> 1984 NFL season\n# Answer:\n1984 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1151", "prediction": ["# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.deceased_person.place_of_death -> London\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> people.person.profession -> Philosopher\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.notable_for -> g.12578zv1w\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> John Bowlby -> influence.influence_node.influenced -> Mary Ainsworth\n# Answer:\nJohn Bowlby", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.article -> m.02_67b\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> influence.influence_node.peers -> Friedrich Engels\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> Karl Marx -> influence.influence_node.influenced_by -> Adam Smith\n# Answer:\nKarl Marx", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0djwmxs\n# Answer:\nDown House"], "ground_truth": ["London", "England", "Shrewsbury", "Downe, Kent"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07999999999999999, "path_precision": 0.125, "path_recall": 0.058823529411764705, "path_ans_f1": 0.16666666666666666, "path_ans_precision": 0.125, "path_ans_recall": 0.25}
{"id": "WebQTest-1152", "prediction": ["# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.actor -> Val Kilmer\n# Answer:\nVal Kilmer", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKITT -> film.film_character.portrayed_in_films -> m.0j7ph8m -> film.performance.film -> Knight Rider: The Movie\n# Answer:\nKnight Rider: The Movie", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.07spfgl -> tv.regular_tv_appearance.series -> Knight Rider\n# Answer:\nKnight Rider", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_episodes -> m.09nfwd3 -> tv.tv_guest_role.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1153", "prediction": ["# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> Miley Cyrus\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> John McCain\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> George W. Bush\n# Answer:\nScotch-Irish American", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> travel.travel_destination.tourist_attractions -> Comerica Park\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> film.film_location.featured_in_films -> 8 Mile\n# Answer:\nDetroit", "# Reasoning Path:\nEminem -> music.artist.origin -> Warren -> location.location.containedby -> United States of America\n# Answer:\nWarren", "# Reasoning Path:\nEminem -> music.artist.origin -> Detroit -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nDetroit"], "ground_truth": ["Marshall Bruce Mathers III"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1154", "prediction": ["# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.spouse -> Gary Chapman\n# Answer:\nGary Chapman", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> people.marriage.spouse -> Vince Gill\n# Answer:\nVince Gill", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0w2ygvd -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nAmy Grant -> people.person.children -> Corrina Grant Gill -> people.person.parents -> Vince Gill\n# Answer:\nCorrina Grant Gill", "# Reasoning Path:\nAmy Grant -> people.person.children -> Gloria Mills Chapman -> people.person.parents -> Gary Chapman\n# Answer:\nGloria Mills Chapman", "# Reasoning Path:\nAmy Grant -> people.person.spouse_s -> m.0h4q7j_ -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nAmy Grant -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["Gary Chapman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1155", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> location.location.contains -> Talamancan montane forests\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> location.country.languages_spoken -> English Language\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.notable_for -> g.12596bzjx\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.02h92xs\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.country.languages_spoken -> English Language\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.article -> m.080_k8\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Condominios Panorama Life Torre I -> location.location.containedby -> North America\n# Answer:\nCondominios Panorama Life Torre I", "# Reasoning Path:\nCentral America -> symbols.namesake.named_after -> Amerigo Vespucci -> symbols.name_source.namesakes -> South America\n# Answer:\nAmerigo Vespucci"], "ground_truth": ["Honduras", "Costa Rica", "El Salvador", "Belize", "Guatemala", "Gran Colombia", "Panama"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.25, "ans_recall": 0.2857142857142857, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.25, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1156", "prediction": ["# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.parents -> Casey Aldridge\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.place_of_birth -> McComb\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> common.topic.notable_for -> g.125g6_bhr\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> people.person.nationality -> United States of America\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nJamie Lynn Spears -> people.person.children -> Maddie Briann Aldridge -> common.topic.notable_types -> Person\n# Answer:\nMaddie Briann Aldridge", "# Reasoning Path:\nJamie Lynn Spears -> people.person.place_of_birth -> McComb -> location.location.people_born_here -> Maddie Briann Aldridge\n# Answer:\nMcComb", "# Reasoning Path:\nJamie Lynn Spears -> freebase.valuenotation.is_reviewed -> Children -> type.property.schema -> Person\n# Answer:\nChildren"], "ground_truth": ["Maddie Briann Aldridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1160", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.nationality -> United States of America\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> common.topic.notable_types -> Person or entity appearing in film\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.gender -> Female\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Tito Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Brandon Jackson\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nKatherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.parents -> Katherine Jackson -> people.person.children -> Jackie Jackson\n# Answer:\nKatherine Jackson"], "ground_truth": ["Katherine Jackson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1161", "prediction": ["# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_for -> g.12568xvfh\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> Clemson University The Tiger -> common.topic.notable_types -> School mascot\n# Answer:\nClemson University The Tiger", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_types -> School mascot\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> business.employer.employees -> m.010h0sdv -> business.employment_tenure.person -> James P. Clements\n# Answer:\nJames P. Clements", "# Reasoning Path:\nClemson University -> education.educational_institution.mascot -> The Tiger and the Cub -> common.topic.notable_for -> g.1258wsn3q\n# Answer:\nThe Tiger and the Cub", "# Reasoning Path:\nClemson University -> award.ranked_item.appears_in_ranked_lists -> m.0h6f984 -> award.ranking.list -> US News & World Report: National University Rankings\n# Answer:\nUS News & World Report: National University Rankings", "# Reasoning Path:\nClemson University -> education.educational_institution.athletics_brand -> Clemson Tigers -> education.athletics_brand.teams -> Clemson Tigers men's basketball\n# Answer:\nClemson Tigers", "# Reasoning Path:\nClemson University -> education.educational_institution.athletics_brand -> Clemson Tigers -> common.topic.webpage -> m.04mc99d\n# Answer:\nClemson Tigers"], "ground_truth": ["The Tiger and the Cub", "Clemson University The Tiger"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1163", "prediction": ["# Reasoning Path:\nSouth America -> location.location.time_zones -> Venezuela Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC\n# Answer:\nVenezuela Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Venezuela Time Zone -> common.topic.article -> m.03gzttq\n# Answer:\nVenezuela Time Zone", "# Reasoning Path:\nSouth America -> travel.travel_destination.tour_operators -> Bunnik Tours -> travel.tour_operator.travel_destinations -> Brazil\n# Answer:\nBunnik Tours", "# Reasoning Path:\nSouth America -> location.location.partially_contains -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Venezuela Time Zone -> common.topic.notable_for -> g.125f_5rg1\n# Answer:\nVenezuela Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Venezuela Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins\n# Answer:\nVenezuela Time Zone", "# Reasoning Path:\nSouth America -> location.location.time_zones -> Venezuela Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nVenezuela Time Zone", "# Reasoning Path:\nSouth America -> travel.travel_destination.tour_operators -> Bunnik Tours -> business.brand.product_lines -> Cruise & Tour\n# Answer:\nBunnik Tours"], "ground_truth": ["Amazon rainforest"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1164", "prediction": ["# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> \u00c9douard Manet\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.book_subject.works -> Monet\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Impressionism -> book.written_work.subjects -> France\n# Answer:\nImpressionism", "# Reasoning Path:\nClaude Monet -> common.image.appears_in_topic_gallery -> November 14\n# Answer:\nNovember 14", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.image -> Pink Bulls\n# Answer:\nModern art", "# Reasoning Path:\nClaude Monet -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art"], "ground_truth": ["Modern art", "Impressionism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1166", "prediction": ["# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.043wpnt\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.argumentmaps.motivation_s.motivation -> m.05z179k -> base.argumentmaps.motivation.intention -> Withdrawal of U.S. troops from Iraq\n# Answer:\nWithdrawal of U.S. troops from Iraq", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> military.military_combatant.military_conflicts -> m.04ftyvf\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> book.written_work.subjects -> United States of America\n# Answer:\nIraq", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> time.event.locations -> Global\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> 20 March 2003 anti-war protest -> common.topic.article -> m.082shh\n# Answer:\n20 March 2003 anti-war protest", "# Reasoning Path:\nIraq War -> base.fight.subject_of_protest.protests_about_this -> Berkeley Marine Corps Recruiting Center protests -> common.topic.notable_for -> g.1255gcll3\n# Answer:\nBerkeley Marine Corps Recruiting Center protests", "# Reasoning Path:\nIraq War -> book.book_subject.works -> Iraq -> common.topic.notable_types -> Book\n# Answer:\nIraq"], "ground_truth": ["2003-03-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1168", "prediction": ["# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> common.topic.article -> m.047q4q\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> music.guitar.brand -> Fender Musical Instruments Corporation\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> common.topic.notable_types -> Guitar\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.guitarist.guitars_played -> Fender Jag-Stang -> common.topic.notable_for -> g.125fxghdg\n# Answer:\nFender Jag-Stang", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Electric guitar -> music.instrument.family -> Guitar\n# Answer:\nElectric guitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Fretless guitar\n# Answer:\nGuitar", "# Reasoning Path:\nKurt Cobain -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Sonic Sense\n# Answer:\nGuitar"], "ground_truth": ["Fender Jag-Stang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1169", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.character -> Raven / Mystique\n# Answer:\nRaven / Mystique", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0s912lb -> film.performance.film -> X-Men: Days of Future Past\n# Answer:\nX-Men: Days of Future Past", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqn6t -> film.performance.film -> X-Men: Apocalypse\n# Answer:\nX-Men: Apocalypse", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.010sqq_b -> film.performance.character -> Joy Mangano\n# Answer:\nJoy Mangano", "# Reasoning Path:\nJennifer Lawrence -> film.actor.film -> m.0cgr66b -> film.performance.film -> X-Men: First Class\n# Answer:\nX-Men: First Class", "# Reasoning Path:\nJennifer Lawrence -> award.award_winner.awards_won -> m.0101mk_y -> award.award_honor.ceremony -> 2014 Kids' Choice Awards\n# Answer:\n2014 Kids' Choice Awards"], "ground_truth": ["Raven / Mystique"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1170", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nJim Webb", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nMark Warner", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cmm -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles"], "ground_truth": ["Bob McDonnell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1171", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> film.film_character.portrayed_in_films -> m.04kc989\n# Answer:\nShmi Skywalker", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.parents -> Shmi Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine\n# Answer:\nShmi Skywalker"], "ground_truth": ["James Earl Jones", "Dr. Smoov", "Abraham Benrubi", "Matt Lanter", "Zac Efron"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.26086956521739135, "ans_precission": 0.375, "ans_recall": 0.2, "path_f1": 0.3157894736842105, "path_precision": 0.375, "path_recall": 0.2727272727272727, "path_ans_f1": 0.26086956521739135, "path_ans_precision": 0.375, "path_ans_recall": 0.2}
{"id": "WebQTest-1173", "prediction": ["# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States, with Territories\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.location.containedby -> Georgia -> location.location.containedby -> United States of America\n# Answer:\nGeorgia", "# Reasoning Path:\nWalton County -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWalton County -> location.us_county.county_seat -> Monroe -> location.location.containedby -> United States of America\n# Answer:\nMonroe", "# Reasoning Path:\nWalton County -> location.statistical_region.population -> g.11b674j1z2\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Atlanta-Sandy Springs-Marietta, GA Metropolitan Statistical Area", "Georgia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2222222222222222, "ans_recall": 0.5, "path_f1": 0.30769230769230765, "path_precision": 0.2222222222222222, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5}
{"id": "WebQTest-1174", "prediction": ["# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Erythema nodosum\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Anemia\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> medicine.risk_factor.diseases -> Intussusception\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease_cause.diseases -> Kyphosis\n# Answer:\nCancer", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> common.topic.notable_types -> Cause Of Death\n# Answer:\nLymphoma", "# Reasoning Path:\nJacqueline Kennedy Onassis -> opera.opera_character.operas_appears_in -> m.0dm2d1m -> opera.opera_character_voice.opera -> Jackie O\n# Answer:\nJackie O", "# Reasoning Path:\nJacqueline Kennedy Onassis -> people.deceased_person.cause_of_death -> Lymphoma -> base.bioventurist.bv_medical_condition.therapeutics -> m.052wqkp\n# Answer:\nLymphoma"], "ground_truth": ["Lymphoma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1175", "prediction": ["# Reasoning Path:\nLauren Conrad -> people.person.education -> m.0gx3gdb -> education.education.institution -> Laguna Beach High School\n# Answer:\nLaguna Beach High School", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.major_field_of_study -> Fashion design\n# Answer:\nFashion design", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.major_field_of_study -> Fashion design\n# Answer:\nFashion design", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063g5hk -> base.popstra.friendship.participant -> Brody Jenner\n# Answer:\nBrody Jenner", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.040zfb0 -> education.education.institution -> Fashion Institute of Design & Merchandising\n# Answer:\nFashion Institute of Design & Merchandising", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.dated -> m.063t50c -> base.popstra.dated.participant -> Cisco Adler\n# Answer:\nCisco Adler", "# Reasoning Path:\nLauren Conrad -> base.popstra.celebrity.friendship -> m.063tm27 -> base.popstra.friendship.participant -> Audrina Patridge\n# Answer:\nAudrina Patridge", "# Reasoning Path:\nLauren Conrad -> people.person.education -> m.04hyzwc -> education.education.institution -> Academy of Art University\n# Answer:\nAcademy of Art University"], "ground_truth": ["Fashion design"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1176", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.article -> m.02_384\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Henri -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nTropical Storm Henri"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1177", "prediction": ["# Reasoning Path:\nMario Lopez -> people.person.education -> m.0g_9bvg -> education.education.institution -> Chula Vista High School\n# Answer:\nChula Vista High School", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> film.actor.film -> m.03jq9tb -> film.performance.film -> Colors\n# Answer:\nColors", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> base.popstra.celebrity.canoodled -> m.0652r5y -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nMario Lopez -> people.person.gender -> Male -> common.topic.image -> male.jpg\n# Answer:\nMale", "# Reasoning Path:\nMario Lopez -> tv.tv_actor.starring_roles -> m.02nnfly -> tv.regular_tv_appearance.series -> Saved by the Bell\n# Answer:\nSaved by the Bell"], "ground_truth": ["Chula Vista High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1178", "prediction": ["# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.people_born_here -> Lumidee\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York City\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.location.containedby -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> Manhattan\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> people.person.place_of_birth -> East Harlem -> location.neighborhood.neighborhood_of -> New York metropolitan area\n# Answer:\nEast Harlem", "# Reasoning Path:\nTupac Shakur -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["East Harlem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1179", "prediction": ["# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.02nv74t\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> film.producer.films_executive_produced -> The Emperor -> film.film.starring -> m.0k4_f9\n# Answer:\nThe Emperor", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Luke Skywalker -> fictional_universe.fictional_character.occupation -> Jedi\n# Answer:\nLuke Skywalker", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Luke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j\n# Answer:\nLuke Skywalker", "# Reasoning Path:\nGeorge Lucas -> film.producer.films_executive_produced -> Return of the Jedi -> film.film.starring -> m.0k3r3j\n# Answer:\nReturn of the Jedi", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Luke Skywalker -> fictional_universe.fictional_character.powers_or_abilities -> Levitation\n# Answer:\nLuke Skywalker", "# Reasoning Path:\nGeorge Lucas -> fictional_universe.fictional_character_creator.fictional_characters_created -> Darth Vader -> film.film_character.portrayed_in_films -> m.01xpnt2\n# Answer:\nDarth Vader", "# Reasoning Path:\nGeorge Lucas -> film.producer.films_executive_produced -> The Emperor -> fictional_universe.fictional_character.powers_or_abilities -> Levitation\n# Answer:\nThe Emperor"], "ground_truth": ["Baron Papanoida"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> military.military_combatant.military_conflicts -> m.04fvd6y\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia"], "ground_truth": ["Saudi Arabia", "Australia", "France", "United Kingdom", "Iraq", "United States of America", "Argentina"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.1846153846153846, "path_precision": 0.75, "path_recall": 0.10526315789473684, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1180", "prediction": ["# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.costume_design_by -> Alexandra Welker\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> common.topic.notable_types -> Film\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> film.film.runtime -> m.0jzmqx\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> film.film_story_contributor.film_story_credits -> How to Deal -> common.topic.webpage -> m.0940d15\n# Answer:\nHow to Deal", "# Reasoning Path:\nSarah Dessen -> people.person.spouse_s -> m.0n4rc8b -> people.marriage.spouse -> Jay Earl Marks\n# Answer:\nJay Earl Marks", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> The Moon and More -> common.topic.notable_types -> Book\n# Answer:\nThe Moon and More", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Along for the Ride -> book.book.characters -> Adam\n# Answer:\nAlong for the Ride", "# Reasoning Path:\nSarah Dessen -> book.author.works_written -> Dreamland -> book.written_work.subjects -> Identity\n# Answer:\nDreamland"], "ground_truth": ["How to Deal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1181", "prediction": ["# Reasoning Path:\nHawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHawaiian Islands -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.location.containedby -> United States of America -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nKauai County -> location.location.containedby -> Hawaii -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nKauai County -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Hawaii\n# Answer:\nUnited States of America"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 0.7142857142857143, "ans_recall": 1.0, "path_f1": 0.8333333333333333, "path_precision": 0.7142857142857143, "path_recall": 1.0, "path_ans_f1": 0.923076923076923, "path_ans_precision": 0.8571428571428571, "path_ans_recall": 1.0}
{"id": "WebQTest-1183", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.geolocation -> m.0cqx58y\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.notable_for -> g.1256km_vm\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.article -> m.018mmb\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> common.topic.webpage -> m.0g5vwbd\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.containedby -> Los Angeles County\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> common.topic.webpage -> m.0773vxj -> common.webpage.resource -> TV and Movie Trivia Tribute\n# Answer:\nTV and Movie Trivia Tribute", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.place_of_burial -> Westwood Village Memorial Park Cemetery -> location.location.containedby -> 90024\n# Answer:\nWestwood Village Memorial Park Cemetery", "# Reasoning Path:\nFarrah Fawcett -> media_common.quotation_subject.quotations_about_this_subject -> Farrah was one of the iconic beauties of our time. Her girl-next-door charm combined with stunning looks made her a star on film, TV and the printed page. -> media_common.quotation.author -> Hugh Hefner\n# Answer:\nFarrah was one of the iconic beauties of our time. Her girl-next-door charm combined with stunning looks made her a star on film, TV and the printed page."], "ground_truth": ["Westwood Village Memorial Park Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1185", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Charles Dickens\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced -> Toni Morrison\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Sherwood Anderson -> influence.influence_node.influenced_by -> Mark Twain\n# Answer:\nSherwood Anderson", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> William Shakespeare\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> people.person.gender -> Male\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> J. D. Salinger\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Herman Melville\n# Answer:\nWilliam Faulkner"], "ground_truth": ["Thomas Malory", "Robert Burns", "Sherwood Anderson", "William Faulkner"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1186", "prediction": ["# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Soviet Union\n# Answer:\nSocialist state", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Sri Lanka\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> common.topic.notable_types -> Form of Government\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Martinique\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nCuba -> location.country.form_of_government -> Republic -> common.topic.notable_types -> Form of Government\n# Answer:\nRepublic", "# Reasoning Path:\nCuba -> location.statistical_region.energy_use_per_capita -> g.1245_22wq\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody"], "ground_truth": ["Semi-presidential system", "Socialist state", "Unitary state", "Republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-1187", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> base.locations.countries.continent -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.capital -> London\n# Answer:\nKingdom of Great Britain"], "ground_truth": ["Honduras", "Kiribati", "Mandatory Palestine", "England", "New Zealand", "State of Palestine", "India", "Guyana", "United Kingdom", "United States of America", "Swaziland", "Antigua and Barbuda", "Lesotho", "Tanzania", "Sudan", "Kingdom of Great Britain", "Tokelau", "Sri Lanka", "Cook Islands", "Vatican City", "Barbados", "Canada", "Philippines", "Zambia", "Cura\u00e7ao", "Hong Kong", "Liberia", "Bangladesh", "South Yemen", "China", "Nauru", "Gibraltar", "Vanuatu", "Grenada", "Brunei", "Pakistan", "Kenya", "Timor-Leste", "Gazankulu", "Marshall Islands", "Guam", "Dominica", "Saint Kitts and Nevis", "Laos", "Ethiopia", "Montserrat", "Namibia", "Qatar", "Bahamas", "Japan", "Israel", "Australia", "Tuvalu", "Bonaire", "Saint Vincent and the Grenadines", "Malaysia", "Saint Lucia", "Jersey", "Sierra Leone", "Indonesia", "Republic of Ireland", "Territory of Papua and New Guinea", "Rwanda", "Samoa", "Isle of Man", "Ghana", "Malta", "Papua New Guinea", "Cyprus", "Fiji", "Belize", "Wales", "Territory of New Guinea", "Puerto Rico", "Gambia", "South Africa", "Cayman Islands", "Bermuda", "Jordan", "Cameroon", "Botswana", "Transkei", "Uganda", "Zimbabwe", "Nigeria", "Singapore", "Turks and Caicos Islands"], "ans_acc": 0.05747126436781609, "ans_hit": 1, "ans_f1": 0.10869565217391304, "ans_precission": 1.0, "ans_recall": 0.05747126436781609, "path_f1": 0.0873634945397816, "path_precision": 0.875, "path_recall": 0.04597701149425287, "path_ans_f1": 0.10869565217391304, "path_ans_precision": 1.0, "path_ans_recall": 0.05747126436781609}
{"id": "WebQTest-1189", "prediction": ["# Reasoning Path:\nItaly -> location.country.capital -> Rome -> travel.travel_destination.tourist_attractions -> Vatican City\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> location.location.containedby -> Lazio\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> periodicals.newspaper_circulation_area.newspapers -> Liberazione\n# Answer:\nRome", "# Reasoning Path:\nItaly -> base.aareas.schema.administrative_area.administrative_children -> Lazio -> base.aareas.schema.administrative_area.administrative_children -> Province of Rieti\n# Answer:\nLazio", "# Reasoning Path:\nItaly -> location.country.capital -> Rome -> sports.sports_team_location.teams -> Pallacanestro Virtus Roma\n# Answer:\nRome", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> base.aareas.schema.administrative_area.administrative_children -> Veneto -> base.aareas.schema.administrative_area.administrative_children -> Province of Venice\n# Answer:\nVeneto", "# Reasoning Path:\nItaly -> base.aareas.schema.administrative_area.administrative_children -> Lazio -> base.aareas.schema.administrative_area.capital -> Rome\n# Answer:\nLazio"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.actor -> Doc Shaw\n# Answer:\nDoc Shaw", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglsdh -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0tly67c -> tv.regular_tv_appearance.character -> Marcus Little\n# Answer:\nMarcus Little", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nErin Cardillo", "# Reasoning Path:\nThe Suite Life on Deck -> media_common.netflix_title.netflix_genres -> The Walt Disney Company -> organization.organization.founders -> Walt Disney\n# Answer:\nThe Walt Disney Company"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1191", "prediction": ["# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Barbados\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nEnglish Language", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6ddy727\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["English Language", "Bahamas Creole English Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-1192", "prediction": ["# Reasoning Path:\nGeorge Michael -> people.person.education -> m.0h3d9q4 -> education.education.institution -> Bushey Meads School\n# Answer:\nBushey Meads School", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.track -> The First Time Ever I Saw Your Face\n# Answer:\nThe First Time Ever I Saw Your Face", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Different Corner -> music.recording.artist -> WHAM!\n# Answer:\nA Different Corner", "# Reasoning Path:\nGeorge Michael -> music.lyricist.lyrics_written -> A Last Request (I Want Your Sex, Part III) -> common.topic.notable_for -> g.1z2sq43ys\n# Answer:\nA Last Request (I Want Your Sex, Part III)", "# Reasoning Path:\nGeorge Michael -> music.artist.genre -> Adult contemporary music -> broadcast.content.broadcast -> Indigo FM\n# Answer:\nAdult contemporary music", "# Reasoning Path:\nGeorge Michael -> music.artist.genre -> Adult contemporary music -> common.topic.notable_for -> g.1255l10w8\n# Answer:\nAdult contemporary music", "# Reasoning Path:\nGeorge Michael -> music.artist.track_contributions -> m.010__qx2 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nGeorge Michael -> award.award_nominee.award_nominations -> m.0105vmz6 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Bushey Meads School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1194", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.match -> UEFA Euro 2012 Quarter-final I\n# Answer:\nUEFA Euro 2012 Quarter-final I", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> common.topic.article -> m.02xt70\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0g5h3bt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.3448275862068966, "path_precision": 0.5555555555555556, "path_recall": 0.25, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1195", "prediction": ["# Reasoning Path:\nBahamas -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nBahamas -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_22wy\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6ddy727\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_7s_2\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBahamas -> location.statistical_region.cpi_inflation_rate -> g.11b60tqm2w\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nBahamas -> location.statistical_region.part_time_employment_percent -> g.1hhc39ggh\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nBahamas -> location.statistical_region.co2_emissions_per_capita -> g.1245_b1hf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1197", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Italian Language", "Brazilian Portuguese", "Portuguese Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.2857142857142857, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1198", "prediction": ["# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Robert Browning\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Elizabeth Barrett Browning\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> people.person.religion -> Atheism\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> John Milton -> influence.influence_node.influenced_by -> William Shakespeare\n# Answer:\nJohn Milton", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Algernon Charles Swinburne -> influence.influence_node.influenced_by -> Alfred Tennyson, 1st Baron Tennyson\n# Answer:\nAlgernon Charles Swinburne", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> William Shakespeare -> influence.influence_node.influenced_by -> Edmund Spenser\n# Answer:\nWilliam Shakespeare", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced_by -> Thomas Hobbes\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nSamuel Taylor Coleridge -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nBaruch Spinoza"], "ground_truth": ["Algernon Charles Swinburne", "Robert Burns", "Emanuel Swedenborg", "Friedrich Wilhelm Joseph Schelling", "Giambattista Vico", "Immanuel Kant", "Thomas Browne", "Baruch Spinoza", "John Milton", "William Shakespeare"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5714285714285715, "path_precision": 1.0, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-1199", "prediction": ["# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> base.descriptive_names.names.descriptive_name -> m.0106hk42\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> people.profession.specialization_of -> Broadcaster\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> TV Personality -> people.profession.corresponding_type -> TV Actor\n# Answer:\nTV Personality", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Fashion designer -> common.topic.notable_types -> Profession\n# Answer:\nFashion designer", "# Reasoning Path:\nKim Kardashian -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer"], "ground_truth": ["Actor", "Fashion designer", "TV Personality", "Businessperson", "Television producer", "Socialite", "Entrepreneur", "Model"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 1.0, "ans_recall": 0.375, "path_f1": 0.5454545454545454, "path_precision": 1.0, "path_recall": 0.375, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_holder -> Mike DeWine\n# Answer:\nMike DeWine", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator"], "ground_truth": ["Ted Strickland", "Return J. Meigs, Jr.", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1200", "prediction": ["# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> base.aubreymaturin.place.mentions -> m.05pwg4d\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> common.topic.notable_for -> g.12559b43k\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0hyq8n7\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.location.containedby -> United States of America\n# Answer:\nWisconsin", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Wisconsin -> location.location.containedby -> United States, with Territories\n# Answer:\nWisconsin", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.statistical_region.population -> m.0t4sg4g\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> geography.river.mouth -> Gulf of Mexico -> location.location.adjoin_s -> m.02_4t9f\n# Answer:\nGulf of Mexico", "# Reasoning Path:\nMississippi River -> location.location.partially_containedby -> Mississippi -> location.location.containedby -> United States of America\n# Answer:\nMississippi"], "ground_truth": ["Gulf of Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1201", "prediction": ["# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.actor -> Timothy Omundson\n# Answer:\nTimothy Omundson", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.0_p48fh -> tv.regular_tv_appearance.actor -> Anthony Michael Hall\n# Answer:\nAnthony Michael Hall", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.character -> Carlton Lassiter\n# Answer:\nCarlton Lassiter", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPsych -> award.award_winning_work.awards_won -> m.0zyrvxx -> award.award_honor.award -> People's Choice Award for Favorite Cable TV Comedy\n# Answer:\nPeople's Choice Award for Favorite Cable TV Comedy", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgl5 -> tv.regular_tv_appearance.actor -> James Roday\n# Answer:\nJames Roday", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgt5 -> tv.regular_tv_appearance.seasons -> Psych - Season 8\n# Answer:\nPsych - Season 8", "# Reasoning Path:\nPsych -> tv.tv_program.regular_cast -> m.02wsgms -> tv.regular_tv_appearance.actor -> Dul\u00e9 Hill\n# Answer:\nDul\u00e9 Hill"], "ground_truth": ["Timothy Omundson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1203", "prediction": ["# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_holder -> Shimon Peres\n# Answer:\nShimon Peres", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.office_position_or_title -> President of Israel\n# Answer:\nPresident of Israel", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.appointed_by -> Benjamin Netanyahu\n# Answer:\nBenjamin Netanyahu", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.010gg9lp -> government.government_position_held.office_holder -> Ehud Olmert\n# Answer:\nEhud Olmert", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j7nj\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0j5wkf1 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.010ggbks -> government.government_position_held.office_holder -> Shimon Peres\n# Answer:\nShimon Peres", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.010phvv4 -> government.government_position_held.office_holder -> Shimon Peres\n# Answer:\nShimon Peres", "# Reasoning Path:\nIsrael -> location.statistical_region.cpi_inflation_rate -> g.11b60st8pp\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nIsrael -> government.governmental_jurisdiction.governing_officials -> m.0114vpvq -> government.government_position_held.governmental_body -> Israeli Ministry of Defense\n# Answer:\nIsraeli Ministry of Defense"], "ground_truth": ["Reuven Rivlin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1204", "prediction": ["# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.article -> m.027v6q\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.image -> The Four Corners region is in the red area on this map\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.article -> m.0ly_\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.notable_for -> g.1257_prs_\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_for -> g.125c_8p32\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> common.topic.notable_types -> Location\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> people.ethnicity.geographic_distribution -> Four Corners -> location.location.events -> Black Hawk War\n# Answer:\nFour Corners", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> base.descriptive_names.names.descriptive_name -> m.011vfn5d\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> freebase.type_hints.included_types -> Topic\n# Answer:\nEthnicity", "# Reasoning Path:\nAncestral Puebloans -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Applies to\n# Answer:\nEthnicity"], "ground_truth": ["Four Corners"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1205", "prediction": ["# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.actor -> Arleen Sorkin\n# Answer:\nArleen Sorkin", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm17r -> tv.regular_tv_appearance.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.seasons -> Batman: The Animated Series - Season 3\n# Answer:\nBatman: The Animated Series - Season 3", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.02wm18b -> tv.regular_tv_appearance.actor -> Mia Sara\n# Answer:\nMia Sara", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.series -> Batman: The Animated Series\n# Answer:\nBatman: The Animated Series", "# Reasoning Path:\nHarley Quinn -> common.topic.article -> m.01t93s\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHarley Quinn -> tv.tv_character.appeared_in_tv_program -> m.0wz39vs -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.romantically_involved_with -> m.030rldt -> fictional_universe.romantic_involvement.partner -> Joker\n# Answer:\nJoker", "# Reasoning Path:\nHarley Quinn -> fictional_universe.fictional_character.employers -> m.02wm19d -> fictional_universe.fictional_employment_tenure.title -> Internship\n# Answer:\nInternship"], "ground_truth": ["Mia Sara", "Hynden Walch", "Arleen Sorkin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.46153846153846156, "path_precision": 0.3333333333333333, "path_recall": 0.75, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1206", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Visual Artist -> freebase.type_hints.included_types -> Person\n# Answer:\nVisual Artist", "# Reasoning Path:\nFrancis Bacon -> book.author.works_written -> Bacon\n# Answer:\nBacon", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Visual Artist -> freebase.type_profile.equivalent_topic -> Artist\n# Answer:\nVisual Artist", "# Reasoning Path:\nFrancis Bacon -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Emphysema\n# Answer:\nMale", "# Reasoning Path:\nFrancis Bacon -> book.written_work.subjects -> 18th century\n# Answer:\n18th century"], "ground_truth": ["Philosopher", "Spy", "Author", "Statesman", "Scientist"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.5, "ans_recall": 0.2, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-1209", "prediction": ["# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Items or Less: 10 Random Looks at NCIS\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> 10 Years Aft\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> common.topic.notable_for -> g.125505_2h\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> A Death in the Family\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> Abby's Lab: Music and Mayhem\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> common.topic.notable_types -> TV Season\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 0 -> tv.tv_series_season.episodes -> Autopsy: Bodies of Work\n# Answer:\nNCIS - Season 0", "# Reasoning Path:\nNCIS -> tv.tv_program.seasons -> NCIS - Season 10 -> tv.tv_series_season.episodes -> Berlin\n# Answer:\nNCIS - Season 10"], "ground_truth": ["NCIS - Season 3"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hj3 -> soccer.football_player_stats.team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hjh -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1210", "prediction": ["# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Bermuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> China\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["O Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1212", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.major_field_of_study -> Computer programming\n# Answer:\nComputer programming", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Computer Science\n# Answer:\nComputer Science", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.major_field_of_study -> Psychology\n# Answer:\nPsychology", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nMercy College", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Computer programming", "Psychology", "Computer Science"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1213", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.artist -> Janet Jackson\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> music.producer.releases_produced -> Yours Faithfully -> common.topic.notable_types -> Musical Release\n# Answer:\nYours Faithfully", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.artist -> Whitney Houston\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.producer -> 1Club.FM\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WSJT -> broadcast.content.artist -> Janet Jackson\n# Answer:\nWSJT"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1215", "prediction": ["# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.industry.companies -> Corporate Citizenship\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> business.product_line.brand -> Resilient Solution\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> common.topic.notable_for -> g.1254yfch4\n# Answer:\nMotivational speaker", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> common.topic.subjects -> Ben Galley\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> organization.organization_sector.organizations_in_this_sector -> Corporate Citizenship\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> base.descriptive_names.names.descriptive_name -> m.0_z7q_5\n# Answer:\nMotivational speaker", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Consultant -> common.topic.subject_of -> Resilient Solution\n# Answer:\nConsultant", "# Reasoning Path:\nStephen Covey -> people.person.profession -> Motivational speaker -> people.profession.specialization_of -> Public speaker\n# Answer:\nMotivational speaker"], "ground_truth": ["Consultant", "Motivational speaker", "Author", "Writer", "Manager", "Professor"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1216", "prediction": ["# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> location.country.capital -> Moscow\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.jurisdiction_of_office -> Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> base.rivalries.rival.rivals -> Space Race\n# Answer:\nSoviet Union", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Russian Empire -> location.country.official_language -> Russian Language\n# Answer:\nRussian Empire", "# Reasoning Path:\nJoseph Stalin -> government.politician.government_positions_held -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nJoseph Stalin -> people.person.nationality -> Soviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k\n# Answer:\nSoviet Union"], "ground_truth": ["Soviet Union"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1217", "prediction": ["# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.sports_played_professionally -> m.0c54277 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nMonta Ellis -> common.topic.article -> m.06rsnr\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0w6fnfv -> sports.sports_team_roster.team -> Dallas Mavericks\n# Answer:\nDallas Mavericks", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0jng27d -> sports.sports_team_roster.team -> Golden State Warriors\n# Answer:\nGolden State Warriors", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0jng7mv -> sports.sports_team_roster.team -> Milwaukee Bucks\n# Answer:\nMilwaukee Bucks", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0w6fnfv -> sports.sports_team_roster.position -> Shooting guard\n# Answer:\nShooting guard", "# Reasoning Path:\nMonta Ellis -> award.award_winner.awards_won -> m.0_qsr_f -> award.award_honor.award -> NBA Most Improved Player Award\n# Answer:\nNBA Most Improved Player Award", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0w6fnfv -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMonta Ellis -> sports.pro_athlete.teams -> m.0jng27d -> sports.sports_team_roster.position -> Guard\n# Answer:\nGuard"], "ground_truth": ["NBA Most Improved Player Award"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1219", "prediction": ["# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> base.aareas.schema.administrative_area.administrative_children -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados -> base.popstra.location.vacationers -> m.064_m37\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions"], "ground_truth": ["Barbados"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Spain\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.image -> Die Plaza Mayor am Abend\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.capital_of_administrative_division.capital_of -> m.0jvvl78\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> United States of America\n# Answer:\nCoronado"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1220", "prediction": ["# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.0118pdct -> film.performance.film -> Bless All the Dear Children\n# Answer:\nBless All the Dear Children", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.012zhxmp -> film.performance.film -> Bukowski\n# Answer:\nBukowski", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.0220tfg -> tv.regular_tv_appearance.series -> Beverly Hills, 90210\n# Answer:\nBeverly Hills, 90210", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.03l0kfx -> film.performance.film -> Nowhere\n# Answer:\nNowhere", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.0118pdct -> film.performance.character -> Jenny Wilder\n# Answer:\nJenny Wilder", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.03l0p88 -> film.performance.film -> Christmas Caper\n# Answer:\nChristmas Caper", "# Reasoning Path:\nShannen Doherty -> film.actor.film -> m.03lmxjd -> film.performance.film -> The Secret of NIMH\n# Answer:\nThe Secret of NIMH", "# Reasoning Path:\nShannen Doherty -> tv.tv_actor.starring_roles -> m.03ltlbv -> tv.regular_tv_appearance.series -> North Shore\n# Answer:\nNorth Shore"], "ground_truth": ["Bukowski", "Night Shift", "Obsessed", "Christmas Caper", "Almost Dead", "Bless All the Dear Children", "Category 7: The End of the World", "Witchslayer Gretl", "Jailbreakers", "Striking Poses", "The Ticket", "The Lost Treasure of the Grand Canyon", "A Burning Passion: The Margaret Mitchell Story", "Another Day", "The Other Lover", "Sleeping with the Devil", "Nightlight", "Jay and Silent Bob Strike Back", "Hell on Heels: The Battle of Mary Kay", "Growing the Big One", "The Secret of NIMH", "Heathers", "Freeze Frame", "Blindfold: Acts of Obsession", "Mallrats", "The Delphi Effect", "Gone in the Night", "The Rendering", "Satan's School for Girls", "Girls Just Want to Have Fun", "Nowhere", "Friends 'Til the End", "Burning Palms"], "ans_acc": 0.15151515151515152, "ans_hit": 1, "ans_f1": 0.24390243902439024, "ans_precission": 0.625, "ans_recall": 0.15151515151515152, "path_f1": 0.2380952380952381, "path_precision": 0.625, "path_recall": 0.14705882352941177, "path_ans_f1": 0.24390243902439024, "path_ans_precision": 0.625, "path_ans_recall": 0.15151515151515152}
{"id": "WebQTest-1221", "prediction": ["# Reasoning Path:\nJoakim Noah -> people.person.education -> m.03p7z1w -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.school -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.0b6vrkl -> education.education.institution -> Poly Prep Country Day School\n# Answer:\nPoly Prep Country Day School", "# Reasoning Path:\nJoakim Noah -> people.person.education -> m.040rzsq -> education.education.institution -> Lawrenceville School\n# Answer:\nLawrenceville School", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nJoakim Noah -> sports.drafted_athlete.drafted -> m.02nqxbv -> sports.sports_league_draft_pick.draft -> 2007 NBA draft\n# Answer:\n2007 NBA draft", "# Reasoning Path:\nJoakim Noah -> sports.pro_athlete.teams -> m.02397ys -> sports.sports_team_roster.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nJoakim Noah -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.0105y97v\n# Answer:\nBasketball player"], "ground_truth": ["University of Florida"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1223", "prediction": ["# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0y81fs9 -> film.performance.actor -> Adam Cooper\n# Answer:\nAdam Cooper", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0jvcy7 -> film.performance.actor -> Jamie Bell\n# Answer:\nJamie Bell", "# Reasoning Path:\nBilly Elliot -> film.film_character.portrayed_in_films -> m.0y81fs9 -> film.performance.actor -> Adam Cooper\n# Answer:\nAdam Cooper", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.actor -> Gary Lewis\n# Answer:\nGary Lewis", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0by4690 -> film.performance.actor -> Joe Renton\n# Answer:\nJoe Renton", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.0by469l -> film.performance.actor -> Billy Fane\n# Answer:\nBilly Fane", "# Reasoning Path:\nBilly Elliot -> award.award_nominated_work.award_nominations -> m.0n4xrzy -> award.award_nomination.award_nominee -> Jamie Bell\n# Answer:\nJamie Bell", "# Reasoning Path:\nBilly Elliot -> film.film.starring -> m.02tbgcw -> film.performance.character -> Jackie Elliot\n# Answer:\nJackie Elliot"], "ground_truth": ["Jamie Bell", "Adam Cooper"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.375, "path_recall": 0.23076923076923078, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1225", "prediction": ["# Reasoning Path:\nKourtney Kardashian -> base.popstra.celebrity.dated -> m.065px1n -> base.popstra.dated.participant -> Scott Disick\n# Answer:\nScott Disick", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.award_winner -> Kim Kardashian\n# Answer:\nKim Kardashian", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.honored_for -> Keeping Up with the Kardashians\n# Answer:\nKeeping Up with the Kardashians", "# Reasoning Path:\nKourtney Kardashian -> common.topic.image -> Kourtney Kardashian 2 2009 -> common.image.size -> m.0632364\n# Answer:\nKourtney Kardashian 2 2009", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.award_winner -> Khlo\u00e9 Kardashian\n# Answer:\nKhlo\u00e9 Kardashian", "# Reasoning Path:\nKourtney Kardashian -> award.award_winner.awards_won -> m.0y76gjw -> award.award_honor.award -> Teen Choice Award for Choice TV Female Reality Star\n# Answer:\nTeen Choice Award for Choice TV Female Reality Star", "# Reasoning Path:\nKourtney Kardashian -> common.topic.webpage -> m.07720_d -> common.webpage.resource -> m.0bkd3qm\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Scott Disick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1226", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.diesel_price_liter -> g.12cp_h_97\n# Answer:\nlocation.statistical_region.diesel_price_liter", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71l32x_\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Cocos (Keeling) Islands\n# Answer:\nFederal monarchy"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1227", "prediction": ["# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> sports.sports_team_roster.team -> Villanova Wildcats football\n# Answer:\nVillanova Wildcats football", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf18 -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nDefensive end", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nHowie Long -> sports.pro_athlete.teams -> m.0hqqf0t -> sports.sports_team_roster.position -> Defensive lineman\n# Answer:\nDefensive lineman", "# Reasoning Path:\nHowie Long -> common.topic.image -> Howie Long - American Football Player TV host -> common.image.size -> m.02d_35f\n# Answer:\nHowie Long - American Football Player TV host", "# Reasoning Path:\nHowie Long -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nHowie Long -> award.award_winner.awards_won -> m.0_bnzjk -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear"], "ground_truth": ["Oakland Raiders", "Villanova Wildcats football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1228", "prediction": ["# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.parent_writing_systems -> Proto-Canaanite alphabet\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.parent_writing_systems -> Latin alphabet\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Phoenician\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> base.schemastaging.context_name.pronunciation -> g.125_pyj6_\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> base.schemastaging.context_name.pronunciation -> m.01314_ny\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> common.topic.notable_for -> g.1257wg5f8\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> language.language_writing_system.languages -> Gothic Language\n# Answer:\nGothic alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.parent_writing_systems -> Phoenician alphabet -> language.language_writing_system.languages -> Araona Language\n# Answer:\nPhoenician alphabet", "# Reasoning Path:\nGreek alphabet -> language.language_writing_system.child_writing_systems -> Gothic alphabet -> base.schemastaging.context_name.pronunciation -> m.01314m6_\n# Answer:\nGothic alphabet"], "ground_truth": ["Phoenician alphabet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1229", "prediction": ["# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.hud_county_place.county -> Hardin County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nHardin County", "# Reasoning Path:\nElizabethtown -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07c9g7m\n# Answer:\nlocation.hud_foreclosure_area.bls_unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rxy8r\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60rwz93\n# Answer:\nlocation.statistical_region.unemployment_rate", "# Reasoning Path:\nElizabethtown -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nElizabethtown -> location.statistical_region.unemployment_rate -> g.11b60thpzf\n# Answer:\nlocation.statistical_region.unemployment_rate"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-123", "prediction": ["# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> law.inventor.inventions -> Google Search\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> common.topic.subjects -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Acxiom Corporation -> business.customer.supplier -> IBM\n# Answer:\nAcxiom Corporation", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Google for Work\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> common.topic.subjects -> Internet Marketing\n# Answer:\nCleverly Engaged Marketing"], "ground_truth": ["Google Maps", "Nexus S", "Google Glass", "Apache Wave", "Google Docs, Sheets, and Slides", "Google Buzz", "Google Drive", "Google Earth", "Nexus Q", "Google Classroom", "Nexus 7", "Chromecast", "Nexus 10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1230", "prediction": ["# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.notable_types -> Museum\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> location.location.contains -> ChristChurch Cathedral, Christchurch -> location.location.containedby -> New Zealand\n# Answer:\nChristChurch Cathedral, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> architecture.structure.architectural_style -> Gothic Revival architecture\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> common.topic.image -> Canterbury Museum1\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Christchurch Botanic Gardens -> common.topic.notable_for -> g.1256xmszx\n# Answer:\nChristchurch Botanic Gardens", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Canterbury Museum, Christchurch -> architecture.museum.type_of_museum -> Natural history museum\n# Answer:\nCanterbury Museum, Christchurch", "# Reasoning Path:\nChristchurch -> location.statistical_region.population -> g.11bcdlk0kf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Christchurch Botanic Gardens -> common.topic.notable_types -> Tourist attraction\n# Answer:\nChristchurch Botanic Gardens", "# Reasoning Path:\nChristchurch -> travel.travel_destination.tourist_attractions -> Christchurch Botanic Gardens -> common.topic.image -> Water fountain at Christchurch Botanical Gardens\n# Answer:\nChristchurch Botanic Gardens"], "ground_truth": ["Air Force Museum of New Zealand", "Ferrymead Heritage Park", "International Antarctic Centre", "Christchurch Arts Centre", "Lyttelton Timeball Station", "TranzAlpine", "Cathedral Square, Christchurch", "Christchurch Botanic Gardens", "Willowbank Wildlife Reserve, Christchurch", "Canterbury Museum, Christchurch"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3181818181818182, "ans_precission": 0.7777777777777778, "ans_recall": 0.2, "path_f1": 0.163265306122449, "path_precision": 0.4444444444444444, "path_recall": 0.1, "path_ans_f1": 0.3181818181818182, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.2}
{"id": "WebQTest-1231", "prediction": ["# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> base.biblioness.bibs_location.state -> North Carolina\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> common.topic.article -> m.0cymlx\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27199\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> sports.pro_athlete.sports_played_professionally -> m.0c54blq -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.location.containedby -> North Carolina\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.citytown.postal_codes -> 27101-2755\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.location.events -> 1993 NCAA Men's Division I Basketball Tournament\n# Answer:\nWinston-Salem", "# Reasoning Path:\nChris Paul -> people.person.place_of_birth -> Winston-Salem -> location.location.containedby -> Area code 336\n# Answer:\nWinston-Salem"], "ground_truth": ["United States of America"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1232", "prediction": ["# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.season -> 2001 Major League Baseball Season\n# Answer:\n2001 Major League Baseball Season", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04ynyvj -> sports.sports_award.award -> National League Rookie of the Year\n# Answer:\nNational League Rookie of the Year", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.draft -> 1999 Major League Baseball Draft\n# Answer:\n1999 Major League Baseball Draft", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnb -> sports.sports_award.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nAlbert Pujols -> sports.sports_award_winner.awards -> m.04xwlnk -> sports.sports_award.season -> 2005 Major League Baseball season\n# Answer:\n2005 Major League Baseball season", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.school -> Maple Woods Community College\n# Answer:\nMaple Woods Community College", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["2001 Major League Baseball Season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1233", "prediction": ["# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Ambiente'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.schemastaging.visual_color_extra.srgb -> m.010q1h63\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa laevigata\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Annapurna'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.petbreeds.dog_coat_color.dog_breeds -> Border Collie\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Cabaret'\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.ontologies.ontology_instance.equivalent_instances -> m.09klnhg\n# Answer:\nWhite", "# Reasoning Path:\nNew York Giants -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa 'Cherry Parfait'\n# Answer:\nWhite"], "ground_truth": ["Red", "Grey", "White", "Blue"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-1234", "prediction": ["# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2k8\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> common.topic.article -> m.025sq3\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> book.book_subject.works -> Masters of Enterprise: Giants of American Business from John Jacob Astor and J.P. Morgan to Bill Gates and Oprah Winfrey\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.0106b2kr\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> common.topic.image -> Tycoonclipper\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> common.topic.notable_types -> Profession\n# Answer:\nBusiness magnate", "# Reasoning Path:\nAndrew Carnegie -> people.person.profession -> Business magnate -> base.descriptive_names.names.descriptive_name -> m.010fn7xm\n# Answer:\nBusiness magnate"], "ground_truth": ["financier", "steel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1235", "prediction": ["# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Eyepiece\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_principle_type.telescopic_classifications_of_this_technique -> Optical telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> base.onephylogeny.type_of_thing.includes -> Objective\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> astronomy.telescope_type.higher_classification -> Optical telescope\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.notable_types -> Invention\n# Answer:\nPet door", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Reflecting telescope -> common.topic.notable_types -> Telescope type\n# Answer:\nReflecting telescope", "# Reasoning Path:\nIsaac Newton -> law.inventor.inventions -> Pet door -> common.topic.article -> m.079vqs\n# Answer:\nPet door"], "ground_truth": ["Pet door", "Reflecting telescope"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1237", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nNBA All-Star Game Most Valuable Player Award", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\n1992 NBA draft", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season"], "ground_truth": ["Orlando Magic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1238", "prediction": ["# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.010qccn6\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.09dznhz\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> fictional_universe.fictional_universe.characters -> Morty Fieldmouse\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> common.topic.notable_for -> g.1255cgp_j\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.0_gjwtk\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> film.director.film -> The Band Concert\n# Answer:\nWalt Disney", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Mouseton -> common.topic.notable_types -> Fictional Universe\n# Answer:\nMouseton", "# Reasoning Path:\nMickey Mouse -> fictional_universe.fictional_character.character_created_by -> Walt Disney -> award.award_winner.awards_won -> m.0gkf0xv\n# Answer:\nWalt Disney"], "ground_truth": ["Ub Iwerks", "Walt Disney"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5555555555555556, "ans_precission": 0.625, "ans_recall": 0.5, "path_f1": 0.5555555555555556, "path_precision": 0.625, "path_recall": 0.5, "path_ans_f1": 0.5555555555555556, "path_ans_precision": 0.625, "path_ans_recall": 0.5}
{"id": "WebQTest-1239", "prediction": ["# Reasoning Path:\nConcord -> common.topic.article -> m.0qyn0\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geolocation -> m.03dnk1k\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.contains -> The Willows Shopping Center\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> location.location.geometry -> m.058s25t\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94529 -> common.topic.notable_types -> Postal Code\n# Answer:\n94529", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> common.topic.notable_types -> Postal Code\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94518 -> location.location.geometry -> m.056d_lq\n# Answer:\n94518", "# Reasoning Path:\nConcord -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07hcczg\n# Answer:\nlocation.hud_foreclosure_area.bls_unemployment_rate", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94520 -> common.topic.notable_for -> g.12552m6w3\n# Answer:\n94520", "# Reasoning Path:\nConcord -> location.citytown.postal_codes -> 94527 -> location.postal_code.country -> United States of America\n# Answer:\n94527"], "ground_truth": ["94519", "94522", "94524", "94529", "94527", "94520", "94518", "94521"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-124", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1240", "prediction": ["# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07vzkbw -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07yc9d6 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07tdd11 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07sh25z -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07tdcdk -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles"], "ground_truth": ["New York Jets"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1241", "prediction": ["# Reasoning Path:\nColumbia River -> geography.river.origin_long_lat -> m.04d0j_1\n# Answer:\ngeography.river.origin_long_lat", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> geography.lake.basin_countries -> Canada\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> common.topic.notable_types -> Lake\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> British Columbia\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.geolocation -> m.02_gdvj\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> travel.travel_destination.tourist_attractions -> Columbia River Gorge -> location.location.containedby -> Skamania County\n# Answer:\nColumbia River Gorge", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> time.event.locations -> North America\n# Answer:\nRobert Gray's Columbia River expedition", "# Reasoning Path:\nColumbia River -> geography.river.origin -> Columbia Lake -> location.location.containedby -> Canada\n# Answer:\nColumbia Lake", "# Reasoning Path:\nColumbia River -> location.location.events -> Robert Gray's Columbia River expedition -> common.topic.image -> Capt Robert Gray\n# Answer:\nRobert Gray's Columbia River expedition"], "ground_truth": ["Columbia Lake"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1242", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.written_work.previous_in_series -> Harry Potter and the Half-Blood Prince\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Book 7) (Deluxe Edition)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.previous_in_series -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Harry Potter)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book_edition.author_editor -> J. K. Rowling\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> book.book_character.appears_in_book -> Harry Potter and the Chamber of Secrets -> book.written_work.previous_in_series -> Harry Potter and the Philosopher's Stone\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.locations -> Azkaban\n# Answer:\nAzkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> book.book.editions -> Harry Potter and the Deathly Hallows (Book 7)\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Fantastic Beasts and Where to Find Them\n# Answer:\nFantastic Beasts and Where to Find Them", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> England\n# Answer:\nHarry Potter and the Chamber of Secrets"], "ground_truth": ["Harry Potter and the Half-Blood Prince", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Chamber of Secrets", "Harry Potter and the Deathly Hallows", "Harry Potter and the Goblet of Fire", "Harry Potter and the Order of the Phoenix"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.8, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1244", "prediction": ["# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Andrew Newport\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> base.descriptive_names.names.descriptive_name -> m.010b1tg9\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> common.topic.notable_for -> g.1258nrk6f\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Anne Lennard, Countess of Sussex\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> common.topic.notable_types -> Deceased Person -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nDeceased Person", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Soldier -> book.book_subject.works -> Holiday Letter for a Poet Gone to War\n# Answer:\nSoldier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> common.topic.notable_types -> Profession\n# Answer:\nCourtier", "# Reasoning Path:\nWalter Raleigh -> people.person.profession -> Courtier -> people.profession.people_with_this_profession -> Anthony Lee\n# Answer:\nCourtier"], "ground_truth": ["Soldier", "Courtier", "Writer", "Sailor", "Poet"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5490196078431373, "ans_precission": 0.875, "ans_recall": 0.4, "path_f1": 0.5490196078431373, "path_precision": 0.875, "path_recall": 0.4, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-1245", "prediction": ["# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.administrative_division.country -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.containedby -> Eurasia\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Vatican City -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nVatican City", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Netherlands -> location.country.currency_used -> United States Dollar\n# Answer:\nNetherlands", "# Reasoning Path:\nEuro -> common.topic.notable_for -> g.12570307k\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> location.location.primarily_containedby -> Finland\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Belgium -> location.location.containedby -> Western Europe\n# Answer:\nBelgium", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> Netherlands -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of the Netherlands\n# Answer:\nNetherlands", "# Reasoning Path:\nEuro -> finance.currency.countries_used -> \u00c5land Islands -> base.aareas.schema.administrative_area.administrative_parent -> Finland\n# Answer:\n\u00c5land Islands"], "ground_truth": ["Saint Pierre and Miquelon", "Slovakia", "Lithuania", "Mayotte", "Luxembourg", "Latvia", "Republic of Ireland", "France", "Vatican City", "Guadeloupe", "Republic of Kosovo", "Andorra", "San Marino", "\u00c5land Islands", "Malta", "Cyprus", "Saint Barth\u00e9lemy", "Monaco", "Italy", "Netherlands", "Portugal", "Province of Varese", "Caribbean special municipalities of the Netherlands", "Spain", "Collectivity of Saint Martin", "Kingdom of the Netherlands", "Montenegro", "Zimbabwe", "Slovenia", "Estonia", "Austria", "Finland", "Belgium", "Germany", "Greece", "Martinique", "Varese"], "ans_acc": 0.1891891891891892, "ans_hit": 1, "ans_f1": 0.1927710843373494, "ans_precission": 0.8888888888888888, "ans_recall": 0.10810810810810811, "path_f1": 0.1927710843373494, "path_precision": 0.8888888888888888, "path_recall": 0.10810810810810811, "path_ans_f1": 0.31197771587743733, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.1891891891891892}
{"id": "WebQTest-1246", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.webpage -> m.04m2ytq\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.image -> Current coins in use.\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37ppn\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37x3k\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1247", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_profile.published -> Published\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Mervin Praison\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nAngelina Jolie -> book.author.book_editions_published -> Notes from My Travels -> common.topic.notable_types -> Book\n# Answer:\nNotes from My Travels", "# Reasoning Path:\nAngelina Jolie -> people.person.profession -> Film director -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nFilm director"], "ground_truth": ["Film Producer", "Actor", "Screenwriter", "Voice Actor", "Author", "Writer", "Model", "Film director"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.525, "ans_precission": 0.875, "ans_recall": 0.375, "path_f1": 0.525, "path_precision": 0.875, "path_recall": 0.375, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1248", "prediction": ["# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.containedby -> Iraq\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.administrative_division.first_level_division_of -> Iraq\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.geolocation -> m.02_hynm\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.administrative_division.capital -> Mosul\n# Answer:\nNineveh Governorate", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Eurasia\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> common.topic.article -> m.05fyy\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.location.containedby -> Arab world\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Nineveh Governorate\n# Answer:\nIraq", "# Reasoning Path:\nNineveh -> common.topic.image -> A proposed flag for the Chaldean people. -> common.image.appears_in_topic_gallery -> Nimrud\n# Answer:\nA proposed flag for the Chaldean people.", "# Reasoning Path:\nNineveh -> location.location.containedby -> Nineveh Governorate -> location.location.contains -> University of Mosul\n# Answer:\nNineveh Governorate"], "ground_truth": ["Nineveh Governorate", "Iraq"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1249", "prediction": ["# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Yucca flower\n# Answer:\nYucca flower", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.symbol -> New Mexico spadefoot toad\n# Answer:\nNew Mexico spadefoot toad", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st870 -> location.location_symbol_relationship.symbol -> Greater roadrunner\n# Answer:\nGreater roadrunner", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st94k -> location.location_symbol_relationship.symbol -> Sandia hairstreak\n# Answer:\nSandia hairstreak", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04stkbz -> location.location_symbol_relationship.symbol -> Cutthroat trout\n# Answer:\nCutthroat trout", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nNew Mexico -> book.book_subject.works -> ...And Now Miguel -> book.written_work.subjects -> United States of America\n# Answer:\n...And Now Miguel", "# Reasoning Path:\nNew Mexico -> government.governmental_jurisdiction.official_symbols -> m.04st6n2 -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian"], "ground_truth": ["Yucca flower"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-125", "prediction": ["# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> people.person.parents -> Hagar\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Jacob\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> people.person.sibling_s -> m.0k6lvj6\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.children -> Esau\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.notable_types -> Deceased Person\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Isaac -> people.person.sibling_s -> m.0k6lvj6\n# Answer:\nIsaac", "# Reasoning Path:\nAbraham -> people.person.children -> Ishmael -> common.topic.webpage -> m.04m0j0k\n# Answer:\nIshmael", "# Reasoning Path:\nAbraham -> people.person.children -> Jokshan -> people.person.sibling_s -> m.048b8rb\n# Answer:\nJokshan"], "ground_truth": ["Jokshan", "Midian", "Medan", "Isaac", "Shuah", "Zimran", "Ishbak", "Ishmael"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 1.0, "ans_recall": 0.375, "path_f1": 0.5454545454545454, "path_precision": 1.0, "path_recall": 0.375, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 1.0, "path_ans_recall": 0.375}
{"id": "WebQTest-1250", "prediction": ["# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registering_agency -> Internal Revenue Service\n# Answer:\nInternal Revenue Service", "# Reasoning Path:\nNational FFA Organization -> organization.organization.headquarters -> m.0wkkztl -> location.mailing_address.citytown -> Alexandria\n# Answer:\nAlexandria", "# Reasoning Path:\nNational FFA Organization -> organization.non_profit_organization.registered_with -> m.0pqg9g3 -> organization.non_profit_registration.registered_as -> 501(c)(3)\n# Answer:\n501(c)(3)", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> AMERICAN CRYSTAL SUGAR CO /MN/\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> freebase.valuenotation.has_value -> NAICS 2007 code\n# Answer:\nAgriculture", "# Reasoning Path:\nNational FFA Organization -> organization.organization.board_members -> m.04hzczj -> organization.organization_board_membership.member -> Douglas C. DeVries\n# Answer:\nDouglas C. DeVries", "# Reasoning Path:\nNational FFA Organization -> organization.organization.sectors -> Agriculture -> business.industry.companies -> AVEBE\n# Answer:\nAgriculture"], "ground_truth": ["Alexandria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1251", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> location.country.languages_spoken -> French\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.location.containedby -> Western Europe\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria"], "ground_truth": ["Austria", "Switzerland", "Belgium", "Luxembourg", "Liechtenstein", "Germany", "East Germany"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.6486486486486486, "path_precision": 0.75, "path_recall": 0.5714285714285714, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1252", "prediction": ["# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.ceremony -> 9th Critics' Choice Awards\n# Answer:\n9th Critics' Choice Awards", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.0b4d5rz -> award.award_nomination.award_nominee -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award -> Critics' Choice Movie Award for Best Acting Ensemble\n# Answer:\nCritics' Choice Movie Award for Best Acting Ensemble", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.0n7xsws -> award.award_honor.award_winner -> Miranda Otto\n# Answer:\nMiranda Otto", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_winning_work.awards_won -> m.09k3pgy -> award.award_honor.award_winner -> Andy Serkis\n# Answer:\nAndy Serkis", "# Reasoning Path:\nThe Lord of the Rings: The Return of the King -> award.award_nominated_work.award_nominations -> m.09k3q0p -> award.award_nomination.award_nominee -> Andy Serkis\n# Answer:\nAndy Serkis"], "ground_truth": ["Miranda Otto"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.5, "path_recall": 0.8, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1254", "prediction": ["# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2013 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> common.topic.notable_for -> g.12556r5p0\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> common.topic.notable_for -> g.1257dt3mw\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2007 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> award.ranked_item.appears_in_ranked_lists -> m.04tk4kl\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> sports.golf_course.designer -> Alister MacKenzie\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> sports.golf_course.facility -> Augusta National Golf Club\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2008 Masters Tournament\n# Answer:\nAugusta National Golf Course", "# Reasoning Path:\n2012 Masters Tournament -> time.event.locations -> Augusta National Golf Course -> location.location.events -> 2009 Masters Tournament\n# Answer:\nAugusta National Golf Course"], "ground_truth": ["Augusta National Golf Course"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1255", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yvette Wilson\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09wbnt0\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer", "Cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1256", "prediction": ["# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09gr3sg -> american_football.player_receiving_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> sports.pro_athlete.teams -> m.05h9hzm -> sports.sports_team_roster.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.09tc6zn -> american_football.player_rushing_statistics.team -> Buffalo Bills\n# Answer:\nBuffalo Bills", "# Reasoning Path:\nTerrell Owens -> sports.pro_athlete.teams -> m.0h_j42w -> sports.sports_team_roster.position -> Wide receiver\n# Answer:\nWide receiver", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.rushing -> m.07t77sv -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTerrell Owens -> american_football.football_player.receiving -> m.09rltd7 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Cincinnati Bengals", "Buffalo Bills", "Texas Revolution"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.13114754098360656, "path_precision": 0.5, "path_recall": 0.07547169811320754, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1257", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> United States, with Territories\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.location.containedby -> United States of America\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.location.containedby -> Bamberg County -> location.location.containedby -> South Carolina\n# Answer:\nBamberg County", "# Reasoning Path:\nDenmark -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> South Carolina\n# Answer:\nUnited States of America", "# Reasoning Path:\nDenmark -> location.location.containedby -> South Carolina -> location.us_state.capital -> Columbia\n# Answer:\nSouth Carolina", "# Reasoning Path:\nDenmark -> location.hud_county_place.county -> Bamberg County -> location.location.containedby -> South Carolina\n# Answer:\nBamberg County", "# Reasoning Path:\nDenmark -> location.statistical_region.population -> g.11b66f43lx\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenmark -> location.statistical_region.population -> g.11bc87sm2q\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["United States of America", "South Carolina", "Bamberg County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1259", "prediction": ["# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> influence.influence_node.influenced_by -> Richard Yates\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> film.film_story_contributor.film_story_credits -> A Farewell to Arms -> book.written_work.subjects -> World War I\n# Answer:\nA Farewell to Arms", "# Reasoning Path:\nErnest Hemingway -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nErnest Hemingway -> film.film_story_contributor.film_story_credits -> Baby Shoes -> film.film.written_by -> Michael Howard\n# Answer:\nBaby Shoes", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Andre Dubus -> people.person.gender -> Male\n# Answer:\nAndre Dubus", "# Reasoning Path:\nErnest Hemingway -> influence.influence_node.influenced -> Ann Beattie -> influence.influence_node.influenced_by -> Richard Yates\n# Answer:\nAnn Beattie", "# Reasoning Path:\nErnest Hemingway -> common.topic.webpage -> m.09wc371 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nErnest Hemingway -> film.film_story_contributor.film_story_credits -> A Farewell to Arms -> media_common.netflix_title.netflix_genres -> Classics\n# Answer:\nA Farewell to Arms"], "ground_truth": ["On Writing", "Wada'an Lilseelah", "Winner Take Nothing (Scribner Classic)", "The Old Man And The Sea", "Neues vom Festland. Stories", "By-Line", "The Fifth Column and Four Stories of the Spanish Civil War", "Hemingway", "Across the River and into the Trees", "In Harry's Bar in Venice and More", "The Hemingway reader", "Notes on dangerous game", "Ernest Hemingway Selected Letters 1917\u20131961", "Vier Stories aus dem spanischen B\u00fcrgerkrieg", "Dear Papa, dear Hotch", "The only thing that counts", "Klokkene ringer for deg", "The Complete Short Stories of Ernest Hemingway: The Finca Vig\u00eda Edition", "Shootism versus sport", "Die sch\u00f6nsten Geschichten Afrikas", "In Another Country", "The Butterfly and the Tank...fiction", "The Snows of Kilimanjaro", "By-Line Ernest Hemingway", "Green Hills of Africa", "Sun Also Rises", "The wild years", "The snows of Kilimanjaro", "On being shot again", "The malady of power", "The Old Man and the Sea (Old Man & the Sea Illus Gift Ed C)", "De vye zanmi", "The old man and the sea. (Lernmaterialien)", "88 Poems", "Across the River and into the Trees (Arrow Classic)", "La quinta columna", "Indian Camp", "L'Etrange contr\u00e9e", "Fifth Column and Four Stories of the Spanish Civil War (Fifth Column & 4 Stories Hre)", "Three stories", "For Whom the Bell Tolls (Vintage Classics)", "Sobranie sochineni\u012d v chetyrekh tomakh", "The dangerous summer", "Defense of dirty words", "Die Stories", "Across the river and into the trees.", "Now I Lay Me", "a.d. in Africa", "A Day's Wait", "The Undefeated", "Proshchai\u0306 oruzhie!", "Old Man and the Sea/ (Cassette)", "The Spanish War", "The sun also rises", "The old man and the sea.", "The Battler", "Give us a prescription, Doctor", "The old man and the sea =", "The garden of Eden.", "Million dollar fright", "Green Hills of Africa (Vintage Classics)", "Cat in the Rain", "The circus", "The soul of Spain with Mc. Almon and Bird the publishers", "On the blue water", "Obras Completas:Por qui\u00e9n doblan las campanas, El viejo y el mar", "Old Man and the Sea", "Green hills of Africa.", "Dateline: Toronto", "Winner take nothing.", "A Farewell To Arms", "In Our Time (In Our Time Hre)", "g.1ym_l5zt2", "The garden of Eden", "The Old Man and the Sea", "Ernest Hemingway's Apprenticeship", "At have og ikke have", "The Old Man and the Sea (York Notes)", "Major Works of Ernest Hemingway", "Voor wien de klok luidt", "True at first light", "To Have and Have Not", "El Toro Fiel / the Faithful Bull", "The great blue river", "The sun also rises.", "The Capital of the World", "For Whom the Bell Tolls (War Promo)", "The Old Man and the Sea (A Scribner Classic)", "The Killers", "El Buen Leon", "Night before battle", "Der alte Mann und das Meer, und andere Meisterwerke", "Men at war", "Fable", "A moveable feast", "Soldier's Home", "Genio after Josie", "Valentine", "Ernest Hemingway: The Collected Stories", "Wings always over Africa", "Marlin off the Morro", "Men Without Women", "Bullfighting, sport & industry", "Winner Take Nothing", "Three novels: The sun also rises; A farewell to arms; The old man and the sea", "Die Wahrheit im Morgenlicht. Eine afrikanische Safari.", "The Short Happy Life of Francis Macomber", "A Farewell to Arms (Farewell to Arms Tr)", "A Moveable Feast (Moveable Feast Srs)", "After the Storm", "In our time", "The Torrents of Spring", "Las Nieves del Kilimanjaro", "A Clean, Well-Lighted Place", "The Sun Also Rises (Archive of Literary Documents)", "To have and have not.", "Ernest Hemingway, selected letters, 1917-1961", "For Whom the Bell Tolls (Audio Library Classics)", "True at First Light", "The TORRENTS OF SPRING", "The tradesman's return", "E.H, apprenti reporter", "A Paris letter", "To Have and Have Not (To Have & Have Not Hre)", "50000 dollars", "The Sun Also Rises (A Scribner classic)", "The president vanquishes", "Death in the afternoon", "Bi xatire\u0302 s\u0131\u0302lehan", "Che Ti Dice La Patria?", "a.d. southern style", "Up in Michigan", "Lao ren yu hai", "A Farewell to Arms", "Los Asesinos", "quarantanove racconti", "For whom the bell tolls", "Old Man and the Sea (Special Student)", "The Garden of Eden", "Farewell to Arms (A Scribner Classic)", "For Whom the Bell Tolls", "Sailfish off Mombasa", "The torrents of spring", "Reportagen 1920 - 1924", "The Nick Adams Stories", "Le chaud et le froid", "Green Hills of Africa (Hudson River editions)", "To Have and Have Not (To Have & Have Not Srs)", "The Revolutionist", "Green hills of Africa", "Ernest Hemingway in high school", "Marlin off Cuba", "A Farewell to Arms (Scribner Classics)", "Hemingway on Fishing", "The colected poems of Ernest Hemingway", "Nieves del Kilimanjaro, Las", "Death in the afternoon.", "Zhan di zhong sheng", "There she breaches!", "The Old Man and The Sea (Annual Review of the Institute for Information Studies)", "Ernest Hemingway: Cub Reporter", "The Old Man and the Sea (MacMillan Literature Series, Signature Edition)", "A folyo n at, a fa k koze", "Three Stories and Ten Poems", "Across The River And Into The Trees", "A Moveable Feast (Scribner Classic)", "The friend of Spain", "Men Without Women (Arrow Classic)", "Men without women", "The enduring Hemingway", "A Farewell to Arms (Vintage Classics)", "The Dangerous Summer", "The old man and the sea", "The essential Hemingway", "Oeuvres Romanesques Vol. 1", "Bullfighting", "Big Two-Hearted River", "The Old Man and the Sea (Vintage Classics)", "A Moveable Feast", "The Sun Also Rises", "Green Hills of Africa (Scribner Classic)", "The Spanish earth", "Hemingway on war", "Der al\u1e6der un der yam", "Best Of Bad Hemingway: Vol 1", "Complete poems", "Across the River and Into the Trees", "To have and have not", "The faithful bull", "Winner take nothing", "Conversations with Ernest Hemingway", "Death in the Afternoon (Death in the Afternoon Hre)", "Collected poems", "Hemingway on hunting", "Hayat Francis Macumber", "True At First Light", "The sights of Whitehead Street", "In Our Time", "The Old Man and the Sea. (Lernmaterialien)", "Under Kilimanjaro", "The snows of Kilimanjaro and other stories", "Hemingway on Hunting (On)", "Sun Also Rises (Library Reprint Editions)", "Hemingway and the mechanism of fame", "The short stories of Ernest Hemingway", "Islands in the Stream", "Death in the Afternoon", "A divine gesture", "By-line, Ernest Hemingway", "The fifth column and four stories of the Spanish Civil War", "The Gambler, the Nun, and the Radio", "Sun Also Rises (Sun Also Rises Tr)", "En ligne", "Relatos Ineditos", "The Fifth Column and the First Forty-Nine Stories", "Old Man and the Sea (New Windmill)", "Storie della guerra de Spagna", "Hills Like White Elephants", "The collected poems of Ernest Hemingway", "Across the river and into the trees", "The good lion", "The End of Something", "He who gets slap happy", "A farewell to arms", "The Letters of Ernest Hemingway: Volume 1, 1907-1922", "Complete Poems", "A Simple Enquiry", "Un corresponsal llamado Hemingway", "The Old Man and the Sea (Old Man & the Sea Tr)", "ACROSS THE RIVER AND INTO THE TREES", "The fifth column, and four stories of the Spanish Civil War.", "The denunciation", "On Paris", "Monologue to the maestro", "g.11b6s8f7gb", "The Cambridge Edition of the Letters of Ernest Hemingway", "The Christmas gift"], "ans_acc": 0.016877637130801686, "ans_hit": 1, "ans_f1": 0.024096385542168672, "ans_precission": 0.25, "ans_recall": 0.012658227848101266, "path_f1": 0.12195121951219512, "path_precision": 0.25, "path_recall": 0.08064516129032258, "path_ans_f1": 0.03319502074688797, "path_ans_precision": 1.0, "path_ans_recall": 0.016877637130801686}
{"id": "WebQTest-126", "prediction": ["# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Visual Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.profession -> Composer\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.lyricist -> Rabindranath Tagore -> people.person.profession -> Visual Artist\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.recordings -> Jana Gana Mana (Indian National Song) -> music.recording.artist -> Leoncie\n# Answer:\nJana Gana Mana (Indian National Song)", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> film.film_story_contributor.film_story_credits -> Uphaar\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Ram Singh Thakur -> common.topic.article -> m.02vx725\n# Answer:\nRam Singh Thakur", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> music.composer.compositions -> Sri Lanka Matha\n# Answer:\nRabindranath Tagore", "# Reasoning Path:\nJana Gana Mana -> music.composition.composer -> Rabindranath Tagore -> people.person.quotations -> Clouds come floating into my life, no longer to carry rain or usher storm, but to add color to my sunset sky.\n# Answer:\nRabindranath Tagore"], "ground_truth": ["Rabindranath Tagore", "Ram Singh Thakur"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1260", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XXXV"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1261", "prediction": ["# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.containedby -> Belgium\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2013 Flanders International Film Festival Ghent\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Passendale -> location.location.geolocation -> m.02_nfnm\n# Answer:\nPassendale", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> First Battle of Ypres\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.people_born_here -> Susannah Hornebolt\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> military.military_post.people_posted_here -> m.011nsjvz\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2009 Tour of Flanders\n# Answer:\nFlanders", "# Reasoning Path:\nBattle of Passchendaele -> time.event.locations -> Flanders -> location.location.events -> 2014 E3 Harelbeke\n# Answer:\nFlanders"], "ground_truth": ["Passendale", "Flanders"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1262", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Midwestern United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nOhio -> location.location.containedby -> Eastern United States -> location.location.containedby -> United States of America\n# Answer:\nEastern United States"], "ground_truth": ["Midwestern United States", "East North Central States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1263", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.matches_played -> m.0g9m454 -> soccer.football_player_match_participation.team -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.match -> UEFA Euro 2012 Quarter-final I\n# Answer:\nUEFA Euro 2012 Quarter-final I", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> common.topic.article -> m.02xt70\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De"], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.3448275862068966, "path_precision": 0.5555555555555556, "path_recall": 0.25, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1264", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00", "Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1265", "prediction": ["# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.schemastaging.context_name.pronunciation -> g.125_lw10d\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Tyler -> people.person.religion -> Episcopal Church -> religion.religious_organization.leaders -> m.0w714g8\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nJohn Tyler -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1266", "prediction": ["# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> common.topic.notable_types -> Profession\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> sports.pro_athlete.teams -> m.04xhkd1 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nPitcher", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7hfc\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> people.profession.specializations -> Infielder\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> book.book_subject.works -> Highpockets\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> people.person.profession -> Baseball player -> base.ontologies.ontology_instance.equivalent_instances -> m.0gy7jbg\n# Answer:\nBaseball player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Baseball Almanac ID\n# Answer:\nBaseball Player", "# Reasoning Path:\nSergio Romo -> common.topic.notable_types -> Baseball Player -> common.topic.article -> m.0d9v52v\n# Answer:\nBaseball Player"], "ground_truth": ["Baseball player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1267", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2gx3\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nJamaica -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6hby7\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Yucat\u00e1n Peninsula\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1988 Atlantic hurricane season\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1268", "prediction": ["# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Eurasia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Asia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.12tb6flp5\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> Indian subcontinent\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.gni_in_ppp_dollars -> g.11b60vv5wn\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.location.containedby -> South Asia\n# Answer:\nMaldives", "# Reasoning Path:\nJay Moriarty -> people.deceased_person.place_of_death -> Maldives -> location.statistical_region.agriculture_as_percent_of_gdp -> g.1hhc37phc\n# Answer:\nMaldives"], "ground_truth": ["Maldives"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1269", "prediction": ["# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.containedby -> Massachusetts\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.notable_for -> g.12574v___\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> location.location.events -> Capture of USS Chesapeake\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> common.topic.article -> m.0ccd9j\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> common.topic.image -> Topographic map of Boston Harbor\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> base.schemastaging.context_name.pronunciation -> g.125_l36xc\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> travel.travel_destination.tourist_attractions -> Boston Harborwalk\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Bumpkin Island\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> The Graves\n# Answer:\nBoston Harbor", "# Reasoning Path:\nBoston Tea Party -> time.event.locations -> Boston Harbor -> geography.body_of_water.islands -> Button Island\n# Answer:\nBoston Harbor"], "ground_truth": ["Boston Harbor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-127", "prediction": ["# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.notable_people_with_this_condition -> Diem Brown\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.parent_disease -> Female urologic disease\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.survival_rates -> m.04nvdb_\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nOvarian cancer", "# Reasoning Path:\nGilda Radner -> people.deceased_person.cause_of_death -> Ovarian cancer -> medicine.disease.treatments -> Liposomal doxorubicin\n# Answer:\nOvarian cancer"], "ground_truth": ["Ovarian cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1270", "prediction": ["# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.064wlqm -> base.popstra.substance_abuse.substance -> Cocaine\n# Answer:\nCocaine", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.substance_abuse -> m.063fx3q -> base.popstra.substance_abuse.substance -> Alcoholic beverage\n# Answer:\nAlcoholic beverage", "# Reasoning Path:\nLindsay Lohan -> celebrities.celebrity.substance_abuse_problems -> m.04hykz6 -> celebrities.substance_abuse_problem.substance -> Alcoholic beverage\n# Answer:\nAlcoholic beverage", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.rehab_stay -> m.063fybd -> base.popstra.rehab_stay.facility -> Promises Treatment Centers\n# Answer:\nPromises Treatment Centers", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.rehab_stay -> m.063fvxb -> base.popstra.rehab_stay.facility -> Wonderland Center\n# Answer:\nWonderland Center", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.friendship -> m.063gnw4 -> base.popstra.friendship.participant -> Britney Spears\n# Answer:\nBritney Spears", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.parties -> m.064wlk4 -> base.popstra.party_attendance_person.party -> Haute & Bothered Launch Party\n# Answer:\nHaute & Bothered Launch Party", "# Reasoning Path:\nLindsay Lohan -> base.popstra.celebrity.rehab_stay -> m.063fvxb -> base.popstra.rehab_stay.substance_abused -> Alcoholic beverage\n# Answer:\nAlcoholic beverage"], "ground_truth": ["Alcoholic beverage", "Cocaine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1271", "prediction": ["# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> sports.sports_championship_event.season -> 2003 NFL season\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> sports.sports_championship_event.championship -> The NFC Championship Game\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.championships -> 2004 NFC Championship Game -> time.event.locations -> Lincoln Financial Field\n# Answer:\n2004 NFC Championship Game", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2008-11-23 -> american_football.football_game.season -> 2008 NFL season\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2008-11-23", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.sport -> American football -> sports.sport.positions -> Wide receiver\n# Answer:\nAmerican football", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Arizona Cardinals, 2009-11-01 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nCarolina Panthers at Arizona Cardinals, 2009-11-01", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.away_games -> Carolina Panthers at Atlanta Falcons, 2009-09-20 -> american_football.football_game.season -> 2009 NFL season\n# Answer:\nCarolina Panthers at Atlanta Falcons, 2009-09-20"], "ground_truth": ["Super Bowl XXXVIII"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1272", "prediction": ["# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.country.official_language -> Burmese Language\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> 2009 Kokang incident\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Ayeyarwady Region -> location.administrative_division.country -> Myanmar\n# Answer:\nAyeyarwady Region", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Second Mongol invasion of Burma\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Sri Lanka -> location.location.events -> 1996 Cricket World Cup\n# Answer:\nSri Lanka", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.location.events -> Battle of Bilin River\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Myanmar -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_mygc\n# Answer:\nMyanmar", "# Reasoning Path:\nCyclone Nargis -> meteorology.tropical_cyclone.affected_areas -> Bangladesh -> location.location.events -> 2005 Bangladesh bombings\n# Answer:\nBangladesh"], "ground_truth": ["2008-04-27"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1274", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nCharles Allen Culberson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_holder -> Price Daniel\n# Answer:\nPrice Daniel", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0f5 -> government.government_position_held.office_holder -> Sam Houston\n# Answer:\nSam Houston", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0g5 -> government.government_position_held.office_holder -> Richard Coke\n# Answer:\nRichard Coke", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0j9 -> government.government_position_held.office_holder -> Miriam A. Ferguson\n# Answer:\nMiriam A. Ferguson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas"], "ground_truth": ["George Tyler Wood", "Allan Shivers", "George W. Bush", "Bill Clements", "James W. Throckmorton", "James Allred", "Jim Hogg", "Dolph Briscoe", "Price Daniel", "Richard B. Hubbard", "John Connally", "Coke R. Stevenson", "Preston Smith", "Francis Lubbock", "W. Lee O'Daniel", "Rick Perry", "Oran Milo Roberts", "John Ireland", "Pat Morris Neff", "Edmund J. Davis", "Ross S. Sterling", "Beauford H. Jester", "Mark White", "J. W. Henderson", "Richard Coke", "Lawrence Sullivan Ross", "Thomas Mitchell Campbell", "Pendleton Murrah", "S. W. T. Lanham", "Andrew Jackson Hamilton", "Ann Richards", "Sam Houston", "Charles Allen Culberson", "Hardin Richard Runnels", "James E. Ferguson", "Fletcher Stockdale", "William P. Hobby", "Oscar Branch Colquitt", "Dan Moody", "Peter Hansborough Bell", "Miriam A. Ferguson", "Edward Clark", "Elisha M. Pease", "Joseph D. Sayers", "James Pinckney Henderson"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.18867924528301885, "ans_precission": 0.625, "ans_recall": 0.1111111111111111, "path_f1": 0.13698630136986303, "path_precision": 0.625, "path_recall": 0.07692307692307693, "path_ans_f1": 0.18867924528301885, "path_ans_precision": 0.625, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1275", "prediction": ["# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> type.property.expected_type -> TV Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Audio Dramatist\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.010wvs8m\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> common.topic.article -> m.087k24\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Camgirl\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> people.profession.specializations -> Character Actor\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.011qs93_\n# Answer:\nActor", "# Reasoning Path:\nHelen Flanagan -> people.person.profession -> Actor -> base.descriptive_names.names.descriptive_name -> m.01012_4j\n# Answer:\nActor"], "ground_truth": ["Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1276", "prediction": ["# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_for -> g.1255hfvqd\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_hints.included_types -> Topic\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Olympics -> base.skosbase.skos_concept.narrower -> Winter Olympics\n# Answer:\nOlympics"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1277", "prediction": ["# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> base.aareas.schema.administrative_area.administrative_children -> Humphreys County\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Mississippi -> location.location.partiallycontains -> m.0wg8_qh\n# Answer:\nMississippi", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> organization.organization_scope.organizations_with_this_scope -> US Notary Public Association\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nTennessee River -> location.location.partially_containedby -> Tennessee -> location.location.partially_contains -> Tuscumbia River\n# Answer:\nTennessee"], "ground_truth": ["Kentucky", "Tennessee", "Alabama", "Mississippi"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5555555555555556, "ans_precission": 0.625, "ans_recall": 0.5, "path_f1": 0.5555555555555556, "path_precision": 0.625, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1278", "prediction": ["# Reasoning Path:\nVenezuela -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> common.topic.notable_for -> g.125773yxl\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.location.events -> 2002 Venezuelan coup d'\u00e9tat attempt -> common.topic.image -> III Bolivar avenue, near to Miraflores\n# Answer:\n2002 Venezuelan coup d'\u00e9tat attempt", "# Reasoning Path:\nVenezuela -> location.location.events -> 2013 Venezuela Transaereo 5074 Britten-Norman Islander crash -> freebase.valuenotation.is_reviewed -> Start date\n# Answer:\n2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> base.newsevents.news_reported_event.news_report_s -> m.07wx5cm\n# Answer:\n2007 RCTV protests", "# Reasoning Path:\nVenezuela -> location.location.events -> 2007 RCTV protests -> common.topic.article -> m.02rf6lg\n# Answer:\n2007 RCTV protests"], "ground_truth": ["Los Maniceros massacre", "Venezuelan crisis of 1902\u201303", "2014 Festival Internacional de Cine Infantil y Juvenil", "Bombardment of Fort San Carlos", "2011 ALBA Games", "2014\u201315 Venezuelan protests", "g.122dshgz", "2002 Venezuelan coup d'\u00e9tat attempt", "Battle of La Victoria", "Machurucuto Incident", "Venezuelan War of Independence", "Dutch\u2013Venezuelan crisis of 1908", "2014 Caribbean Earthquake", "Dutch\u2013Venezuela War", "12th G-15 summit", "2014 NACAM Rally Championship", "2007 RCTV protests", "2013 Venezuela Transaereo 5074 Britten-Norman Islander crash", "Battle of Santa In\u00e9s"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.2608695652173913, "ans_precission": 0.75, "ans_recall": 0.15789473684210525, "path_f1": 0.2608695652173913, "path_precision": 0.75, "path_recall": 0.15789473684210525, "path_ans_f1": 0.2608695652173913, "path_ans_precision": 0.75, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-1279", "prediction": ["# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> music.instrument.family -> Guitar\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> music.instrument.variation -> Kalindula\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> education.field_of_study.students_majoring -> m.0cn9pt_\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Bass guitar -> music.instrument.family -> String instrument\n# Answer:\nBass guitar", "# Reasoning Path:\nCorey Taylor -> music.group_member.instruments_played -> Drums -> common.topic.notable_for -> g.125brs154\n# Answer:\nDrums", "# Reasoning Path:\nCorey Taylor -> music.artist.track -> X-M@S -> common.topic.notable_types -> Musical Recording\n# Answer:\nX-M@S"], "ground_truth": ["Bass guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-128", "prediction": ["# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> common.topic.notable_for -> g.1254x65b_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.0g51m5_ -> film.performance.film -> Spy Kids: All the Time in the World\n# Answer:\nSpy Kids: All the Time in the World", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9ttl -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nJuni Cortez -> common.topic.article -> m.02ql78\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJuni Cortez -> tv.tv_character.appeared_in_tv_program -> g.11b7vvcxwb\n# Answer:\ntv.tv_character.appeared_in_tv_program", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9tvb -> film.performance.film -> Spy Kids 2: The Island of Lost Dreams\n# Answer:\nSpy Kids 2: The Island of Lost Dreams", "# Reasoning Path:\nJuni Cortez -> film.film_character.portrayed_in_films -> m.02s9trw -> film.performance.film -> Spy Kids\n# Answer:\nSpy Kids"], "ground_truth": ["Daryl Sabara"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1280", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.person.nationality -> United States of America\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.person.place_of_birth -> Detroit\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> people.person.parents -> Eleanor Roosevelt\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> government.politician.party -> m.03gjgf8\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> government.us_president.vice_president -> John Nance Garner -> people.deceased_person.place_of_death -> Uvalde\n# Answer:\nJohn Nance Garner", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> people.person.sibling_s -> m.0vp4wvk\n# Answer:\nFranklin D. Roosevelt, Jr.", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> common.topic.notable_types -> Deceased Person\n# Answer:\nFranklin D. Roosevelt, Jr."], "ground_truth": ["Harry S. Truman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1281", "prediction": ["# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> United States of America\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Madelyn Kate Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nMadelyn Kate Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> base.aareas.schema.administrative_area.administrative_parent -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> common.topic.article -> m.05qbc_f\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKate Gosselin -> common.topic.webpage -> m.046s2cx -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nKate Gosselin -> people.person.place_of_birth -> Philadelphia -> location.location.containedby -> Pennsylvania\n# Answer:\nPhiladelphia", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Madelyn Kate Gosselin -> medicine.notable_person_with_medical_condition.condition -> Preterm birth\n# Answer:\nMadelyn Kate Gosselin", "# Reasoning Path:\nKate Gosselin -> people.person.children -> Cara Nicole Gosselin -> people.person.place_of_birth -> Penn State Milton S. Hershey Medical Center\n# Answer:\nCara Nicole Gosselin"], "ground_truth": ["Philadelphia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1282", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71srh9q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1284", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> travel.tourist_attraction.near_travel_destination -> Blois\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> architecture.structure.architectural_style -> Neoclassicism\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.article -> m.0m6_b\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Domenico da Cortona\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Galerie Claude Bernard -> common.topic.notable_types -> Museum\n# Answer:\nGalerie Claude Bernard", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.image -> Chateau de Chambord\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> location.location.containedby -> France\n# Answer:\nArc de Triomphe"], "ground_truth": ["Centre Georges Pompidou", "Mus\u00e9e d'Orsay", "Panth\u00e9on", "Place de la Concorde", "Parc Ast\u00e9rix", "Sacr\u00e9-C\u0153ur, Paris", "Mus\u00e9e du quai Branly", "Arc de Triomphe", "Grande Arche", "Mus\u00e9e Maillol", "Bois de Boulogne", "Ch\u00e2teau de Chambord", "Galerie Claude Bernard", "Caf\u00e9 Volpini", "Galerie nationale du Jeu de Paume", "Champs-\u00c9lys\u00e9es", "Jardin du Luxembourg", "Les Invalides", "Eiffel Tower", "Notre Dame de Paris", "The Louvre", "La Maison Rouge", "Petit Palais", "Disneyland Paris", "Gare d'Orsay", "Folies Berg\u00e8re", "Mus\u00e9e de l'Orangerie", "Verdon Gorge", "\u00cele de la Cit\u00e9", "Mus\u00e9e des Arts et M\u00e9tiers", "Disneyland Park"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.17647058823529413, "ans_precission": 1.0, "ans_recall": 0.0967741935483871, "path_f1": 0.17647058823529413, "path_precision": 1.0, "path_recall": 0.0967741935483871, "path_ans_f1": 0.17647058823529413, "path_ans_precision": 1.0, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-1285", "prediction": ["# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> location.location.containedby -> Shasta County\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Shasta County -> location.location.containedby -> California\n# Answer:\nShasta County", "# Reasoning Path:\nLassen Peak -> common.topic.notable_for -> g.125cb11p0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> common.topic.notable_for -> g.1255yn2p5\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> Lassen Volcanic National Park -> symbols.namesake.named_after -> Peter Lassen\n# Answer:\nLassen Volcanic National Park", "# Reasoning Path:\nLassen Peak -> common.topic.image -> Lassen-Peak-Large -> common.image.appears_in_topic_gallery -> Upstate California\n# Answer:\nLassen-Peak-Large", "# Reasoning Path:\nLassen Peak -> location.location.geolocation -> m.02_dlb6\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.location.containedby -> Americas\n# Answer:\nUnited States of America", "# Reasoning Path:\nLassen Peak -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> California\n# Answer:\nUnited States of America"], "ground_truth": ["Lassen Volcanic National Park", "Shasta County", "North America", "United States of America", "California", "Cascade Range"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5833333333333334, "ans_precission": 0.7, "ans_recall": 0.5, "path_f1": 0.5833333333333334, "path_precision": 0.7, "path_recall": 0.5, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1287", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.symbol -> Tomato juice\n# Answer:\nTomato juice", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04stkcq -> location.location_symbol_relationship.symbol -> Walleye\n# Answer:\nWalleye", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st75g -> location.location_symbol_relationship.Kind_of_symbol -> State beverage\n# Answer:\nState beverage", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04stq1y -> location.location_symbol_relationship.symbol -> Aesculus glabra\n# Answer:\nAesculus glabra", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04strcz -> location.location_symbol_relationship.symbol -> Racer\n# Answer:\nRacer", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.official_symbols -> m.04st87j -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird"], "ground_truth": ["With God, all things are possible"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1289", "prediction": ["# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Opera -> common.topic.notable_types -> Composition type\n# Answer:\nOpera", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> book.book_subject.works -> W.A. Mozart\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> music.genre.subgenre -> Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> common.topic.subjects -> ABRSM\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Classical music -> organization.organization_type.organizations_of_this_type -> ABRSM\n# Answer:\nClassical music", "# Reasoning Path:\nWolfgang Amadeus Mozart -> music.artist.genre -> Art song -> music.compositional_form.subforms -> Lied\n# Answer:\nArt song"], "ground_truth": ["Chamber music", "Art song", "Ballet", "Opera", "Classical music"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.888888888888889, "path_precision": 1.0, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-129", "prediction": ["# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Comedian -> people.profession.specializations -> Stand-up comedian\n# Answer:\nComedian", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Comedian -> common.topic.notable_types -> Profession\n# Answer:\nComedian", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Actor -> owl#inverseOf -> Film performances\n# Answer:\nActor", "# Reasoning Path:\nLucille Ball -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer"], "ground_truth": ["Actor", "Singer", "Television producer", "Comedian", "Model"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1291", "prediction": ["# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.notable_types -> Family member\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Kathleen Cavendish -> people.person.place_of_birth -> Brookline\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Ted Kennedy -> people.person.children -> Patrick J. Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Robert F. Kennedy -> people.person.children -> Rory Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.article -> m.01kyp0\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> people.person.sibling_s -> m.03lfqsd\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> people.person.nationality -> United States of America\n# Answer:\nRosemary Kennedy", "# Reasoning Path:\nJoseph P. Kennedy, Sr. -> people.person.children -> Rosemary Kennedy -> common.topic.image -> Rose Kennedy and her children, 1922. Bachrach Studios photograph in the John F. Kennedy Presidential Library and Museum, Boston.\n# Answer:\nRosemary Kennedy"], "ground_truth": ["John F. Kennedy", "Eunice Kennedy Shriver", "Robert F. Kennedy", "Ted Kennedy", "Kathleen Cavendish", "Joseph P. Kennedy, Jr.", "Jean Kennedy Smith", "Rosemary Kennedy", "Patricia Kennedy Lawford"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 1.0, "ans_recall": 0.4444444444444444, "path_f1": 0.6153846153846153, "path_precision": 1.0, "path_recall": 0.4444444444444444, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 1.0, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-1292", "prediction": ["# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Air Nostrum\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.serves -> El Prat de Llobregat\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Vueling\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> location.location.geolocation -> m.02_h61n\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Iberia\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.operator -> ENAIRE\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.statistical_region.population -> g.11b7w16s2h\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.hub_for -> Clickair\n# Answer:\nBarcelona\u2013El Prat Airport", "# Reasoning Path:\nBarcelona -> location.location.nearby_airports -> Barcelona\u2013El Prat Airport -> aviation.airport.serves -> Adventures by Disney - Italy: Tuscany\n# Answer:\nBarcelona\u2013El Prat Airport"], "ground_truth": ["Barcelona\u2013El Prat Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1293", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> language.language_family.sub_families -> Germanic languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Indo-European languages -> common.topic.notable_types -> Language Family\n# Answer:\nIndo-European languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> language.language_family.languages -> Frisian languages\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.article -> m.082j1_\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> Anglo-Frisian languages -> common.topic.notable_types -> Language Family\n# Answer:\nAnglo-Frisian languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nEnglish Language -> language.human_language.language_family -> West Germanic languages -> common.topic.notable_types -> Language Family\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nEnglish Language -> base.rosetta.languoid.parent -> English Group -> base.rosetta.languoid.parent -> West Germanic Group\n# Answer:\nEnglish Group"], "ground_truth": ["Indo-European languages", "Anglo-Frisian languages", "West Germanic languages"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1294", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Poland\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> location.location.partially_contains -> Soviet Union\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> common.topic.subject_of -> Carnegie Moscow Center\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> common.topic.notable_types -> Continent\n# Answer:\nEurope", "# Reasoning Path:\nWorld War II -> time.event.locations -> Southeast Asia -> location.location.contains -> Philippines\n# Answer:\nSoutheast Asia"], "ground_truth": ["Battle of Falmouth"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1295", "prediction": ["# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0k4kfm -> film.performance.film -> The Bourne Supremacy\n# Answer:\nThe Bourne Supremacy", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.09fs7t3 -> award.award_nomination.nominated_for -> Invictus\n# Answer:\nInvictus", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0k1fdk -> film.performance.film -> Ocean's Eleven\n# Answer:\nOcean's Eleven", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.0n4s483 -> award.award_nomination.nominated_for -> Ocean's Eleven\n# Answer:\nOcean's Eleven", "# Reasoning Path:\nMatt Damon -> film.actor.film -> g.11b6v52kzy\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0k4kfm -> film.performance.character -> Jason Bourne\n# Answer:\nJason Bourne", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.04dn0w4 -> award.award_nomination.nominated_for -> Saving Private Ryan\n# Answer:\nSaving Private Ryan", "# Reasoning Path:\nMatt Damon -> award.award_nominee.award_nominations -> m.09fs7t3 -> award.award_nomination.ceremony -> 67th Golden Globe Awards\n# Answer:\n67th Golden Globe Awards", "# Reasoning Path:\nMatt Damon -> film.actor.film -> m.0114bqjl -> film.performance.film -> Judge Not: In Defense of Dogma\n# Answer:\nJudge Not: In Defense of Dogma"], "ground_truth": ["Mystic Pizza", "Oh, What a Lovely Tea Party", "Push, Nevada", "Stuck on You", "Finding Forrester", "Ocean's Thirteen", "The Bourne Ultimatum", "The People Speak", "Ocean's Eleven", "Invictus", "True Grit", "Confessions of a Dangerous Mind", "School Ties", "Saving Private Ryan", "The Bourne Identity", "The Informant!", "Inside Job", "The Good Old Boys", "All the Pretty Horses", "Field of Dreams", "Elysium", "Rounders", "Extreme Realities", "The Zero Theorem", "Promised Land", "Interstellar", "Spirit: Stallion of the Cimarron", "Che: Part Two", "Jay and Silent Bob Strike Back", "Hereafter", "Green Zone", "Syriana", "Courage Under Fire", "EuroTrip", "Good Will Hunting", "Unauthorized: The Harvey Weinstein Project", "The Talented Mr. Ripley", "Behind the Screens", "The Great Wall", "Judge Not: In Defense of Dogma", "Behind the Candelabra", "Youth Without Youth", "The Martian", "The Bourne Supremacy", "Chasing Amy", "Geronimo: An American Legend", "Jersey Girl", "Magnificent Desolation: Walking On The Moon 3D", "Titan A.E.", "We Bought a Zoo", "Gerry", "The Monuments Men", "Howard Zinn: You Can\u00b4t Be Neutral on a Moving Train", "Ocean's Twelve", "The Good Shepherd", "The Adjustment Bureau", "The Brothers Grimm", "Happy Feet Two", "Margaret", "Glory Daze", "The Rainmaker", "Rising Son", "Contagion", "The Majestic", "The Good Mother", "The Departed", "Dogma", "The Third Wheel", "Rounders 2", "The Legend of Bagger Vance"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.1518987341772152, "ans_precission": 0.6666666666666666, "ans_recall": 0.08571428571428572, "path_f1": 0.08888888888888889, "path_precision": 0.6666666666666666, "path_recall": 0.047619047619047616, "path_ans_f1": 0.1518987341772152, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1296", "prediction": ["# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> common.topic.notable_types -> Profession\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.0105y97v\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Basketball players\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> common.topic.article -> m.0gm8s29\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> common.topic.notable_types -> Basketball Player -> type.type.expected_by -> Basketball player\n# Answer:\nBasketball Player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wj6\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> common.topic.notable_types -> Basketball Player -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBasketball Player", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> base.descriptive_names.names.descriptive_name -> m.01069wkk\n# Answer:\nBasketball player"], "ground_truth": ["Actor", "Basketball player"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1297", "prediction": ["# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> location.location.contains_major_portion_of -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.location.containedby -> DVD Region 1 -> location.location.contains -> United States of America\n# Answer:\nDVD Region 1", "# Reasoning Path:\nCanada -> location.location.containedby -> North America -> base.locations.continents.planet -> Earth\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> base.locations.countries.continent -> North America -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nNorth America", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp"], "ground_truth": ["Americas", "North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.6, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1298", "prediction": ["# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> people.person.profession -> American football player -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nAmerican football player", "# Reasoning Path:\nGeorge Wilson -> american_football.football_player.position_s -> Safety -> common.topic.notable_types -> Sports position\n# Answer:\nSafety", "# Reasoning Path:\nGeorge Wilson -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player"], "ground_truth": ["American football player"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.activism.activist.area_of_activism -> Anti-communism\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nPatrick Bouvier Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1300", "prediction": ["# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> London\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.containedby -> England\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Aragon House\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.geolocation -> m.0ckr6qf\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.arena_stadium -> Craven Cottage -> location.location.containedby -> London\n# Answer:\nCraven Cottage", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.notable_for -> g.1254xtmhh\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> common.topic.image -> Putney Bridge\n# Answer:\nFulham", "# Reasoning Path:\nFulham F.C. -> sports.sports_team.location -> Fulham -> location.location.contains -> Duke of Cumberland, Fulham\n# Answer:\nFulham"], "ground_truth": ["Fulham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1301", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> symbols.namesake.named_after -> John Sutter\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.article -> m.0962k\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> location.location.containedby -> California\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Crocker Art Museum -> base.usnris.nris_listing.significance_level -> State\n# Answer:\nCrocker Art Museum", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Folsom Lake -> geography.lake.basin_countries -> United States of America\n# Answer:\nFolsom Lake", "# Reasoning Path:\nSacramento -> sports.sports_team_location.teams -> Sacramento State Hornets men's basketball -> sports.sports_team.sport -> Basketball\n# Answer:\nSacramento State Hornets men's basketball", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Folsom Lake -> location.location.containedby -> Northern California\n# Answer:\nFolsom Lake", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.image -> Sutters Fort 1846\n# Answer:\nSutter's Fort"], "ground_truth": ["Sacramento History Museum", "B Street Theatre", "Sacramento Zoo", "California State Capitol Museum", "Sutter's Fort", "California State Indian Museum", "California Automobile Museum", "Raging Waters Sacramento", "California State Railroad Museum", "Folsom Lake", "Crocker Art Museum"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4158415841584158, "ans_precission": 0.875, "ans_recall": 0.2727272727272727, "path_f1": 0.4158415841584158, "path_precision": 0.875, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4158415841584158, "path_ans_precision": 0.875, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1303", "prediction": ["# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nJosh Hutchersonm -> film.actor.film -> m.0gy8k_1 -> film.performance.film -> Party Wagon\n# Answer:\nParty Wagon", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award_nominee -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.010wr2c8 -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.award -> MTV Movie Award for Best Fight\n# Answer:\nMTV Movie Award for Best Fight", "# Reasoning Path:\nJosh Hutchersonm -> award.award_nominee.award_nominations -> m.0z83x3f -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards"], "ground_truth": ["Miracle Dogs", "The Forger", "The Kids Are All Right", "The Hunger Games: Mockingjay, Part 2", "House Blend", "Detention", "One Last Ride", "Winged Creatures", "7 Days in Havana", "Red Dawn", "Party Wagon", "Cirque du Freak: The Vampire's Assistant", "Zathura", "In Dubious Battle", "Firehouse Dog", "Bridge to Terabithia", "The Hunger Games: Catching Fire", "American Splendor", "Epic", "The Hunger Games: Mockingjay, Part 1", "RV", "Motocross Kids", "Journey to the Center of the Earth", "Wilder Days", "Kicking & Screaming", "Little Manhattan", "The Third Rule", "Escobar: Paradise Lost", "Journey 2: The Mysterious Island", "The Polar Express", "The Long Home", "The Hunger Games"], "ans_acc": 0.09375, "ans_hit": 1, "ans_f1": 0.15789473684210525, "ans_precission": 0.5, "ans_recall": 0.09375, "path_f1": 0.12307692307692307, "path_precision": 0.5, "path_recall": 0.07017543859649122, "path_ans_f1": 0.15789473684210525, "path_ans_precision": 0.5, "path_ans_recall": 0.09375}
{"id": "WebQTest-1304", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Hermann G\u00f6ring\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.parent -> m.0w1p81d\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.0btmmq1 -> government.political_party_tenure.party -> German Workers' Party\n# Answer:\nGerman Workers' Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.symbols.symbolized_concept.symbolized_by -> m.05bt196\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Anton Drexler\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> The Hidden Life of Otto Frank\n# Answer:\nNazi Party"], "ground_truth": ["Hitler Youth", "Waffen-SS", "1st SS Panzer Division Leibstandarte SS Adolf Hitler", "Sturmabteilung", "Gestapo", "Wehrmacht", "Nazi Party", "Schutzstaffel"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3783783783783784, "ans_precission": 0.7777777777777778, "ans_recall": 0.25, "path_f1": 0.3783783783783784, "path_precision": 0.7777777777777778, "path_recall": 0.25, "path_ans_f1": 0.3783783783783784, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.25}
{"id": "WebQTest-1305", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kq0 -> award.award_nomination.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.nominated_for -> Bus Stop\n# Answer:\nBus Stop", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.07ynmx5 -> award.award_honor.award -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress", "# Reasoning Path:\nMarilyn Monroe -> award.award_winner.awards_won -> m.0m20gt1 -> award.award_honor.award -> Golden Globe Henrietta Award for World Film Favorites\n# Answer:\nGolden Globe Henrietta Award for World Film Favorites", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.0909kmj -> award.award_nomination.ceremony -> 14th Golden Globe Awards\n# Answer:\n14th Golden Globe Awards", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn0h0 -> award.award_nomination.award -> BAFTA Award for Best Foreign Actress\n# Answer:\nBAFTA Award for Best Foreign Actress"], "ground_truth": ["Playboy Playmate", "Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Musical or Comedy", "David di Donatello Golden Plate Award", "Golden Globe Henrietta Award for World Film Favorites"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.5, "path_recall": 0.4, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1306", "prediction": ["# Reasoning Path:\nIran -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraq\n# Answer:\nParliamentary system", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwyx0z8\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6fdrh\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Unitary state", "Islamic republic", "Theocracy", "Presidential system", "Parliamentary system"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.22222222222222224, "ans_precission": 0.25, "ans_recall": 0.2, "path_f1": 0.22222222222222224, "path_precision": 0.25, "path_recall": 0.2, "path_ans_f1": 0.22222222222222224, "path_ans_precision": 0.25, "path_ans_recall": 0.2}
{"id": "WebQTest-1307", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmyl -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1308", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nKing Edward's School, Birmingham", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nFirst Class Honours", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bx3 -> education.education.institution -> St. Philip's School\n# Answer:\nSt. Philip's School", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1309", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p3lg\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> interests.collection_category.collectors -> m.09vb4f2\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> common.topic.image -> Jan Vermeer van Delft\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Der Blaue Reiter -> common.topic.notable_for -> g.12596yl82\n# Answer:\nDer Blaue Reiter"], "ground_truth": ["Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-131", "prediction": ["# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> influence.influence_node.influenced -> Martin Luther King, Jr.\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> influence.influence_node.influenced -> Michael Jackson\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> common.topic.notable_types -> Author\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> people.person.gender -> Male\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Reinhold Niebuhr -> people.person.nationality -> United States of America\n# Answer:\nReinhold Niebuhr", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Nipsey Russell -> people.person.ethnicity -> African American\n# Answer:\nNipsey Russell", "# Reasoning Path:\nBarack Obama -> influence.influence_node.influenced_by -> Saul Alinsky -> common.topic.notable_types -> Author\n# Answer:\nSaul Alinsky"], "ground_truth": ["Reinhold Niebuhr", "Nipsey Russell", "Saul Alinsky"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1310", "prediction": ["# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> sports.sports_championship_event.champion -> Liverpool F.C.\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2010 Football League Cup Final -> sports.sports_championship_event.champion -> Manchester United F.C.\n# Answer:\n2010 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2013 Football League Cup Final -> sports.sports_championship_event.champion -> Swansea City A.F.C.\n# Answer:\n2013 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.article -> m.012dc4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> common.topic.notable_types -> Football Competition -> type.type.domain -> Soccer\n# Answer:\nFootball Competition", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2002 Football League Cup Final -> soccer.football_match.held_at -> Millennium Stadium\n# Answer:\n2002 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2012 Football League Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nFootball League Cup -> sports.sports_championship.events -> 2013 Football League Cup Final -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\n2013 Football League Cup Final"], "ground_truth": ["Liverpool F.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14285714285714285, "path_precision": 0.1111111111111111, "path_recall": 0.2, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1311", "prediction": ["# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Sagrada Fam\u00edlia -> architecture.structure.architect -> Carles Buigas\n# Answer:\nSagrada Fam\u00edlia", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> William Burges\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha\n# Answer:\nWilliam Morris", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> Eug\u00e8ne Viollet-le-Duc -> influence.influence_node.influenced -> Benjamin Bucknall\n# Answer:\nEug\u00e8ne Viollet-le-Duc", "# Reasoning Path:\nAntoni Gaud\u00ed -> architecture.architect.structures_designed -> Sagrada Fam\u00edlia -> architecture.structure.architect -> Dom\u00e8nec Sugra\u00f1es i Gras\n# Answer:\nSagrada Fam\u00edlia", "# Reasoning Path:\nAntoni Gaud\u00ed -> influence.influence_node.influenced_by -> William Morris -> people.person.profession -> Visual Artist\n# Answer:\nWilliam Morris"], "ground_truth": ["William Morris", "Eug\u00e8ne Viollet-le-Duc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1312", "prediction": ["# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Greenland -> location.administrative_division.country -> Denmark\n# Answer:\nGreenland", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> base.rosetta.languoid.parent -> Danish Group -> base.rosetta.languoid.parent -> Danish-Bokmal Group\n# Answer:\nDanish Group", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.country.languages_spoken -> Faroese\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> language.human_language.main_country -> Denmark -> location.country.languages_spoken -> German Language\n# Answer:\nDenmark", "# Reasoning Path:\nDanish Language -> media_common.netflix_genre.titles -> R -> media_common.netflix_title.netflix_genres -> Thriller\n# Answer:\nR", "# Reasoning Path:\nDanish Language -> language.human_language.countries_spoken_in -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1313", "prediction": ["# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> fictional_universe.fictional_character.parents -> Zeus\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> common.topic.article -> m.08nkrwy\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> common.topic.notable_types -> Film character\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.08nknvb\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0j85m5t -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nZeus", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.children -> Persephone -> film.film_character.portrayed_in_films -> m.0bvrrhh\n# Answer:\nPersephone", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.siblings -> m.0gwhv5j -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Zeus\n# Answer:\nZeus", "# Reasoning Path:\nDemeter -> fictional_universe.fictional_character.gender -> Female -> medicine.risk_factor.diseases -> Acne\n# Answer:\nFemale"], "ground_truth": ["Zeus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1314", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> The Ritz garden caf\u00e9 by the Swiss artist, Pierre-Georges Jeanniot (1848-1934)\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.image -> Hotel Ritz Paris\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Bernard Gaucherel\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architectural_style -> Classical architecture\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.webpage -> m.0g5c9ry\n# Answer:\nH\u00f4tel de Crillon", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> common.topic.article -> m.056n88\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel Ritz Paris -> architecture.structure.architect -> Charles Mew\u00e8s\n# Answer:\nH\u00f4tel Ritz Paris", "# Reasoning Path:\nParis -> travel.travel_destination.accommodation -> H\u00f4tel de Crillon -> common.topic.image -> H\u00c3\u00b4tel de Crillon 25 08 2007\n# Answer:\nH\u00f4tel de Crillon"], "ground_truth": ["H\u00f4tel Ritz Paris", "H\u00f4tel de Crillon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1316", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> symbols.name_source.namesakes -> Franklin Township\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.spouse_s -> m.0j6f4rh\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1317", "prediction": ["# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Iberian Peninsula\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Zaragoza -> location.location.containedby -> Aragon\n# Answer:\nZaragoza", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_parent -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> location.location.containedby -> Community of Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> P\u00fablico\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> base.aareas.schema.administrative_area.administrative_area_type -> City/Town/Village\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.capital -> Madrid -> periodicals.newspaper_circulation_area.newspapers -> Qu\u00e9!\n# Answer:\nMadrid", "# Reasoning Path:\nSpain -> location.country.second_level_divisions -> Zaragoza -> base.aareas.schema.administrative_area.administrative_parent -> Aragon\n# Answer:\nZaragoza"], "ground_truth": ["Catalonia"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1319", "prediction": ["# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxm3 -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0z9w091 -> sports.sports_team_roster.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxnp -> baseball.batting_statistics.team -> Detroit Tigers\n# Answer:\nDetroit Tigers", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxkk -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\n2004 Major League Baseball season", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> baseball.baseball_player.batting_stats -> m.06sbxlb -> baseball.batting_statistics.season -> 2005 Major League Baseball season\n# Answer:\n2005 Major League Baseball season", "# Reasoning Path:\nIv\u00e1n Rodr\u00edguez -> sports.pro_athlete.teams -> m.0ywy2gy -> sports.sports_team_roster.team -> Texas Rangers\n# Answer:\nTexas Rangers"], "ground_truth": ["Detroit Tigers", "Texas Rangers", "Miami Marlins"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.4137931034482759, "path_precision": 0.75, "path_recall": 0.2857142857142857, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-132", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.gender -> Male\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q2m7 -> base.popstra.dated.participant -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1320", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> base.playball.baseball_stadium.naming_rights -> m.05lm6s0\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> sports.sports_facility.home_venue_for -> m.0wz1z23\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> Eppstein Uhen Architects\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> projects.project_focus.projects -> Initial design and construction of Miller Park\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> common.topic.notable_for -> g.125fg3gk6\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.previously_known_as -> Seattle Pilots -> sports.sports_team.arena_stadium -> Sick's Stadium\n# Answer:\nSeattle Pilots", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.arena_stadium -> Miller Park -> architecture.structure.architecture_firm -> NBBJ\n# Answer:\nMiller Park", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Miller Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1321", "prediction": ["# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvn_0\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyd\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.subjects -> World War I\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnyq\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.contains -> ANZAC Cove\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> film.film.release_date_s -> m.04mvnz0\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle for No.3 Post\n# Answer:\nGallipoli", "# Reasoning Path:\nGallipoli Campaign -> time.event.locations -> Gallipoli -> location.location.events -> Battle of Chunuk Bair\n# Answer:\nGallipoli"], "ground_truth": ["Gallipoli"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1322", "prediction": ["# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0dlskzm -> award.award_nomination.nominated_for -> Inception\n# Answer:\nInception", "# Reasoning Path:\nTom Hardy -> film.actor.film -> g.11b77pmp46\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0hjbfqm -> award.award_nomination.nominated_for -> Warrior\n# Answer:\nWarrior", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0dlskzm -> award.award_nomination.ceremony -> 37th People's Choice Awards\n# Answer:\n37th People's Choice Awards", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0hkhtqz -> award.award_nomination.nominated_for -> Tinker Tailor Soldier Spy\n# Answer:\nTinker Tailor Soldier Spy", "# Reasoning Path:\nTom Hardy -> film.actor.film -> m.010s_rgp -> film.performance.film -> Legend\n# Answer:\nLegend", "# Reasoning Path:\nTom Hardy -> freebase.valuenotation.has_no_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0dlskzm -> award.award_nomination.award -> People's Choice Award for Favorite On-Screen Chemistry\n# Answer:\nPeople's Choice Award for Favorite On-Screen Chemistry", "# Reasoning Path:\nTom Hardy -> award.award_nominee.award_nominations -> m.0g5lkfw -> award.award_nomination.ceremony -> 64th British Academy Film Awards\n# Answer:\n64th British Academy Film Awards"], "ground_truth": ["London Road", "W\u0394Z", "The Drop", "Sergeant Slaughter, My Big Brother", "Colditz", "RocknRolla", "Lethal Dose", "The Revenant", "Splinter Cell", "Flood", "The Reckoning", "This Means War", "Scenes of a Sexual Nature", "Thick as Thieves", "Bronson", "Star Trek Nemesis", "Gideon's Daughter", "Locke", "Tinker Tailor Soldier Spy", "EMR", "The Inheritance", "Dot the I", "The Virgin Queen", "Deserter", "Sweeney Todd", "The Outsider", "Legend", "Perfect", "Lawless", "Marie Antoinette", "Warrior", "Child 44", "Sucker Punch", "Minotaur", "Everest", "Black Hawk Down", "Inception", "Mad Max: Fury Road", "Layer Cake", "The Dark Knight Rises"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.163265306122449, "ans_precission": 0.4444444444444444, "ans_recall": 0.1, "path_f1": 0.12121212121212122, "path_precision": 0.4444444444444444, "path_recall": 0.07017543859649122, "path_ans_f1": 0.163265306122449, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.1}
{"id": "WebQTest-1323", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> common.topic.notable_types -> Sports Facility\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome"], "ground_truth": ["Cobb Energy Performing Arts Centre", "Atlanta Jewish Film Festival", "Margaret Mitchell House & Museum", "Four Seasons Hotel Atlanta", "World of Coca-Cola", "Masquerade", "Turner Field", "Fernbank Museum of Natural History", "Omni Coliseum", "Atlanta History Center", "Fernbank Science Center", "CNN Center", "Georgia State Capitol", "Philips Arena", "Atlanta Ballet", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Georgia World Congress Center", "Fox Theatre", "Woodruff Arts Center", "Hyatt Regency Atlanta", "Atlanta Symphony Orchestra", "Martin Luther King, Jr. National Historic Site", "Underground Atlanta", "Six Flags White Water", "Zoo Atlanta", "The Tabernacle", "Jimmy Carter Library and Museum", "Variety Playhouse", "Arbor Place Mall", "Peachtree Road Race", "Atlanta Marriott Marquis", "Georgia Aquarium", "Six Flags Over Georgia", "Center for Puppetry Arts", "Georgia Dome"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615385, "ans_precission": 1.0, "ans_recall": 0.08333333333333333, "path_f1": 0.15384615384615385, "path_precision": 1.0, "path_recall": 0.08333333333333333, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 1.0, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1324", "prediction": ["# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.institution -> Carnegie Mellon College of Engineering\n# Answer:\nCarnegie Mellon College of Engineering", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> education.education.degree -> Doctorate\n# Answer:\nDoctorate", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.0n0rj5w -> education.education.institution -> Bluefield High School\n# Answer:\nBluefield High School", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.02kq1bj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.03kwg72 -> education.education.major_field_of_study -> Mathematics\n# Answer:\nMathematics", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> people.person.education -> m.0v8ycxg -> education.education.institution -> Carnegie Mellon College of Engineering\n# Answer:\nCarnegie Mellon College of Engineering", "# Reasoning Path:\nJohn Forbes Nash, Jr. -> education.academic.advisors -> Albert W. Tucker -> education.academic.advisors -> Solomon Lefschetz\n# Answer:\nAlbert W. Tucker"], "ground_truth": ["Carnegie Mellon College of Engineering", "Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.375, "path_recall": 0.6, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1325", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cyprus\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nMalta -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_kbr9\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["English Language", "Maltese Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-1326", "prediction": ["# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02ctc4t\n# Answer:\ncommon.image.size", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.notable_types -> Invention\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Quadruplex telegraph -> common.topic.notable_for -> g.1257t43gt\n# Answer:\nQuadruplex telegraph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> award.ranked_item.appears_in_ranked_lists -> m.0h717jf\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Quadruplex telegraph -> common.topic.article -> m.02r0w8t\n# Answer:\nQuadruplex telegraph"], "ground_truth": ["Carbon microphone", "Incandescent light bulb", "Phonograph cylinder", "Movie camera", "Phonograph", "Mimeograph", "Electric Power Distribution", "Quadruplex telegraph"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3902439024390244, "ans_precission": 0.8888888888888888, "ans_recall": 0.25, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.3902439024390244, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.25}
{"id": "WebQTest-1327", "prediction": ["# Reasoning Path:\nShakira -> people.person.place_of_birth -> Barranquilla -> location.location.containedby -> Colombia\n# Answer:\nBarranquilla", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.languages_spoken -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> common.topic.notable_types -> Country\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> people.person.nationality -> Colombia -> location.country.official_language -> Spanish Language\n# Answer:\nColombia", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.0v3dwp1 -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nShakira -> music.artist.label -> Sony Music Latin -> music.record_label.artist -> Draco Rosa\n# Answer:\nSony Music Latin", "# Reasoning Path:\nShakira -> tv.tv_actor.guest_roles -> m.09nb6_1 -> tv.tv_guest_role.episodes_appeared_in -> and then there were 9...\n# Answer:\nand then there were 9...", "# Reasoning Path:\nShakira -> music.artist.label -> Sony Music Latin -> common.topic.notable_types -> Record label\n# Answer:\nSony Music Latin"], "ground_truth": ["Colombia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1328", "prediction": ["# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> common.topic.subject_of -> Jimi Agbaje\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nRutherford B. Hayes -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nRutherford B. Hayes -> book.book_subject.works -> Reunion and reaction -> book.written_work.subjects -> United States of America\n# Answer:\nReunion and reaction"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1329", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc6p4 -> film.performance.actor -> Dominic Monaghan\n# Answer:\nDominic Monaghan", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nThe Lord of the Rings: The Fellowship of the Ring -> film.film.starring -> m.02vc8t6 -> film.performance.character -> Old Bilbo\n# Answer:\nfilm.performance.character"], "ground_truth": ["Ian Holm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-133", "prediction": ["# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Willadeene Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.hud_county_place.county -> Sevier County\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Stella Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.containedby -> Tennessee\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.containedby -> Sevier County\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> music.artist.origin -> Sevierville -> location.location.people_born_here -> Avie Lee Owens\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> people.person.place_of_birth -> Sevierville -> location.location.people_born_here -> Willadeene Parton\n# Answer:\nSevierville", "# Reasoning Path:\nDolly Parton -> common.topic.notable_types -> TV Episode\n# Answer:\nTV Episode"], "ground_truth": ["Sevierville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1330", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> base.locations.states_and_provences.places_within -> Oak Hill\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Pennsylvania -> location.location.containedby -> Contiguous United States\n# Answer:\nPennsylvania", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.administrative_divisions -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois"], "ground_truth": ["New York"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1331", "prediction": ["# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nBarry Goldwater", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.office_holder -> Barry Goldwater\n# Answer:\nBarry Goldwater", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w120p -> government.government_position_held.office_holder -> Henry F. Ashurst\n# Answer:\nHenry F. Ashurst", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0_n4 -> government.government_position_held.office_holder -> Marcus A. Smith\n# Answer:\nMarcus A. Smith", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w0zkh -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nArizona -> government.political_district.representatives -> m.09w11bv -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate"], "ground_truth": ["Paul Fannin", "Ernest McFarland", "Dennis DeConcini", "Barry Goldwater", "Ralph H. Cameron", "Henry F. Ashurst", "Carl Hayden", "Jon Kyl", "Jeff Flake", "Marcus A. Smith", "John McCain"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.4597701149425288, "ans_precission": 0.625, "ans_recall": 0.36363636363636365, "path_f1": 0.45454545454545453, "path_precision": 0.625, "path_recall": 0.35714285714285715, "path_ans_f1": 0.4597701149425288, "path_ans_precision": 0.625, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-1332", "prediction": ["# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.force_deployments -> m.02h782s\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.force_deployments -> m.02h7m00\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.casualties -> m.043wptp\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 20th century -> time.event.includes_event -> 1970s\n# Answer:\n20th century", "# Reasoning Path:\nVietnam War -> time.event.included_in_event -> 1960s -> time.event.includes_event -> 1960\n# Answer:\n1960s", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> military.military_combatant.casualties -> m.04kb4m4\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Asia -> location.location.events -> World War II\n# Answer:\nAsia"], "ground_truth": ["1975-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1333", "prediction": ["# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Cairo\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Kentucky\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> geography.river.mouth -> Mississippi River\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.containedby -> United States of America\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> location.location.partially_containedby -> Ohio\n# Answer:\nOhio River", "# Reasoning Path:\nTennessee River -> geography.river.mouth -> Ohio River -> common.topic.image -> Ohio River viewed from Liberty Hill in Ripley, Ohio\n# Answer:\nOhio River"], "ground_truth": ["Ohio River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1334", "prediction": ["# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> award.award_nominee.award_nominations -> m.0ycq_cj\n# Answer:\nJim Lovell", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> freebase.valuenotation.has_value -> Parents\n# Answer:\nJim Lovell", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jeffrey Kluger -> award.award_nominee.award_nominations -> m.0ycq_cj\n# Answer:\nJeffrey Kluger", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> people.person.gender -> Male\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> film.actor.film -> g.11b70lbbvb\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> people.person.profession -> Writer\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_nominee.award_nominations -> m.0n1vpsh -> award.award_nomination.nominated_for -> Philadelphia\n# Answer:\nPhiladelphia", "# Reasoning Path:\nApollo 13 -> film.film.story_by -> Jim Lovell -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nJim Lovell", "# Reasoning Path:\nTom Hanks -> award.award_nominee.award_nominations -> m.010b2qn2 -> award.award_nomination.nominated_for -> Captain Phillips\n# Answer:\nCaptain Phillips"], "ground_truth": ["Jim Lovell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.5263157894736842, "path_precision": 0.5555555555555556, "path_recall": 0.5, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1335", "prediction": ["# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> Dagbani Language -> base.rosetta.languoid.parent -> Southeast Western Group\n# Answer:\nDagbani Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Nigeria\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nGhana -> location.statistical_region.external_debt_stock -> g.11b71px2jn\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["\u00c9w\u00e9 Language", "Dagaare language", "Asante dialect", "Fula language", "English Language", "Dangme Language", "Dagbani Language", "Kasem Language", "Akan Language", "Gonja Language", "Ga Language", "Nzema Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.27586206896551724, "ans_precission": 0.8, "ans_recall": 0.16666666666666666, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.27586206896551724, "path_ans_precision": 0.8, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1336", "prediction": ["# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.location.containedby -> Utah\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.geolocation -> m.0cnlcc_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Capitol Hill -> location.location.containedby -> Utah\n# Answer:\nCapitol Hill", "# Reasoning Path:\nUtah State Capitol -> common.topic.image -> Utah State Capitol in 2002 -> common.image.appears_in_topic_gallery -> Utah State Legislature\n# Answer:\nUtah State Capitol in 2002", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11b7tm7k1_\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> tv.tv_location.tv_shows_filmed_here -> The Stand\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> location.location.containedby -> Salt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nSalt Lake City", "# Reasoning Path:\nUtah State Capitol -> common.topic.notable_for -> g.1258b7gl7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nUtah -> base.aareas.schema.administrative_area.capital -> Salt Lake City -> location.statistical_region.population -> g.11btt563rr\n# Answer:\nSalt Lake City"], "ground_truth": ["Salt Lake City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1337", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.article -> m.013cqz\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.notable_types -> Product category\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove"], "ground_truth": ["Bifocals", "Franklin stove", "Lightning rod", "Glass harmonica"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8076923076923077, "path_precision": 0.875, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1339", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-134", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Thriller\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.genre -> Mystery\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Death on the Nile -> film.film.genre -> Detective fiction\n# Answer:\nDeath on the Nile", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> book.written_work.subjects -> England\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Death on the Nile -> book.written_work.subjects -> England\n# Answer:\nDeath on the Nile", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> The Getaway Guide to Agatha Christie's England -> book.written_work.subjects -> England\n# Answer:\nThe Getaway Guide to Agatha Christie's England", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Evil Under the Sun -> film.film.genre -> Crime Fiction\n# Answer:\nEvil Under the Sun", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Screenwriter", "Novelist", "Writer", "Playwright", "Poet"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1340", "prediction": ["# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.languages_spoken -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> base.biblioness.bibs_location.country -> South Africa\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.official_language -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.place_of_birth -> Johannesburg -> location.location.containedby -> South Africa\n# Answer:\nJohannesburg", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nSteve Nash -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada"], "ground_truth": ["Canada", "United Kingdom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1341", "prediction": ["# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.0481629 -> government.government_position_held.office_holder -> Arlen Specter\n# Answer:\nArlen Specter", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04dxp_0 -> government.government_position_held.office_holder -> Bob Casey, Jr.\n# Answer:\nBob Casey, Jr.", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04flnp7 -> government.government_position_held.office_holder -> Michael J. Holston\n# Answer:\nMichael J. Holston", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nJim Cawley", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.04j5vfm -> government.government_position_held.office_holder -> James Buchanan\n# Answer:\nJames Buchanan", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.09pvmbw -> government.government_position_held.office_holder -> William Maclay\n# Answer:\nWilliam Maclay", "# Reasoning Path:\nPennsylvania -> government.political_district.representatives -> m.0481629 -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform"], "ground_truth": ["William Wilkins", "George Logan", "Edward Martin", "George M. Dallas", "Richard Brodhead", "Abner Lacock", "Philander C. Knox", "Richard Schweiker", "Rick Santorum", "H. John Heinz III", "Walter Lowrie", "Simon Cameron", "George T. Oliver", "Francis J. Myers", "Matthew Quay", "Jonathan Roberts", "Joseph S. Clark, Jr.", "James Ross", "Albert Gallatin", "Arlen Specter", "Michael Leib", "Harris Wofford", "Robert Morris", "Boies Penrose", "David A. Reed", "William Findlay", "Hugh Scott", "William E. Crow", "Peter Muhlenberg", "William Marks", "Pat Toomey", "Daniel Sturgeon", "James Buchanan", "Joseph R. Grundy", "Samuel Maclay", "William Scott Vare", "Isaac D. Barnard", "George W. Pepper", "Joseph F. Guffey", "William Bingham", "Bob Casey, Jr.", "William Bigler", "William Maclay", "James J. Davis", "James H. Duff", "Samuel McKean", "Andrew Gregg"], "ans_acc": 0.0851063829787234, "ans_hit": 1, "ans_f1": 0.14545454545454545, "ans_precission": 0.5, "ans_recall": 0.0851063829787234, "path_f1": 0.12903225806451613, "path_precision": 0.5, "path_recall": 0.07407407407407407, "path_ans_f1": 0.14545454545454545, "path_ans_precision": 0.5, "path_ans_recall": 0.0851063829787234}
{"id": "WebQTest-1342", "prediction": ["# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.language_family -> Romance languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Yeniche Language -> language.human_language.main_country -> Germany\n# Answer:\nYeniche Language", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> Yeniche Language -> language.human_language.region -> Europe\n# Answer:\nYeniche Language", "# Reasoning Path:\nFrance -> location.country.languages_spoken -> French -> language.human_language.language_family -> Romance languages\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nFrench", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1343", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.degree -> Secondary education\n# Answer:\nSecondary education", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nBachelor of Divinity", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.major_field_of_study -> Systematic theology\n# Answer:\nSystematic theology", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.institution -> Washington High School\n# Answer:\nWashington High School"], "ground_truth": ["PhD", "Bachelor of Divinity", "Bachelor of Arts", "Secondary education"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1344", "prediction": ["# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis -> base.biblioness.bibs_location.state -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_children -> Franklin County\n# Answer:\nMissouri", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Area code 314 -> location.location.containedby -> Missouri\n# Answer:\nArea code 314", "# Reasoning Path:\nSt. Louis -> location.location.containedby -> Missouri -> location.location.time_zones -> Central Time Zone\n# Answer:\nMissouri"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1348", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.school -> University of Tennessee\n# Answer:\nUniversity of Tennessee", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.draft -> 1998 NFL draft\n# Answer:\n1998 NFL draft", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Ole Miss Rebels football", "New Orleans Saints", "Minnesota Vikings", "Houston Oilers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1350", "prediction": ["# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's long jump\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's long jump", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrr2 -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 200 metres\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's 200 metres", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrrk -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's 4 \u00d7 100 metres relay", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrs6 -> olympics.olympic_medal_honor.event -> Athletics at the 1936 Summer Olympics \u2013 Men's 100 metres\n# Answer:\nAthletics at the 1936 Summer Olympics \u2013 Men's 100 metres", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.olympics -> 1936 Summer Olympics\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nCleveland", "# Reasoning Path:\nJesse Owens -> olympics.olympic_athlete.medals_won -> m.04hdrqm -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal"], "ground_truth": ["Associated Press Male Athlete of the Year"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1351", "prediction": ["# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_types -> Currency\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.notable_for -> g.125cdv3sb\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.article -> m.0b4243\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> common.topic.article -> m.016w_5\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPeru -> location.country.currency_used -> Peruvian nuevo sol -> common.topic.webpage -> m.04m8gvj\n# Answer:\nPeruvian nuevo sol", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc37ctc\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.1hhc38qlv\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_9gm0\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["Peruvian nuevo sol"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1353", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Line Digital Ltd\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1354", "prediction": ["# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.spouse_s -> m.0w7r8xb\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Mary Jane Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.place_of_birth -> Tennessee\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> common.topic.notable_for -> g.1255nt3hr\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.deceased_person.cause_of_death -> Meningitis\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Francis Avent Gumm -> people.person.children -> Dorothy Virginia Gumm\n# Answer:\nFrancis Avent Gumm", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.deceased_person.place_of_death -> Los Angeles\n# Answer:\nEthel Marion Milne", "# Reasoning Path:\nJudy Garland -> people.person.parents -> Ethel Marion Milne -> people.person.children -> Mary Jane Gumm\n# Answer:\nEthel Marion Milne"], "ground_truth": ["Francis Avent Gumm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1355", "prediction": ["# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04yst0l -> education.education.institution -> University of Virginia\n# Answer:\nUniversity of Virginia", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.04hddhr -> education.education.institution -> Bates College\n# Answer:\nBates College", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.02wp24k -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.040qpmj -> education.education.institution -> University of Virginia School of Law\n# Answer:\nUniversity of Virginia School of Law", "# Reasoning Path:\nRobert F. Kennedy -> people.person.education -> m.040qpmp -> education.education.institution -> Harvard College\n# Answer:\nHarvard College", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.notable_for -> g.12573hgz4\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Anthony Shriver\n# Answer:\nKennedy family", "# Reasoning Path:\nRobert F. Kennedy -> people.family_member.family -> Kennedy family -> common.topic.image -> Kennedy Arms\n# Answer:\nKennedy family"], "ground_truth": ["Harvard College", "University of Virginia School of Law", "Bates College", "Harvard University", "Milton Academy", "University of Virginia"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.625, "ans_recall": 0.8333333333333334, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.625, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1356", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.article -> m.0bwj2gk\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.notable_for -> g.125cvd366\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Stephen Ward\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> common.topic.notable_types -> Cause Of Death\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.parent_cause_of_death -> Substance abuse\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Pier Angeli\n# Answer:\nBarbiturate overdose", "# Reasoning Path:\nMarilyn Monroe -> award.award_nominee.award_nominations -> m.07zn09y -> award.award_nomination.ceremony -> 9th British Academy Film Awards\n# Answer:\n9th British Academy Film Awards", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.cause_of_death -> Barbiturate overdose -> people.cause_of_death.people -> Judy Garland\n# Answer:\nBarbiturate overdose"], "ground_truth": ["Barbiturate overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1357", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> common.topic.notable_for -> g.125fm8vqg\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.subdividing_type -> Japanese prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> common.topic.notable_for -> g.1259_1vs2\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.pertinent_type -> Japanese prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nJapan"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1358", "prediction": ["# Reasoning Path:\nLos Angeles -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064c8fd -> base.popstra.vacation_choice.vacationer -> David Schwimmer\n# Answer:\nDavid Schwimmer", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064sn_4 -> base.popstra.vacation_choice.vacationer -> Hilary Duff\n# Answer:\nHilary Duff", "# Reasoning Path:\nLos Angeles -> travel.travel_destination.climate -> m.052lsdb -> travel.travel_destination_monthly_climate.month -> July\n# Answer:\nJuly", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings -> book.periodical.subjects -> Catholicism\n# Answer:\nThe Tidings", "# Reasoning Path:\nLos Angeles -> base.popstra.location.vacationers -> m.064sw_0 -> base.popstra.vacation_choice.vacationer -> Madonna\n# Answer:\nMadonna", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Hoy -> book.newspaper.circulation_areas -> New York City\n# Answer:\nHoy", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings -> common.topic.notable_types -> Newspaper\n# Answer:\nThe Tidings", "# Reasoning Path:\nLos Angeles -> periodicals.newspaper_circulation_area.newspapers -> Urdu Times -> book.newspaper.circulation_areas -> Washington, D.C.\n# Answer:\nUrdu Times"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1359", "prediction": ["# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.parents -> Alice Kim\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.place_of_birth -> New York City\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.children -> Lucian Augustus Coppola Cage\n# Answer:\nWeston Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> people.person.sibling_s -> m.0pcd733\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> common.topic.notable_types -> Person\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nHeterosexuality", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Kal-El Coppola Cage -> common.topic.notable_for -> g.125br8kb3\n# Answer:\nKal-El Coppola Cage", "# Reasoning Path:\nNicolas Cage -> people.person.children -> Weston Coppola Cage -> people.person.profession -> Singer\n# Answer:\nWeston Coppola Cage"], "ground_truth": ["Weston Coppola Cage", "Kal-El Coppola Cage"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1361", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71yj1cp\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1362", "prediction": ["# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.appointed_by -> Rick Scott\n# Answer:\nRick Scott", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0jsjn35 -> government.government_position_held.office_holder -> Rick Scott\n# Answer:\nRick Scott", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.office_holder -> Kurt S. Browning\n# Answer:\nKurt S. Browning", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.basic_title -> Secretary of state\n# Answer:\nSecretary of state", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115c969 -> government.government_position_held.office_holder -> Jan Fortune\n# Answer:\nJan Fortune", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0jsjn35 -> government.government_position_held.office_position_or_title -> Governor of Florida\n# Answer:\nGovernor of Florida", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0115gjyq -> government.government_position_held.office_holder -> Joe Davis\n# Answer:\nJoe Davis", "# Reasoning Path:\nFlorida -> government.governmental_jurisdiction.governing_officials -> m.0hz6vzk -> government.government_position_held.office_position_or_title -> Secretary of State of Florida\n# Answer:\nSecretary of State of Florida"], "ground_truth": ["Rick Scott"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1363", "prediction": ["# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> people.marriage.spouse -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 29.10 -> base.saturdaynightlive.snl_episode.musical_guest_performance -> m.04p501g\n# Answer:\nSNL - 29.10", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.0113nnph -> people.marriage.spouse -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> celebrities.celebrity.sexual_relationships -> m.010gz96f -> celebrities.romantic_relationship.celebrity -> Eric Johnson\n# Answer:\nEric Johnson", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.07sl20z -> people.marriage.spouse -> Nick Lachey\n# Answer:\nNick Lachey", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJessica Simpson -> base.saturdaynightlive.snl_host.episodes_hosted -> SNL - 29.10 -> base.saturdaynightlive.snl_episode.host -> Nick Lachey\n# Answer:\nSNL - 29.10", "# Reasoning Path:\nJessica Simpson -> people.person.spouse_s -> m.010gz8v_ -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership"], "ground_truth": ["Nick Lachey", "Eric Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1364", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> base.schemastaging.context_name.pronunciation -> g.125_nyqwz\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.region -> Europe\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.countries_spoken_in -> England\n# Answer:\nOld English"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1365", "prediction": ["# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.01066g6n\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> common.topic.article -> m.01pxq\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.schemastaging.context_name.pronunciation -> g.125_m09_2\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.0106_ymb\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> people.profession.specialization_of -> Scientist\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.010f2qwv\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> base.descriptive_names.names.descriptive_name -> m.010f2qyn\n# Answer:\nChemist", "# Reasoning Path:\nJohn Dalton -> people.person.profession -> Chemist -> people.profession.specializations -> Biochemist\n# Answer:\nChemist"], "ground_truth": ["Physicist", "Chemist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1366", "prediction": ["# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.album.album_content_type -> Studio album\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Chicken Fried -> music.recording.releases -> The Foundation\n# Answer:\nChicken Fried", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.recording.featured_artists -> Alan Jackson\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> music.composition.composer -> Zac Brown\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> common.topic.webpage -> m.0460qbn -> common.webpage.resource -> m.0bjwcfr\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> As She's Walking Away -> common.topic.notable_types -> Composition\n# Answer:\nAs She's Walking Away", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Chicken Fried -> common.topic.notable_for -> g.12yxh5hrx\n# Answer:\nChicken Fried", "# Reasoning Path:\nZac Brown Band -> music.artist.album -> Colder Weather -> common.topic.notable_for -> g.126srqk7h\n# Answer:\nColder Weather"], "ground_truth": ["I Shall Be Released", "Bar", "Jolene", "Keep Me In Mind", "Let It Go (Live In Atlanta)", "Free / Into the Mystic", "Forever and Ever, Amen", "Nothing", "Homegrown", "Who Knows", "A Different Kind of Fine", "Human", "Intro", "The Muse", "Castaway", "The Night They Drove Old Dixie Down", "Goodbye In Her Eyes", "Not OK", "I Lost It (live)", "Highway 20 Ride", "Every Little Bit (live)", "Uncaged", "Heavy Is the Head", "Sic 'em on a Chicken", "Blackbird", "Tomorrow Never Comes (acoustic version)", "Knee Deep (Feat. Jimmy Buffett)", "Day That I Die", "All Alright", "Can't You See", "Keep Me in Mind", "Alabama Jubilee", "Martin (Live In Atlanta)", "Where the Boat Leaves From / One Love", "Every Little Bit", "Last But Not Least", "DJ", "We're Gonna Make This Day", "Black Water", "I Shall Be Released (live)", "Sic 'em on the Chicken", "Valentines", "I'll Be Your Man (Song For a Daughter)", "Beautiful Drug", "The Devil Went Down to Georgia", "Quiet Your Mind", "Martin", "Whatever It Is", "Remedy", "Loving You Easy", "Toes", "Free", "As She's Walking Away", "I Play the Road", "Colder Weather", "Chicken Fried (Full Version)", "No Hurry", "Trouble", "It's Not OK", "Tax Man Shoes", "Heather", "Wildfire", "These Days", "Cold Hearted", "Whiskey's Gone", "Dress Blues", "The Wind", "Tomorrow Never Comes", "Overnight (Feat. Trombone Shorty)", "Natural Disaster", "Jump Right In", "Smoke Rise", "Let It Rain", "Make This Day", "Chicken Fried", "It's Not Okay", "America the Beautiful", "Violin Intro to Free", "Oh My Sweet Carolina (live)", "Trying to Drive", "Harmony", "Mary", "Curse Me", "Island Song", "The Night They Drove Old Dixie Down (live)", "Settle Me Down", "Bad Moon Rising", "Day for the Dead", "Let It Go", "Where the Boat Leaves From", "Junkyard", "On This Train", "Bittersweet", "One Day", "Mango Tree", "Lance's Song", "Different Kind of Fine", "Day That I Die (Feat. Amos Lee)", "Better Day", "Young and Wild", "Sweet Annie"], "ans_acc": 0.0297029702970297, "ans_hit": 1, "ans_f1": 0.05745554035567715, "ans_precission": 0.875, "ans_recall": 0.0297029702970297, "path_f1": 0.3806646525679759, "path_precision": 0.875, "path_recall": 0.24324324324324326, "path_ans_f1": 0.05745554035567715, "path_ans_precision": 0.875, "path_ans_recall": 0.0297029702970297}
{"id": "WebQTest-1367", "prediction": ["# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> Mendip -> location.location.containedby -> South West England\n# Answer:\nMendip", "# Reasoning Path:\nGlastonbury -> travel.travel_destination.tourist_attractions -> The Tribunal, Glastonbury -> location.location.containedby -> United Kingdom\n# Answer:\nThe Tribunal, Glastonbury", "# Reasoning Path:\nGlastonbury -> location.location.geolocation -> m.05d0fsf\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGlastonbury -> location.location.containedby -> United Kingdom -> location.country.second_level_divisions -> South West England\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom", "Mendip"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1368", "prediction": ["# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> base.activism.activist.area_of_activism -> Labor rights\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> people.person.profession -> Labor Union Leader\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.article -> m.01z4fw\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> organization.organization_founder.organizations_founded -> Federation of Organized Trades and Labor Unions\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.notable_for -> g.1257s1xp1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> people.person.place_of_birth -> London\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> organization.organization_founder.organizations_founded -> Cigar Makers' International Union\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.founders -> Samuel Gompers -> book.author.works_written -> America's fight for the preservation of democracy\n# Answer:\nSamuel Gompers", "# Reasoning Path:\nAmerican Federation of Labor -> common.topic.image -> AFL-label -> common.image.size -> m.04sn0nx\n# Answer:\nAFL-label", "# Reasoning Path:\nAmerican Federation of Labor -> organization.organization.geographic_scope -> United States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nUnited States of America"], "ground_truth": ["Samuel Gompers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1369", "prediction": ["# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> architecture.structure.architectural_style -> Romanesque architecture\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> location.location.geolocation -> m.02_dk4s\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b66hmvpg\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> base.usnris.nris_listing.significance_level -> State\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Museum of Nature and Science -> location.location.containedby -> 80205\n# Answer:\nDenver Museum of Nature and Science", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Chamberlin Observatory -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h7xk_\n# Answer:\nChamberlin Observatory", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Museum of Nature and Science -> common.topic.notable_for -> g.125f0jmy6\n# Answer:\nDenver Museum of Nature and Science", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Westin Westminster -> common.topic.image -> The Westin Westminster\n# Answer:\nWestin Westminster", "# Reasoning Path:\nDenver -> location.statistical_region.population -> g.11b7tnnw1n\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nDenver -> travel.travel_destination.tourist_attractions -> Denver Museum of Nature and Science -> location.location.geolocation -> m.02_s_lr\n# Answer:\nDenver Museum of Nature and Science"], "ground_truth": ["Molly Brown House", "Westin Westminster", "Elitch Gardens", "Forney Transportation Museum", "Museum of Contemporary Art Denver", "Denver Mint", "Denver Museum of Nature and Science", "Black American West Museum & Heritage Center", "Chamberlin Observatory", "Children's Museum of Denver", "Frederic C. Hamilton Building", "North Building", "Denver Botanic Gardens", "Festivus Film Festival", "Colorado State Capitol", "Denver Firefighters Museum"], "ans_acc": 0.1875, "ans_hit": 1, "ans_f1": 0.3037974683544304, "ans_precission": 0.8, "ans_recall": 0.1875, "path_f1": 0.3037974683544304, "path_precision": 0.8, "path_recall": 0.1875, "path_ans_f1": 0.3037974683544304, "path_ans_precision": 0.8, "path_ans_recall": 0.1875}
{"id": "WebQTest-1370", "prediction": ["# Reasoning Path:\nThomas Hobbes -> people.person.places_lived -> m.0jvv0vn -> people.place_lived.location -> Paris\n# Answer:\nParis", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_types -> Location\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.notable_for -> g.1256l2xh5\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> location.location.geolocation -> m.0wms3sk\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.location.containedby -> United Kingdom\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.deceased_person.place_of_death -> Derbyshire -> location.administrative_division.country -> United Kingdom\n# Answer:\nDerbyshire", "# Reasoning Path:\nThomas Hobbes -> people.person.place_of_birth -> Westport, Wiltshire -> common.topic.article -> m.0gfjdy7\n# Answer:\nWestport, Wiltshire", "# Reasoning Path:\nThomas Hobbes -> people.person.nationality -> United Kingdom -> location.country.first_level_divisions -> Scotland\n# Answer:\nUnited Kingdom"], "ground_truth": ["Paris"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1373", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> location.location.containedby -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.deceased_person.place_of_death -> Atlantic Ocean\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dealey Plaza\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> Words to remember -> common.topic.notable_for -> g.1258tnz1j\n# Answer:\nWords to remember", "# Reasoning Path:\nJohn F. Kennedy -> military.military_commander.military_commands -> m.0w5t4qv -> military.military_command.military_conflict -> Cold War\n# Answer:\nCold War"], "ground_truth": ["Dallas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1374", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Kingdom of Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic"], "ground_truth": ["Parliamentary republic", "Unitary state", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1376", "prediction": ["# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> location.location.containedby -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Canada -> base.aareas.schema.administrative_area.administrative_children -> Qu\u00e9bec\n# Answer:\nCanada", "# Reasoning Path:\nMontreal -> base.biblioness.bibs_location.state -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nMontreal -> location.location.containedby -> Urban agglomeration of Montreal -> location.location.containedby -> Qu\u00e9bec\n# Answer:\nUrban agglomeration of Montreal"], "ground_truth": ["Qu\u00e9bec"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1378", "prediction": ["# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7qz5rxs\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11b7xkpw1f\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> g.11bws79p_d\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKellan Lutz -> common.topic.webpage -> m.0bvwg64 -> common.webpage.resource -> Kellan Lutz Pictures\n# Answer:\nKellan Lutz Pictures", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> m.010rckmk -> film.performance.character -> Tarzan\n# Answer:\nTarzan", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> m.012zhjcm -> film.performance.film -> After Dusk They Come\n# Answer:\nAfter Dusk They Come", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nKellan Lutz -> film.actor.film -> m.010rckmk -> film.performance.film -> Tarzan\n# Answer:\nTarzan", "# Reasoning Path:\nKellan Lutz -> people.person.education -> m.0gyg65t -> education.education.institution -> Chapman University\n# Answer:\nChapman University", "# Reasoning Path:\nKellan Lutz -> people.person.gender -> Male -> biology.hybrid_parent_gender.hybrids -> m.0zm9_69\n# Answer:\nMale"], "ground_truth": ["Rick Leland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1379", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Film Score Composer -> people.profession.specialization_of -> Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Virtuoso -> people.profession.specialization_of -> Musician\n# Answer:\nVirtuoso", "# Reasoning Path:\nLife of Franz Liszt -> common.topic.notable_types -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Film Score Composer -> common.resource.annotations -> m.0ggn923\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Film Score Composer -> common.topic.notable_types -> Profession\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Pianist -> people.profession.specialization_of -> Musician\n# Answer:\nPianist", "# Reasoning Path:\nLife of Franz Liszt -> film.music_contributor.film -> The Loves of Liszt -> film.film.music -> Ferenc Farkas\n# Answer:\nThe Loves of Liszt", "# Reasoning Path:\nLife of Franz Liszt -> people.person.profession -> Film Score Composer -> base.descriptive_names.names.descriptive_name -> m.0101j3hm\n# Answer:\nFilm Score Composer"], "ground_truth": ["Composer", "Film Score Composer", "Virtuoso", "Priest", "Pianist"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8358208955223881, "ans_precission": 0.875, "ans_recall": 0.8, "path_f1": 0.8358208955223881, "path_precision": 0.875, "path_recall": 0.8, "path_ans_f1": 0.8358208955223881, "path_ans_precision": 0.875, "path_ans_recall": 0.8}
{"id": "WebQTest-138", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> France\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.region -> Europe\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> common.topic.notable_types -> Human Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Basque Language", "Catalan language", "Occitan language", "Spanish Language", "Galician Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.5, "path_precision": 0.6666666666666666, "path_recall": 0.4, "path_ans_f1": 0.7164179104477612, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.6}
{"id": "WebQTest-1380", "prediction": ["# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> common.topic.notable_for -> g.1258bjttw\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> people.profession.specialization_of -> Scientist\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> base.descriptive_names.names.descriptive_name -> m.0101hljz\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Inventor -> common.topic.article -> m.03sbl\n# Answer:\nInventor", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nNikola Tesla -> people.person.profession -> Futurist -> common.topic.notable_types -> Profession\n# Answer:\nFuturist"], "ground_truth": ["Mechanical Engineer", "Futurist", "Physicist", "Scientist", "Electrical engineer", "Inventor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1381", "prediction": ["# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.geolocation -> m.03dtqfp\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> location.location.containedby -> United States of America\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.postal_code.country -> United States of America\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08541 -> common.topic.notable_types -> Postal Code\n# Answer:\n08541", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.contains -> 08540-6449\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540 -> location.location.containedby -> United States of America\n# Answer:\n08540", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08540-6449 -> common.topic.notable_types -> Postal Code\n# Answer:\n08540-6449", "# Reasoning Path:\nPrinceton -> location.citytown.postal_codes -> 08542 -> location.location.contains -> 08542-4511\n# Answer:\n08542"], "ground_truth": ["08542-4511", "08542", "08544", "08540-6449", "08543", "08541", "08540"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 1.0, "ans_recall": 0.5714285714285714, "path_f1": 0.4615384615384615, "path_precision": 0.5, "path_recall": 0.42857142857142855, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1382", "prediction": ["# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Basketball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Brazil\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Chile\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nArgentina -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nArgentina -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60prvn4\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Bolivia", "Brazil", "Chile", "Paraguay", "Uruguay"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.0273972602739726, "path_precision": 0.2222222222222222, "path_recall": 0.014598540145985401, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 0.4}
{"id": "WebQTest-1383", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.writing_system -> Icelandic orthography\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> common.topic.article -> m.01gmks\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6h4x_\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1384", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Samsara -> film.film.featured_film_locations -> Western Wall\n# Answer:\nSamsara", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Defamation -> media_common.netflix_title.netflix_genres -> Judaism\n# Answer:\nDefamation", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Samsara -> film.film.country -> United States of America\n# Answer:\nSamsara", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j7nj\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nIsrael -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc38hll\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1385", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nGeorge Seifert", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera"], "ground_truth": ["Dom Capers", "John Fox", "George Seifert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.5, "path_recall": 0.8, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1387", "prediction": ["# Reasoning Path:\nToronto -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.resource -> City of Toronto Web site\n# Answer:\nCity of Toronto Web site", "# Reasoning Path:\nToronto -> periodicals.newspaper_circulation_area.newspapers -> Toronto Star -> book.newspaper.circulation_areas -> Ontario\n# Answer:\nToronto Star", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.02nc8t6 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> GO Transit -> common.topic.notable_types -> Mass Transportation System\n# Answer:\nGO Transit", "# Reasoning Path:\nToronto -> periodicals.newspaper_circulation_area.newspapers -> Xtra! -> common.topic.article -> m.02dlch\n# Answer:\nXtra!", "# Reasoning Path:\nToronto -> travel.travel_destination.local_transportation -> Toronto rapid transit -> common.topic.article -> m.02skzz\n# Answer:\nToronto rapid transit", "# Reasoning Path:\nToronto -> common.topic.webpage -> m.059h8f0 -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1388", "prediction": ["# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.primarily_containedby -> Western Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.location.partially_containedby -> Europe\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> location.country.languages_spoken -> French\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.place_of_birth -> Saint-Domingue -> common.topic.notable_types -> Location\n# Answer:\nSaint-Domingue", "# Reasoning Path:\nToussaint Louverture -> people.person.nationality -> France -> periodicals.newspaper_circulation_area.newspapers -> La Croix\n# Answer:\nFrance", "# Reasoning Path:\nToussaint Louverture -> people.person.children -> Saint-Jean Louverture -> people.person.gender -> Male\n# Answer:\nSaint-Jean Louverture"], "ground_truth": ["French Revolutionary Army", "Haitian Rebellion of 1891", "French Army"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1389", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard"], "ground_truth": ["Bard", "Writer", "Poet", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-139", "prediction": ["# Reasoning Path:\nNational Rifle Association -> organization.organization.headquarters -> m.04300ty -> location.mailing_address.citytown -> Fairfax\n# Answer:\nFairfax", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> freebase.type_profile.strict_included_types -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nNational Rifle Association -> organization.organization.headquarters -> m.04300ty -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nNational Rifle Association -> organization.organization.leadership -> m.0p9ky_9 -> organization.leadership.person -> David Keene\n# Answer:\nDavid Keene", "# Reasoning Path:\nNational Rifle Association -> common.topic.notable_types -> Membership organization -> type.type.expected_by -> Organization\n# Answer:\nMembership organization", "# Reasoning Path:\nNational Rifle Association -> organization.organization.headquarters -> m.04300ty -> common.topic.notable_for -> g.125h79z0s\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNational Rifle Association -> common.topic.webpage -> m.03lsdwh -> common.webpage.resource -> m.0bkhqtx\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Fairfax"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1390", "prediction": ["# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_holder -> Mark Shurtleff\n# Answer:\nMark Shurtleff", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_holder -> John Swallow\n# Answer:\nJohn Swallow", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zxk98b -> government.government_position_held.office_holder -> Sean Reyes\n# Answer:\nSean Reyes", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0zbfgvy -> government.government_position_held.office_holder -> Jon Cox\n# Answer:\nJon Cox", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0jsklnk -> government.government_position_held.office_holder -> Gary Herbert\n# Answer:\nGary Herbert", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0hzbyqy -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nUtah Attorney General", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0nbwncq -> government.government_position_held.office_position_or_title -> Utah Attorney General\n# Answer:\nUtah Attorney General"], "ground_truth": ["John Swallow", "Sean Reyes", "Mark Shurtleff"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1392", "prediction": ["# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> common.topic.notable_types -> Human Language\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Indonesian Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nIndonesian Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Tobelo Language -> language.human_language.region -> Asia\n# Answer:\nTobelo Language", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Timor-Leste\n# Answer:\nEnglish Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nBali Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.country.languages_spoken -> Bali Language -> language.human_language.region -> Asia\n# Answer:\nBali Language"], "ground_truth": ["Tobelo Language", "English Language", "Malay Language", "Bali Language", "Dutch Language", "Madura Language", "Batak Language", "Sunda Language", "Indonesian Language", "Javanese Language"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.509090909090909, "ans_precission": 0.7, "ans_recall": 0.4, "path_f1": 0.509090909090909, "path_precision": 0.7, "path_recall": 0.4, "path_ans_f1": 0.509090909090909, "path_ans_precision": 0.7, "path_ans_recall": 0.4}
{"id": "WebQTest-1394", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm4\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjm_\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> people.person.profession -> Coach\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> sports.pro_athlete.sports_played_professionally -> m.0d5vt2h\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjmf\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1395", "prediction": ["# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> location.location.partially_containedby -> Lesotho\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> freebase.valuenotation.has_value -> Prominence\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> common.topic.notable_types -> Mountain\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Cleft Peak -> location.location.partially_containedby -> Lesotho\n# Answer:\nCleft Peak", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> geography.mountain.mountain_range -> Drakensberg\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Emlembe -> location.location.partially_containedby -> Swaziland\n# Answer:\nEmlembe", "# Reasoning Path:\nSouth Africa -> location.location.partially_contains -> Njesuthi -> location.location.containedby -> Africa\n# Answer:\nNjesuthi", "# Reasoning Path:\nSouth Africa -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Mozambique\n# Answer:\nAfrica"], "ground_truth": ["Namibia", "Swaziland", "Botswana", "Mozambique", "Zimbabwe", "Lesotho"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.07894736842105263, "path_precision": 0.375, "path_recall": 0.04411764705882353, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1396", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi"], "ground_truth": ["Sebastian Shaw", "David Prowse", "James Earl Jones"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6451612903225806, "ans_precission": 0.625, "ans_recall": 0.6666666666666666, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.6451612903225806, "path_ans_precision": 0.625, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1398", "prediction": ["# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.nominated_for -> Possessed\n# Answer:\nPossessed", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.nominated_for -> Sudden Fear\n# Answer:\nSudden Fear", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.05bz0fg -> award.award_nomination.nominated_for -> Mildred Pierce\n# Answer:\nMildred Pierce", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.07zn0yc -> award.award_nomination.nominated_for -> What Ever Happened to Baby Jane?\n# Answer:\nWhat Ever Happened to Baby Jane?", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.0908_n7 -> award.award_nomination.nominated_for -> Sudden Fear\n# Answer:\nSudden Fear", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlww8 -> award.award_nomination.ceremony -> 20th Academy Awards\n# Answer:\n20th Academy Awards", "# Reasoning Path:\nJoan Crawford -> award.award_nominee.award_nominations -> m.03mlwz0 -> award.award_nomination.ceremony -> 25th Academy Awards\n# Answer:\n25th Academy Awards"], "ground_truth": ["Flamingo Road", "I Saw What You Did", "They All Kissed the Bride", "Sudden Fear", "Trog", "Rain", "The Best of Everything", "Laughing Sinners", "Della", "Four Walls", "I Live My Life", "Love on the Run", "Queen Bee", "The Gorgeous Hussy", "The Understanding Heart", "Chained", "Untamed", "The Damned Don't Cry!", "Strait-Jacket", "The Shining Hour", "Humoresque", "The Unknown", "Sally, Irene and Mary", "Paris", "West Point", "This Modern Age", "The Merry Widow", "Our Blushing Brides", "When Ladies Meet", "The Story of Esther Costello", "The Ice Follies of 1939", "Old Clothes", "The Caretakers", "Harriet Craig", "Tramp, Tramp, Tramp", "Possessed", "Today We Live", "Mildred Pierce", "A Woman's Face", "It's a Great Feeling", "Reunion in France", "Across to Singapore", "Our Modern Maidens", "Twelve Miles Out", "Winners of the Wilderness", "Female on the Beach", "This Woman Is Dangerous", "A Slave of Fashion", "Berserk!", "The Taxi Dancer", "The Karate Killers", "Paid", "Sadie McKee", "Tide of Empire", "The Stolen Jools", "Spring Fever", "Rose-Marie", "Hush\u2026 Hush, Sweet Charlotte", "Autumn Leaves", "The Bride Wore Red", "Susan and God", "Our Dancing Daughters", "Johnny Guitar", "The Duke Steps Out", "Dancing Lady", "Great Day", "Torch Song", "Dream of Love", "Above Suspicion", "Dance, Fools, Dance", "Letty Lynton", "Pretty Ladies", "The Boob", "Mannequin", "What Ever Happened to Baby Jane?", "No More Ladies", "Strange Cargo", "The Hollywood Revue of 1929", "Forsaking All Others", "The Circle", "The Women", "The Law of the Range", "The Last of Mrs. Cheyney", "Harry Langdon: The Forgotten Clown", "Daisy Kenyon", "Montana Moon", "Goodbye, My Fancy", "Grand Hotel"], "ans_acc": 0.045454545454545456, "ans_hit": 1, "ans_f1": 0.08474576271186442, "ans_precission": 0.625, "ans_recall": 0.045454545454545456, "path_f1": 0.10752688172043011, "path_precision": 0.625, "path_recall": 0.058823529411764705, "path_ans_f1": 0.08474576271186442, "path_ans_precision": 0.625, "path_ans_recall": 0.045454545454545456}
{"id": "WebQTest-1399", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Upper Midwest American English -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nUpper Midwest American English", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Northern Mariana Islands\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Nebraska -> location.location.containedby -> United States, with Territories\n# Answer:\nNebraska", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi -> location.location.containedby -> United States, with Territories\n# Answer:\nMississippi", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nSpanish Language", "# Reasoning Path:\nUnited States of America -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nLojban"], "ground_truth": ["Automotive industry", "Food Manufacturing", "Pharmaceutical Preparation", "Industrial Organic Chemicals, NEC"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> common.topic.notable_for -> g.125fm8vqg\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.subdividing_type -> Japanese prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 8 -> common.topic.notable_for -> g.1259_1vs2\n# Answer:\nFukushima I \u2013 8", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.pertinent_type -> Japanese prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1402", "prediction": ["# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.symbol -> Cardinal\n# Answer:\nCardinal", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.0hz9xtw -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.symbol -> Brachiopod\n# Answer:\nBrachiopod", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.symbol -> Liriodendron\n# Answer:\nLiriodendron", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04st742 -> location.location_symbol_relationship.symbol -> Milk\n# Answer:\nMilk", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04st93t -> location.location_symbol_relationship.symbol -> Viceroy\n# Answer:\nViceroy", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0d65 -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nKentucky -> government.governmental_jurisdiction.official_symbols -> m.04l0nqp -> location.location_symbol_relationship.Kind_of_symbol -> State tree\n# Answer:\nState tree"], "ground_truth": ["Cardinal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1403", "prediction": ["# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.office_holder -> Jean Chr\u00e9tien\n# Answer:\nJean Chr\u00e9tien", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.office_holder -> Brian Mulroney\n# Answer:\nBrian Mulroney", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kmh11 -> government.government_position_held.office_holder -> Alexander Mackenzie\n# Answer:\nAlexander Mackenzie", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.jurisdiction_of_office -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.0bghq8m -> government.government_position_held.office_holder -> Alexander Mackenzie\n# Answer:\nAlexander Mackenzie", "# Reasoning Path:\nPrime Minister of Canada -> base.schemastaging.context_name.pronunciation -> g.125_rtp7r\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kc_7_ -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nParliament of Canada", "# Reasoning Path:\nPrime Minister of Canada -> base.firsts.achievement.firsts -> m.04wvp27 -> base.firsts.first_achievement.first -> Kim Campbell\n# Answer:\nKim Campbell", "# Reasoning Path:\nPrime Minister of Canada -> government.government_office_or_title.office_holders -> m.04kk22r -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister"], "ground_truth": ["Kim Campbell", "Brian Mulroney", "Jean Chr\u00e9tien", "Alexander Mackenzie"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.6666666666666667, "path_precision": 0.5555555555555556, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1405", "prediction": ["# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> common.topic.notable_types -> Venue\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> location.location.geolocation -> m.0cnclfr\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> common.topic.image -> Red Butte Garden2\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Hogle Zoo -> common.topic.notable_types -> Zoo\n# Answer:\nHogle Zoo", "# Reasoning Path:\nSalt Lake City -> location.location.nearby_airports -> Salt Lake City International Airport -> aviation.airport.hub_for -> Western Airlines\n# Answer:\nSalt Lake City International Airport", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Red Butte Garden and Arboretum -> common.topic.article -> m.05qp9w\n# Answer:\nRed Butte Garden and Arboretum", "# Reasoning Path:\nSalt Lake City -> location.statistical_region.population -> g.11b66h2b_k\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSalt Lake City -> travel.travel_destination.tourist_attractions -> Hogle Zoo -> location.location.geolocation -> m.0clz46_\n# Answer:\nHogle Zoo", "# Reasoning Path:\nSalt Lake City -> location.location.nearby_airports -> Salt Lake City International Airport -> aviation.airport.hub_for -> Ameriflight\n# Answer:\nSalt Lake City International Airport"], "ground_truth": ["Church History Museum", "Antelope Island State Park", "Hogle Zoo", "Sugar House Park", "Days of '47 Parade", "Temple Square", "Bryce Canyon National Park", "Discovery Gateway", "Salt Lake City Public Library", "Clark Planetarium", "Salt Lake Temple", "Zion National Park", "g.121xqqc4", "Red Butte Garden and Arboretum", "Trolley Square"], "ans_acc": 0.13333333333333333, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.6666666666666666, "ans_recall": 0.13333333333333333, "path_f1": 0.2222222222222222, "path_precision": 0.6666666666666666, "path_recall": 0.13333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.13333333333333333}
{"id": "WebQTest-1408", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Armed Services -> government.legislative_committee.members -> m.02_wjvp\n# Answer:\nUnited States Senate Committee on Armed Services", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Commerce, Science, and Transportation -> government.legislative_committee.members -> m.02_wjxj\n# Answer:\nUnited States Senate Committee on Commerce, Science, and Transportation", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.jurisdiction -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Appropriations -> government.legislative_committee.members -> m.05nls2p\n# Answer:\nUnited States Senate Committee on Appropriations", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.committees -> United States Senate Committee on Armed Services -> government.legislative_committee.members -> m.02_wk09\n# Answer:\nUnited States Senate Committee on Armed Services"], "ground_truth": ["Sherrod Brown", "Rob Portman"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1409", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nEnglish Language"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-141", "prediction": ["# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0gwn5ny\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> fictional_universe.fictional_character.organizations -> Thorin and Company\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> film.film_character.portrayed_in_films -> m.0y623dj\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> common.topic.notable_types -> Film character\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.parents -> Gloin -> book.book_character.appears_in_book -> The Hobbit\n# Answer:\nGloin", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03l6qbh -> film.performance.actor -> John Rhys-Davies\n# Answer:\nJohn Rhys-Davies", "# Reasoning Path:\nGimli -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Middle-earth -> fictional_universe.fictional_universe.species -> Hobbit\n# Answer:\nMiddle-earth", "# Reasoning Path:\nGimli -> film.film_character.portrayed_in_films -> m.03wlpyc -> film.performance.actor -> John Rhys-Davies\n# Answer:\nJohn Rhys-Davies"], "ground_truth": ["Gloin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1410", "prediction": ["# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> book.book.editions -> Saint Nicholas of Myra, Bari, and Manhattan : biography of a legend\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> common.topic.notable_for -> g.1256xmt38\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> book.book.genre -> Biography\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> common.topic.article -> m.0f9qj\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSaint Nicholas -> book.book_subject.works -> St. Nicholas of Myra, Bari and Manhattan: Biography of a Legend -> common.topic.notable_types -> Book\n# Answer:\nSt. Nicholas of Myra, Bari and Manhattan: Biography of a Legend", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> location.location.people_born_here -> Aedesius of Alexandria\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> people.person.place_of_birth -> Patara, Lycia -> travel.travel_destination.tourist_attractions -> Patara Beach\n# Answer:\nPatara, Lycia", "# Reasoning Path:\nSaint Nicholas -> symbols.name_source.namesakes -> St. Nicholas' Collegiate Church -> common.topic.article -> m.01bx26\n# Answer:\nSt. Nicholas' Collegiate Church", "# Reasoning Path:\nSaint Nicholas -> common.topic.notable_types -> Deceased Person -> type.type.expected_by -> Letter Recipient\n# Answer:\nDeceased Person"], "ground_truth": ["Bishop"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1414", "prediction": ["# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Myocardial Ischemia\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Old age\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.symptoms -> Ventricular tachycardia\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> people.deceased_person.cause_of_death -> heart attack -> medicine.risk_factor.diseases -> Major depression\n# Answer:\nheart attack", "# Reasoning Path:\nPope John Paul II -> common.topic.webpage -> m.09xfw5c -> common.webpage.resource -> One pope to rule them all...\n# Answer:\nOne pope to rule them all..."], "ground_truth": ["Parkinson's disease", "heart attack"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-1416", "prediction": ["# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.subjects -> Nathalie Kosciusko-Morizet\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.subject_of -> Jimi Agbaje\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> people.person.profession -> Politician -> common.topic.subjects -> Akinwunmi Ambode\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin Pierce -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1417", "prediction": ["# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> sports.sports_facility.teams -> Hartford Whalers\n# Answer:\nBoston Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Celtics\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 2011 Stanley Cup Finals -> time.event.locations -> Rogers Arena\n# Answer:\n2011 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> location.location.geolocation -> m.0clmg5x\n# Answer:\nBoston Garden", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1929 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1929 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.championships -> 1939 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1939 Stanley Cup Finals", "# Reasoning Path:\nBoston Bruins -> sports.sports_team.arena_stadium -> Boston Garden -> sports.sports_facility.teams -> Boston Braves\n# Answer:\nBoston Garden"], "ground_truth": ["2013 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1418", "prediction": ["# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.married_to -> m.0gx44zk -> fictional_universe.marriage_of_fictional_characters.spouses -> Derek Shepherd\n# Answer:\nDerek Shepherd", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.actor -> Giuppy Izzo\n# Answer:\nGiuppy Izzo", "# Reasoning Path:\nMeredith Grey -> tv.tv_character.appeared_in_tv_program -> m.02nv3j_ -> tv.regular_tv_appearance.actor -> Ellen Pompeo\n# Answer:\nEllen Pompeo", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.title -> Surgeon\n# Answer:\nSurgeon", "# Reasoning Path:\nMeredith Grey -> base.italiantv.adapted_tv_character.dubbing_performances -> m.09tms_9 -> base.italiantv.tv_dubbing_performance.tv_program -> Italian adaptation of Grey's Anatomy\n# Answer:\nItalian adaptation of Grey's Anatomy", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.employers -> m.0k53dtw -> fictional_universe.fictional_employment_tenure.employer -> Seattle Grace Mercy West Hospital\n# Answer:\nSeattle Grace Mercy West Hospital", "# Reasoning Path:\nMeredith Grey -> fictional_universe.fictional_character.character_created_by -> Shonda Rhimes -> common.topic.webpage -> m.09w006q\n# Answer:\nShonda Rhimes", "# Reasoning Path:\nMeredith Grey -> tv.tv_character.appeared_in_tv_program -> m.02nv3j_ -> tv.regular_tv_appearance.series -> Grey's Anatomy\n# Answer:\nGrey's Anatomy"], "ground_truth": ["Chris Ivery"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1419", "prediction": ["# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Alesso Baldovinetti\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Melozzo da Forl\u00ec -> influence.influence_node.influenced_by -> Piero della Francesca\n# Answer:\nMelozzo da Forl\u00ec", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Masaccio\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Lorenzo de' Medici -> influence.influence_node.influenced -> Giovanni Pico della Mirandola\n# Answer:\nLorenzo de' Medici", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Girolamo Savonarola -> influence.influence_node.influenced -> John Colet\n# Answer:\nGirolamo Savonarola", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Donatello -> influence.influence_node.influenced_by -> Lorenzo de' Medici\n# Answer:\nDonatello", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> influence.influence_node.influenced_by -> Andrea del Verrocchio\n# Answer:\nDomenico Ghirlandaio", "# Reasoning Path:\nMichelangelo -> influence.influence_node.influenced_by -> Domenico Ghirlandaio -> visual_art.visual_artist.associated_periods_or_movements -> Italian Renaissance\n# Answer:\nDomenico Ghirlandaio"], "ground_truth": ["Girolamo Savonarola", "Domenico Ghirlandaio", "Donatello", "Giovanni Pico della Mirandola", "Melozzo da Forl\u00ec", "Lorenzo de' Medici"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9090909090909091, "ans_precission": 1.0, "ans_recall": 0.8333333333333334, "path_f1": 0.9090909090909091, "path_precision": 1.0, "path_recall": 0.8333333333333334, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-142", "prediction": ["# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> law.inventor.inventions -> The Internet\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> base.schemastaging.context_name.pronunciation -> g.125_qdtmg\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.gender -> Male\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> common.topic.notable_for -> g.12580mc43\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> law.inventor.inventions -> Internet Protocol\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Vint Cerf -> law.inventor.inventions -> The Internet\n# Answer:\nVint Cerf", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1q_\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> base.argumentmaps.original_idea.innovator -> Robert  E. Kahn -> law.inventor.inventions -> The Internet\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> people.person.employment_history -> m.04_j1qm\n# Answer:\nRobert  E. Kahn", "# Reasoning Path:\nTransmission Control Protocol -> law.invention.inventor -> Robert  E. Kahn -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nRobert  E. Kahn"], "ground_truth": ["Vint Cerf", "Robert  E. Kahn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1420", "prediction": ["# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AMERICAN CRYSTAL SUGAR CO /MN/\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> common.topic.article -> m.0hkr\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> freebase.valuenotation.has_value -> NAICS 2007 code\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> organization.organization.child -> m.09pdql9 -> organization.organization_relationship.child -> Mahyco Monsanto Biotech (I) Pvt. Ltd.\n# Answer:\nMahyco Monsanto Biotech (I) Pvt. Ltd.", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> AVEBE\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agrochemical -> base.nasagcmd.gcmd_keyword1.includes_keyword_s -> Pesticide\n# Answer:\nAgrochemical", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> Adler Seeds\n# Answer:\nAgriculture", "# Reasoning Path:\nMonsanto -> business.business_operation.industry -> Agriculture -> business.industry.companies -> Aeromeh\n# Answer:\nAgriculture"], "ground_truth": ["Seed", "Agriculture", "Agrochemical", "Chemical industry"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-1421", "prediction": ["# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.character -> Norm Peterson\n# Answer:\nNorm Peterson", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.0wyv_l7 -> tv.regular_tv_appearance.character -> Paul Krapence\n# Answer:\nPaul Krapence", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0vm_d1v -> tv.regular_tv_appearance.series -> Game Change\n# Answer:\nGame Change", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.02t8y53 -> tv.regular_tv_appearance.character -> Woody Boyd\n# Answer:\nWoody Boyd", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.actor -> George Wendt\n# Answer:\nGeorge Wendt", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.01y0qy_ -> tv.regular_tv_appearance.character -> Frasier Crane\n# Answer:\nFrasier Crane", "# Reasoning Path:\nWoody Harrelson -> tv.tv_actor.starring_roles -> m.0_0mgc5 -> tv.regular_tv_appearance.character -> Martin Hart\n# Answer:\nMartin Hart", "# Reasoning Path:\nCheers -> tv.tv_program.regular_cast -> m.02hsl63 -> tv.regular_tv_appearance.seasons -> Cheers - Season 11\n# Answer:\nCheers - Season 11"], "ground_truth": ["Woody Boyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1422", "prediction": ["# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> language.human_language.region -> Europe\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.official_language -> Swedish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSwedish Language", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.country.languages_spoken -> Yiddish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nYiddish Language"], "ground_truth": ["Finnish Language", "Romani language", "Yiddish Language", "Swedish Language", "Swedish Sign Language", "Me\u00e4nkieli", "Turoyo Language"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1423", "prediction": ["# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> India\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.country.languages_spoken -> Dagbani Language -> base.rosetta.languoid.parent -> Southeast Western Group\n# Answer:\nDagbani Language", "# Reasoning Path:\nGhana -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Nigeria\n# Answer:\nEnglish Language", "# Reasoning Path:\nGhana -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60ywwvy\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nGhana -> location.statistical_region.external_debt_stock -> g.11b71px2jn\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["\u00c9w\u00e9 Language", "Dagaare language", "Asante dialect", "Fula language", "English Language", "Dangme Language", "Dagbani Language", "Kasem Language", "Akan Language", "Gonja Language", "Ga Language", "Nzema Language"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.27586206896551724, "ans_precission": 0.8, "ans_recall": 0.16666666666666666, "path_f1": 0.1818181818181818, "path_precision": 0.2, "path_recall": 0.16666666666666666, "path_ans_f1": 0.27586206896551724, "path_ans_precision": 0.8, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1424", "prediction": ["# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Thailand\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Republic\n# Answer:\nRepublic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.statistical_region.external_debt_stock -> g.11b71qs22v\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Singapore\n# Answer:\nUnitary state", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Indonesia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPhilippines -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> South Korea\n# Answer:\nUnitary state"], "ground_truth": ["Republic", "Constitutional republic", "Unitary state", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-1425", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0125cyb9 -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.religion -> Mormonism -> religion.religion.organizations -> Brigham Young University\n# Answer:\nMormonism", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["Harvard Business School", "Harvard Law School", "Harvard University", "Stanford University", "Brigham Young University"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.375, "ans_recall": 0.6, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.5, "path_ans_recall": 0.8}
{"id": "WebQTest-1426", "prediction": ["# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Kingdom of the Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.administrative_division.second_level_division_of -> Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.location.containedby -> Netherlands\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> soccer.football_player.statistics -> m.0w8x1wl -> soccer.football_player_stats.team -> PSV Eindhoven\n# Answer:\nPSV Eindhoven", "# Reasoning Path:\nIbrahim Afellay -> common.image.size -> m.04zt8k9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nIbrahim Afellay -> soccer.football_player.statistics -> m.0w9fm16 -> soccer.football_player_stats.team -> Netherlands national football team\n# Answer:\nNetherlands national football team", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> location.statistical_region.population -> g.1jmcbdzl0\n# Answer:\nUtrecht", "# Reasoning Path:\nIbrahim Afellay -> soccer.football_player.statistics -> m.0w9fn38 -> soccer.football_player_stats.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nIbrahim Afellay -> people.person.place_of_birth -> Utrecht -> travel.travel_destination.tourist_attractions -> Centraal Museum\n# Answer:\nUtrecht"], "ground_truth": ["Utrecht"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1427", "prediction": ["# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nEdward Livingston", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_holder -> Louis McLane\n# Answer:\nLouis McLane", "# Reasoning Path:\nAndrew Jackson -> government.us_president.vice_president -> Martin Van Buren -> government.us_president.vice_president -> Richard Mentor Johnson\n# Answer:\nMartin Van Buren", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.08v_t1g -> government.government_position_held.office_holder -> James Alexander Hamilton\n# Answer:\nJames Alexander Hamilton", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04m1bbr -> government.government_position_held.office_holder -> Louis McLane\n# Answer:\nLouis McLane", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.0bws0wh -> government.government_position_held.office_position_or_title -> United States Secretary of State\n# Answer:\nUnited States Secretary of State", "# Reasoning Path:\nAndrew Jackson -> government.political_appointer.appointees -> m.04kq1q9 -> government.government_position_held.office_position_or_title -> United States Secretary of the Treasury\n# Answer:\nUnited States Secretary of the Treasury"], "ground_truth": ["Edward Livingston", "Martin Van Buren", "James Alexander Hamilton", "Louis McLane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1428", "prediction": ["# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nRon Stoppable -> tv.tv_character.appeared_in_tv_program -> m.03jspv_ -> tv.regular_tv_appearance.actor -> Will Friedle\n# Answer:\nWill Friedle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale", "# Reasoning Path:\nRon Stoppable -> film.film_character.portrayed_in_films -> m.0j_gwp -> film.performance.actor -> Will Friedle\n# Answer:\nWill Friedle", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRon Stoppable -> common.topic.article -> m.09w3my\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nRon Stoppable -> fictional_universe.fictional_character.gender -> Male -> common.topic.article -> m.05zpq8\n# Answer:\nMale"], "ground_truth": ["Will Friedle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.2222222222222222, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1429", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Tokelau\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Cook Islands\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> location.country.currency_used -> New Zealand dollar -> finance.currency.countries_used -> Niue\n# Answer:\nNew Zealand dollar", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> film.film.country -> United States of America\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> location.statistical_region.gni_in_ppp_dollars -> g.1245_22l4\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> olympics.olympic_participating_country.medals_won -> m.04gdvs5 -> olympics.olympic_medal_honor.medal -> Gold medal\n# Answer:\nGold medal", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> film.film.genre -> Family\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> common.topic.notable_types -> Film\n# Answer:\nYogi Bear"], "ground_truth": ["New Zealand dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1432", "prediction": ["# Reasoning Path:\nWest Midlands -> base.aareas.schema.administrative_area.administrative_children -> Birmingham -> location.administrative_division.country -> United Kingdom\n# Answer:\nBirmingham", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nWest Midlands -> location.administrative_division.second_level_division_of -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWest Midlands -> location.administrative_division.country -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Edgbaston", "Stowheath", "Woodcross", "Bilston", "Tettenhall", "Boldmere", "Wednesfield", "Horseley Fields", "Bromford", "Warwickshire", "University of Wolverhampton, Compton Park Campus", "Lyndon School, Solihull", "King Edward VI College, Stourbridge", "Rushall, West Midlands", "Pensnett", "Smith's Wood", "Aston", "Clayhanger, West Midlands", "Aldridge", "Oxley, Wolverhampton", "Bickenhill", "University of Wolverhampton, Walsall Campus", "Gornal, West Midlands", "Cradley Heath", "Quinton, Birmingham", "Bloxwich", "Weoley Castle", "Catherine-de-Barnes", "Park Village", "Brownhills", "Sandwell College, West Bromwich", "Cheylesmore", "Wall Heath", "Lozells and East Handsworth", "Bearwood, West Midlands", "Sutton Trinity", "Yardley Wood", "Selly Park", "Fordhouses", "Blakenall Heath", "Sutton New Hall", "Redhill School, Stourbridge", "New Frankley", "Amblecote", "Hockley", "Wollaston", "Mary Arden's Farm", "Lye", "Chad Valley, Birmingham", "Bartley Green", "Erdington", "Stourbridge", "National School of Blacksmithing, Holme Lacy campus", "Telford and Wrekin", "Soho, West Midlands", "Northfield, Birmingham", "Earlsdon", "Aston Ramsden", "Sheldon, West Midlands", "Coventry University", "Handsworth, West Midlands", "Oldbury", "Netherton", "Ashted", "Knowle, West Midlands", "Low Hill", "Shropshire", "Walsall", "Saltley", "Bordesley Green", "The Sixth Form College, Solihull", "Redditch", "Stechford and Yardley North", "Perry Barr", "Springfield, Birmingham", "Brierley Hill", "Wylde Green", "Handsworth Wood", "Woodsetton, Dudley", "Chelmsley Wood", "West Heath, West Midlands", "Rednal", "Sandwell College, Smethwick", "Tividale", "Ashmore Park", "City College, Birmingham", "Solihull", "Herefordshire", "Darlaston", "Bradley, West Midlands", "West Bromwich", "Kings Heath", "Bournville", "South Yardley", "Longbridge", "Scotlands Estate", "Great Bridge, West Midlands", "Dovecotes", "Sutton Four Oaks", "Druids Heath", "Meriden", "Kings Norton", "Metropolitan Borough of Walsall", "Moor Pool", "Nechells", "Allesley", "University of Wolverhampton, City Campus", "Brownhills West", "Barston", "Great Barr", "City College Coventry", "Staffordshire", "Middleton", "Castle Bromwich", "Fordbridge", "Pelsall", "St Francis of Assisi Catholic Technology College", "Sandwell", "Berkswell", "Wordsley", "Hampton-in-Arden", "Aston Business School", "Walsall College, Wisemore campus", "Stone Cross, West Midlands", "Whitmore Reans", "Willenhall", "Smethwick", "Marston Green", "Oscott", "Chapelfields", "Dudley College", "Sedgley", "Patton", "Dunstall Hill", "Birmingham", "Billesley, West Midlands", "Compton, Wolverhampton", "Tudor Hill", "Sparkbrook", "Tipton", "Brandhall", "Dorridge", "Harborne", "City of Wolverhampton College", "Norton, Dudley", "Tyburn, West Midlands", "Walsall Wood", "The Public, West Bromwich", "Alumwell Business and Enterprise College", "Hall Green", "Selly Oak", "Washwood Heath", "Stockland Green", "Coventry", "Wolverhampton", "Brandwood End", "Moseley and Kings Heath", "Turners Hill, West Midlands", "Dudley", "Bushbury", "Hamstead, West Midlands", "Olton", "Shard End", "Brandwood", "Brades Village", "Bournbrook", "Streetly", "Chadwick End", "Merridale", "Kingstanding", "Hodge Hill", "Worcestershire", "Oakham, West Midlands", "Bishopgate Green", "Pendeford", "Coseley", "Acocks Green", "Castlecroft", "Kingswinford", "Aston University, Birmingham", "Rowley Regis", "Walsall College, Green Lane campus", "Sutton Vesey", "Gorsebrook", "Wednesbury", "Balsall Common", "Sandwell College, Oldbury", "Lode Heath School and Specialist College", "Allesley Green", "Metropolitan Borough of Dudley", "Blackheath", "Blakenhall", "Ladywood", "Stoke-on-Trent", "Metropolitan Borough of Solihull", "California, Birmingham", "Stourton", "Shirley, West Midlands", "Walmley", "Jewellery Quarter", "West Midlands", "Birmingham City University", "Castle Vale", "Barr Beacon", "Kingshurst", "Blossomfield", "Quarry Bank", "Radford, Coventry", "Halesowen", "Hockley Heath"], "ans_acc": 0.009523809523809525, "ans_hit": 1, "ans_f1": 0.009345794392523366, "ans_precission": 0.25, "ans_recall": 0.004761904761904762, "path_f1": 0.08163265306122448, "path_precision": 1.0, "path_recall": 0.0425531914893617, "path_ans_f1": 0.01886792452830189, "path_ans_precision": 1.0, "path_ans_recall": 0.009523809523809525}
{"id": "WebQTest-1433", "prediction": ["# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> common.topic.image -> CarlSandburgNHS\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> travel.tourist_attraction.near_travel_destination -> Roanoke\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> protected_sites.protected_site.governing_body -> U.S. National Park Service\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> location.statistical_region.population -> g.11b66ff3kh\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> common.topic.notable_for -> g.125hbcvj0\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> transportation.road.end2 -> m.0123xlpk\n# Answer:\nBlue Ridge Parkway", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Carl Sandburg Home National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h13hb\n# Answer:\nCarl Sandburg Home National Historic Site", "# Reasoning Path:\nAsheville -> location.location.nearby_airports -> Asheville Regional Airport -> aviation.airport.serves -> Western Carolina University\n# Answer:\nAsheville Regional Airport", "# Reasoning Path:\nAsheville -> travel.travel_destination.tourist_attractions -> Blue Ridge Parkway -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBlue Ridge Parkway"], "ground_truth": ["Asheville Art Museum", "Blue Ridge Parkway", "Pisgah National Forest", "Biltmore Estate", "The Omni Grove Park Inn", "Thomas Wolfe House", "Smith-McDowell House", "Folk Art Center", "Carl Sandburg Home National Historic Site", "Bele Chere"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.3181818181818182, "ans_precission": 0.7777777777777778, "ans_recall": 0.2, "path_f1": 0.3181818181818182, "path_precision": 0.7777777777777778, "path_recall": 0.2, "path_ans_f1": 0.3181818181818182, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.2}
{"id": "WebQTest-1434", "prediction": ["# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> baseball.baseball_team.current_manager -> Tim Bogar\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> sports.sports_team.previously_known_as -> Kansas City Blues\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> business.employer.employees -> g.11b88cjhcb\n# Answer:\nbusiness.employer.employees", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> sports.professional_sports_team.draft_picks -> m.04vw_l5\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> baseball.baseball_team.team_stats -> m.05lk3qd\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Minnesota Twins -> baseball.baseball_team.current_manager -> Ron Gardenhire\n# Answer:\nMinnesota Twins", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Texas Rangers -> sports.professional_sports_team.draft_picks -> m.04vw_n1\n# Answer:\nTexas Rangers", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> baseball.baseball_team.team_stats -> m.05n5_09\n# Answer:\nWashington Senators", "# Reasoning Path:\nAmerican League -> baseball.baseball_league.teams -> Washington Senators -> freebase.valuenotation.is_reviewed -> Member of\n# Answer:\nWashington Senators"], "ground_truth": ["Detroit Tigers", "Anaheim Angels", "Seattle Mariners", "Kansas City Royals", "Texas Rangers", "Chicago White Sox", "Oakland Athletics", "New York Highlanders", "Houston Astros", "Chicago White Stockings", "Baltimore Orioles", "California Angels", "Los Angeles Angels of Anaheim", "Boston Red Sox", "Seattle Pilots", "Washington Senators", "Tampa Bay Rays", "Tampa Bay Devil Rays", "Cleveland Indians", "Toronto Blue Jays", "Minnesota Twins", "Cleveland Naps", "Philadelphia Athletics", "Milwaukee Brewers", "St. Louis Browns", "New York Yankees", "Kansas City Athletics"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19753086419753085, "ans_precission": 0.8888888888888888, "ans_recall": 0.1111111111111111, "path_f1": 0.16161616161616163, "path_precision": 0.8888888888888888, "path_recall": 0.08888888888888889, "path_ans_f1": 0.19753086419753085, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1435", "prediction": ["# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.team -> St. Louis Rams\n# Answer:\nSt. Louis Rams", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd999 -> sports.sports_team_roster.team -> New York Giants\n# Answer:\nNew York Giants", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.045wt3q -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.04_ngdv -> sports.sports_team_roster.team -> Amsterdam Admirals\n# Answer:\nAmsterdam Admirals", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.05bd99t -> sports.sports_team_roster.team -> Iowa Barnstormers\n# Answer:\nIowa Barnstormers", "# Reasoning Path:\nKurt Warner -> sports.pro_athlete.teams -> m.0hprztb -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nKurt Warner -> american_football.football_player.passing -> m.09tcvy0 -> american_football.player_passing_statistics.team -> Arizona Cardinals\n# Answer:\nArizona Cardinals"], "ground_truth": ["Amsterdam Admirals", "St. Louis Rams", "Arizona Cardinals", "Iowa Barnstormers", "New York Giants", "Northern Iowa Panthers football"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.7894736842105262, "ans_precission": 0.75, "ans_recall": 0.8333333333333334, "path_f1": 0.17910447761194032, "path_precision": 0.75, "path_recall": 0.1016949152542373, "path_ans_f1": 0.7894736842105262, "path_ans_precision": 0.75, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1436", "prediction": ["# Reasoning Path:\nDirk Nowitzki -> people.person.children -> Malaika Nowitzki -> common.topic.notable_for -> g.11b75n3xz8\n# Answer:\nMalaika Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.children -> Malaika Nowitzki -> common.topic.notable_types -> Person\n# Answer:\nMalaika Nowitzki", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.spouse -> Jessica Olsson\n# Answer:\nJessica Olsson", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nDirk Nowitzki -> people.person.languages -> English Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nEnglish Language", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nDirk Nowitzki -> people.person.spouse_s -> m.0r8dkzz -> people.marriage.location_of_ceremony -> Dallas\n# Answer:\nDallas", "# Reasoning Path:\nDirk Nowitzki -> award.award_winner.awards_won -> m.010f1rwg -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work"], "ground_truth": ["Jessica Olsson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1441", "prediction": ["# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> travel.travel_destination.tourist_attractions -> Museo Picasso M\u00e1laga\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> people.deceased_person.place_of_death -> Paris\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> people.person.places_lived -> m.0wfdzk8 -> people.place_lived.location -> Madrid\n# Answer:\nMadrid", "# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> location.location.time_zones -> Central European Time Zone\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> people.person.place_of_birth -> M\u00e1laga -> common.topic.notable_types -> City/Town/Village\n# Answer:\nM\u00e1laga", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> \u00c9douard Manet -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\n\u00c9douard Manet"], "ground_truth": ["M\u00e1laga", "France", "Madrid"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.13793103448275865, "path_precision": 0.5, "path_recall": 0.08, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1442", "prediction": ["# Reasoning Path:\nCroatia -> common.topic.notable_for -> g.1256bxl73\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.article -> m.02z8k3\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.notable_types -> Currency\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.notable_for -> g.125b9qw7_\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.11b60t8rf2\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1ls_\n# Answer:\nCroatian kuna", "# Reasoning Path:\nCroatia -> location.statistical_region.labor_participation_rate -> g.11b71w7m15\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nCroatia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8p\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nCroatia -> location.statistical_region.gdp_growth_rate -> g.1245_1jbw\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nCroatia -> location.country.currency_used -> Croatian kuna -> common.topic.webpage -> m.04m1lsv\n# Answer:\nCroatian kuna"], "ground_truth": ["Croatian kuna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1443", "prediction": ["# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjg3 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> sports.pro_athlete.teams -> m.0zs_gld -> sports.sports_team_roster.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjhh -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjk4 -> baseball.batting_statistics.team -> Seattle Mariners\n# Answer:\nSeattle Mariners", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjc3 -> baseball.batting_statistics.season -> 1996 Major League Baseball Season\n# Answer:\n1996 Major League Baseball Season", "# Reasoning Path:\nRa\u00fal Iba\u00f1ez -> baseball.baseball_player.batting_stats -> m.06qyjdq -> baseball.batting_statistics.season -> 1997 Major League Baseball Season\n# Answer:\n1997 Major League Baseball Season"], "ground_truth": ["Kansas City Royals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1445", "prediction": ["# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0vz6kq1 -> award.award_nomination.nominated_for -> Eleanor and Franklin: The White House Years\n# Answer:\nEleanor and Franklin: The White House Years", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7v5 -> award.award_nomination.nominated_for -> Reasonable Doubts\n# Answer:\nReasonable Doubts", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0vz6kq1 -> award.award_nomination.ceremony -> 29th Primetime Emmy Awards\n# Answer:\n29th Primetime Emmy Awards", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0n4mxrn -> award.award_nomination.nominated_for -> NCIS\n# Answer:\nNCIS", "# Reasoning Path:\nMark Harmon -> tv.tv_actor.starring_roles -> m.0bgtlxt -> tv.regular_tv_appearance.series -> From the Earth to the Moon\n# Answer:\nFrom the Earth to the Moon", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.0vz6kq1 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Performance By A Supporting Actor In A Comedy Or Drama Special\n# Answer:\nPrimetime Emmy Award for Outstanding Performance By A Supporting Actor In A Comedy Or Drama Special", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.07t74mx -> award.award_nomination.nominated_for -> The West Wing\n# Answer:\nThe West Wing", "# Reasoning Path:\nMark Harmon -> award.award_nominee.award_nominations -> m.090f7q8 -> award.award_nomination.nominated_for -> Reasonable Doubts\n# Answer:\nReasonable Doubts"], "ground_truth": ["The Dream Merchants", "From the Earth to the Moon", "Reasonable Doubts", "Flamingo Road", "NCIS", "Eleanor and Franklin: The White House Years", "St. Elsewhere", "Centennial", "240-Robert", "Charlie Grace", "Chicago Hope", "Sam"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.43478260869565216, "ans_precission": 0.625, "ans_recall": 0.3333333333333333, "path_f1": 0.37037037037037035, "path_precision": 0.625, "path_recall": 0.2631578947368421, "path_ans_f1": 0.43478260869565216, "path_ans_precision": 0.625, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1446", "prediction": ["# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2005 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> United States of America\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2014 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 657\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.events -> 2009 SoCal Independent Film Festival\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area code 714\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> location.location.containedby -> Area codes 657 and 714\n# Answer:\nHuntington Beach", "# Reasoning Path:\nThe Rev -> people.deceased_person.place_of_death -> Huntington Beach -> common.topic.image -> Huntington Beach Pier\n# Answer:\nHuntington Beach"], "ground_truth": ["2009-12-28"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1447", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nJake Lloyd", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.actor -> Michael Fox\n# Answer:\nMichael Fox", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.02sg5rn -> film.performance.character -> Darth Vader\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l6zw -> film.performance.actor -> Dr. Smoov\n# Answer:\nDr. Smoov", "# Reasoning Path:\nStar Wars Episode I: The Phantom Menace -> film.film.starring -> m.0130_9qs -> film.performance.actor -> Peter Serafinowicz\n# Answer:\nPeter Serafinowicz"], "ground_truth": ["Jake Lloyd"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1448", "prediction": ["# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.places_lived -> m.04hdch1 -> people.place_lived.location -> Orange County\n# Answer:\nOrange County", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> California\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.primarily_containedby -> Area codes 657 and 714\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.hud_county_place.county -> Orange County\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> people.person.place_of_birth -> Santa Ana -> location.location.containedby -> Orange County\n# Answer:\nSanta Ana", "# Reasoning Path:\nMichelle Pfeiffer -> award.award_nominee.award_nominations -> m.021yhcs -> award.award_nomination.award -> Academy Award for Best Actress\n# Answer:\nAcademy Award for Best Actress"], "ground_truth": ["Orange County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1449", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Teacher -> common.topic.notable_types -> Profession\n# Answer:\nTeacher", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> common.topic.notable_types -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> common.topic.subjects -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nLaura Ingalls Wilder -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Edmund Grey\n# Answer:\nJournalist"], "ground_truth": ["Author", "Writer", "Journalist", "Teacher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8076923076923077, "path_precision": 0.875, "path_recall": 0.75, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-145", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k03_k5 -> education.education.institution -> ETH Zurich\n# Answer:\nETH Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0j_z4g5 -> education.education.institution -> Luitpold Gymnasium\n# Answer:\nLuitpold Gymnasium", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.has_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0ngycdd -> education.education.institution -> Aargau Cantonal School\n# Answer:\nAargau Cantonal School", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> freebase.valuenotation.is_reviewed -> Minor\n# Answer:\nMinor"], "ground_truth": ["ETH Zurich", "University of Zurich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.375, "path_recall": 0.6, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1452", "prediction": ["# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> base.biblioness.bibs_location.state -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0w7fk1d -> sports.sports_team_roster.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nDwight Howard -> basketball.basketball_player.player_statistics -> m.04qh0tc -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.hud_county_place.county -> Fulton County\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0w7fk1d -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDwight Howard -> sports.pro_athlete.teams -> m.0kq8zyx -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers"], "ground_truth": ["Houston Rockets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1453", "prediction": ["# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> people.person.parents -> Vanessa Lachey\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_for -> g.1259plkqz\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> people.marriage.spouse -> Jessica Simpson\n# Answer:\nJessica Simpson", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> common.topic.notable_types -> Person\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.children -> Camden John Lachey -> people.person.gender -> Male\n# Answer:\nCamden John Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.0gyp51b -> people.marriage.spouse -> Vanessa Lachey\n# Answer:\nVanessa Lachey", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNick Lachey -> people.person.spouse_s -> m.07sl20z -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Vanessa Lachey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1454", "prediction": ["# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.containedby -> United States of America\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Madonna Louise Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> people.person.place_of_birth -> Bay City -> location.location.people_born_here -> Christopher Ciccone\n# Answer:\nBay City", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Causing a Commotion -> music.composition.composer -> Stephen Bray\n# Answer:\nCausing a Commotion", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.composer -> Mirwais Ahmadza\u00ef\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> music.composer.compositions -> This Used to Be My Playground -> music.composition.composer -> Shep Pettibone\n# Answer:\nThis Used to Be My Playground", "# Reasoning Path:\nMadonna -> music.composer.compositions -> Die Another Day -> music.composition.recordings -> Die Another Day (radio edit)\n# Answer:\nDie Another Day", "# Reasoning Path:\nMadonna -> music.composer.compositions -> 4 Minutes -> music.composition.composer -> Timbaland\n# Answer:\n4 Minutes"], "ground_truth": ["Bay City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1455", "prediction": ["# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> base.biblioness.bibs_location.city -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> location.location.containedby -> Federal District\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.country.capital -> Bras\u00edlia -> location.location.time_zones -> Bras\u00edlia Time Zone\n# Answer:\nBras\u00edlia", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.country.administrative_divisions -> S\u00e3o Paulo -> periodicals.newspaper_circulation_area.newspapers -> O Estado de S. Paulo\n# Answer:\nS\u00e3o Paulo", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.administrative_divisions -> S\u00e3o Paulo -> food.beer_country_region.beers_from_here -> Paulist\u00e2nia\n# Answer:\nS\u00e3o Paulo", "# Reasoning Path:\nBrazil -> location.country.administrative_divisions -> S\u00e3o Paulo -> base.aareas.schema.administrative_area.administrative_area_type -> Brazilian state\n# Answer:\nS\u00e3o Paulo", "# Reasoning Path:\nBrazil -> location.country.first_level_divisions -> Tocantins -> base.aareas.schema.administrative_area.administrative_area_type -> Brazilian state\n# Answer:\nTocantins"], "ground_truth": ["Bras\u00edlia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1456", "prediction": ["# Reasoning Path:\nJohn Stockton -> people.person.education -> m.04hd6_9 -> education.education.institution -> Gonzaga University\n# Answer:\nGonzaga University", "# Reasoning Path:\nJohn Stockton -> people.person.education -> m.0n1l1ft -> education.education.institution -> Gonzaga Preparatory School\n# Answer:\nGonzaga Preparatory School", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.category -> Basketball player\n# Answer:\nBasketball player", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> people.person.children -> David Stockton -> people.person.place_of_birth -> Spokane\n# Answer:\nDavid Stockton", "# Reasoning Path:\nJohn Stockton -> award.hall_of_fame_inductee.hall_of_fame_inductions -> m.0bjck8b -> award.hall_of_fame_induction.hall_of_fame -> Naismith Memorial Basketball Hall of Fame\n# Answer:\nNaismith Memorial Basketball Hall of Fame", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.schema -> Person\n# Answer:\nDate of birth", "# Reasoning Path:\nJohn Stockton -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["Gonzaga University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1457", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71yj1cp\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1458", "prediction": ["# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.01322yt1 -> organization.organization_relationship.parent -> Fiat Chrysler Automobiles\n# Answer:\nFiat Chrysler Automobiles", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.0gb6_f8 -> organization.organization_relationship.parent -> Government of Canada\n# Answer:\nGovernment of Canada", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.05ckhgd -> organization.organization_relationship.parent -> Fiat S.p.A.\n# Answer:\nFiat S.p.A.", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.04kjv4n -> organization.organization_relationship.parent -> Cerberus Capital Management\n# Answer:\nCerberus Capital Management", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.04lsswl -> organization.organization_relationship.parent -> Daimler AG\n# Answer:\nDaimler AG", "# Reasoning Path:\nChrysler Group LLC -> organization.organization.parent -> m.05ckhgd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> base.cars_refactor.make.model_s -> Plymouth Acclaim\n# Answer:\nPlymouth", "# Reasoning Path:\nChrysler Group LLC -> automotive.company.make_s -> Plymouth -> common.topic.notable_for -> g.125773lm0\n# Answer:\nPlymouth"], "ground_truth": ["Government of Canada", "Fiat S.p.A.", "Fiat Chrysler Automobiles", "Federal government of the United States", "Voluntary Employee Beneficiary Association"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.375, "ans_recall": 0.6, "path_f1": 0.4615384615384615, "path_precision": 0.375, "path_recall": 0.6, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.375, "path_ans_recall": 0.6}
{"id": "WebQTest-1459", "prediction": ["# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> film.film.edited_by -> Jay Freund\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee in King Arthur's Court -> book.written_work.previous_in_series -> Adventures of Huckleberry Finn\n# Answer:\nA Connecticut Yankee in King Arthur's Court", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My father\n# Answer:\nClara Clemens", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Prince and the Pauper -> book.written_work.next_in_series -> Life on the Mississippi\n# Answer:\nThe Prince and the Pauper", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> media_common.netflix_title.netflix_genres -> Drama\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> The Man That Corrupted Hadleyburg -> film.film.other_crew -> m.0w45cv2\n# Answer:\nThe Man That Corrupted Hadleyburg", "# Reasoning Path:\nMark Twain -> film.film_story_contributor.film_story_credits -> A Connecticut Yankee -> film.film.written_by -> William Conselman\n# Answer:\nA Connecticut Yankee", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> book.author.works_written -> My husband, Gabrilowitsch\n# Answer:\nClara Clemens"], "ground_truth": ["A Connecticut Yankee in King Arthur's Court, 1889 (novel) (IN RUSSIAN LANGUAGE) / (Ein Yankee am Hofe des K\u00f6nig Artus / Janki iz Konnektikuta pri dvore korolja Artura)", "The adventures of Tom Sawyer ; Tom Sawyer abroad", "The Innocents Abroad (Classic Books on Cassettes Collection)", "Adventures of Tom Sawyer. Adventure stories typically. Prince and pauper. Stories", "Adventures of Tom Sawyer (Deluxe Watermill Classics)", "A Tramp Abroad (Konemann Classics)", "The Adventures of Huckleberry Finn", "Tom Sawyer - Detective", "Grant and Twain: The Story of a Friendship That Changed America", "Sketches New and Old (Complete)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman's Library)", "The Innocents Abroad, vol. 1: The Authorized Uniform Edition", "Tom Sawyer Detective", "Life on the Mississippi", "A Connecticut Yankee in King Arthur's Court (Penguin Classics)", "Tom Sawyer abroad", "Old Times on the Mississippi", "The Prince and the Pauper (Dover Children's Thrift Classics)", "Tom Sawyer Abroad (Large Print Edition)", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7 12, R 120 ALP)", "Adventures of Huckleberry Finn (Illustrated Edition) (Dodo Press)", "The American Claimant (Large Print)", "The Innocents Abroad", "The adventures of Huckleberry Finn (Tom Sawyer's comrade)", "The adventures of Huckleberry Finn", "Adventures of Tom Sawyer (Everyman's Classics S.)", "Personal Recollections of Joan of Arc Volume 1 (Large Print Edition)", "The Prince And the Pauper", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Everyman Paperbacks)", "The American Claimant (1896)", "Sketches New And Old", "Personal Recollections of Joan of Arc Volume 1", "Adventures Of Tom Sawyer (Whole Story)", "Tom Sawyer Abroad (Tor Classics)", "The Prince and the Pauper", "The Adventures of Tom Sawyer, 1876 (IN RUSSIAN LANGUAGE) / (Las aventuras de Tom Sawyer / les Aventures de Tom Sawyer / Die Abenteuer des Tom Sawyer)", "Adventures of Tom Sawyer (Dramascripts)", "A Connecticut Yankee in King Arthur's Court Readalong", "Adventures of Tom Sawyer (Webster's Korean Thesaurus Edition)", "Christian Science (Large Print)", "Personal Recollections of Joan of Arc Volume 2 (Large Print Edition)", "The Adventures of Tom Sawyer", "Roughing It (1872) (The Oxford Mark Twain)", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper", "Adventures of Tom Sawyer (Cassette Sac 967)", "Adventures of Tom Sawyer (Saddleback Classics)", "Personal Recollections of Joan of Arc", "Following the equator", "A Tramp Abroad (1880) (The Oxford Mark Twain)", "Personal Recollections of Joan of Arc (1896) (The Oxford Mark Twain)", "Tom Sawyer Abroad (Watermill Classics Library)", "A Connecticut Yankee in King Arthur's Court (Tor Classics)", "Adventures of Tom Sawyer (Family Classics Library)", "A Tramp Abroad", "The innocents abroad", "Tom Sawyer, Detective", "Adventures of Tom Sawyer and Huckleberry Finn", "How to Tell a Story and Other Essays (1897) (The Oxford Mark Twain)", "Wild Nights!", "Autobiography of Mark Twain", "The Prince and the Pauper (Webster's Chinese-Simplified Thesaurus Edition)", "Tom Sawyer Detective (Austral Juvenil)", "The American Claimant (Large Print Edition)", "The Innocents Abroad (Signet Classics)", "The American Claimant", "Adventures of Tom Sawyer (Streamline Books)", "Letters from the earth", "How to tell a story, and other essays", "The Adventures of Tom Sawyer Adventure Classic (Adventure Classics)", "Personal Recollections of Joan of Arc Volume 2", "The American Claimant (1892) (The Oxford Mark Twain)", "Adventures of Tom Sawyer (Classics)", "Who Is Mark Twain?", "Roughing It", "Adventures of Huckleberry Finn (1885) (The Oxford Mark Twain)", "Adventures of Tom Sawyer Promo (Action Packs)", "American Claimant (Writings of Mark Twain)", "The mysterious stranger", "Adventures of Huckleberry Finn With Reader's Guide (Amsco Literature Program Series Grade 7-12)", "Old Times on the Mississippi.", "Personal recollections of Joan of Arc", "Christian Science (1907) (The Oxford Mark Twain)", "The Adventures of Huckleberry Finn (Signet Classics)", "The adventures of Tom Sawyer ; The adventures of Huckleberry Finn ; The prince and the pauper ; Pudd'nhead Wilson ; Short stories ; A Connecticut Yankee at King Arthur's court", "The Adventures of Tom Sawyer and the Adventures of Huckleberry Finn (Signet Classics)", "Sketches New and Old", "The Trouble Begins at 8: A Life of Mark Twain in the Wild, Wild West", "Roughing It (Centre for Editions of American Authors)", "Tom Sawyer Abroad (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "Roughing It (Works of Mark Twain, Volume One)", "Adventures of Tom Sawyer (Longman Simplified English Series)", "Adventures of Tom Sawyer (Classic Library)", "The wit and wisdom of Mark Twain", "Adventures of Tom Sawyer (Ec04)", "Tom Sawyer Abroad (Penguin Classics)", "Old times on the Mississippi", "Tom Sawyer Abroad", "The Adventures of Huckleberry Finn, Complete (Large Print)", "Tom Sawyer, detective", "A Tramp Abroad (Large Print Edition)", "Adventures of Tom Sawyer (New Windmill)", "American Claimant", "Adventures of Tom Sawyer & Huck Finn Collector's Library Volume I (Collector's Library of Classics, Volume 1)", "The Adventures Of Tom Sawyer", "Adventures of Tom Sawyer (Children's Classics)", "A Connecticut Yankee in King Arthur's Court (Enriched Classics Series)", "Roughing it.", "Following the Equator", "Sketches New And Old (The Works Of Mark Twain - 25 Volumes - Author's National Edition)", "1601", "The American claimant", "The Prince and the Pauper (New Method Supplementary Readers)", "The prince and the pauper", "Adventures of Tom Sawyer (Children's Illustrated Classics)", "The Innocents Abroad (1869) (The Oxford Mark Twain)", "Adventures of Tom Sawyer GB", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classical Books)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn", "Adventures of Tom Sawyer (Modern Library Classics (Sagebrush))", "Adventures of Tom Sawyer (Penguin Classics)", "The Prince and the Pauper (Webster's Korean Thesaurus Edition)", "The Adventures of Tom Sawyer (Puffin Books)", "Adventures of Tom Sawyer (Episodes from/Cdl 51205)", "Roughing It (Classics of the Old West)", "The Mysterious Stranger (Signet Classics)", "Adventures of Tom Sawyer, The", "The adventures of Tom Sawyer", "Christian Science", "The Adventures of Tom Sawyer - Literary Touchstone Edition", "Adventures of Tom Sawyer (Progress English)", "The Prince and the Pauper (Webster's Chinese-Traditional Thesaurus Edition)", "Letters from the Earth", "The Adventures of Huckleberry Finn CD set (Cambridge Literature)", "Sketches, new and old", "Adventures of Tom Sawyer - Huckleberry Finn (Classic Compendium)", "Adventures of Huckleberry Finn/Tom Sawyer (Junior Classics)", "Adventures of Tom Sawyer (Fiction)", "Personal Recollections Of Joan Of Arc", "Adventures of Tom Sawyer (Oxford Progressive English Readers)", "Tom Sawyer Abroad (1894) (The Oxford Mark Twain)", "Roughing It (Signet Classics)", "Tom Sawyer Abroad (Watermill Classic)", "Adventures of Huckleberry Finn (Tom Sawyer's comrade)", "Adventures of Huckleberry Finn", "The Mysterious Stranger", "A Connecticut Yankee in King Arthur's Court (Classics Read By Celebrities Series)", "The Adventures of Tom Sawyer (The Classic Collection)", "Christian Science (Large Print Edition)", "Ignorance, Confidence, and Filthy Rich Friends: The Business Adventures of Mark Twain, Chronic Speculator and Entrepreneur", "The Adventures of Tom Sawyer With Audio CD (Hear It Read It)", "The Adventures of Tom Sawyer and The Adventures of Huckleberry Finn (Signet Classics)", "Tom Sawyer, Detective (Hesperus Classics)", "Roughing It (Mark Twain Library)", "A Connecticut Yankee in King Arthur's Court, Set", "Adventures of Tom Sawyer, The (Classic Collection)", "A Connecticut Yankee in King Arthur's Court", "Personal Recollections of Joan of Arc (Complete)", "Tom Sawyer Detective (Watermill Classic)", "Personal Recollections of Joan of Arc, V2", "Sketches, New and Old (1875) (The Oxford Mark Twain)", "Roughing it", "Tom Sawyer, Detective (Tor Classics)", "The Mysterious Stranger (Large Print Edition)", "Old Times On The Mississippi", "Tom Sawyer, Detective (Dover Evergreen Classics)", "The Prince and the Pauper (Webster's Italian Thesaurus Edition)", "Adventures of Tom Sawyer (08454) (Deans Childrens Classics)", "The Adventures of Tom Sawyer, (Classic Books on CDs) [UNABRIDGED] (Classic Books on CD)", "Roughing It (Konemann Classics)"], "ans_acc": 0.047337278106508875, "ans_hit": 1, "ans_f1": 0.04324324324324324, "ans_precission": 0.25, "ans_recall": 0.023668639053254437, "path_f1": 0.21698113207547173, "path_precision": 0.25, "path_recall": 0.19166666666666668, "path_ans_f1": 0.07960199004975124, "path_ans_precision": 0.25, "path_ans_recall": 0.047337278106508875}
{"id": "WebQTest-146", "prediction": ["# Reasoning Path:\nJordan -> sports.sports_team_location.teams -> Al-Wehdat SC -> sports.sports_team.location -> Amman\n# Answer:\nAl-Wehdat SC", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> people.person.place_of_birth -> Amman\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71xk1k6\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nJordan -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37hbj\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nJordan -> location.statistical_region.gdp_growth_rate -> g.11b60tf0cj\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nJordan -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6hc5l\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> royalty.monarch.royal_line -> Hashemites\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Hussein of Jordan -> government.politician.government_positions_held -> m.0k_ktq_\n# Answer:\nHussein of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> government.politician.government_positions_held -> m.0k_ktrb\n# Answer:\nAbdullah I of Jordan", "# Reasoning Path:\nJordan -> royalty.kingdom.rulers -> Abdullah I of Jordan -> people.person.nationality -> Ottoman Empire\n# Answer:\nAbdullah I of Jordan"], "ground_truth": ["Jordan national football team", "Al-Wehdat SC"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1460", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> people.profession.specialization_of -> Scientist\n# Answer:\nBotanist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Abhishiktananda\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.image -> StAnthony\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> common.topic.notable_types -> Profession\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> people.profession.people_with_this_profession -> Adrian Urmanov\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Monk -> base.descriptive_names.names.descriptive_name -> m.0100v_8n\n# Answer:\nMonk", "# Reasoning Path:\nGregor Mendel -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist"], "ground_truth": ["Scientist", "Botanist", "Monk"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1461", "prediction": ["# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nKristie Raye", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqw9x -> people.sibling_relationship.sibling -> John Allen\n# Answer:\nJohn Allen", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwb8 -> people.sibling_relationship.sibling -> Kim Allen\n# Answer:\nKim Allen", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nConnecticut Huskies men's basketball", "# Reasoning Path:\nRay Allen -> people.person.children -> Wynstan Ryan Allen -> common.topic.notable_for -> g.1256c2b0z\n# Answer:\nWynstan Ryan Allen", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0ztqwbs -> people.sibling_relationship.sibling -> Talisha Allen\n# Answer:\nTalisha Allen", "# Reasoning Path:\nRay Allen -> people.person.education -> m.0bmyybb -> education.education.institution -> University of Connecticut\n# Answer:\nUniversity of Connecticut", "# Reasoning Path:\nRay Allen -> people.person.children -> Wynstan Ryan Allen -> people.person.parents -> Shannon Walker Williams\n# Answer:\nWynstan Ryan Allen"], "ground_truth": ["Shannon Walker Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1462", "prediction": ["# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Politician\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.ethnicity -> Irish American\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> base.politicalconventions.vice_presidential_nominee.convention_nominated_at -> 2008 Democratic National Convention\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> common.topic.subject_of -> President Barack Obama and the Message Beyond the Photograph\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.nationality -> United States of America\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> government.us_president.vice_president -> Joe Biden -> people.person.profession -> Lawyer\n# Answer:\nJoe Biden", "# Reasoning Path:\nBarack Obama -> people.person.sibling_s -> m.0kv53ff -> people.sibling_relationship.sibling -> George Obama\n# Answer:\nGeorge Obama"], "ground_truth": ["Joe Biden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1463", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nEnglish Language"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1464", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Cabbagetown, Toronto -> location.location.geolocation -> m.0dg55zz\n# Answer:\nCabbagetown, Toronto", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Cabbagetown, Toronto -> common.topic.notable_for -> g.125f4z3yg\n# Answer:\nCabbagetown, Toronto", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> location.location.geolocation -> m.0cqsh3n\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> geography.island.body_of_water -> Lake Ontario\n# Answer:\nToronto Islands", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> theater.theater.theater_production_venue_relationship -> m.0102mq55\n# Answer:\nToronto Centre for the Arts"], "ground_truth": ["Bare Oaks Family Naturist Park", "Canada's Wonderland", "Edwards Gardens", "Heartland Town Centre", "Black Creek Pioneer Village", "St. Lawrence Market", "Spadina House", "Nathan Phillips Square", "Chinatown, Toronto", "Casa Loma", "Ontario Place", "Queen Street West", "Danforth Avenue", "Yorkville, Toronto", "Toronto Eaton Centre", "Corktown Common", "Luminato", "Roy Thomson Hall", "Ontario Science Centre", "Toronto Zoo", "BMO Field", "Museum of Contemporary Canadian Art", "McMichael Canadian Art Collection", "Rogers Centre", "Planet in Focus", "Gardiner Museum", "Queens Quay", "Textile Museum of Canada", "CN Tower", "Hanlan's Point Beach", "Air Canada Centre", "Kensington Market", "Toronto Islands", "Royal Ontario Museum", "Toronto Waterfront Marathon", "Royal Alexandra Theatre", "High Park", "Bayview Village Shopping Centre", "The Beaches", "Sugar Beach", "Distillery District", "Bata Shoe Museum", "Cabbagetown, Toronto", "First Toronto Post Office", "Princess of Wales Theatre", "Hockey Hall of Fame", "Art Gallery of Ontario", "Sony Centre for the Performing Arts", "Toronto Centre for the Arts"], "ans_acc": 0.061224489795918366, "ans_hit": 1, "ans_f1": 0.11538461538461538, "ans_precission": 1.0, "ans_recall": 0.061224489795918366, "path_f1": 0.11538461538461538, "path_precision": 1.0, "path_recall": 0.061224489795918366, "path_ans_f1": 0.11538461538461538, "path_ans_precision": 1.0, "path_ans_recall": 0.061224489795918366}
{"id": "WebQTest-1465", "prediction": ["# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.11b60sjxs7\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Argentina\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Chile\n# Answer:\nUnitary state", "# Reasoning Path:\nSouth Korea -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nSouth Korea -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g_c4\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nSouth Korea -> location.statistical_region.official_development_assistance -> g.1hhc3_gkr\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Constitutional republic", "Unitary state", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1467", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> symbols.name_source.namesakes -> Franklin Township\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.spouse_s -> m.0j6f4rh\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1468", "prediction": ["# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> location.location.events -> 1987 grenade attack in the Sri Lankan Parliament\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Sri Jayawardenepura Kotte -> common.topic.image -> The Parliament of Sri Lanka\n# Answer:\nSri Jayawardenepura Kotte", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0s28\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSri Lanka -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vqmm4\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m56\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSri Lanka -> location.statistical_region.labor_participation_rate -> g.11b71x3fyz\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nSri Lanka -> location.country.capital -> Colombo -> location.location.events -> Battle of Mulleriyawa\n# Answer:\nColombo", "# Reasoning Path:\nSri Lanka -> location.statistical_region.official_development_assistance -> g.12cp_k6qr\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nSri Lanka -> location.statistical_region.electricity_consumption_per_capita -> g.1245_67k2\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita"], "ground_truth": ["Colombo", "Sri Jayawardenepura Kotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-147", "prediction": ["# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_holder -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.010178yn -> government.government_position_held.office_holder -> Andrew Farmer\n# Answer:\nAndrew Farmer", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.04j5sk4 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.01017wwj -> government.government_position_held.office_holder -> Timothy Hill\n# Answer:\nTimothy Hill", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.0101b172 -> government.government_position_held.office_holder -> Gloria Johnson\n# Answer:\nGloria Johnson", "# Reasoning Path:\nTennessee -> government.governmental_jurisdiction.governing_officials -> m.0101fcmc -> government.government_position_held.office_holder -> Susan Lynn\n# Answer:\nSusan Lynn", "# Reasoning Path:\nTennessee -> government.political_district.representatives -> m.0bfmtrp -> government.government_position_held.office_holder -> Andrew Jackson\n# Answer:\nAndrew Jackson"], "ground_truth": ["William Haslam"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1470", "prediction": ["# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Second Polish Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Slovakia\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Soviet Union\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Turkey\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.12cp_j3lx\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Democracy -> organization.organization_sector.organizations_in_this_sector -> Council of Europe\n# Answer:\nDemocracy", "# Reasoning Path:\nPoland -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc385gk\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nPoland -> location.statistical_region.consumer_price_index -> g.11b60v4rtw\n# Answer:\nlocation.statistical_region.consumer_price_index", "# Reasoning Path:\nPoland -> location.statistical_region.size_of_armed_forces -> g.1hhc39krm\n# Answer:\nlocation.statistical_region.size_of_armed_forces"], "ground_truth": ["Parliamentary republic", "Democracy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1471", "prediction": ["# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_holder -> John Carney\n# Answer:\nJohn Carney", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.office_holder -> Carl Danberg\n# Answer:\nCarl Danberg", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbwj4 -> government.government_position_held.office_holder -> Harriet Smith Windsor\n# Answer:\nHarriet Smith Windsor", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Delaware\n# Answer:\nLieutenant Governor of Delaware", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.04x_m_h -> government.government_position_held.office_holder -> Ruth Ann Minner\n# Answer:\nRuth Ann Minner", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbq1t -> government.government_position_held.basic_title -> Lieutenant Governor\n# Answer:\nLieutenant Governor", "# Reasoning Path:\nDelaware -> location.location.people_born_here -> A.C. Golden -> common.topic.article -> m.0c0311g\n# Answer:\nA.C. Golden", "# Reasoning Path:\nDelaware -> government.governmental_jurisdiction.governing_officials -> m.010hbqbl -> government.government_position_held.basic_title -> Attorney general\n# Answer:\nAttorney general"], "ground_truth": ["Griffin Seward", "Rebecca Lee Crumpler", "Eddie Paskey", "John Sedwick", "Chris Dapkins", "Solomon Bayley", "Patrick Kearney", "Ed Haskett", "Outerbridge Horsey IV", "David McElwee", "Edward Groesbeck Voss", "Chris Gutierrez", "Andrew Cebulka", "Katharine Pyle", "Joe Garcio", "Norman Hutchins", "Bill Indursky", "Ann Marie Borghese", "Alfred I. du Pont", "Huck Betts", "William Grassie", "Tully Satre", "James B. Clark, Jr.", "Jeffrey W. Bullock", "Hugh T. Broomall", "Hampton Del Ruth", "R. R. M. Carpenter", "Steve Ressel", "Gimel \\\"Young Guru\\\" Keaton", "Francine Fournier", "Jeremy Conway", "Jacqueline Jones", "Rex Carlton", "Nathaniel Harrington Bannister", "Reuben James", "Billy Ficca", "Fred Lonberg-Holm", "Tom Peszek", "Collins J. Seitz", "Herbert Bennett Fenn", "Jim Wilson", "William King", "A.C. Golden", "Matt Stawicki", "James Tilton"], "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.03773584905660377, "ans_precission": 0.125, "ans_recall": 0.022222222222222223, "path_f1": 0.03773584905660377, "path_precision": 0.125, "path_recall": 0.022222222222222223, "path_ans_f1": 0.03773584905660377, "path_ans_precision": 0.125, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-1472", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.office_position_or_title -> Governor of Massachusetts\n# Answer:\nGovernor of Massachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.jurisdiction_of_office -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Senator\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["2003-01-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1473", "prediction": ["# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Republic of Kosovo\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Republic of Ireland\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71y1l3w\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nCyprus -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nCyprus -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc3_4dv\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1475", "prediction": ["# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Bass guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.family -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> common.topic.subject_of -> Reverb\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Guitar -> music.instrument.variation -> Electric guitar\n# Answer:\nGuitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flflz\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> music.group_member.instruments_played -> Slide guitar -> music.performance_role.guest_performances -> m.04flfm5\n# Answer:\nSlide guitar", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr5q -> award.award_nomination.nominated_for -> Pirates of the Caribbean: The Curse of the Black Pearl\n# Answer:\nPirates of the Caribbean: The Curse of the Black Pearl", "# Reasoning Path:\nJohnny Depp -> award.award_nominee.award_nominations -> m.03mlr72 -> award.award_nomination.ceremony -> 77th Academy Awards\n# Answer:\n77th Academy Awards"], "ground_truth": ["Slide guitar", "Guitar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1477", "prediction": ["# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bd32ns -> tv.tv_guest_role.episodes_appeared_in -> Hooked\n# Answer:\nHooked", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.starring_roles -> m.0bb15h2 -> tv.regular_tv_appearance.seasons -> American Idol - Season 4\n# Answer:\nAmerican Idol - Season 4", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvtx58 -> tv.tv_guest_role.episodes_appeared_in -> Winner Is Crowned\n# Answer:\nWinner Is Crowned", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvvlfq -> tv.tv_guest_role.episodes_appeared_in -> January 20, 2007\n# Answer:\nJanuary 20, 2007", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvtzgw -> tv.tv_guest_role.episodes_appeared_in -> Steve Carell, Blake Lively, Carrie Underwood\n# Answer:\nSteve Carell, Blake Lively, Carrie Underwood", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.starring_roles -> m.0zd9t42 -> tv.regular_tv_appearance.character -> Maria Rainer\n# Answer:\nMaria Rainer", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.guest_roles -> m.0bvvr1y -> tv.tv_guest_role.episodes_appeared_in -> The Finale\n# Answer:\nThe Finale", "# Reasoning Path:\nCarrie Underwood -> tv.tv_actor.starring_roles -> m.0bd3jst -> tv.regular_tv_appearance.series -> CMT Invitation Only\n# Answer:\nCMT Invitation Only"], "ground_truth": ["Hooked"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1478", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.original_language -> English Language\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.author -> Todd C. Peppers\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk"], "ground_truth": ["Sonia Sotomayor", "Anthony Kennedy", "Ruth Bader Ginsburg", "Samuel Alito", "Antonin Scalia", "John Roberts", "Stephen Breyer", "Clarence Thomas", "Elena Kagan"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.11764705882352941, "path_ans_precision": 0.125, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1479", "prediction": ["# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> people.profession.specialization_of -> Athlete\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> common.topic.notable_types -> Profession\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> people.profession.specializations -> Professional golfer\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> common.topic.article -> m.04n4sw8\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> common.topic.notable_for -> g.125cdby1b\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> celebrities.celebrity.sexual_relationships -> m.0wpd79b -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cygr\n# Answer:\nGolfer", "# Reasoning Path:\nDustin Johnson -> celebrities.celebrity.sexual_relationships -> m.0wpd79b -> celebrities.romantic_relationship.celebrity -> Paulina Gretzky\n# Answer:\nPaulina Gretzky", "# Reasoning Path:\nDustin Johnson -> people.person.profession -> Golfer -> base.descriptive_names.names.descriptive_name -> m.0106cyt1\n# Answer:\nGolfer"], "ground_truth": ["Golfer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1480", "prediction": ["# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.parents -> Gregg Allman\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.profession -> Singer\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> people.person.parents -> Sonny Bono\n# Answer:\nChaz Bono", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> common.topic.image -> Elijah_Blue_Allman.jpg\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> award.award_nominee.award_nominations -> m.0r66c_b\n# Answer:\nChaz Bono", "# Reasoning Path:\nCher -> base.popstra.celebrity.breakup -> m.064j9rj -> base.popstra.breakup.participant -> Sonny Bono\n# Answer:\nSonny Bono", "# Reasoning Path:\nCher -> people.person.children -> Elijah Blue Allman -> people.person.gender -> Male\n# Answer:\nElijah Blue Allman", "# Reasoning Path:\nCher -> people.person.children -> Chaz Bono -> people.person.nationality -> United States of America\n# Answer:\nChaz Bono"], "ground_truth": ["Sonny Bono", "Gregg Allman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.25, "path_precision": 0.375, "path_recall": 0.1875, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1481", "prediction": ["# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> education.educational_institution.subsidiary_or_constituent_schools -> University of Oregon School of Law -> location.location.containedby -> United States of America\n# Answer:\nUniversity of Oregon School of Law", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Eugene -> location.location.containedby -> United States of America\n# Answer:\nEugene", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Oregon\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> Oregon -> location.location.containedby -> United States of America\n# Answer:\nOregon", "# Reasoning Path:\nUniversity of Oregon -> location.location.containedby -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of Oregon -> education.educational_institution.subsidiary_or_constituent_schools -> University of Oregon School of Law -> location.location.containedby -> Eugene\n# Answer:\nUniversity of Oregon School of Law", "# Reasoning Path:\nUniversity of Oregon -> education.university.number_of_undergraduates -> m.0h77tx1\n# Answer:\neducation.university.number_of_undergraduates"], "ground_truth": ["Eugene"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1482", "prediction": ["# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.actor -> Alexa Vega\n# Answer:\nAlexa Vega", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.0zfn9cq -> film.performance.actor -> Addisyn Fair\n# Answer:\nAddisyn Fair", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9tsc -> film.performance.character -> Baby Carmen\n# Answer:\nBaby Carmen", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9trw -> film.performance.actor -> Daryl Sabara\n# Answer:\nDaryl Sabara", "# Reasoning Path:\nSpy Kids -> common.topic.webpage -> http://elfman.filmmusic.com/spykids/\n# Answer:\nhttp://elfman.filmmusic.com/spykids/", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0pft -> film.performance.actor -> Carla Gugino\n# Answer:\nCarla Gugino", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.01z0ph5 -> film.performance.actor -> Alan Cumming\n# Answer:\nAlan Cumming", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.0zfn9cq -> film.performance.character -> Baby Carmen\n# Answer:\nBaby Carmen", "# Reasoning Path:\nSpy Kids -> film.film.starring -> m.02s9trw -> film.performance.character -> Juni Cortez\n# Answer:\nJuni Cortez"], "ground_truth": ["Addisyn Fair", "Alexa Vega"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1483", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71xcxgk\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic"], "ground_truth": ["Parliamentary republic", "Representative democracy", "Constitutional republic", "Federal republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1484", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Virgil\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Menander\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Virgil -> influence.influence_node.influenced_by -> Homer\n# Answer:\nVirgil", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Homer\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form"], "ground_truth": ["Seneca the Younger", "Plautus", "Geoffrey Chaucer", "Thomas More", "Ovid", "Virgil", "Lucian", "Christopher Marlowe", "Michel de Montaigne", "John Pory", "Thomas Kyd", "Edmund Spenser", "Terence", "Plutarch"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.43243243243243246, "ans_precission": 0.8888888888888888, "ans_recall": 0.2857142857142857, "path_f1": 0.43243243243243246, "path_precision": 0.8888888888888888, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5095541401273885, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-1485", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxt\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1486", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nMercy College", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nPhillips Exeter Academy", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0jwchnr -> education.education.institution -> Ardsley High School\n# Answer:\nArdsley High School", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.major_field_of_study -> Computer programming\n# Answer:\nComputer programming", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMark Zuckerberg -> common.topic.notable_for -> g.125621s53\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Mercy College", "Harvard University", "Phillips Exeter Academy", "Ardsley High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1487", "prediction": ["# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03plx6x -> people.place_lived.location -> Reading\n# Answer:\nReading", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.places_lived -> m.03pv20m -> people.place_lived.location -> Bucklebury\n# Answer:\nBucklebury", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> location.location.containedby -> United Kingdom\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> location.location.geolocation -> m.02_scs7\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.webpage -> m.0fq430w\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.notable_types -> Hospital\n# Answer:\nRoyal Berkshire Hospital", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.nationality -> England -> government.governmental_jurisdiction.government_bodies -> National Industrial Relations Court\n# Answer:\nEngland", "# Reasoning Path:\nCatherine, Duchess of Cambridge -> people.person.place_of_birth -> Royal Berkshire Hospital -> common.topic.article -> m.062rv_\n# Answer:\nRoyal Berkshire Hospital"], "ground_truth": ["Bucklebury", "Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1488", "prediction": ["# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> Aberjhani\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> business.product_theme.products -> Salvador Dali Registry\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> visual_art.art_subject.artwork_on_the_subject -> The Piano Body\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced_by -> Edvard Munch\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> book.school_or_movement.associated_authors -> Ion C\u0103lug\u0103ru\n# Answer:\nSurrealism", "# Reasoning Path:\nMarc Chagall -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art"], "ground_truth": ["Expressionism", "Surrealism", "Modern art", "Cubism", "Fauvism"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5714285714285715, "path_precision": 1.0, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-1489", "prediction": ["# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.character -> Lando Calrissian\n# Answer:\nLando Calrissian", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.0h2jz23 -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.011rh2vj -> film.performance.character -> Bartender (voice)\n# Answer:\nBartender (voice)", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.01xpnsw -> film.performance.character -> Lando Calrissian\n# Answer:\nLando Calrissian", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.010g_8m5 -> film.performance.film -> The Right of the People\n# Answer:\nThe Right of the People", "# Reasoning Path:\nBilly Dee Williams -> people.person.place_of_birth -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nBilly Dee Williams -> film.actor.film -> m.012stjnz -> film.performance.film -> Dangerous Passion\n# Answer:\nDangerous Passion"], "ground_truth": ["Lando Calrissian"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-149", "prediction": ["# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Mozambique\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.region -> Europe\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.statistical_region.cpi_inflation_rate -> g.11b60zrgdp\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nPortugal -> location.country.official_language -> Portuguese Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.country.languages_spoken -> Portuguese Language -> language.human_language.countries_spoken_in -> Angola\n# Answer:\nPortuguese Language", "# Reasoning Path:\nPortugal -> location.statistical_region.deposit_interest_rate -> g.1hhc3msbv\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1490", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan"], "ground_truth": ["Guaran\u00ed language", "Quechuan languages", "Yiddish Language", "Spanish Language", "Italian Language"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8358208955223881, "ans_precission": 0.875, "ans_recall": 0.8, "path_f1": 0.8358208955223881, "path_precision": 0.875, "path_recall": 0.8, "path_ans_f1": 0.8358208955223881, "path_ans_precision": 0.875, "path_ans_recall": 0.8}
{"id": "WebQTest-1493", "prediction": ["# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> education.education.institution -> University of Reading\n# Answer:\nUniversity of Reading", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> education.education.institution -> Wakeman School\n# Answer:\nWakeman School", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> education.education.major_field_of_study -> Botany\n# Answer:\nBotany", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_profile.published -> Published\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.010hdpvg -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nWilfred Owen -> common.topic.notable_types -> Fictional Character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFictional Character", "# Reasoning Path:\nWilfred Owen -> people.person.education -> m.0n18h9_ -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree"], "ground_truth": ["Wakeman School", "University of Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1494", "prediction": ["# Reasoning Path:\nKevin Jonas -> base.popstra.celebrity.dated -> m.064hjjh -> base.popstra.dated.participant -> Danielle Deleasa\n# Answer:\nDanielle Deleasa", "# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_j -> music.recording_contribution.album -> Jonas L.A.\n# Answer:\nJonas L.A.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.children -> Nick Jonas\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> tv.tv_actor.starring_roles -> m.09j06y6 -> tv.regular_tv_appearance.series -> Jonas L.A.\n# Answer:\nJonas L.A.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.children -> Frankie Jonas\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Paul Kevin Jonas, Sr. -> people.person.nationality -> United States of America\n# Answer:\nPaul Kevin Jonas, Sr.", "# Reasoning Path:\nKevin Jonas -> music.artist.contribution -> m.0cqz7_2 -> music.recording_contribution.album -> JONAS\n# Answer:\nJONAS", "# Reasoning Path:\nKevin Jonas -> people.person.parents -> Denise Miller-Jonas -> people.person.spouse_s -> m.0nbwt6z\n# Answer:\nDenise Miller-Jonas"], "ground_truth": ["Danielle Deleasa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1495", "prediction": ["# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Atlanta\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> United States of America\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.number_of_passengers -> m.02k9dqr\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> common.topic.notable_types -> Airport\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Bullpen Rib House\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.hub_for -> Air Atlanta\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.hubs -> General Mitchell International Airport -> aviation.airport.focus_city_for -> Republic Airlines\n# Answer:\nGeneral Mitchell International Airport", "# Reasoning Path:\nAirTran Airways -> aviation.airline.focus_cities -> Fort Lauderdale\u2013Hollywood International Airport -> aviation.airport.serves -> Fort Lauderdale\n# Answer:\nFort Lauderdale\u2013Hollywood International Airport"], "ground_truth": ["Logan International Airport", "Denver International Airport", "Raleigh\u2013Durham International Airport", "LaGuardia Airport", "Philadelphia International Airport", "Baltimore\u2013Washington International Airport"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1496", "prediction": ["# Reasoning Path:\nParis -> symbols.name_source.namesakes -> 21 Lutetia -> symbols.namesake.named_after -> Lutetia\n# Answer:\n21 Lutetia", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> ParisiiCoins\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.article -> m.0283_dp\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.namesake.named_after -> Parisii -> common.topic.image -> The Vellavi sited south of the Arverni\u2014 whose name is embodied in Auvergne\n# Answer:\nParisii", "# Reasoning Path:\nParis -> symbols.name_source.namesakes -> 21 Lutetia -> astronomy.astronomical_discovery.discovery_site -> Paris Observatory\n# Answer:\n21 Lutetia", "# Reasoning Path:\nParis -> symbols.name_source.namesakes -> 21 Lutetia -> astronomy.celestial_object.category -> Asteroid\n# Answer:\n21 Lutetia", "# Reasoning Path:\nParis -> symbols.name_source.namesakes -> 21 Lutetia -> common.topic.notable_for -> g.1259rkhcy\n# Answer:\n21 Lutetia", "# Reasoning Path:\nParis -> symbols.name_source.namesakes -> Paris Hilton -> people.person.religion -> Catholicism\n# Answer:\nParis Hilton"], "ground_truth": ["Parisii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1499", "prediction": ["# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pshj -> base.popstra.dated.participant -> Ryan Gosling\n# Answer:\nRyan Gosling", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065p_f4 -> base.popstra.dated.participant -> Michael Mailer\n# Answer:\nMichael Mailer", "# Reasoning Path:\nSandra Bullock -> people.person.spouse_s -> m.0y5g7_0 -> people.marriage.spouse -> Ryan Gosling\n# Answer:\nRyan Gosling", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.canoodled -> m.0652w91 -> base.popstra.canoodled.participant -> Ben Affleck\n# Answer:\nBen Affleck", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pps3 -> base.popstra.dated.participant -> Matthew McConaughey\n# Answer:\nMatthew McConaughey", "# Reasoning Path:\nSandra Bullock -> people.person.spouse_s -> m.0y5g7_0 -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065pss_ -> base.popstra.dated.participant -> Tate Donovan\n# Answer:\nTate Donovan", "# Reasoning Path:\nSandra Bullock -> base.popstra.celebrity.dated -> m.065ptyv -> base.popstra.dated.participant -> Ben Affleck\n# Answer:\nBen Affleck"], "ground_truth": ["Jesse James"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-150", "prediction": ["# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Haiti\n# Answer:\nHaitian Creole", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nCuba -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc38hmp\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nCuba -> location.statistical_region.energy_use_per_capita -> g.1245_22wq\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["Spanish Language", "Haitian Creole", "Lucumi Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1500", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> United States, with Territories\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.population -> g.11b66j25ww\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07ftnh2\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> United States of America\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.statistical_region.rent50_4 -> m.05ggltc\n# Answer:\nJefferson County"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-1501", "prediction": ["# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> location.mailing_address.citytown -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> common.topic.notable_for -> g.1254yl5sx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> book.written_work.subjects -> Photography\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum", "# Reasoning Path:\nUnited Nations -> organization.organization.founders -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> location.mailing_address.state_province_region -> New York\n# Answer:\nNew York", "# Reasoning Path:\nUnited Nations -> organization.organization.headquarters -> m.04300z7 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nUnited Nations -> book.book_subject.works -> Report on 2011 International Year Part 7: Photography of The African Continuum -> base.reviews.review.mentions -> New York City\n# Answer:\nReport on 2011 International Year Part 7: Photography of The African Continuum"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1502", "prediction": ["# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Connecticut\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> Connecticut -> location.location.containedby -> United States of America\n# Answer:\nConnecticut", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Yale Law School -> location.location.containedby -> United States of America\n# Answer:\nYale Law School", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Berkeley College -> location.location.containedby -> New Haven\n# Answer:\nBerkeley College", "# Reasoning Path:\nUniversity Yale -> location.location.containedby -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity Yale -> education.educational_institution.subsidiary_or_constituent_schools -> Branford College -> location.location.containedby -> United States of America\n# Answer:\nBranford College"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1503", "prediction": ["# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> common.topic.subject_of -> The Approaching 100th Anniversary of the Harlem Renaissance (part 2)\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> people.deceased_person.place_of_death -> New York City -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNew York City", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.dates -> m.0c12k2g\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After his death in New York City in 1809, only six mourners attended his funeral. -> base.kwebbase.kwsentence.previous_sentence -> In 1806, he was refused the right to vote because he was not an American citizen.\n# Answer:\nAfter his death in New York City in 1809, only six mourners attended his funeral.", "# Reasoning Path:\nThomas Paine -> base.kwebbase.kwtopic.has_sentences -> After working as corset-maker for a number of years, Paine became an excise officer. -> base.kwebbase.kwsentence.next_sentence -> The 1760s were a decade of rising prices, falling wages, and sporadic food riots.\n# Answer:\nAfter working as corset-maker for a number of years, Paine became an excise officer.", "# Reasoning Path:\nThomas Paine -> people.person.quotations -> A bad cause will never be supported by bad means and bad men. -> media_common.quotation.subjects -> Causes\n# Answer:\nA bad cause will never be supported by bad means and bad men."], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1504", "prediction": ["# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Siaya County\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.containedby -> Nyanza Province\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> location.location.geolocation -> m.04nvlsq\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> common.topic.article -> m.04f2ymv\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.place_of_birth -> Nyang'oma Kogelo -> common.topic.notable_for -> g.125frds2b\n# Answer:\nNyang'oma Kogelo", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.religion -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nBarack Obama, Sr. -> people.person.spouse_s -> m.0j4l4yg -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Nyang'oma Kogelo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1506", "prediction": ["# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> location.location.containedby -> Foxborough\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> sports.sports_facility.teams -> New England Revolution\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> soccer.football_pitch.matches -> 2014 Pre-World Cup Friendly Mexico vs Portugal football match\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> common.topic.image -> Gillette Stadium2\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.schemastaging.visual_color_extra.srgb -> m.010q1h63\n# Answer:\nWhite", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.fight_song -> I'm Shipping Up to Boston -> music.composition.subjects -> Boston\n# Answer:\nI'm Shipping Up to Boston", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.arena_stadium -> Gillette Stadium -> location.location.events -> 2003 FIFA Women's World Cup\n# Answer:\nGillette Stadium", "# Reasoning Path:\nNew England Patriots -> sports.sports_team.colors -> White -> base.roses.rose_color.roses_of_this_color -> Rosa laevigata\n# Answer:\nWhite"], "ground_truth": ["Gillette Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1507", "prediction": ["# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Actuary\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Novelist -> fictional_universe.character_occupation.characters_with_this_occupation -> Seth Mortensen\n# Answer:\nNovelist", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Agent\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Antiquarian\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> common.topic.subjects -> Krishna Mali\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Ted Cole\n# Answer:\nAuthor", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> people.profession.specializations -> Art dealer\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nKate Chopin -> common.image.size -> m.0klbs8\n# Answer:\ncommon.image.size"], "ground_truth": ["Author", "Writer", "Novelist", "Businessperson"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.8135593220338982, "path_precision": 0.8888888888888888, "path_recall": 0.75, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-1508", "prediction": ["# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> education.education.institution -> Edmund A. Walsh School of Foreign Service\n# Answer:\nEdmund A. Walsh School of Foreign Service", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02hvncd -> education.education.institution -> University College, Oxford\n# Answer:\nUniversity College, Oxford", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0125cddf -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.02k6ph3 -> education.education.institution -> Yale Law School\n# Answer:\nYale Law School", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0w5q498 -> education.education.institution -> University Yale\n# Answer:\nUniversity Yale", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> education.education.major_field_of_study -> Foreign Service\n# Answer:\nForeign Service", "# Reasoning Path:\nBill Clinton -> people.person.education -> m.0h1khf8 -> education.education.degree -> Bachelor of Science\n# Answer:\nBachelor of Science"], "ground_truth": ["Edmund A. Walsh School of Foreign Service", "University of Oxford", "University College, Oxford", "Yale Law School", "Georgetown University", "University Yale"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.625, "ans_recall": 0.8333333333333334, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.625, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1510", "prediction": ["# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.descriptive_names.names.descriptive_name -> m.010x329h\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.popstra.product.sold_to -> m.063p_sk\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.image.appears_in_topic_gallery -> Pit bull\n# Answer:\nPit bull", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.petsupplyandaccessory.pet_apparel_user.pet_apparel_company -> The Woofer\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.image.size -> m.02bqvx0\n# Answer:\ncommon.image.size", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.descriptive_names.names.descriptive_name -> m.010x32bc\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> base.petbreeds.dog_breed.group -> Terrier Group -> base.petbreeds.dog_breed_group.dog_breeds -> Bull Terrier\n# Answer:\nTerrier Group", "# Reasoning Path:\nStaffordshire Bull Terrier -> biology.animal_breed.breed_of -> Dog -> base.pethealth.pet_with_medical_condition.diseases_and_other_conditions_of_this_pet -> Ehrlichiosis\n# Answer:\nDog", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> type.type.expected_by -> Breed\n# Answer:\nAnimal breed", "# Reasoning Path:\nStaffordshire Bull Terrier -> common.topic.notable_types -> Animal breed -> freebase.type_profile.strict_included_types -> Animal\n# Answer:\nAnimal breed"], "ground_truth": ["Dog"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1513", "prediction": ["# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> symbols.namesake.named_after -> John Sutter\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.article -> m.0962k\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> location.location.containedby -> California\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Crocker Art Museum -> base.usnris.nris_listing.significance_level -> State\n# Answer:\nCrocker Art Museum", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> B Street Theatre -> architecture.building.building_function -> Theatre\n# Answer:\nB Street Theatre", "# Reasoning Path:\nSacramento -> sports.sports_team_location.teams -> Sacramento State Hornets men's basketball -> sports.sports_team.arena_stadium -> Colberg Court\n# Answer:\nSacramento State Hornets men's basketball", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Sutter's Fort -> common.topic.image -> Sutters Fort 1846\n# Answer:\nSutter's Fort", "# Reasoning Path:\nSacramento -> travel.travel_destination.tourist_attractions -> Crocker Art Museum -> common.topic.image -> Crocker2 35484\n# Answer:\nCrocker Art Museum"], "ground_truth": ["Sacramento History Museum", "B Street Theatre", "Sacramento Zoo", "California State Capitol Museum", "Sutter's Fort", "California State Indian Museum", "California Automobile Museum", "Raging Waters Sacramento", "California State Railroad Museum", "Folsom Lake", "Crocker Art Museum"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.4158415841584158, "ans_precission": 0.875, "ans_recall": 0.2727272727272727, "path_f1": 0.4158415841584158, "path_precision": 0.875, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4158415841584158, "path_ans_precision": 0.875, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-1515", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.spouse_s -> m.0s940f6\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> common.topic.notable_for -> g.125c8dhdm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.employment_history -> m.0jb95tm\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> freebase.valuenotation.has_value -> Parents\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Steven Fox\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> William Fox -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fox", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> people.person.parents -> Phyllis Piper -> people.person.children -> Kelli Fox\n# Answer:\nPhyllis Piper"], "ground_truth": ["Phyllis Piper", "William Fox"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1516", "prediction": ["# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.institution -> University of Michigan\n# Answer:\nUniversity of Michigan", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.institution -> University of Michigan Health System\n# Answer:\nUniversity of Michigan Health System", "# Reasoning Path:\nSanjay Gupta -> education.honorary_degree_recipient.honorary_degrees -> m.0xn83q1 -> education.honorary_degree.institution -> University of Michigan\n# Answer:\nUniversity of Michigan", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.degree -> Bachelor's degree\n# Answer:\nBachelor's degree", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.0n1j65x -> education.education.institution -> Novi High School\n# Answer:\nNovi High School", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.02wnv1q -> education.education.major_field_of_study -> Biomedical Sciences\n# Answer:\nBiomedical Sciences", "# Reasoning Path:\nSanjay Gupta -> people.person.education -> m.05b8sml -> education.education.degree -> Doctor of Medicine\n# Answer:\nDoctor of Medicine", "# Reasoning Path:\nSanjay Gupta -> people.person.children -> Sage Ayla Gupta -> common.topic.notable_types -> Person\n# Answer:\nSage Ayla Gupta"], "ground_truth": ["University of Michigan", "University of Michigan Health System", "Novi High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1518", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nNorth American Free Trade Agreement -> common.topic.notable_for -> g.125563pkq\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.nationality -> United States of America\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#range -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.ethnicity -> White people\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfg53 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded"], "ground_truth": ["United States of America", "Canada", "Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1519", "prediction": ["# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Americas -> location.location.containedby -> Western Hemisphere\n# Answer:\nAmericas", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> common.topic.notable_for -> g.12586lyg9\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> Greater Antilles -> location.location.containedby -> Antilles\n# Answer:\nGreater Antilles", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nCayman Islands -> location.administrative_division.first_level_division_of -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.location.partially_contains -> Normandy\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nCayman Islands -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Akrotiri and Dhekelia\n# Answer:\nUnited Kingdom, with Dependencies and Territories"], "ground_truth": ["Americas", "North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1520", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Italian Language", "Brazilian Portuguese", "Portuguese Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.2857142857142857, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1522", "prediction": ["# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j8kwh4 -> base.schemastaging.athlete_salary.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j34ls4 -> base.schemastaging.athlete_salary.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j8kwh4 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j8kwgx -> base.schemastaging.athlete_salary.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> base.schemastaging.athlete_extra.salary -> m.0j34ls4 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nKroy Biermann -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.09tcrdr -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nKroy Biermann -> american_football.football_player.games -> m.07nvpcg -> american_football.player_game_statistics.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons"], "ground_truth": ["Atlanta Falcons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.3448275862068965, "path_precision": 0.625, "path_recall": 0.23809523809523808, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1523", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.runner_up -> Arizona Cardinals\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvv1h\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> sports.sports_championship_event.season -> 1995 NFL season\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.season -> 2005 NFL season\n# Answer:\n2006 AFC Championship Game"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1524", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.founding_figures -> Muhammad in Islam -> book.author.works_written -> Sayings of Mohammed\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> book.book_subject.works -> Muhammad\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> influence.influence_node.influenced -> Muhammad ibn Ya'qub al-Kulayni\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Ali -> people.person.religion -> Shia Islam\n# Answer:\nAli", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Muhammad in Islam -> people.person.spouse_s -> m.0j4jg5y\n# Answer:\nMuhammad in Islam", "# Reasoning Path:\nIslam -> religion.religion.notable_figures -> Abu Bakr -> people.person.parents -> Uthman Abu Quhafa\n# Answer:\nAbu Bakr", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.founding_figures -> Abraham\n# Answer:\nAbrahamic religions"], "ground_truth": ["Muhammad in Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1525", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> military.military_combatant.military_conflicts -> m.04fvd6y\n# Answer:\nSaudi Arabia"], "ground_truth": ["Saudi Arabia", "Australia", "France", "United Kingdom", "Iraq", "United States of America", "Argentina"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.1846153846153846, "path_precision": 0.75, "path_recall": 0.10526315789473684, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1526", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Romansh language -> language.human_language.region -> Europe\n# Answer:\nRomansh language"], "ground_truth": ["German Language", "Italian Language", "French", "Romansh language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1527", "prediction": ["# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> government.politician.election_campaigns -> Barack Obama presidential campaign, 2012 -> government.election_campaign.party -> Democratic Party\n# Answer:\nBarack Obama presidential campaign, 2012", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> common.topic.notable_types -> Event\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.election_campaigns -> Barack Obama presidential campaign, 2012 -> common.topic.image -> Obama 2012\n# Answer:\nBarack Obama presidential campaign, 2012", "# Reasoning Path:\nBarack Obama -> government.politician.election_campaigns -> Barack Obama 1996 Illinois Senate Campaign -> government.election_campaign.party -> Democratic Party\n# Answer:\nBarack Obama 1996 Illinois Senate Campaign"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1528", "prediction": ["# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.film -> The Butterfly Effect\n# Answer:\nThe Butterfly Effect", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.film -> 3:10 to Yuma\n# Answer:\n3:10 to Yuma", "# Reasoning Path:\nLogan Lerman -> common.topic.article -> m.04f1sc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.04ls5pn -> film.performance.film -> My One and Only\n# Answer:\nMy One and Only", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq21w -> film.performance.character -> Evan Treborn\n# Answer:\nEvan Treborn", "# Reasoning Path:\nLogan Lerman -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.0646ffc -> film.performance.film -> Percy Jackson & the Olympians: The Lightning Thief\n# Answer:\nPercy Jackson & the Olympians: The Lightning Thief", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.07lhlx2 -> film.performance.film -> Gamer\n# Answer:\nGamer", "# Reasoning Path:\nLogan Lerman -> film.actor.film -> m.03jq2p4 -> film.performance.character -> William Evans\n# Answer:\nWilliam Evans"], "ground_truth": ["My One and Only", "The Patriot", "Percy Jackson: Sea of Monsters", "Riding in Cars with Boys", "Fury", "The Number 23", "Meet Bill", "Hoot", "The Scribe", "Noah", "The Three Musketeers", "The Butterfly Effect", "The Only Living Boy in New York", "A Painted House", "What Women Want", "3:10 to Yuma", "Stuck in Love", "The Flannerys", "The Perks of Being a Wallflower", "Gamer", "Percy Jackson & the Olympians: The Lightning Thief"], "ans_acc": 0.23809523809523808, "ans_hit": 1, "ans_f1": 0.33333333333333326, "ans_precission": 0.5555555555555556, "ans_recall": 0.23809523809523808, "path_f1": 0.2380952380952381, "path_precision": 0.5555555555555556, "path_recall": 0.15151515151515152, "path_ans_f1": 0.33333333333333326, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.23809523809523808}
{"id": "WebQTest-1529", "prediction": ["# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.containedby -> Turkey\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_for -> g.1255y5dbj\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Aaron ben Elijah\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Ni\u0161 Constantine the Great Airport -> location.location.containedby -> Yugoslavia\n# Answer:\nNi\u0161 Constantine the Great Airport", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.geolocation -> m.023775s\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> people.deceased_person.place_of_death -> Nicomedia -> location.location.people_born_here -> Arrian\n# Answer:\nNicomedia", "# Reasoning Path:\nConstantine the Great -> symbols.name_source.namesakes -> Ni\u0161 Constantine the Great Airport -> aviation.airport.serves -> Ni\u0161\n# Answer:\nNi\u0161 Constantine the Great Airport"], "ground_truth": ["Nicomedia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-153", "prediction": ["# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> location.location.geolocation -> m.02_sq5m\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> common.topic.image -> Thyssen-Bornemisza\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> travel.travel_destination.tourist_attractions -> Thyssen-Bornemisza Museum -> location.location.geolocation -> m.02_sq5m\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> architecture.museum.type_of_museum -> Art Gallery\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> location.statistical_region.population -> g.11b7vbxnhf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Thyssen-Bornemisza Museum -> visual_art.art_owner.artworks_owned -> m.01012z21\n# Answer:\nThyssen-Bornemisza Museum", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.04sg179\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> base.arthist.helynevek.intezmeny -> Museo Nacional Del Prado -> base.schemastaging.organization_extra.contact_webpages -> m.010hbjzz\n# Answer:\nMuseo Nacional Del Prado", "# Reasoning Path:\nMadrid -> travel.travel_destination.tourist_attractions -> Thyssen-Bornemisza Museum -> common.topic.image -> Thyssen-Bornemisza\n# Answer:\nThyssen-Bornemisza Museum"], "ground_truth": ["Madrid Arena", "Festimad", "El Escorial", "Temple of Debod", "Museo Nacional Centro de Arte Reina Sof\u00eda", "Buen Retiro Park", "La Vaguada", "Plaza de Cibeles", "Summercase", "Sorolla Museum", "Museum of L\u00e1zaro Galdiano", "Thyssen-Bornemisza Museum", "Palace of la Bolsa de Madrid", "Parque Warner Madrid", "Almudena Cathedral", "Puerta del Sol", "Royal Palace of Madrid", "Madrid Marathon", "Plaza Mayor, Madrid", "Gran V\u00eda", "Paseo del Prado", "Museo Nacional Del Prado"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.16494845360824742, "ans_precission": 0.8888888888888888, "ans_recall": 0.09090909090909091, "path_f1": 0.16, "path_precision": 0.6666666666666666, "path_recall": 0.09090909090909091, "path_ans_f1": 0.16494845360824742, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-1531", "prediction": ["# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.containedby -> Africa\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> book.book_subject.works -> Sundiata\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.contains -> Togo\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> organization.organization_scope.organizations_with_this_scope -> g.1237p_yl\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> common.topic.notable_types -> Location\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc16\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nMali -> location.location.containedby -> West Africa -> location.location.contains -> Benin\n# Answer:\nWest Africa", "# Reasoning Path:\nMali -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nMali -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6flnr\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["West Africa", "Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1533", "prediction": ["# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.place_of_birth -> g.120mc0gk\n# Answer:\nfictional_universe.fictional_character.place_of_birth", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Darth Vader\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> common.topic.notable_for -> g.1256pb32r\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Beru Lars\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.fictional_characters_born_here -> Darth Vader\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> book.book_subject.works -> Secrets of Tatooine\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> C-3PO\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.works_set_here -> Return of the Jedi\n# Answer:\nTatooine", "# Reasoning Path:\nLuke Skywalker -> fictional_universe.fictional_character.places_lived -> Tatooine -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Greedo\n# Answer:\nTatooine"], "ground_truth": ["Polis Massa"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1534", "prediction": ["# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> sports.pro_athlete.sports_played_professionally -> m.0d6m4sl\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> Coach\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.profession -> American football player\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.place_of_birth -> West Haven\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh5s -> american_football.football_historical_coach_position.coach -> Lane Kiffin\n# Answer:\nLane Kiffin", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.current_head_coach -> Tony Sparano -> people.person.gender -> Male\n# Answer:\nTony Sparano", "# Reasoning Path:\nOakland Raiders -> sports.sports_team.coaches -> m.0_msn30 -> sports.sports_team_coach_tenure.coach -> Joe Woods\n# Answer:\nJoe Woods", "# Reasoning Path:\nOakland Raiders -> american_football.football_team.historical_coaching_staff -> m.04ykh_2 -> american_football.football_historical_coach_position.coach -> Mike Shanahan\n# Answer:\nMike Shanahan"], "ground_truth": ["Tony Sparano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1535", "prediction": ["# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> location.location.containedby -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.place_founded -> Albuquerque -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAlbuquerque", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization_sector.organizations_in_this_sector -> Exclaimer -> organization.organization.place_founded -> United Kingdom\n# Answer:\nExclaimer", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> location.mailing_address.citytown -> Redmond\n# Answer:\nRedmond", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nMicrosoft Corporation -> business.business_operation.current_liabilities -> m.0906wt9 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMicrosoft Corporation -> computer.programming_language_developer.programming_languages_developed -> F* -> computer.programming_language.language_paradigms -> Imperative programming\n# Answer:\nF*", "# Reasoning Path:\nMicrosoft Corporation -> organization.organization.headquarters -> m.02shzm7 -> common.topic.notable_types -> Location\n# Answer:\nLocation"], "ground_truth": ["Redmond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1536", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> education.education.institution -> Salinas High School\n# Answer:\nSalinas High School", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.0n1dbm0 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJohn Steinbeck -> award.award_nominee.award_nominations -> m.0b9zmns -> award.award_nomination.ceremony -> 1955 National Book Awards\n# Answer:\n1955 National Book Awards", "# Reasoning Path:\nJohn Steinbeck -> people.person.education -> m.02wpt95 -> education.education.major_field_of_study -> Marine biology\n# Answer:\nMarine biology", "# Reasoning Path:\nJohn Steinbeck -> book.author.works_written -> A Russian Journal -> common.topic.notable_for -> g.1256s0x_d\n# Answer:\nA Russian Journal", "# Reasoning Path:\nJohn Steinbeck -> people.person.spouse_s -> m.0j2z_hc -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Stanford University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1537", "prediction": ["# Reasoning Path:\nNiels Henrik David Bohr -> base.nobelprizes.nobel_prize_winner.nobel_honor -> m.064wn6r -> base.nobelprizes.nobel_honor.subject_area -> Quantum mechanics\n# Answer:\nQuantum mechanics", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> common.topic.notable_for -> g.1258dvpk0\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> influence.influence_node.influenced -> Lise Meitner -> influence.influence_node.influenced_by -> Max Planck\n# Answer:\nLise Meitner", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> business.employer.employees -> m.04kr8tr\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> 3948 Bohr -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\n3948 Bohr", "# Reasoning Path:\nNiels Henrik David Bohr -> symbols.name_source.namesakes -> Niels Bohr Institute -> common.topic.article -> m.0dz_p\n# Answer:\nNiels Bohr Institute", "# Reasoning Path:\nNiels Henrik David Bohr -> influence.influence_node.influenced -> Lise Meitner -> influence.influence_node.influenced -> Otto Hahn\n# Answer:\nLise Meitner", "# Reasoning Path:\nNiels Henrik David Bohr -> base.kwebbase.kwtopic.connections_from -> niels henrik david bohr activist with julius robert oppenheimer -> base.kwebbase.kwconnection.other -> J. Robert Oppenheimer\n# Answer:\nniels henrik david bohr activist with julius robert oppenheimer"], "ground_truth": ["Quantum mechanics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1538", "prediction": ["# Reasoning Path:\nLouis Sachar -> common.topic.article -> m.01bq8n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> common.topic.notable_types -> Book\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> Richard Price -> influence.influence_node.influenced_by -> Hubert Selby, Jr.\n# Answer:\nRichard Price", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Walt Disney Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Holes -> book.written_work.next_in_series -> Stanley Yelnats' Survival Guide to Camp Green Lake\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.starring -> m.02tb466\n# Answer:\nHoles"], "ground_truth": ["Sideways Arithmetic From Wayside School", "Sixth grade secrets", "Small Steps (Readers Circle)", "There's a boy in the girls bathroom", "Wayside School is falling down", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Someday Angeline", "Louis Sacher Collection", "Sixth Grade Secrets (Apple Paperbacks)", "Someday Angeline (Avon/Camelot Book)", "L\u00f6cher", "Small steps", "Why Pick on Me?", "Wayside School gets a little stranger", "Johnny's in the Basement", "Wayside School Boxed Set", "Monkey soup", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Kidnapped at Birth?", "Dogs Don't Tell Jokes", "Johnny's in the basement", "Small Steps", "Super Fast, Out of Control!", "More Sideways Arithmetic from Wayside School", "Wayside School Gets A Little Stranger", "Marvin Redpost.", "Pequenos Pasos/ Small Steps", "There's a Boy in the Girls' Bathroom", "Class President (A Stepping Stone Book(TM))", "Class President", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Wayside School Collection", "Holes (Cascades)", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "Holes Activity Pack", "The Cardturner", "Boy Who Lost His Face", "More Sideways Arithmetic From Wayside School", "Wayside School is Falling Down", "Wayside School Gets a Little Stranger", "A magic crystal?", "Hoyos/Holes", "Holes (Readers Circle)", "Alone in His Teacher's House", "Holes. (Lernmaterialien)", "The boy who lost his face", "Why Pick on Me? (A Stepping Stone Book(TM))", "Sideways stories from Wayside School", "Hay Un Chico En El Bano De Las Chicas", "Sixth Grade Secrets", "Sideways Arithmetic from Wayside School", "Stanley Yelnats Survival Guide to Camp Green Lake", "Marvin Redpost", "The Boy Who Lost His Face", "Holes (Yearling Books)", "Holes (Listening Library)", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "Il y a un gar\u00e7on dans les toilettes des filles", "A Flying Birthday Cake?", "Der Fluch des David Ballinger. ( Ab 11 J.).", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Holes", "Holes (World Book Day 2001)", "Sixth Grade Secrets (An Apple Paperback)", "g.1218f5g0", "Wayside School Is Falling Down", "Sideways Stories from Wayside School"], "ans_acc": 0.10294117647058823, "ans_hit": 1, "ans_f1": 0.1343570057581574, "ans_precission": 0.7777777777777778, "ans_recall": 0.07352941176470588, "path_f1": 0.21666666666666665, "path_precision": 0.3333333333333333, "path_recall": 0.16049382716049382, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.10294117647058823}
{"id": "WebQTest-1539", "prediction": ["# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.spouse_s -> m.0pbmp4r\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrd3d -> people.marriage.spouse -> Scarlett Johansson\n# Answer:\nScarlett Johansson", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Patrick Reynolds\n# Answer:\nTammy Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrd3d -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Jim Reynolds -> people.person.children -> Jeff Reynolds\n# Answer:\nJim Reynolds", "# Reasoning Path:\nRyan Reynolds -> people.person.spouse_s -> m.0ygrgyh -> people.marriage.spouse -> Blake Lively\n# Answer:\nBlake Lively", "# Reasoning Path:\nRyan Reynolds -> people.person.parents -> Tammy Reynolds -> people.person.children -> Terry Reynolds\n# Answer:\nTammy Reynolds"], "ground_truth": ["Blake Lively"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-154", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.original_language -> English Language\n# Answer:\nTrumpet"], "ground_truth": ["Cornet", "Trumpet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1540", "prediction": ["# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> people.marriage.spouse -> Marianne Ginther\n# Answer:\nMarianne Ginther", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j17836 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0j1782r -> people.marriage.spouse -> Jackie Battley\n# Answer:\nJackie Battley", "# Reasoning Path:\nNewt Gingrich -> people.person.children -> Jackie Gingrich Cushman -> people.person.parents -> Jackie Battley\n# Answer:\nJackie Gingrich Cushman", "# Reasoning Path:\nNewt Gingrich -> people.person.profession -> g.121bkpjb\n# Answer:\npeople.person.profession", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.spouse -> Callista Gingrich\n# Answer:\nCallista Gingrich", "# Reasoning Path:\nNewt Gingrich -> people.person.children -> Kathy Lubbers -> people.person.spouse_s -> m.03sc8bv\n# Answer:\nKathy Lubbers", "# Reasoning Path:\nNewt Gingrich -> people.person.spouse_s -> m.0hybx80 -> people.marriage.location_of_ceremony -> Alexandria\n# Answer:\nAlexandria"], "ground_truth": ["Callista Gingrich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1541", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> common.topic.article -> m.0ptt\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> ORSRADIO.COM Classical\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> common.topic.notable_for -> g.125dwc37n\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.content.broadcast -> BR-Klassik\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> base.musiteca.musicalstyle.concerts -> ConciertoTest\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music"], "ground_truth": ["Classical music", "Baroque music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1542", "prediction": ["# Reasoning Path:\nJames Franco -> people.person.education -> m.0n1n3d5 -> education.education.institution -> Rhode Island School of Design\n# Answer:\nRhode Island School of Design", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0cjcrvl -> education.education.institution -> Tisch School of the Arts\n# Answer:\nTisch School of the Arts", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.institution -> Brooklyn College\n# Answer:\nBrooklyn College", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0d0_10p -> education.education.institution -> Palo Alto High School\n# Answer:\nPalo Alto High School", "# Reasoning Path:\nJames Franco -> people.person.education -> m.064zvgv -> education.education.institution -> University of California, Los Angeles\n# Answer:\nUniversity of California, Los Angeles", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0gxbcv6 -> education.education.degree -> Master of Fine Arts\n# Answer:\nMaster of Fine Arts", "# Reasoning Path:\nJames Franco -> common.topic.webpage -> m.098dnd6 -> common.webpage.resource -> Where are the ''Freaks and Geeks'' now?\n# Answer:\nWhere are the ''Freaks and Geeks'' now?", "# Reasoning Path:\nJames Franco -> people.person.education -> m.0cjcrvl -> education.education.major_field_of_study -> Filmmaking\n# Answer:\nFilmmaking"], "ground_truth": ["Rhode Island School of Design", "Palo Alto High School", "Warren Wilson College", "Brooklyn College", "Tisch School of the Arts", "University of California, Los Angeles", "University Yale", "Columbia University School of the Arts"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.625, "ans_precission": 0.625, "ans_recall": 0.625, "path_f1": 0.625, "path_precision": 0.625, "path_recall": 0.625, "path_ans_f1": 0.625, "path_ans_precision": 0.625, "path_ans_recall": 0.625}
{"id": "WebQTest-1543", "prediction": ["# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specializations -> Raconteur\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> people.profession.specialization_of -> Writer\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> common.topic.notable_for -> g.1256kn4dk\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Humorist -> common.topic.notable_types -> Profession\n# Answer:\nHumorist", "# Reasoning Path:\nMark Twain -> people.person.profession -> Teacher -> common.topic.notable_types -> Profession\n# Answer:\nTeacher", "# Reasoning Path:\nMark Twain -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nMark Twain -> people.person.children -> Clara Clemens -> people.person.profession -> Singer\n# Answer:\nClara Clemens"], "ground_truth": ["Humorist", "Journalist", "Teacher", "Author", "Writer", "Novelist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1544", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.01xrx8q -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.02h6phn -> government.government_position_held.office_holder -> George W. Bush\n# Answer:\nGeorge W. Bush", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident"], "ground_truth": ["John Tyler"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1546", "prediction": ["# Reasoning Path:\nRick Santorum -> people.person.education -> m.0n11xm8 -> education.education.institution -> Carmel High School\n# Answer:\nCarmel High School", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.012gg3yy -> education.education.institution -> Joseph M. Katz Graduate School of Business\n# Answer:\nJoseph M. Katz Graduate School of Business", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.02wn695 -> education.education.institution -> Pennsylvania State University\n# Answer:\nPennsylvania State University", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.04hx35s -> education.education.institution -> University of Pittsburgh\n# Answer:\nUniversity of Pittsburgh", "# Reasoning Path:\nRick Santorum -> people.person.education -> m.062tbvf -> education.education.institution -> Pennsylvania State University - Dickinson Law\n# Answer:\nPennsylvania State University - Dickinson Law", "# Reasoning Path:\nRick Santorum -> business.board_member.organization_board_memberships -> m.09tymds -> organization.organization_board_membership.organization -> Universal Health Services\n# Answer:\nUniversal Health Services", "# Reasoning Path:\nRick Santorum -> government.politician.party -> m.03gjgj7 -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nRick Santorum -> people.person.languages -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language"], "ground_truth": ["Carmel High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1547", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_types -> Book\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> The George W. Bush foreign policy reader -> common.topic.notable_for -> g.125b58jd4\n# Answer:\nThe George W. Bush foreign policy reader", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.author -> Michael Herskowitz\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> book.written_work.next_in_series -> Decision Points\n# Answer:\nA Charge to Keep", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> Avec l'aide de Dieu -> common.topic.notable_types -> Book\n# Answer:\nAvec l'aide de Dieu", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> event.public_speaker.speeches_or_presentations -> m.05_fz1k -> event.speech_or_presentation.type_or_format_of_presentation -> State of the Union\n# Answer:\nState of the Union", "# Reasoning Path:\nGeorge W. Bush -> book.author.works_written -> A Charge to Keep -> common.topic.article -> m.03cwz0w\n# Answer:\nA Charge to Keep"], "ground_truth": ["In My Time: A Personal and Political Memoir"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1548", "prediction": ["# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> location.location.contains -> Luxor Obelisk\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> common.topic.article -> m.0kx9q\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> base.schemastaging.context_name.pronunciation -> g.11b7zfzkkp\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> common.topic.image -> Place de la concorde\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> people.deceased_person.place_of_death -> Place de la Concorde -> location.location.geolocation -> m.02_vqwf\n# Answer:\nPlace de la Concorde", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Joseph Priestley -> people.deceased_person.place_of_death -> Pennsylvania\n# Answer:\nJoseph Priestley", "# Reasoning Path:\nAntoine Lavoisier -> freebase.valuenotation.is_reviewed -> Place of death -> type.property.schema -> Deceased Person\n# Answer:\nPlace of death", "# Reasoning Path:\nAntoine Lavoisier -> influence.influence_node.influenced_by -> Guillaume-Fran\u00e7ois Rouelle -> people.deceased_person.place_of_death -> Paris\n# Answer:\nGuillaume-Fran\u00e7ois Rouelle"], "ground_truth": ["Place de la Concorde"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-155", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> time.event.instance_of_recurring_event -> United States presidential inauguration\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> time.event.instance_of_recurring_event -> United States presidential inauguration\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> common.topic.image -> GWBush1.jpg\n# Answer:\nGeorge W. Bush 2001 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.election -> United States presidential election, 2000\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2005 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nGeorge W. Bush 2005 presidential inauguration", "# Reasoning Path:\nGeorge W. Bush -> base.inaugurations.inauguration_speaker.inauguration -> George W. Bush 2001 presidential inauguration -> common.topic.notable_for -> g.1258jzwbw\n# Answer:\nGeorge W. Bush 2001 presidential inauguration"], "ground_truth": ["2004-11-02", "2000-11-07"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1550", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.spouse_s -> m.0j4k1c9 -> people.marriage.spouse -> Queen Elizabeth The Queen Mother\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Prince Edward, Earl of Wessex\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.family_member.family -> House of Windsor\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Anne, Princess Royal\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.children -> David Armstrong-Jones, Viscount Linley\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.spouse_s -> m.028zms8\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Charles, Prince of Wales\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Prince Andrew, Duke of York\n# Answer:\nElizabeth II"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1551", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> sports.sports_championship_event.season -> 2013 NFL season\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Season\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsdt -> sports.sports_team_season_record.season -> 2005 NFL season\n# Answer:\n2005 NFL season", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.season_record -> m.075fsnp -> sports.sports_team_season_record.season -> 1976 NFL season\n# Answer:\n1976 NFL season"], "ground_truth": ["Super Bowl XL", "Super Bowl XLVIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1553", "prediction": ["# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> base.biblioness.bibs_location.country -> Iran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.places_lived -> m.03pn2p8 -> people.place_lived.location -> Tehran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> base.biblioness.bibs_location.country -> Iran\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.spouse_s -> m.0n8jnbt -> people.marriage.spouse -> Mariam Yusuf\n# Answer:\nMariam Yusuf", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> common.topic.notable_types -> City/Town/Village\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> music.artist.origin -> Tehran -> location.location.events -> 1983 Asian Wrestling Championships\n# Answer:\nTehran", "# Reasoning Path:\nSami Yusuf -> people.person.place_of_birth -> Tehran -> location.location.events -> 1974 Tehran International Short Film Festival\n# Answer:\nTehran"], "ground_truth": ["Tehran"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1554", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.country.official_language -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Portuguese Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-1555", "prediction": ["# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Nepali Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.official_language -> Nepali Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Bhojpuri Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.currency_used -> Nepalese rupee\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> base.locations.countries.continent -> Asia\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Awadhi Language\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> government.governmental_body.jurisdiction -> Nepal -> location.country.languages_spoken -> Bajjika\n# Answer:\nNepal", "# Reasoning Path:\nParliament of Nepal -> common.topic.image -> Nepal Coat of Arms -> common.image.appears_in_topic_gallery -> Emblem of Nepal\n# Answer:\nNepal Coat of Arms", "# Reasoning Path:\nParliament of Nepal -> common.topic.article -> m.0c5yqq\n# Answer:\ncommon.topic.article"], "ground_truth": ["Parliament of Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1556", "prediction": ["# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Fenway Park\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> common.topic.article -> m.01q9lj\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1986 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1986 World Series", "# Reasoning Path:\nNew York Mets -> organization.organization.board_members -> m.05_428v -> organization.organization_board_membership.member -> Fred Wilpon\n# Answer:\nFred Wilpon", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> time.event.locations -> Shea Stadium\n# Answer:\n1969 World Series", "# Reasoning Path:\nNew York Mets -> sports.sports_team.championships -> 1969 World Series -> film.film_subject.films -> 1969 World Series Highlight Film\n# Answer:\n1969 World Series"], "ground_truth": ["1986 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1557", "prediction": ["# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> sports.sports_facility.teams -> Miami Matadors\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> location.location.containedby -> Sunrise\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> sports.sports_facility.home_venue_for -> m.0wz2lgr\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> location.location.containedby -> Miami\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> location.location.geolocation -> m.02_vl7c\n# Answer:\nBB&T Center", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> common.topic.article -> m.039xm3\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> Miami Arena -> common.topic.notable_types -> Sports Facility\n# Answer:\nMiami Arena", "# Reasoning Path:\nFlorida Panthers -> sports.sports_team.arena_stadium -> BB&T Center -> location.location.events -> Armageddon 1999\n# Answer:\nBB&T Center"], "ground_truth": ["BB&T Center", "Miami Arena"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1558", "prediction": ["# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.spouse_s -> m.0kbm513\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> people.marriage.spouse -> Elizabeth Huberdeau\n# Answer:\nElizabeth Huberdeau", "# Reasoning Path:\nJohn Cena -> people.person.parents -> John Cena Sr. -> people.person.spouse_s -> m.0kbm513\n# Answer:\nJohn Cena Sr.", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.children -> Dan Cena\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> common.topic.notable_for -> g.125hbdcdp\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.spouse_s -> m.07n79hl -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> common.topic.notable_types -> Person\n# Answer:\nCarol Cena", "# Reasoning Path:\nJohn Cena -> people.person.parents -> Carol Cena -> people.person.children -> Matt Cena\n# Answer:\nCarol Cena"], "ground_truth": ["Elizabeth Huberdeau"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1559", "prediction": ["# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> location.location.geolocation -> m.0d4qkys\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.webpage -> m.03l4p6m\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.notable_types -> Zoo\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Cinequest Film Festival -> film.film_festival.individual_festivals -> 1990 Cinequest Film Festival\n# Answer:\nCinequest Film Festival", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> common.topic.article -> m.072p6d\n# Answer:\nSantana Row", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Happy Hollow Park & Zoo -> common.topic.webpage -> m.0bd3sxn\n# Answer:\nHappy Hollow Park & Zoo", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Cinequest Film Festival -> common.topic.notable_types -> Film festival\n# Answer:\nCinequest Film Festival", "# Reasoning Path:\nSan Jose -> travel.travel_destination.tourist_attractions -> Santana Row -> location.location.containedby -> United States of America\n# Answer:\nSantana Row"], "ground_truth": ["Alum Rock Park", "Kelley Park", "Happy Hollow Park & Zoo", "Downtown San Jose", "San Jose Municipal Rose Garden", "San Jose Museum of Quilts & Textiles", "San Jose Museum of Art", "Rosicrucian Egyptian Museum", "The Tech Museum of Innovation", "Japantown", "Rock \u2018n\u2019 Roll San Jose Half Marathon", "Santana Row", "Cinequest Film Festival", "Children's Discovery Museum of San Jose", "Winchester Mystery House"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-156", "prediction": ["# Reasoning Path:\nCalifornia -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> The Cell -> film.film.country -> United States of America\n# Answer:\nThe Cell", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> time.event.instance_of_recurring_event -> Wine Country Film Festival\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> film.film_location.featured_in_films -> Crazy in Alabama -> film.film.country -> United States of America\n# Answer:\nCrazy in Alabama", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> freebase.valuenotation.has_value -> Official website\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1987 Wine Country Film Festival -> common.topic.notable_types -> Film festival event\n# Answer:\n1987 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> location.location.events -> 1988 Wine Country Film Festival -> time.event.locations -> Sonoma Valley\n# Answer:\n1988 Wine Country Film Festival", "# Reasoning Path:\nCalifornia -> government.political_district.representatives -> m.02_ws7d -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1560", "prediction": ["# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> organization.organization_founder.organizations_founded -> American Ethnological Society\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> people.person.profession -> Teacher\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> book.author.works_written -> Correspondence and documents relative to the attempt to negotiate for the release of the American captives at Algiers\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> common.topic.notable_types -> Organization founder\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> common.topic.image -> Noah's book Travels in England, France, Spain, and the Barbary States, in the Years 1813-14 and 15\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Mordecai Manuel Noah -> people.person.profession -> Diplomat\n# Answer:\nMordecai Manuel Noah", "# Reasoning Path:\nNew York University -> organization.organization.founders -> Albert Gallatin -> people.deceased_person.place_of_death -> Astoria\n# Answer:\nAlbert Gallatin", "# Reasoning Path:\nNew York University -> law.litigant.party_to_cases -> m.0dggt0t -> law.legal_case_party_relationship.case -> New York University v. Ariel Fund Ltd\n# Answer:\nNew York University v. Ariel Fund Ltd"], "ground_truth": ["Albert Gallatin", "Mordecai Manuel Noah"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1561", "prediction": ["# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> tv.tv_program_creator.programs_created -> Dino Babies\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.notable_types -> Production company\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> tv.tv_program_creator.programs_created -> Flight Squad\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> tv.tv_program_creator.programs_created -> Postcards from Buster\n# Answer:\nMarc Brown", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> film.production_company.films -> Voulez-vous coucher avec God?\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Marc Brown -> fictional_universe.fictional_character_creator.fictional_characters_created -> Arthur Read\n# Answer:\nMarc Brown", "# Reasoning Path:\nArthur -> tv.tv_program.program_creator -> Cookie Jar Group -> common.topic.webpage -> m.04m3pcs\n# Answer:\nCookie Jar Group", "# Reasoning Path:\nArthur -> award.award_nominated_work.award_nominations -> m.010g23zh -> award.award_nomination.award_nominee -> Alan Cumming\n# Answer:\nAlan Cumming"], "ground_truth": ["WGBH", "Cookie Jar Group", "Marc Brown"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1562", "prediction": ["# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> United Kingdom\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Sweden\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> government.governmental_jurisdiction.governing_officials -> m.010pyz2k\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Republic of Ireland\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> organization.organization_scope.organizations_with_this_scope -> Council of Europe\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> organization.organization.geographic_scope -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nEuropean Union -> internet.website_owner.websites_owned -> http://europa.eu/index_en.htm -> common.topic.notable_types -> Website\n# Answer:\nhttp://europa.eu/index_en.htm", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\nlocation.statistical_region.broadband_penetration_rate"], "ground_truth": ["Slovakia", "Lithuania", "Latvia", "Luxembourg", "Republic of Ireland", "France", "Poland", "Denmark", "Malta", "Bulgaria", "Cyprus", "Croatia", "Italy", "United Kingdom", "Netherlands", "Sweden", "Portugal", "Spain", "Slovenia", "Romania", "Estonia", "Austria", "Finland", "Belgium", "Germany", "Greece", "Hungary", "Czech Republic"], "ans_acc": 0.17857142857142858, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.11267605633802817, "path_precision": 0.4, "path_recall": 0.06557377049180328, "path_ans_f1": 0.2631578947368421, "path_ans_precision": 0.5, "path_ans_recall": 0.17857142857142858}
{"id": "WebQTest-1563", "prediction": ["# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Africa\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Suez Governorate -> location.location.contains -> Ain Sukhna\n# Answer:\nSuez Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.containedby -> Arab world\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.time_zones -> East Africa Time Zone\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.location.partiallycontains -> m.0wg8_26\n# Answer:\nSudan", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Ptolemaic Kingdom -> location.location.containedby -> Southern Europe\n# Answer:\nPtolemaic Kingdom", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Suez Governorate -> location.administrative_division.country -> Egypt\n# Answer:\nSuez Governorate", "# Reasoning Path:\nNorth Africa -> location.location.contains -> Sudan -> location.country.official_language -> English Language\n# Answer:\nSudan"], "ground_truth": ["Western Roman Empire", "Roman Republic", "United Arab Republic", "Sudan", "Caliphate of C\u00f3rdoba", "Rashidun Caliphate", "Ptolemaic Kingdom"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4137931034482759, "ans_precission": 0.75, "ans_recall": 0.2857142857142857, "path_f1": 0.4137931034482759, "path_precision": 0.75, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4137931034482759, "path_ans_precision": 0.75, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1564", "prediction": ["# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> people.deceased_person.cause_of_death -> Surgical complications -> common.topic.notable_for -> g.12565x30z\n# Answer:\nSurgical complications", "# Reasoning Path:\nBobby Darin -> tv.tv_actor.guest_roles -> m.0bnfq0r -> tv.tv_guest_role.episodes_appeared_in -> Bobby Darin / Connie Francis / The Crests / Wilbert Harrison / The G-Notes\n# Answer:\nBobby Darin / Connie Francis / The Crests / Wilbert Harrison / The G-Notes", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> I'll Be There -> music.composition.recordings -> I\u2019ll Be There\n# Answer:\nI'll Be There", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Boss Barracuda -> music.composition.composer -> Terry Melcher\n# Answer:\nBoss Barracuda", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> As Long as I'm Singing -> music.single.versions -> As Long as I'm Singing (Quickstep, 52mpm)\n# Answer:\nAs Long as I'm Singing", "# Reasoning Path:\nBobby Darin -> music.composer.compositions -> Boss Barracuda -> music.composition.recordings -> Boss Baracuda\n# Answer:\nBoss Barracuda"], "ground_truth": ["Surgical complications"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1565", "prediction": ["# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> United States of America\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.notable_types -> Sports Facility\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.article -> m.0413pb7\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> common.topic.notable_for -> g.12577x3hc\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.location -> Houston -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHouston", "# Reasoning Path:\nHouston Dynamo -> sports.sports_team.arena_stadium -> BBVA Compass Stadium -> location.location.containedby -> East Downtown Houston\n# Answer:\nBBVA Compass Stadium", "# Reasoning Path:\nHouston Dynamo -> common.topic.notable_types -> Football team -> freebase.type_profile.strict_included_types -> Abstract\n# Answer:\nFootball team"], "ground_truth": ["Houston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1566", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Neotraditional country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> common.topic.article -> m.0mhf_\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Rock music\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.subgenre -> Southern rock\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Blues rock -> music.genre.parent_genre -> Jam band\n# Answer:\nBlues rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> common.topic.notable_types -> Musical genre\n# Answer:\nCountry rock"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1567", "prediction": ["# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1568", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nPunahou School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nNoelani Elementary School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.specialization -> International relations\n# Answer:\nInternational relations", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science"], "ground_truth": ["Columbia University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1569", "prediction": ["# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> common.topic.notable_types -> Currency\n# Answer:\nJamaican dollar", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2gx3\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nJamaica -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6hby7\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Yucat\u00e1n Peninsula\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1988 Atlantic hurricane season\n# Answer:\nTropical Storm Keith", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-157", "prediction": ["# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> United States of America\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> common.topic.article -> m.0gbsh\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> California\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> 93001 -> location.location.containedby -> Ventura County\n# Answer:\n93001", "# Reasoning Path:\nMission San Buenaventura -> common.topic.notable_for -> g.1259zm5db\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.location.people_born_here -> George E. Outland\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.hud_foreclosure_area.estimated_number_foreclosures -> m.07hb128\n# Answer:\nVentura County", "# Reasoning Path:\nMission San Buenaventura -> location.location.geolocation -> m.0cm5d5z\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nMission San Buenaventura -> location.location.containedby -> Ventura County -> location.statistical_region.population -> g.11b66gyw72\n# Answer:\nVentura County"], "ground_truth": ["93001", "Ventura County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1570", "prediction": ["# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.containedby -> Glendale\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> common.topic.image -> UofPStadiumLogo\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 BCS National Championship Game\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> sports.sports_facility.home_venue_for -> m.0wz1z7v\n# Answer:\nUniversity of Phoenix Stadium", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> law.court_jurisdiction_area.courts -> United States Bankruptcy Court, District of Arizona\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.location -> Arizona -> organization.organization_scope.organizations_with_this_scope -> Brazilian Consulate General, Los Angeles\n# Answer:\nArizona", "# Reasoning Path:\nArizona Cardinals -> sports.sports_team.arena_stadium -> University of Phoenix Stadium -> location.location.events -> 2007 Fiesta Bowl\n# Answer:\nUniversity of Phoenix Stadium"], "ground_truth": ["Arizona"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1571", "prediction": ["# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.article -> m.01kffv\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> base.schemastaging.context_name.pronunciation -> g.125_lhw3p\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> common.topic.image -> Isaacnewton\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's laws of motion -> common.topic.article -> m.0fchz\n# Answer:\nNewton's laws of motion", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton's law of universal gravitation -> common.topic.image -> WMAP image of the (extremely tiny) anisotropies in the cosmic background radiation\n# Answer:\nNewton's law of universal gravitation", "# Reasoning Path:\nIsaac Newton -> base.argumentmaps.innovator.original_ideas -> Newton's law of universal gravitation -> base.schemastaging.context_name.pronunciation -> g.125_lhw3p\n# Answer:\nNewton's law of universal gravitation"], "ground_truth": ["Newton's law of universal gravitation"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1572", "prediction": ["# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> M. H. de Young Memorial Museum\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> AIDS Memorial Grove\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> California Academy of Sciences\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> Conservatory of Flowers\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> location.location.containedby -> California\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> San Francisco Museum of Modern Art -> architecture.building_occupant.buildings_occupied -> m.0jjfd0b\n# Answer:\nSan Francisco Museum of Modern Art", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> travel.travel_destination.tourist_attractions -> Kezar Stadium\n# Answer:\nGolden Gate Park", "# Reasoning Path:\nSan Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park -> base.usnris.nris_listing.significance_level -> National\n# Answer:\nGolden Gate Park"], "ground_truth": ["Mus\u00e9e M\u00e9canique", "San Francisco Fire Department Museum", "St. Regis Museum Tower", "Palace of the Legion of Honor", "Ripley's Believe It or Not! Museum", "Angel Island", "Coit Tower", "San Francisco Ferry Building", "Cartoon Art Museum", "Crissy Field", "Presidio of San Francisco", "San Francisco City Hall", "Asian Art Museum of San Francisco", "Chinatown", "San Francisco cable car system", "Fisherman's Wharf", "Ghirardelli Square", "Camera Obscura", "Haas-Lilienthal House", "Travefy", "San Francisco Railway Museum", "Twin Peaks", "Contemporary Jewish Museum", "San Francisco Museum of Modern Art", "Golden Gate Park", "Yerba Buena Center for the Arts", "Consulate General of Mexico, San Francisco", "Baker Beach", "Golden Gate Bridge", "Japanese Tea Garden", "Lombard Street", "Exploratorium", "Museum of the African Diaspora", "Alcatraz Island", "Union Square"], "ans_acc": 0.05714285714285714, "ans_hit": 1, "ans_f1": 0.1081081081081081, "ans_precission": 1.0, "ans_recall": 0.05714285714285714, "path_f1": 0.1081081081081081, "path_precision": 1.0, "path_recall": 0.05714285714285714, "path_ans_f1": 0.1081081081081081, "path_ans_precision": 1.0, "path_ans_recall": 0.05714285714285714}
{"id": "WebQTest-1573", "prediction": ["# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced_by -> Peter Paul Rubens\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Paul C\u00e9zanne -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix\n# Answer:\nPaul C\u00e9zanne", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> book.author.school_or_movement -> Romanticism\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Eug\u00e8ne Delacroix -> people.person.gender -> Male\n# Answer:\nEug\u00e8ne Delacroix", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Diego Vel\u00e1zquez -> influence.influence_node.influenced -> \u00c9douard Manet\n# Answer:\nDiego Vel\u00e1zquez", "# Reasoning Path:\nPablo Picasso -> influence.influence_node.influenced_by -> Sara Murphy -> influence.influence_node.influenced -> Robert Benchley\n# Answer:\nSara Murphy"], "ground_truth": ["\u00c9douard Manet", "Vincent van Gogh", "Paul C\u00e9zanne", "Henri Rousseau", "Sara Murphy", "Henri de Toulouse-Lautrec", "Jean-Auguste-Dominique Ingres", "Eug\u00e8ne Delacroix", "Alfred Jarry", "Paul Gauguin", "Francisco Goya", "Diego Vel\u00e1zquez"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5882352941176471, "path_ans_precision": 1.0, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-1574", "prediction": ["# Reasoning Path:\nPennsylvania -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_for -> g.125fnb_r1\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.person -> Rosemarie B. Greco\n# Answer:\nRosemarie B. Greco", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.image -> Timezones2008 UTC-5\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> location.location.time_zones -> UTC\u221205:00 -> common.topic.article -> m.02_384\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nPennsylvania -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Henri -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nTropical Storm Henri"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1575", "prediction": ["# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Gmail\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Chrome -> common.topic.subjects -> Google Apps for Work\n# Answer:\nGoogle Chrome", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization_partnership.members -> Redcorus\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subjects -> Google for Work\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> common.topic.subject_of -> Google for Education\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.email -> m.012hr_p8\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> common.topic.subjects -> Internet Marketing\n# Answer:\nCleverly Engaged Marketing", "# Reasoning Path:\nGoogle -> organization.organization_partnership.members -> Cleverly Engaged Marketing -> organization.organization.founders -> Corey Barnett\n# Answer:\nCleverly Engaged Marketing"], "ground_truth": ["Google Maps", "Nexus S", "Google Glass", "Apache Wave", "Google Docs, Sheets, and Slides", "Google Buzz", "Google Drive", "Google Earth", "Nexus Q", "Google Classroom", "Nexus 7", "Chromecast", "Nexus 10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1577", "prediction": ["# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r7541 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r7541 -> baseball.batting_statistics.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r7524 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> sports.pro_athlete.teams -> m.0j2qj2k -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74_5 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74vn -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nCliff Lee -> base.schemastaging.athlete_extra.salary -> m.0j2spt6 -> base.schemastaging.athlete_salary.team -> Philadelphia Phillies\n# Answer:\nPhiladelphia Phillies", "# Reasoning Path:\nCliff Lee -> baseball.baseball_player.batting_stats -> m.06r74x1 -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians"], "ground_truth": ["Philadelphia Phillies"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1580", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Marshall Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1582", "prediction": ["# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Lantau Island\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.11b60qhw67\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Lamma Island\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxn\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Asia\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6g4jl\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nHong Kong -> location.statistical_region.official_development_assistance -> g.1hhc388l5\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nHong Kong -> location.location.time_zones -> Hong Kong Time Zone -> time.time_zone.locations_in_this_time_zone -> Central and Western District\n# Answer:\nHong Kong Time Zone", "# Reasoning Path:\nHong Kong -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60qlhr9\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Hong Kong Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1583", "prediction": ["# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.honored_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0z837pz -> award.award_nomination.nominated_for -> Abduction\n# Answer:\nAbduction", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0kb9_qw -> award.award_honor.award -> Teen Choice Award for Choice Movie Actor: Drama/Action Adventure\n# Answer:\nTeen Choice Award for Choice Movie Actor: Drama/Action Adventure", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0g4t6qq -> award.award_honor.honored_for -> Eclipse\n# Answer:\nEclipse", "# Reasoning Path:\nTaylor Lautner -> award.award_nominee.award_nominations -> m.0sgl6rq -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nTaylor Lautner -> film.actor.film -> m.0h7fzt3 -> film.performance.film -> Field of Dreams 2: Lockout\n# Answer:\nField of Dreams 2: Lockout", "# Reasoning Path:\nTaylor Lautner -> award.award_winner.awards_won -> m.0nf3bp2 -> award.award_honor.honored_for -> The Twilight Saga\n# Answer:\nThe Twilight Saga"], "ground_truth": ["Abduction", "The Ridiculous Six", "He's a Bully, Charlie Brown", "The Twilight Saga: New Moon", "Run the Tide", "Tracers", "The Adventures of Sharkboy and Lavagirl", "Twilight", "Cheaper by the Dozen 2", "Field of Dreams 2: Lockout", "The Twilight Saga: Breaking Dawn - Part 1", "Northern Lights", "Grown Ups 2", "Shadow Fury", "The Twilight Saga: Breaking Dawn - Part 2", "Incarceron", "Eclipse", "The Nick and Jessica Variety Hour", "Valentine's Day"], "ans_acc": 0.2631578947368421, "ans_hit": 1, "ans_f1": 0.38961038961038963, "ans_precission": 0.75, "ans_recall": 0.2631578947368421, "path_f1": 0.13513513513513511, "path_precision": 0.625, "path_recall": 0.07575757575757576, "path_ans_f1": 0.38961038961038963, "path_ans_precision": 0.75, "path_ans_recall": 0.2631578947368421}
{"id": "WebQTest-1584", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Weight loss\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Cutaneous manifestations of sarcoidosis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Garland's triad\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Uveoparotitis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.notable_for -> g.1254zhcvf\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Complications from pneumonia -> common.topic.notable_for -> g.1yl5vldtz\n# Answer:\nComplications from pneumonia", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> people.cause_of_death.people -> Joseph Scoren\n# Answer:\nSarcoidosis"], "ground_truth": ["Sarcoidosis", "Complications from pneumonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1585", "prediction": ["# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.actor -> Steven Blum\n# Answer:\nSteven Blum", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.actor -> K\u014dichi Yamadera\n# Answer:\nK\u014dichi Yamadera", "# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.actor -> K\u014dichi Yamadera\n# Answer:\nK\u014dichi Yamadera", "# Reasoning Path:\nSpike Spiegel -> base.schemastaging.tv_character_extra.regular_dubbing_performances -> m.0nh3wqf -> base.schemastaging.tv_star_dubbing_performance.program -> Cowboy Bebop\n# Answer:\nCowboy Bebop", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg03s -> film.performance.film -> Cowboy Bebop: The Movie\n# Answer:\nCowboy Bebop: The Movie", "# Reasoning Path:\nSpike Spiegel -> tv.tv_character.appeared_in_tv_program -> m.0gj1nnq -> tv.regular_tv_appearance.series -> Cowboy Bebop\n# Answer:\nCowboy Bebop", "# Reasoning Path:\nSpike Spiegel -> film.film_character.portrayed_in_films -> m.03lg034 -> film.performance.actor -> Steven Blum\n# Answer:\nSteven Blum"], "ground_truth": ["K\u014dichi Yamadera", "Steven Blum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1586", "prediction": ["# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> John, Constable of Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.parents -> Isabel of Barcelos\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.nationality -> Portugal\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.children -> Alfonso, Prince of Asturias\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> people.person.place_of_birth -> Toledo\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Catherine of Lancaster\n# Answer:\nJohn II of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> Isabella of Portugal, Queen of Castile -> common.topic.image -> Isabella del Portogallo\n# Answer:\nIsabella of Portugal, Queen of Castile", "# Reasoning Path:\nIsabella I of Castile -> people.person.parents -> John II of Castile -> people.person.parents -> Henry III of Castile\n# Answer:\nJohn II of Castile"], "ground_truth": ["Isabella of Portugal, Queen of Castile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1587", "prediction": ["# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> people.person.parents -> Elsa Pataky\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> people.person.gender -> Female\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Tristan Hemsworth -> people.person.place_of_birth -> West Hollywood\n# Answer:\nTristan Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> common.topic.notable_types -> Person\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> India Rose Hemsworth -> people.person.place_of_birth -> London\n# Answer:\nIndia Rose Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.children -> Tristan Hemsworth -> people.person.gender -> Male\n# Answer:\nTristan Hemsworth", "# Reasoning Path:\nChris Hemsworth -> people.person.spouse_s -> m.0g5bzqc -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony"], "ground_truth": ["Elsa Pataky"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1589", "prediction": ["# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.nearby_airports -> Winnipeg James Armstrong Richardson International Airport\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area code 204\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> base.biblioness.bibs_location.state -> Manitoba\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area code 431\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Area codes 204 and 431\n# Answer:\nWinnipeg", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Rural Municipality of Headingley -> location.location.containedby -> Winnipeg Capital Region\n# Answer:\nRural Municipality of Headingley", "# Reasoning Path:\nMTS Iceplex -> common.topic.notable_for -> g.125b420ty\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMTS Iceplex -> location.location.containedby -> Winnipeg -> location.location.containedby -> Canada\n# Answer:\nWinnipeg"], "ground_truth": ["Winnipeg", "Rural Municipality of Headingley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-159", "prediction": ["# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.article -> m.02lctt\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.geolocation -> m.0kbnpg\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> common.topic.notable_for -> g.125bn2ktb\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> common.topic.article -> m.0r_cs\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHilo -> location.location.time_zones -> Hawaii-Aleutian Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_r37m_\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nHilo -> location.location.contains -> Waiakea Mission Station-Hilo Station -> location.location.containedby -> Hawaii\n# Answer:\nWaiakea Mission Station-Hilo Station", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Derek Shizuto -> common.topic.notable_for -> g.125b5498b\n# Answer:\nDerek Shizuto", "# Reasoning Path:\nHilo -> location.location.people_born_here -> Albert Spencer Wilcox -> people.person.nationality -> United States of America\n# Answer:\nAlbert Spencer Wilcox"], "ground_truth": ["Hawaii-Aleutian Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1590", "prediction": ["# Reasoning Path:\nIsrael -> location.statistical_region.religions -> m.0468s5y -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.person.religion -> Judaism\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Samsara -> film.film.country -> United States of America\n# Answer:\nSamsara", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.religion -> Judaism\n# Answer:\nAaron Friedman", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Defamation -> media_common.netflix_title.netflix_genres -> Judaism\n# Answer:\nDefamation", "# Reasoning Path:\nIsrael -> symbols.namesake.named_after -> Jacob -> people.deceased_person.place_of_death -> Egypt\n# Answer:\nJacob", "# Reasoning Path:\nIsrael -> film.film_location.featured_in_films -> Samsara -> film.film.featured_film_locations -> Western Wall\n# Answer:\nSamsara", "# Reasoning Path:\nIsrael -> media_common.quotation.author -> Aaron Friedman -> people.person.nationality -> United States of America\n# Answer:\nAaron Friedman"], "ground_truth": ["Judaism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1591", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Author -> book.book_subject.works -> C-SPAN's LCV 2011 U.S. Cities Tour Wraps up Savannah Shoot, Now in Charleston\n# Answer:\nAuthor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Politician -> type.type.domain -> Government\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Law professor -> people.profession.specialization_of -> g.121bkpjb\n# Answer:\nLaw professor", "# Reasoning Path:\nBarack Obama -> people.person.profession -> Law professor -> common.topic.notable_types -> Profession\n# Answer:\nLaw professor"], "ground_truth": ["Law professor", "Author", "Writer", "Lawyer", "Politician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1592", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> Under the Mistletoe -> freebase.valuenotation.is_reviewed -> Artist\n# Answer:\nUnder the Mistletoe", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.album.releases -> As Long As You Love Me (remixes)\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.release_type -> Album\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.composer -> Nasri\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.1257jn46q\n# Answer:\nAs Long as You Love Me"], "ground_truth": ["Right Here", "Eenie Meenie", "Live My Life", "Bad Day", "PYD", "Lolly", "Never Say Never", "Recovery", "Roller Coaster", "All That Matters", "Home to Mama", "Die in Your Arms", "Never Let You Go", "Boyfriend", "Wait for a Minute", "#thatPower", "First Dance", "All Around The World", "All Bad", "Beautiful", "Thought Of You", "Pray", "Hold Tight", "As Long as You Love Me", "Bigger", "Baby", "Heartbreaker", "Somebody to Love", "Beauty And A Beat", "Turn to You (Mother's Day Dedication)", "Confident", "Change Me"], "ans_acc": 0.0625, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 0.5, "ans_recall": 0.0625, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.5, "path_ans_recall": 0.0625}
{"id": "WebQTest-1593", "prediction": ["# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> location.location.containedby -> Gloucestershire\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.notable_for -> g.1258953bg\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.article -> m.02pj_ts\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> common.topic.notable_types -> Location\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> geography.river.origin -> Thames Head -> location.location.geolocation -> m.04njq15\n# Answer:\nThames Head", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> base.locations.countries.continent -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nRiver Thames -> location.location.containedby -> United Kingdom -> periodicals.newspaper_circulation_area.newspapers -> i\n# Answer:\ni"], "ground_truth": ["Thames Head"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1595", "prediction": ["# Reasoning Path:\nRobert Pattinson -> tv.tv_actor.starring_roles -> m.0gfv1f7 -> tv.regular_tv_appearance.series -> Dark Kingdom: The Dragon King\n# Answer:\nDark Kingdom: The Dragon King", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> tv.tv_actor.starring_roles -> m.0gfv1f7 -> tv.regular_tv_appearance.character -> Giselher\n# Answer:\nGiselher", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nKristen Stewart", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0b3w_ry -> award.award_nomination.nominated_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards"], "ground_truth": ["Maps to the Stars", "The Childhood of a Leader", "Love & Distrust", "Bel Ami", "Hold on to Me", "Dark Kingdom: The Dragon King", "The Haunted Airman", "Harry Potter and the Goblet of Fire", "The Twilight Saga: New Moon", "Queen of the Desert", "Cosmopolis", "Water for Elephants", "Twilight", "Vanity Fair", "Remember Me", "The Bad Mother's Handbook", "The Twilight Saga: Breaking Dawn - Part 1", "Unbound Captives", "Mission: Blacklist", "Harry Potter and the Order of the Phoenix", "How to Be", "Little Ashes", "Life", "The Twilight Saga: Breaking Dawn - Part 2", "The Summer House", "The Rover", "Eclipse"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0.23952095808383234, "ans_precission": 0.625, "ans_recall": 0.14814814814814814, "path_f1": 0.09999999999999999, "path_precision": 0.625, "path_recall": 0.05434782608695652, "path_ans_f1": 0.23952095808383234, "path_ans_precision": 0.625, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-1596", "prediction": ["# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Devanagari\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Persian alphabet\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.countries_spoken_in -> India\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nEnglish Language", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.11b60w91bg\n# Answer:\nlocation.statistical_region.consumer_price_index", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> common.topic.notable_types -> Human Language\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.labor_participation_rate -> g.11b71mdmvg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nPakistan -> location.country.languages_spoken -> Hindustani language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHindustani language", "# Reasoning Path:\nPakistan -> location.statistical_region.consumer_price_index -> g.12tb6gb2b\n# Answer:\nlocation.statistical_region.consumer_price_index"], "ground_truth": ["English Language", "Urdu Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-1597", "prediction": ["# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Fabricated Rubber Products, NEC (wet suits)\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.parent_industry -> Other Miscellaneous Manufacturing\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> freebase.equivalent_topic.equivalent_type -> Sports Equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Footwear Manufacturing -> business.industry.child_industry -> Athletic Shoes & Apparel\n# Answer:\nFootwear Manufacturing", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Ice hockey equipment\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Athletic Shoes & Apparel -> common.topic.notable_types -> Industry\n# Answer:\nAthletic Shoes & Apparel", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> base.onephylogeny.type_of_thing.includes -> Golf club\n# Answer:\nSports equipment", "# Reasoning Path:\nNike, Inc. -> business.business_operation.industry -> Sports equipment -> business.industry.child_industry -> Sporting and Athletic Goods\n# Answer:\nSports equipment"], "ground_truth": ["Fashion accessory", "Sports equipment", "Footwear Manufacturing", "Footwear", "Clothing", "Athletic Shoes & Apparel"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1598", "prediction": ["# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Rebekah Johnson Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.spouse_s -> m.0j4k73z -> people.marriage.spouse -> Lady Bird Johnson\n# Answer:\nLady Bird Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Claudia Taylor Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.spouse_s -> m.05kg486\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Lynda Bird Johnson Robb -> people.person.children -> Catherine Lewis Robb\n# Answer:\nLynda Bird Johnson Robb", "# Reasoning Path:\nLyndon B. Johnson -> base.famouspets.pet_owner.pets_owned -> m.05hqsbs -> base.famouspets.pet_ownership.owner -> Lady Bird Johnson\n# Answer:\nLady Bird Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> people.person.children -> Nicole Marie Nugent\n# Answer:\nLuci Baines Johnson", "# Reasoning Path:\nLyndon B. Johnson -> people.person.children -> Luci Baines Johnson -> organization.organization_member.member_of -> m.05kg44b\n# Answer:\nLuci Baines Johnson"], "ground_truth": ["Lady Bird Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.25, "path_recall": 0.2222222222222222, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1599", "prediction": ["# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.administrative_division.first_level_division_of -> North Korea\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kumsusan Palace of the Sun\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kim Il-sung Stadium\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> common.topic.notable_types -> City/Town/Village\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Arch of Triumph\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.events -> Death and state funeral of Kim Il-sung\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Kim Chaek University of Technology\n# Answer:\nPyongyang", "# Reasoning Path:\nKim Jong-il -> people.deceased_person.place_of_death -> Pyongyang -> location.location.contains -> Korea Central Zoo\n# Answer:\nPyongyang"], "ground_truth": ["Pyongyang"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Line Digital Ltd\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-160", "prediction": ["# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xj -> business.employment_tenure.company -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nRudolf Virchow -> people.person.employment_history -> m.04hx7xp -> business.employment_tenure.company -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.sentence -> At the University of Berlin for a time he worked as personal assistant to the pathologist Virchow.\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Karl Weigert\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_from -> rudolf carl virchow a fan of louis pasteur -> base.kwebbase.kwconnection.other -> Louis Pasteur\n# Answer:\nrudolf carl virchow a fan of louis pasteur", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> carl weigert assistant to rudolf carl virchow -> base.kwebbase.kwconnection.relation -> assistant to\n# Answer:\ncarl weigert assistant to rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> base.kwebbase.kwtopic.connections_to -> ernst haeckel taught by rudolf carl virchow -> base.kwebbase.kwconnection.subject -> Ernst Haeckel\n# Answer:\nernst haeckel taught by rudolf carl virchow", "# Reasoning Path:\nRudolf Virchow -> education.academic.advisees -> Oscar Liebreich -> education.academic.advisors -> Carl Remigius Fresenius\n# Answer:\nOscar Liebreich"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1603", "prediction": ["# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> Religion\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.book.editions -> Tao te ching\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> Music\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> book.book_subject.works -> Tao Te Ching -> book.written_work.subjects -> Religion\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.author -> Jacob Needleman\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> book.written_work.subjects -> New Age\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Tao Te Ching -> media_common.quotation_source.quotations -> A violent wind does not last for a whole morning...\n# Answer:\nTao Te Ching", "# Reasoning Path:\nTaoism -> religion.religion.texts -> Daozang -> common.topic.notable_types -> Religious Text\n# Answer:\nDaozang"], "ground_truth": ["Zhuang Zhou", "I Ching", "Daozang", "Tao Te Ching"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-1604", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.location.containedby -> British Isles\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.form_of_government -> Parliamentary system\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.capital -> London\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> Kingdom of Great Britain -> location.country.currency_used -> Pound sterling\n# Answer:\nKingdom of Great Britain", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand"], "ground_truth": ["Canada", "Australia", "Republic of Ireland", "New Zealand", "Kingdom of Great Britain", "United Kingdom", "United States of America"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.4307692307692308, "path_precision": 0.875, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1607", "prediction": ["# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.season -> 2012 PGA Tour\n# Answer:\n2012 PGA Tour", "# Reasoning Path:\nFedEx Cup -> common.topic.article -> m.08q5wy\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFedEx Cup -> sports.sports_award_type.winners -> m.0n1v8cy -> sports.sports_award.award_winner -> Brandt Snedeker\n# Answer:\nBrandt Snedeker", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_for -> g.12559n8g_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_types -> Sports League Award Type -> freebase.type_profile.published -> Published\n# Answer:\nSports League Award Type", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_types -> Sports League Award Type -> type.type.expected_by -> Award\n# Answer:\nSports League Award Type", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_types -> Sports League Award Type -> common.topic.article -> m.06zxtxj\n# Answer:\nSports League Award Type", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_types -> Sports League Award Type -> freebase.type_hints.included_types -> Topic\n# Answer:\nSports League Award Type", "# Reasoning Path:\nFedEx Cup -> common.topic.notable_types -> Sports League Award Type -> type.type.domain -> Sports\n# Answer:\nSports League Award Type"], "ground_truth": ["Brandt Snedeker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1608", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Siwi Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nSiwi Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Siwi Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSiwi Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Siwi Language -> language.human_language.region -> Africa\n# Answer:\nSiwi Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nModern Standard Arabic", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Modern Standard Arabic -> common.topic.notable_types -> Human Language\n# Answer:\nModern Standard Arabic"], "ground_truth": ["Egyptian Arabic", "Bedawi Arabic", "Sa'idi Arabic", "Siwi Language"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.3333333333333333, "ans_recall": 0.25, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.25}
{"id": "WebQTest-1609", "prediction": ["# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> Working Girl\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> The Tony Randall Show\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> common.topic.notable_for -> g.1259mrnmt\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> award.award_nominated_work.award_nominations -> m.0z8z081 -> award.award_nomination.award_nominee -> Paul Fusco\n# Answer:\nPaul Fusco", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_program_creator.programs_created -> Buffalo Bill\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_producer.programs_produced -> m.0n5dfsq\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Tom Patchett -> tv.tv_writer.tv_programs -> m.0v91p0m\n# Answer:\nTom Patchett", "# Reasoning Path:\nALF -> tv.tv_program.program_creator -> Paul Fusco -> people.person.profession -> Voice Actor\n# Answer:\nPaul Fusco"], "ground_truth": ["Paul Fusco"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-161", "prediction": ["# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.institution -> University of Edinburgh\n# Answer:\nUniversity of Edinburgh", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qt_h -> education.education.institution -> Kirkcaldy High School\n# Answer:\nKirkcaldy High School", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.0k1qvbm -> education.education.institution -> Kirkcaldy West Primary School\n# Answer:\nKirkcaldy West Primary School", "# Reasoning Path:\nGordon Brown -> people.person.profession -> Teacher -> base.lightweight.profession.professions_similar -> Librarian\n# Answer:\nTeacher", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown Davos Jan 08 -> common.image.appears_in_topic_gallery -> United Kingdom general election, 2010\n# Answer:\nGordon Brown Davos Jan 08", "# Reasoning Path:\nGordon Brown -> people.person.education -> m.02wp5xk -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nGordon Brown -> common.topic.image -> Gordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999 -> common.image.appears_in_topic_gallery -> United Kingdom corporation tax\n# Answer:\nGordon Brown, the Chancellor of the Exchequer who abolished ACT and introduced the quarterly instalment r\u00e9gime in 1999"], "ground_truth": ["University of Edinburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1610", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.parents -> John D. Rockefeller Jr.\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfg4c\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_vice_president.to_president -> Richard Nixon -> people.person.nationality -> United States of America\n# Answer:\nRichard Nixon", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.parents -> Abby Aldrich Rockefeller\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_vice_president.to_president -> Richard Nixon -> film.film_subject.films -> All the President's Men\n# Answer:\nRichard Nixon"], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1611", "prediction": ["# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> finance.currency.countries_used -> Nagorno-Karabakh Republic\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.webpage -> m.04m1q3h\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.notable_for -> g.125gnq5zv\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_391m\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.notable_types -> Currency\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.country.currency_used -> Armenian dram -> common.topic.article -> m.033xrd\n# Answer:\nArmenian dram", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60znv7_\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_f6xm\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> location.statistical_region.internet_users_percent_population -> g.11b60xs9cw\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_jsph\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["Armenian dram"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1612", "prediction": ["# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> visual_art.art_subject.artwork_on_the_subject -> Ukrayina (Video art)\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> interests.hobby.people_with_this_hobby -> Mukil Elango\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.0103397j\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> common.topic.article -> m.02mx4l\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> education.field_of_study.students_majoring -> m.010bfdc7\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Thomas Cole\n# Answer:\nDrawing", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> business.product_theme.products -> Banksy Registry\n# Answer:\nPainting", "# Reasoning Path:\nRaphael -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting"], "ground_truth": ["Drawing", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1613", "prediction": ["# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Botswana\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cyprus\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nMalta -> location.statistical_region.deposit_interest_rate -> g.1hhc37h6m\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nMalta -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_kbr9\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["English Language", "Maltese Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-1614", "prediction": ["# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Church\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Basilica\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> people.person.religion -> Catholicism -> religion.religion.types_of_places_of_worship -> Cathedral\n# Answer:\nCatholicism", "# Reasoning Path:\nLouis XIV of France -> royalty.monarch.royal_line -> House of Bourbon -> royalty.royal_line.monarchs_from_this_line -> Robert I, Duke of Parma\n# Answer:\nHouse of Bourbon", "# Reasoning Path:\nLouis XIV of France -> royalty.monarch.royal_line -> House of Bourbon -> people.family.country -> France\n# Answer:\nHouse of Bourbon", "# Reasoning Path:\nLouis XIV of France -> royalty.monarch.royal_line -> House of Bourbon -> royalty.royal_line.monarchs_from_this_line -> Ferdinand I of the Two Sicilies\n# Answer:\nHouse of Bourbon", "# Reasoning Path:\nLouis XIV of France -> royalty.monarch.royal_line -> House of Bourbon -> royalty.royal_line.monarchs_from_this_line -> Ferdinand, Duke of Parma\n# Answer:\nHouse of Bourbon", "# Reasoning Path:\nLouis XIV of France -> organization.organization_founder.organizations_founded -> Royal Academy of Dance -> education.educational_institution.students_graduates -> m.0w5fchy\n# Answer:\nRoyal Academy of Dance"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1615", "prediction": ["# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> sports.sports_team_roster.position -> Defensive end\n# Answer:\nDefensive end", "# Reasoning Path:\nMike Wright -> sports.pro_athlete.teams -> m.0j3qfq_ -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMike Wright -> common.topic.article -> m.07twzg\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07nvh58 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nMike Wright -> american_football.football_player.games -> m.07sgss2 -> american_football.player_game_statistics.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nMike Wright -> american_football.football_player.position_s -> Tackle -> common.topic.article -> m.01b8d0\n# Answer:\nTackle", "# Reasoning Path:\nMike Wright -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Nebraska\n# Answer:\nUnited States of America"], "ground_truth": ["New England Patriots"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.3333333333333333, "path_recall": 0.16666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1616", "prediction": ["# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Heroin overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Suicide'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> fictional_universe.medical_condition_in_fiction.characters_with_this_condition -> Leo Taylor\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.parent_cause_of_death -> Assassination by 'Accident'\n# Answer:\nDrug overdose", "# Reasoning Path:\nShannon Hoon -> common.topic.notable_types -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nShannon Hoon -> people.deceased_person.cause_of_death -> Drug overdose -> common.topic.webpage -> m.09wl9dn\n# Answer:\nDrug overdose"], "ground_truth": ["Drug overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1617", "prediction": ["# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> base.schemastaging.context_name.pronunciation -> g.125_rtnb2\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> common.topic.image -> Heliocentric\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> base.argumentmaps.theory.arguments_that_this_is_true -> Apparent retrograde motion\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> base.argumentmaps.innovator.original_ideas -> Heliocentrism -> common.topic.article -> m.01kf98\n# Answer:\nHeliocentrism", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Copernicus -> common.topic.article -> m.01z1rs\n# Answer:\nCopernicus", "# Reasoning Path:\nNicolaus Copernicus -> symbols.name_source.namesakes -> Wroc\u0142aw\u2013Copernicus Airport -> common.topic.notable_types -> Airport\n# Answer:\nWroc\u0142aw\u2013Copernicus Airport"], "ground_truth": ["Copernican Revolution", "Copernican heliocentrism", "Heliocentrism"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1618", "prediction": ["# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> sports.sports_team_roster.team -> Chicago Cubs\n# Answer:\nChicago Cubs", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.0w483f0 -> sports.sports_team_roster.team -> Chicago White Sox\n# Answer:\nChicago White Sox", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.04fvqyp -> sports.sports_team_roster.team -> Boston Red Sox\n# Answer:\nBoston Red Sox", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.0w483lq -> sports.sports_team_roster.team -> Oakland Athletics\n# Answer:\nOakland Athletics", "# Reasoning Path:\nManny Ramirez -> common.topic.article -> m.06tjlm\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.04fcvk4 -> sports.sports_team_roster.team -> Cleveland Indians\n# Answer:\nCleveland Indians", "# Reasoning Path:\nManny Ramirez -> sports.pro_athlete.teams -> m.010p78y8 -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nManny Ramirez -> baseball.baseball_player.batting_stats -> m.06s7m7d -> baseball.batting_statistics.team -> Cleveland Indians\n# Answer:\nCleveland Indians"], "ground_truth": ["Boston Red Sox", "Los Angeles Dodgers", "Tampa Bay Rays", "Cleveland Indians", "Texas Rangers", "Chicago Cubs", "EDA Rhinos", "Chicago White Sox", "Oakland Athletics"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.606060606060606, "ans_precission": 0.6666666666666666, "ans_recall": 0.5555555555555556, "path_f1": 0.31578947368421056, "path_precision": 0.6666666666666666, "path_recall": 0.20689655172413793, "path_ans_f1": 0.606060606060606, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-1619", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Wild River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.containedby -> United States, with Territories\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> White Mountain National Forest\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.capital -> Quebec City\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> White Mountains\n# Answer:\nNew Hampshire"], "ground_truth": ["North America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-162", "prediction": ["# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Albania\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Bulgaria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nGreece -> location.location.adjoin_s -> m.05g_674 -> location.adjoining_relationship.adjoins -> Republic of Macedonia\n# Answer:\nRepublic of Macedonia", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nGreece -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Bulgaria", "Albania", "Turkey", "Republic of Macedonia"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.16666666666666666, "ans_precission": 0.125, "ans_recall": 0.25, "path_f1": 0.07058823529411765, "path_precision": 0.375, "path_recall": 0.03896103896103896, "path_ans_f1": 0.5, "path_ans_precision": 0.375, "path_ans_recall": 0.75}
{"id": "WebQTest-1620", "prediction": ["# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> event.public_speaker.speeches_or_presentations -> m.05rlb3g -> event.speech_or_presentation.speech_topic -> World War I\n# Answer:\nWorld War I", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> 20th century -> time.event.includes_event -> World War I\n# Answer:\n20th century", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> History of the United States\n# Answer:\nHistory of the United States", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> New Jersey\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nWoodrow Wilson -> common.topic.notable_for -> g.1255d7s1s\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["1913-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1621", "prediction": ["# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> government.election_campaign.party -> Democratic Party\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> common.topic.notable_for -> g.1255xf_wq\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> common.topic.notable_types -> Election campaign\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.040ptrq -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nJerry Brown -> government.politician.election_campaigns -> Jerry Brown for Governor, 2010 -> government.election_campaign.election -> California gubernatorial election, 2010\n# Answer:\nJerry Brown for Governor, 2010", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California Attorney General election, 2006\n# Answer:\nCalifornia Attorney General election, 2006", "# Reasoning Path:\nJerry Brown -> government.politician.party -> m.0lr11c0 -> government.political_party_tenure.party -> California Democratic Party\n# Answer:\nCalifornia Democratic Party", "# Reasoning Path:\nJerry Brown -> common.image.appears_in_topic_gallery -> California gubernatorial election, 1974\n# Answer:\nCalifornia gubernatorial election, 1974", "# Reasoning Path:\nJerry Brown -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJerry Brown -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["California Democratic Party", "Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1622", "prediction": ["# Reasoning Path:\nHawaii -> location.location.containedby -> United States, with Territories -> location.country.first_level_divisions -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nHawaii -> location.location.containedby -> United States, with Territories -> base.aareas.schema.earth.sovereign_domain.sovereign_state -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nHawaii -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii County", "# Reasoning Path:\nHawaii -> location.location.containedby -> United States, with Territories -> location.country.first_level_divisions -> Wake Island\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nHawaii -> location.location.containedby -> United States, with Territories -> base.aareas.schema.administrative_area.subdividing_place -> United States of America\n# Answer:\nUnited States, with Territories"], "ground_truth": ["Pacific Ocean"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1623", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1624", "prediction": ["# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Mauritania\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Mauritania\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Egypt\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Libya\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Morocco\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTunisia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60srfh8\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nTunisia -> location.statistical_region.gdp_real -> g.11b60vk8j7\n# Answer:\nlocation.statistical_region.gdp_real"], "ground_truth": ["Arabic Language", "French"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-1625", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0nk45qs\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_for -> g.1yl5vgrll\n# Answer:\nTriple Barrel Canon"], "ground_truth": ["Anemometer", "Double hull", "Aerial Screw", "Parachute", "33-Barreled Organ", "Triple Barrel Canon", "Diving suit", "Ornithopter", "Armored car", "Viola organista"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.30303030303030304, "path_precision": 0.625, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-1626", "prediction": ["# Reasoning Path:\nSt Trinian's School -> common.topic.notable_for -> g.12596535g\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Fictional universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.article -> m.01nxmh\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> freebase.type_hints.included_types -> Topic\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> fictional_universe.fictional_universe.created_by -> Ronald Searle -> people.person.profession -> Cartoonist\n# Answer:\nRonald Searle", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears In These Fictional Universes\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> freebase.type_profile.kind -> Definition\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.domain -> Fictional Universes\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.expected_by -> Appears in Fictional Universe\n# Answer:\nFictional Universe", "# Reasoning Path:\nSt Trinian's School -> common.topic.notable_types -> Fictional Universe -> type.type.properties -> Created by\n# Answer:\nFictional Universe"], "ground_truth": ["Ronald Searle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1627", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.article -> m.01_c9_\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.official_language -> Malaysian language -> common.topic.article -> m.0cz8t9v\n# Answer:\nMalaysian language", "# Reasoning Path:\nMalaysia -> location.location.time_zones -> Malaysia Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nMalaysia Time Zone", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.country.official_language -> Malaysian language -> common.topic.notable_for -> g.1259ycpbw\n# Answer:\nMalaysian language", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1628", "prediction": ["# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Guatemala\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guatemala\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71yj1cp\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1629", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Republic\n# Answer:\nRepublic", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_types -> Currency\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_for -> g.12565x5sk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Representative democracy -> government.form_of_government.countries -> Cayman Islands\n# Answer:\nRepresentative democracy", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Spain\n# Answer:\nUnitary state", "# Reasoning Path:\nDominican Republic -> location.statistical_region.electricity_consumption_per_capita -> g.1245_22hm\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-163", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Championship\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_for -> g.1z2spvm2w\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> time.event.locations -> CenturyLink Field\n# Answer:\n2006 NFC Championship Game", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> Super Bowl XLVIII -> freebase.valuenotation.is_reviewed -> Season\n# Answer:\nSuper Bowl XLVIII", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.championships -> 2006 NFC Championship Game -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2006 NFC Championship Game"], "ground_truth": ["Super Bowl XLVIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1630", "prediction": ["# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Lenny Henry\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> influence.influence_node.influenced -> Redd Foxx\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> influence.influence_node.influenced -> Nipsey Russell\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Nipsey Russell -> influence.influence_node.influenced_by -> James Brown\n# Answer:\nNipsey Russell", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> music.artist.genre -> Funk\n# Answer:\nJames Brown", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Charlie Chaplin -> people.person.gender -> Male\n# Answer:\nCharlie Chaplin", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> Redd Foxx -> influence.influence_node.influenced_by -> Charlie Chaplin\n# Answer:\nRedd Foxx", "# Reasoning Path:\nMichael Jackson -> influence.influence_node.influenced_by -> James Brown -> broadcast.artist.content -> 1.FM Euro 80's\n# Answer:\nJames Brown"], "ground_truth": ["Redd Foxx", "Charlie Chaplin", "Walt Disney", "Nipsey Russell", "James Brown"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.888888888888889, "path_precision": 1.0, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1631", "prediction": ["# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_holder -> Charles Allen Culberson\n# Answer:\nCharles Allen Culberson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0k1 -> government.government_position_held.office_holder -> Price Daniel\n# Answer:\nPrice Daniel", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.office_position_or_title -> Governor of Texas\n# Answer:\nGovernor of Texas", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.0101mqt4 -> government.government_position_held.office_holder -> Susan King\n# Answer:\nSusan King", "# Reasoning Path:\nTexas -> government.political_district.representatives -> m.0bfmmss -> government.government_position_held.office_holder -> Morris Sheppard\n# Answer:\nMorris Sheppard", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.04ks0gy -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.0101p41x -> government.government_position_held.office_holder -> Phil Stephenson\n# Answer:\nPhil Stephenson", "# Reasoning Path:\nTexas -> government.governmental_jurisdiction.governing_officials -> m.0102lshj -> government.government_position_held.office_holder -> Ron Simmons\n# Answer:\nRon Simmons"], "ground_truth": ["Morgan C. Hamilton", "Kay Bailey Hutchison", "Price Daniel", "Samuel B. Maxey", "John Tower", "W. Lee O'Daniel", "Matthias Ward", "Morris Sheppard", "John Hemphill", "John Henninger Reagan", "Richard Coke", "James W. Flanagan", "John Cornyn", "William A. Blakley", "Horace Chilton", "Ted Cruz", "Louis Wigfall", "Ralph Yarborough", "Andrew Jackson Houston", "Rienzi Melville Johnston", "Sam Houston", "Bob Krueger", "Charles Allen Culberson", "Lloyd Bentsen", "Joseph Weldon Bailey", "Tom Connally", "Thomas Jefferson Rusk", "Earle Bradford Mayfield", "Lyndon B. Johnson", "Roger Q. Mills", "James Pinckney Henderson", "Phil Gramm"], "ans_acc": 0.09375, "ans_hit": 1, "ans_f1": 0.15, "ans_precission": 0.375, "ans_recall": 0.09375, "path_f1": 0.1090909090909091, "path_precision": 0.375, "path_recall": 0.06382978723404255, "path_ans_f1": 0.15, "path_ans_precision": 0.375, "path_ans_recall": 0.09375}
{"id": "WebQTest-1632", "prediction": ["# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Diana, Princess of Wales Memorial Fountain -> common.topic.image -> The Diana, Princess of Wales Memorial Fountain, Hyde Park\n# Answer:\nDiana, Princess of Wales Memorial Fountain", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> United Kingdom\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Diana, Princess of Wales Memorial Fountain -> common.topic.notable_types -> Tourist attraction\n# Answer:\nDiana, Princess of Wales Memorial Fountain", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Diana, Princess of Wales Memorial Fountain -> common.topic.notable_for -> g.1256k9rjc\n# Answer:\nDiana, Princess of Wales Memorial Fountain", "# Reasoning Path:\nLondon -> travel.travel_destination.tourist_attractions -> Victoria and Albert Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0jx2dvh\n# Answer:\nVictoria and Albert Museum", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_containedby -> Kent\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> travel.travel_destination.how_to_get_here -> m.052w4gr -> travel.transportation.transport_operator -> National Rail\n# Answer:\nNational Rail"], "ground_truth": ["Central London", "Olympia", "Tower of London", "London Zoo", "Bank of England Museum", "British Museum", "Wasps RFC", "Crystal Palace Park", "William Wilkins's Building", "Royal Institution", "London Palladium", "Trafalgar Square", "Imperial College London", "Smithfield, London", "Travefy", "St Paul's Cathedral", "V&A Museum of Childhood", "London Bridge", "New London Architecture", "Natural History Museum, London", "Westminster Abbey", "Duke of York's Headquarters", "British Museum Reading Room", "Imperial War Museum London", "Serpentine Galleries", "National Police Memorial", "City University London", "G-A-Y", "Henman Hill", "Jewish Museum London", "Museum of London", "Tate Modern, London", "Horniman Museum", "London Victoria station", "Holloway", "London Underground", "Liverpool Street station", "Buckingham Palace", "Whetstone, London", "Science Museum, London", "Tower Bridge", "Design Museum", "Selfridges, Oxford Street", "Chokushi-Mon", "London Charterhouse", "Leighton House Museum", "National Portrait Gallery, London", "University College London", "Madame Tussauds London", "The Clink", "Newington, London", "Hyde Park", "Churchill War Rooms", "Tate Gallery, Britain", "Sir John Soane's Museum", "Queen's House", "London School of Economics and Political Science", "Sutcliffe Park", "Apsley House", "Palace of Westminster", "Big Ben", "RAF Bomber Command Memorial", "Strand, London", "National Maritime Museum", "Southgate, London", "London Marathon", "Waterlily House", "Tramp", "Sainsbury Wing", "The Building Centre", "Regent's Park", "Wimbledon", "Chinawhite", "Euston railway station", "London Paddington station", "Victoria and Albert Museum", "Polish Institute and Sikorski Museum", "The Nash Conservatory", "Earls Court Exhibition Centre", "Museum of London Docklands", "Wellcome Collection", "Wallington, London", "Barbican Centre", "Hippodrome, London", "St. James's Park", "Chessington World of Adventures", "Diana, Princess of Wales Memorial Fountain", "London Eye", "University of London", "Royal Albert Hall, London", "Wallace Collection"], "ans_acc": 0.03296703296703297, "ans_hit": 1, "ans_f1": 0.042105263157894736, "ans_precission": 0.5, "ans_recall": 0.02197802197802198, "path_f1": 0.06122448979591836, "path_precision": 0.5, "path_recall": 0.03260869565217391, "path_ans_f1": 0.061855670103092786, "path_ans_precision": 0.5, "path_ans_recall": 0.03296703296703297}
{"id": "WebQTest-1633", "prediction": ["# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> A Young Man Being Introduced to the Seven Liberal Arts\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Altarpiece of Ober-St. Veit\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Angel\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artists -> Alessandro Tiarini\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Annunciation\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfqtv\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artworks -> Atlas Slave\n# Answer:\nThe Renaissance", "# Reasoning Path:\nMona Lisa -> visual_art.artwork.period_or_movement -> The Renaissance -> visual_art.art_period_movement.associated_artists -> Alfred Stevens\n# Answer:\nThe Renaissance"], "ground_truth": ["Portrait"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1634", "prediction": ["# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Glenn Quagmire\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Brian Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Stewie Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Avery Bullock\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Chris Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Voice Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.010hsx_r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nPeter Griffin -> fictional_universe.fictional_character.parents -> Thelma Griffin -> tv.tv_character.appeared_in_tv_episodes -> m.05st127\n# Answer:\nThelma Griffin"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1635", "prediction": ["# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.065pl45 -> base.popstra.friendship.participant -> Carrie Underwood\n# Answer:\nCarrie Underwood", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.09tclvf -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTony Romo -> american_football.football_player.games -> m.07nvmcd -> american_football.player_game_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTony Romo -> base.popstra.celebrity.friendship -> m.063hcgf -> base.popstra.friendship.participant -> Carrie Underwood\n# Answer:\nCarrie Underwood", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.0791bch -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.07mmkjm -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nTony Romo -> american_football.football_player.passing -> m.09tclvf -> american_football.player_passing_statistics.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys"], "ground_truth": ["2002"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1637", "prediction": ["# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> time.holiday.day_of_year -> November 1\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> The Father -> common.topic.notable_types -> Deity\n# Answer:\nThe Father", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Euthymius the Great's Feast Day -> common.topic.notable_types -> Holiday\n# Answer:\nEuthymius the Great's Feast Day", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> Fat Thursday -> common.topic.notable_for -> g.1z2v7jg83\n# Answer:\nFat Thursday", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> common.topic.notable_for -> g.1255bx1xr\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> The Father -> common.topic.article -> m.077_qp0\n# Answer:\nThe Father", "# Reasoning Path:\nCatholicism -> religion.religion.holidays -> All Saints' Day -> base.schemastaging.context_name.pronunciation -> g.125_rh8zf\n# Answer:\nAll Saints' Day", "# Reasoning Path:\nCatholicism -> religion.religion.deities -> The Father -> common.topic.notable_for -> g.1258ng2v4\n# Answer:\nThe Father"], "ground_truth": ["St Nicholas Day", "Corpus Christi", "Fat Thursday", "Feast of Assumption", "Euthymius the Great's Feast Day", "Palm Sunday", "Feast of St Francis of Assisi", "Feast of the Cross", "Saints Cyril and Methodius Day", "The Feast of Our Lady of Mount Carmel", "St. Spiridon Day", "Feast of Our Lady of Sorrows", "St. Stephen's Day", "Our Lady of Guadalupe Day", "Our Lady of Aparecida's day", "Feast of St. Margaret of the \u00c1rp\u00e1d House", "Olsok", "St. Anthony's Day", "Solemnity of Mary, Mother of God", "Feast of Christ the King", "Feast of Our Lady of the Rosary", "Name day", "World Day of Peace", "All Saints' Day", "St Joseph's Day", "Father Damien Day", "Saint Patrick's Day", "Feast of Our Lady of Hungary", "St Casimir's Day", "Nativity of Mary", "Feast of the Immaculate Conception", "Saint Nicholas' Eve", "Feast of Saints Peter and Paul", "Maundy Thursday", "St Crispin's Day"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15075376884422112, "ans_precission": 0.625, "ans_recall": 0.08571428571428572, "path_f1": 0.15075376884422112, "path_precision": 0.625, "path_recall": 0.08571428571428572, "path_ans_f1": 0.15075376884422112, "path_ans_precision": 0.625, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1638", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Luxembourg\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.language_family -> Indo-European languages\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> West Flemish -> language.human_language.language_family -> West Germanic languages\n# Answer:\nWest Flemish", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.main_country -> France\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.countries_spoken_in -> South Africa\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["German Language", "French", "Dutch Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.717948717948718, "ans_precission": 0.7777777777777778, "ans_recall": 0.6666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.717948717948718, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1639", "prediction": ["# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Dorothy in The Wizard of Oz\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> base.barbie.barbie_theme.dolls_with_this_theme -> Barbie as Glinda the Good Witch in The Wizard of Oz #14901\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> location.location.containedby -> United States of America\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> film.film_story_contributor.film_story_credits -> Wizard of Oz -> theater.theater_character.plays_appears_in -> The Wiz\n# Answer:\nWizard of Oz", "# Reasoning Path:\nL. Frank Baum -> people.deceased_person.place_of_death -> Hollywood -> location.location.containedby -> Los Angeles\n# Answer:\nHollywood", "# Reasoning Path:\nL. Frank Baum -> freebase.valuenotation.is_reviewed -> Place of death -> type.property.schema -> Deceased Person\n# Answer:\nPlace of death"], "ground_truth": ["Hollywood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-164", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> common.topic.image -> Johannes Hevelius\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Astronomers\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> freebase.type_hints.included_types -> Person\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> people.profession.specialization_of -> Writer\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> common.topic.notable_types -> Profession\n# Answer:\nScience writer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Astronomer -> common.topic.image -> Galileo\n# Answer:\nAstronomer", "# Reasoning Path:\nStephen Hawking -> people.person.profession -> Science writer -> base.descriptive_names.names.descriptive_name -> m.01039sc4\n# Answer:\nScience writer"], "ground_truth": ["Astrophysicist", "Author", "Physicist", "Writer", "Astronomer", "Science writer", "Professor", "Scientist", "Mathematician", "Cosmologist"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 1.0, "ans_recall": 0.3, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-1640", "prediction": ["# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Arab world\n# Answer:\nArab world", "# Reasoning Path:\nTunisia -> location.location.containedby -> Arab world -> location.location.contains -> Algeria\n# Answer:\nArab world", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> base.locations.countries.continent -> Africa\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nEarth", "# Reasoning Path:\nTunisia -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nTunis -> location.location.containedby -> Tunisia -> location.location.containedby -> Greater Maghreb\n# Answer:\nGreater Maghreb", "# Reasoning Path:\nTunisia -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica"], "ground_truth": ["Africa", "Tunisia", "Tunis Governorate"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1641", "prediction": ["# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.canoodled -> m.0652vh7 -> base.popstra.canoodled.participant -> Elizabeth McGovern\n# Answer:\nElizabeth McGovern", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.0w_j3fy -> people.marriage.spouse -> Christina Fulton\n# Answer:\nChristina Fulton", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.0w_j3fy -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.canoodled -> m.0652rbm -> base.popstra.canoodled.participant -> Pen\u00e9lope Cruz\n# Answer:\nPen\u00e9lope Cruz", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.canoodled -> m.0652sv4 -> base.popstra.canoodled.participant -> Maria Conchita\n# Answer:\nMaria Conchita", "# Reasoning Path:\nNicolas Cage -> people.person.spouse_s -> m.0w_j3fy -> freebase.valuenotation.has_no_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNicolas Cage -> celebrities.celebrity.sexual_orientation -> m.05n7hz_ -> celebrities.sexual_orientation_phase.sexual_orientation -> Heterosexuality\n# Answer:\nHeterosexuality", "# Reasoning Path:\nNicolas Cage -> base.popstra.celebrity.dated -> m.063g718 -> base.popstra.dated.participant -> Brooke Shields\n# Answer:\nBrooke Shields"], "ground_truth": ["Alice Kim"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1643", "prediction": ["# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.actor -> Dinah Manoff\n# Answer:\nDinah Manoff", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jt26n -> film.performance.actor -> Alex Vincent\n# Answer:\nAlex Vincent", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.09hxz1j -> film.performance.actor -> Jack Colvin\n# Answer:\nJack Colvin", "# Reasoning Path:\nChild's Play -> film.film.directed_by -> Tom Holland -> film.director.film -> We All Scream for Ice Cream\n# Answer:\nTom Holland", "# Reasoning Path:\nChild's Play -> film.film.distributors -> m.08cv9m4 -> film.film_film_distributor_relationship.distributor -> Rogue\n# Answer:\nRogue", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.0gckfkt -> film.performance.actor -> Tommy Swerdlow\n# Answer:\nTommy Swerdlow", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.0jwb95 -> film.performance.actor -> Catherine Hicks\n# Answer:\nCatherine Hicks", "# Reasoning Path:\nChild's Play -> film.film.starring -> m.03jssz4 -> film.performance.character -> Maggie Peterson\n# Answer:\nMaggie Peterson"], "ground_truth": ["John Franklin", "Brad Dourif", "Edan Gross"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1644", "prediction": ["# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> The Selected Papers of Margaret Sanger, Volume 2\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.includes_issues -> Reproductive rights\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> conferences.conference_subject.specific_conferences_about_this -> Seneca Falls Convention\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Vroomfondle\n# Answer:\nPhilosopher", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> book.book_subject.works -> The Margaret Sanger papers microfilm edition\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.activists -> Abby Morton Diaz\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> organization.organization_sector.organizations_in_this_sector -> Abortion Law Reform Association of New Zealand\n# Answer:\nWomen's rights", "# Reasoning Path:\nMary Wollstonecraft -> base.activism.activist.area_of_activism -> Women's rights -> base.activism.activism_issue.includes_issues -> Women's suffrage\n# Answer:\nWomen's rights"], "ground_truth": ["Women's rights"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1646", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crtdbg -> sports.sports_league_participation.league -> National League East\n# Answer:\nNational League East", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> common.topic.image -> 1980 World Series Program\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.league -> m.0crt4k0 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> sports.sports_championship_event.championship -> National League Championship Series\n# Answer:\n2008 National League Championship Series", "# Reasoning Path:\nPhiladelphia Phillies -> base.schemastaging.organization_extra.phone_number -> m.010dbpyt -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 1980 World Series -> common.topic.notable_for -> g.125cxv1w7\n# Answer:\n1980 World Series", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.championships -> 2008 National League Championship Series -> time.event.locations -> Citizens Bank Park\n# Answer:\n2008 National League Championship Series"], "ground_truth": ["1883"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1648", "prediction": ["# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.writing_system -> Icelandic orthography\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> language.human_language.region -> Europe\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.languages_spoken -> Icelandic Language -> base.rosetta.languoid.local_name -> Icelandic\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.writing_system -> Icelandic orthography\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1t5\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nIceland -> location.country.official_language -> Icelandic Language -> language.human_language.language_family -> North Germanic languages\n# Answer:\nIcelandic Language", "# Reasoning Path:\nIceland -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6h4x_\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Icelandic Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1649", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> common.topic.subject_of -> Michael Godard\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> common.topic.article -> m.02g43\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_2f_\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Visual Artist\n# Answer:\nArtist"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-165", "prediction": ["# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.actor -> Catherine Tate\n# Answer:\nCatherine Tate", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.series -> Doctor Who\n# Answer:\nDoctor Who", "# Reasoning Path:\nDonna Noble -> tv.tv_character.appeared_in_tv_program -> m.010pxvzy -> tv.regular_tv_appearance.seasons -> Doctor Who Series 4 (2008)\n# Answer:\nDoctor Who Series 4 (2008)", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> fictional_universe.character_occupation.characters_with_this_occupation -> Wendy Watson\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.married_to -> m.04gf2fn -> fictional_universe.marriage_of_fictional_characters.spouses -> Lee McAvoy\n# Answer:\nLee McAvoy", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> common.topic.notable_for -> g.1259jvdvh\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.occupation -> office temp-worker -> common.topic.notable_types -> Fictional job title\n# Answer:\noffice temp-worker", "# Reasoning Path:\nDonna Noble -> fictional_universe.fictional_character.gender -> Female -> biology.hybrid_parent_gender.hybrids -> m.0blp57t\n# Answer:\nFemale"], "ground_truth": ["Catherine Tate"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.07999999999999999, "path_precision": 0.125, "path_recall": 0.058823529411764705, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1650", "prediction": ["# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.notable_types -> Organization\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> New York Media, LLC -> organization.organization.headquarters -> m.0hn2t5l\n# Answer:\nNew York Media, LLC", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.article -> m.02vvc40\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> National Women's Political Caucus -> common.topic.notable_for -> g.12591d_31\n# Answer:\nNational Women's Political Caucus", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Feminist Majority Foundation -> organization.organization.board_members -> m.05ncdr9\n# Answer:\nFeminist Majority Foundation", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> organization.organization.sectors -> United States pro-choice movement\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> organization.organization_founder.organizations_founded -> Choice USA -> common.topic.notable_for -> g.1256hswv1\n# Answer:\nChoice USA", "# Reasoning Path:\nGloria Steinem -> base.activism.activist.area_of_activism -> Reproductive rights -> base.activism.activism_issue.parent_issue -> Women's rights\n# Answer:\nReproductive rights"], "ground_truth": ["New York Media, LLC", "Choice USA", "Feminist Majority Foundation", "Coalition of Labor Union Women", "National Women's Political Caucus", "Women's Action Alliance", "Ms. Foundation for Women"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.691358024691358, "ans_precission": 0.875, "ans_recall": 0.5714285714285714, "path_f1": 0.691358024691358, "path_precision": 0.875, "path_recall": 0.5714285714285714, "path_ans_f1": 0.691358024691358, "path_ans_precision": 0.875, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1652", "prediction": ["# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.main_country -> United Kingdom\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish English -> base.schemastaging.context_name.pronunciation -> g.125_nyqwz\n# Answer:\nScottish English", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.countries_spoken_in -> Kingdom of Great Britain\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.language_family -> Indo-European languages\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Scottish Gaelic -> language.human_language.region -> Europe\n# Answer:\nScottish Gaelic", "# Reasoning Path:\nScotland -> location.country.languages_spoken -> Old English -> language.human_language.countries_spoken_in -> England\n# Answer:\nOld English"], "ground_truth": ["English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1653", "prediction": ["# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.institution -> Mercy College\n# Answer:\nMercy College", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.04hc7zn -> education.education.institution -> Phillips Exeter Academy\n# Answer:\nPhillips Exeter Academy", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.02n93cn -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0jwchnr -> education.education.institution -> Ardsley High School\n# Answer:\nArdsley High School", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> education.education.major_field_of_study -> Computer programming\n# Answer:\nComputer programming", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMark Zuckerberg -> people.person.education -> m.0j_gm2q -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Phillips Exeter Academy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1654", "prediction": ["# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Kugelis\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Borscht\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Kishka\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Bialy\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> common.topic.notable_types -> Cuisine\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> common.topic.notable_types -> Dish -> type.type.properties -> Type of dish\n# Answer:\nDish", "# Reasoning Path:\nBigos -> common.topic.notable_for -> g.125flpgtm\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.region_of_origin -> Poland\n# Answer:\nPolish food", "# Reasoning Path:\nBigos -> food.dish.cuisine -> Polish food -> dining.cuisine.dishes -> Babka\n# Answer:\nPolish food"], "ground_truth": ["Stew"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1655", "prediction": ["# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.parents -> Ike Turner\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Ike Turner\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.place_of_birth -> Los Angeles\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> people.person.spouse_s -> m.0t60dj8\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Ronnie Turner -> common.topic.notable_types -> Film actor\n# Answer:\nRonnie Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.parents -> Raymond Hill\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.nationality -> United States of America\n# Answer:\nRaymond Craig Turner", "# Reasoning Path:\nTina Turner -> people.person.children -> Raymond Craig Turner -> people.person.place_of_birth -> United States of America\n# Answer:\nRaymond Craig Turner"], "ground_truth": ["Ronnie Turner", "Raymond Craig Turner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1656", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_for -> g.1254z0xjb\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> common.topic.notable_types -> Musical instrument\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> base.argumentmaps.innovator.original_ideas -> Viola organista -> music.performance_role.track_performances -> m.0nk45qs\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Viola organista -> music.performance_role.track_performances -> m.0112zmz9\n# Answer:\nViola organista", "# Reasoning Path:\nLeonardo da Vinci -> law.inventor.inventions -> Triple Barrel Canon -> common.topic.notable_for -> g.1yl5vgrll\n# Answer:\nTriple Barrel Canon"], "ground_truth": ["Anemometer", "Double hull", "Aerial Screw", "Parachute", "33-Barreled Organ", "Triple Barrel Canon", "Diving suit", "Ornithopter", "Armored car", "Viola organista"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.30303030303030304, "path_precision": 0.625, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-1657", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.draft -> 1992 NBA draft\n# Answer:\n1992 NBA draft", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.season -> 1999\u20132000 NBA season\n# Answer:\n1999\u20132000 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.award -> NBA Rookie of the Year Award\n# Answer:\nNBA Rookie of the Year Award", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nNBA All-Star Game Most Valuable Player Award"], "ground_truth": ["1992 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1658", "prediction": ["# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.official_language -> Albanian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Macedonia -> location.location.containedby -> Eurasia\n# Answer:\nRepublic of Macedonia", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.official_language -> Serbian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Macedonia -> location.country.languages_spoken -> Serbian language\n# Answer:\nRepublic of Macedonia", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.country.languages_spoken -> Bosnian language\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.writing_system -> Turkish alphabet -> language.language_writing_system.languages -> Ladino Language\n# Answer:\nTurkish alphabet", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Republic of Kosovo -> location.location.contains -> Pristina\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nTurkish Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> German Language\n# Answer:\nAustria"], "ground_truth": ["Austria", "Cyprus", "Turkey", "Republic of Macedonia", "Republic of Kosovo"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.711864406779661, "ans_precission": 0.875, "ans_recall": 0.6, "path_f1": 0.711864406779661, "path_precision": 0.875, "path_recall": 0.6, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-1659", "prediction": ["# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> common.topic.notable_for -> g.1258sqjf5\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.geolocation -> m.03dnkgn\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> location.postal_code.country -> United States of America\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.location.containedby -> Contra Costa County\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94595 -> location.location.geolocation -> m.03dnkgg\n# Answer:\n94595", "# Reasoning Path:\nWalnut Creek -> location.statistical_region.population -> g.11b66b709v\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWalnut Creek -> location.location.contains -> g.1hc1b02b7\n# Answer:\nlocation.location.contains", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94596 -> location.postal_code.country -> United States of America\n# Answer:\n94596", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94597 -> common.topic.notable_types -> Postal Code\n# Answer:\n94597", "# Reasoning Path:\nWalnut Creek -> location.citytown.postal_codes -> 94598 -> location.location.containedby -> Contra Costa County\n# Answer:\n94598"], "ground_truth": ["94596", "94595", "94598", "94597"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-166", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.0n1kynd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.04ytk85 -> education.education.institution -> Dartmouth College\n# Answer:\nDartmouth College", "# Reasoning Path:\nTheodore Lesieg -> people.person.education -> m.03p87mv -> education.education.institution -> Lincoln College, Oxford\n# Answer:\nLincoln College, Oxford", "# Reasoning Path:\nTheodore Lesieg -> common.topic.article -> m.02g43\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_for -> g.125fbm0pz\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> ARE YOU MY MOTHER MINI PB -> common.topic.notable_types -> Book\n# Answer:\nARE YOU MY MOTHER MINI PB", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.artist -> Thurl Ravenscroft\n# Answer:\nA Quarter of Dawn", "# Reasoning Path:\nTheodore Lesieg -> book.author.works_written -> And to Think That I Saw It on Mulberry Street -> common.topic.article -> m.05_th_\n# Answer:\nAnd to Think That I Saw It on Mulberry Street", "# Reasoning Path:\nTheodore Lesieg -> music.artist.track -> A Quarter of Dawn -> music.recording.contributions -> m.0110d72d\n# Answer:\nA Quarter of Dawn"], "ground_truth": ["University of Oxford", "Lincoln College, Oxford", "Dartmouth College"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1661", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1662", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America"], "ground_truth": ["Honduras", "Kiribati", "Mandatory Palestine", "England", "New Zealand", "State of Palestine", "India", "Guyana", "United Kingdom", "United States of America", "Swaziland", "Antigua and Barbuda", "Lesotho", "Tanzania", "Sudan", "Kingdom of Great Britain", "Tokelau", "Sri Lanka", "Cook Islands", "Vatican City", "Barbados", "Canada", "Philippines", "Zambia", "Cura\u00e7ao", "Hong Kong", "Liberia", "Bangladesh", "South Yemen", "China", "Nauru", "Gibraltar", "Vanuatu", "Grenada", "Brunei", "Pakistan", "Kenya", "Timor-Leste", "Gazankulu", "Marshall Islands", "Guam", "Dominica", "Saint Kitts and Nevis", "Laos", "Ethiopia", "Montserrat", "Namibia", "Qatar", "Bahamas", "Japan", "Israel", "Australia", "Tuvalu", "Bonaire", "Saint Vincent and the Grenadines", "Malaysia", "Saint Lucia", "Jersey", "Sierra Leone", "Indonesia", "Republic of Ireland", "Territory of Papua and New Guinea", "Rwanda", "Samoa", "Isle of Man", "Ghana", "Malta", "Papua New Guinea", "Cyprus", "Fiji", "Belize", "Wales", "Territory of New Guinea", "Puerto Rico", "Gambia", "South Africa", "Cayman Islands", "Bermuda", "Jordan", "Cameroon", "Botswana", "Transkei", "Uganda", "Zimbabwe", "Nigeria", "Singapore", "Turks and Caicos Islands"], "ans_acc": 0.04597701149425287, "ans_hit": 1, "ans_f1": 0.08791208791208792, "ans_precission": 1.0, "ans_recall": 0.04597701149425287, "path_f1": 0.08791208791208792, "path_precision": 1.0, "path_recall": 0.04597701149425287, "path_ans_f1": 0.08791208791208792, "path_ans_precision": 1.0, "path_ans_recall": 0.04597701149425287}
{"id": "WebQTest-1663", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Swing music -> music.genre.parent_genre -> Ragtime\n# Answer:\nSwing music", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Dixieland -> music.genre.parent_genre -> Jazz\n# Answer:\nDixieland", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.artist.genre -> Dixieland -> music.genre.albums -> Jazz\n# Answer:\nDixieland"], "ground_truth": ["Scat singing", "Traditional pop music", "Jazz", "Swing music", "Dixieland"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.38709677419354843, "ans_precission": 0.375, "ans_recall": 0.4, "path_f1": 0.38709677419354843, "path_precision": 0.375, "path_recall": 0.4, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.375, "path_ans_recall": 0.6}
{"id": "WebQTest-1664", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1665", "prediction": ["# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> base.ontologies.ontology_instance.equivalent_instances -> m.09dvm1q\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Gallo-Italic of Sicily\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_types -> Language Family\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Angevin dialect\n# Answer:\nItalic languages"], "ground_truth": ["Indo-European languages", "Italic languages"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-1666", "prediction": ["# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.image -> Thermae of Caracalla Panorama\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.notable_for -> g.1255mqf0k\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> common.topic.image -> Caraca\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Domus Aurea -> location.location.time_zones -> Central European Time Zone\n# Answer:\nDomus Aurea", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Hadrian's Villa -> common.topic.article -> m.06519t\n# Answer:\nHadrian's Villa", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Baths of Caracalla -> location.location.geolocation -> m.0clhqgh\n# Answer:\nBaths of Caracalla", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Domus Aurea -> common.topic.notable_types -> Structure\n# Answer:\nDomus Aurea", "# Reasoning Path:\nRome -> location.statistical_region.population -> g.11bcdlm_x0\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Domus Aurea -> location.location.geolocation -> m.02_sny6\n# Answer:\nDomus Aurea"], "ground_truth": ["Palazzo Barberini", "Bioparco di Roma", "Pyramid of Cestius", "Appian Way", "Palazzo Venezia", "Vatican City", "Policlinico Umberto I", "Santa Maria sopra Minerva", "Catacombs of Rome", "Hadrian's Villa", "Rome Observatory", "Roman Forum", "Circus Maximus", "Roma-Ostia Half Marathon", "Basilica di Santa Maria Maggiore", "Archbasilica of St. John Lateran", "St. Peter's Basilica", "Capitoline Hill", "Via Veneto", "Castel Sant'Angelo", "Domus Aurea", "Pantheon", "Seven Pilgrim Churches of Rome", "Fontana delle Naiadi", "Villa d'Este", "Altare della Patria", "Spanish Steps", "Trevi Fountain", "Colosseum", "Basilica of Saint Paul Outside the Walls", "Palazzo Farnese", "Tabularium", "San Nicola in Carcere", "Sistine Chapel", "Palazzo Chigi", "Piazza Venezia", "Apostolic Palace", "Baths of Caracalla", "Galleria Borghese", "Raphael Rooms", "Piazza del Popolo", "Palazzo Spada", "Quirinal Palace", "Piazza Navona", "Bocca della Verit\u00e0", "Vatican Museums"], "ans_acc": 0.06521739130434782, "ans_hit": 1, "ans_f1": 0.12151898734177215, "ans_precission": 0.8888888888888888, "ans_recall": 0.06521739130434782, "path_f1": 0.12151898734177215, "path_precision": 0.8888888888888888, "path_recall": 0.06521739130434782, "path_ans_f1": 0.12151898734177215, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.06521739130434782}
{"id": "WebQTest-1667", "prediction": ["# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> people.person.place_of_birth -> Ethiopia\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.children -> Abijah of Judah\n# Answer:\nRehoboam", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> common.topic.article -> m.01qn7g\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> people.person.gender -> Male\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Menelik I -> freebase.valuenotation.has_value -> Date of death\n# Answer:\nMenelik I", "# Reasoning Path:\nSolomon -> people.person.children -> Rehoboam -> people.person.religion -> Judaism\n# Answer:\nRehoboam"], "ground_truth": ["Menelik I", "Rehoboam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1668", "prediction": ["# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Coronary artery disease\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Myocardial Ischemia\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> sports.pro_athlete.sports_played_professionally -> m.0z6442d -> sports.pro_sports_played.sport -> Surfing\n# Answer:\nSurfing", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Vasculitis\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> ACE inhibitor\n# Answer:\nheart attack", "# Reasoning Path:\nAndy Irons -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1669", "prediction": ["# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> base.aareas.schema.administrative_area.administrative_children -> Lee County\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Auburn University Historic District -> location.location.containedby -> Alabama\n# Answer:\nAuburn University Historic District", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> Alabama -> location.location.containedby -> United States of America\n# Answer:\nAlabama", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Georgia\n# Answer:\nUnited States of America", "# Reasoning Path:\nAuburn University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America", "Auburn", "Alabama", "Auburn University Historic District"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-167", "prediction": ["# Reasoning Path:\nQueensland -> symbols.namesake.named_after -> Queen Victoria\n# Answer:\nQueen Victoria", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_position_or_title -> Australian Senator\n# Answer:\nAustralian Senator", "# Reasoning Path:\nQueensland -> symbols.flag_referent.flag -> m.0525kx7 -> symbols.flag_use.flag -> Flag of Queensland\n# Answer:\nFlag of Queensland", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.office_holder -> Len Harris\n# Answer:\nLen Harris", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_8828 -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nAustralian Senate", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0104n88_ -> government.government_position_held.office_holder -> Francis Isidore Power\n# Answer:\nFrancis Isidore Power", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_fgk9 -> government.government_position_held.office_holder -> Brenda Gibbs\n# Answer:\nBrenda Gibbs", "# Reasoning Path:\nQueensland -> government.governmental_jurisdiction.governing_officials -> m.0107h9yk -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo", "# Reasoning Path:\nQueensland -> government.political_district.representatives -> m.04_87_t -> government.government_position_held.governmental_body -> Australian Senate\n# Answer:\nAustralian Senate"], "ground_truth": ["Queen Victoria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1670", "prediction": ["# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> common.topic.notable_for -> g.125980l9b\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Anastasia N. Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> freebase.valuenotation.has_value -> Parents\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.gender -> Female\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.spouse_s -> m.0r8wjcd\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sonia Ali -> people.person.children -> Kimberly Ali\n# Answer:\nSonia Ali", "# Reasoning Path:\nTatyana Ali -> people.person.parents -> Sheriff Ali -> common.topic.notable_types -> Person\n# Answer:\nSheriff Ali", "# Reasoning Path:\nTatyana Ali -> people.person.sibling_s -> m.0jwd_jx -> people.sibling_relationship.sibling -> Kimberly Ali\n# Answer:\nKimberly Ali"], "ground_truth": ["Sonia Ali", "Sheriff Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1671", "prediction": ["# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_types -> Statistical region\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> location.location.people_born_here -> Daniel Gunnarsson\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.notable_for -> g.1259m4hv9\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> common.topic.article -> m.02vwg33\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLeif Eriksson -> people.person.place_of_birth -> K\u00f6ping -> common.topic.article -> m.02vmwv\n# Answer:\nK\u00f6ping", "# Reasoning Path:\nLeif Eriksson -> sports.pro_athlete.sports_played_professionally -> m.0d5r_4t -> sports.pro_sports_played.sport -> Football\n# Answer:\nFootball", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> people.person.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nLeif Eriksson -> common.topic.notable_types -> Football player -> common.topic.article -> m.02ktm6x\n# Answer:\nFootball player"], "ground_truth": ["K\u00f6ping"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1672", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.article -> m.054w4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.main_country -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> base.descriptive_names.names.descriptive_name -> m.011vfn5d\n# Answer:\nEthnicity"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1673", "prediction": ["# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Hermann G\u00f6ring\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> base.schemastaging.context_name.pronunciation -> g.125_pt37m\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.0btmmq1 -> government.political_party_tenure.party -> German Workers' Party\n# Answer:\nGerman Workers' Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> government.political_party.ideology -> Far-right politics\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.founders -> Anton Drexler\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.parent -> m.0w1p81d\n# Answer:\nWaffen-SS", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> book.book_subject.works -> The Hidden Life of Otto Frank\n# Answer:\nNazi Party", "# Reasoning Path:\nAdolf Hitler -> government.politician.party -> m.075rkrk -> government.political_party_tenure.party -> Nazi Party\n# Answer:\nNazi Party"], "ground_truth": ["German Workers' Party", "Nazi Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1674", "prediction": ["# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nKim Il-sung", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.010r1v7b -> government.government_position_held.office_holder -> Kim Il-sung\n# Answer:\nKim Il-sung", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_h__ -> government.government_position_held.office_holder -> Kim Jong-il\n# Answer:\nKim Jong-il", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0f -> government.government_position_held.office_position_or_title -> President of North Korea\n# Answer:\nPresident of North Korea", "# Reasoning Path:\nNorth Korea -> military.military_combatant.military_commanders -> m.0bb_lfx -> military.military_command.military_conflict -> Korean War\n# Answer:\nKorean War", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0j5vvhx -> government.government_position_held.office_holder -> Kim Jong-un\n# Answer:\nKim Jong-un", "# Reasoning Path:\nNorth Korea -> government.governmental_jurisdiction.governing_officials -> m.0s91b0n -> government.government_position_held.office_holder -> Choe Yong-rim\n# Answer:\nChoe Yong-rim"], "ground_truth": ["Kim Jong-un"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1675", "prediction": ["# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Wild River\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> White Mountain National Forest\n# Answer:\nNew Hampshire", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Jersey -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Amy\n# Answer:\nNew Jersey", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> Prince Edward Island -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nPrince Edward Island", "# Reasoning Path:\nAppalachian Mountains -> location.location.partially_containedby -> New Hampshire -> location.location.partially_contains -> Androscoggin River\n# Answer:\nNew Hampshire"], "ground_truth": ["Saint Pierre and Miquelon", "New Jersey", "Alabama", "Virginia", "Tennessee", "New Brunswick", "Newfoundland and Labrador", "Ohio", "New Hampshire", "Vermont", "Kentucky", "South Carolina", "New York", "Connecticut", "Qu\u00e9bec", "Pennsylvania", "Prince Edward Island", "West Virginia", "Georgia", "Massachusetts", "New England", "North Carolina", "Nova Scotia", "Maine"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 1.0, "ans_recall": 0.16666666666666666, "path_f1": 0.2857142857142857, "path_precision": 1.0, "path_recall": 0.16666666666666666, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 1.0, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1676", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Portugal\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_formerly_used -> Peseta -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nPeseta", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Germany\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc38hhk\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1677", "prediction": ["# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> United Kingdom\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.containedby -> Macclesfield\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> common.topic.notable_types -> English civil parish\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> common.topic.article -> m.013p_p\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> common.topic.article -> m.01425q\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> common.topic.notable_for -> g.125c3ypwn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> base.popstra.location.arrestee -> m.0ghc35h\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nTerry Waite -> people.person.place_of_birth -> Bollington -> location.location.people_born_here -> Libby Clegg\n# Answer:\nBollington", "# Reasoning Path:\nTerry Waite -> people.person.nationality -> United Kingdom -> periodicals.newspaper_circulation_area.newspapers -> i\n# Answer:\ni"], "ground_truth": ["Bollington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1678", "prediction": ["# Reasoning Path:\nBattle of San Juan Hill -> common.image.size -> m.02cgdw0\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.containedby -> Santiago de Cuba Province\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Turquino National Park\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.nearby_airports -> Antonio Maceo Airport\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> common.topic.image -> San Juan Hill by Kurz and Allison -> common.image.appears_in_topic_gallery -> Kurz and Allison\n# Answer:\nSan Juan Hill by Kurz and Allison", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> La Gran Piedra (The Big Rock)\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.statistical_region.population -> g.11b7tdz1nw\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> location.location.containedby -> Cuba\n# Answer:\nSantiago de Cuba", "# Reasoning Path:\nBattle of San Juan Hill -> time.event.locations -> Santiago de Cuba -> travel.travel_destination.tourist_attractions -> Baconao\n# Answer:\nSantiago de Cuba"], "ground_truth": ["Santiago de Cuba"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1679", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> South Korea\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71srh9q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-168", "prediction": ["# Reasoning Path:\nJames Madison -> book.author.works_written -> Mr. Madison's motion for commercial restrictions -> common.topic.notable_for -> g.1256d9fcs\n# Answer:\nMr. Madison's motion for commercial restrictions", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 9 -> book.written_work.previous_in_series -> The Papers of James Madison, Secretary of State Series, Vol. 8\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 9", "# Reasoning Path:\nJames Madison -> book.author.works_written -> Mr. Madison's motion for commercial restrictions -> book.written_work.subjects -> United States of America\n# Answer:\nMr. Madison's motion for commercial restrictions", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 4 -> book.written_work.previous_in_series -> The Papers of James Madison, Secretary of State Series, Vol. 3\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 4", "# Reasoning Path:\nJames Madison -> book.author.works_written -> Mr. Madison's motion for commercial restrictions -> book.book.editions -> Mr. Madison's motion for commercial restrictions in a committee of the whole House on the report of the Secretary of State\n# Answer:\nMr. Madison's motion for commercial restrictions", "# Reasoning Path:\nJames Madison -> book.author.works_written -> The Papers of James Madison, Secretary of State Series, Vol. 9 -> freebase.valuenotation.has_value -> Date written\n# Answer:\nThe Papers of James Madison, Secretary of State Series, Vol. 9", "# Reasoning Path:\nJames Madison -> organization.organization_founder.organizations_founded -> The United States Constitutional Convention -> time.event.locations -> Philadelphia\n# Answer:\nThe United States Constitutional Convention", "# Reasoning Path:\nJames Madison -> book.author.works_written -> Mr. Madison's motion for commercial restrictions -> common.topic.notable_types -> Book\n# Answer:\nMr. Madison's motion for commercial restrictions"], "ground_truth": ["The Papers of James Madison, Secretary of State Series, Vol. 5", "The Papers of James Madison, Secretary of State Series, Vol. 4", "James Madison: Writings", "Federalist No. 47", "The Papers of James Madison, Presidential Series Vol. 1", "The reply of Mr. Madison, in answer to Mr. Rose, in discussing the affair of the Chesapeake", "Federalist No. 50", "A vocabulary of New Jersey Delaware", "Federalist No. 43", "Federalist No. 40", "Calendar of the correspondence of James Madison", "Extract of a letter from the Secretary of State to Mr. Monroe, relative to impressments", "Federalist No. 10", "Federalist No. 62", "Federalist No. 45", "An examination of the British doctrine", "The Papers of James Madison, Presidential Series Vol. 7", "Federalist No. 46", "Federalist No. 54", "Federalist No. 51", "The Papers of James Madison, Secretary of State Series, Vol. 9", "Selections from the private correspondence of James Madison, from 1813 to 1836", "James Madison, 1751-1836", "The Papers of James Madison, Presidential Series Vol. 5", "Federalist No. 18", "The Papers of James Madison, Vol. 4", "Federalist No. 49", "Mr. Madison's motion for commercial restrictions", "The Papers of James Madison, Secretary of State Series, Vol. 8", "Federalist No. 37", "Federalist No. 19", "The Papers of James Madison, Presidential Series Vol. 6", "All impressments unlawful and inadmissible", "Federalist No. 63", "The Papers of James Madison Congressional Series, Vol. 12: 2 October 1789 - 20 January 1790", "Federalist No. 57", "Federalist No. 20", "Federalist No. 55", "The James Madison Papers, 1723 - 1836", "Federalist No. 41", "Federalist No. 39", "The Papers of James Madison, Secretary of State Series, Vol. 1", "The Papers of James Madison, Secretary of State Series, Vol. 3", "An examination of the British doctrine, which subjects to capture a neutral trade, not open in time of peace", "Jonathan Bull and Mary Bull", "President Madison's inaugural speech", "The Papers of James Madison, Presidential Series Vol. 2", "Federalist No. 42", "Federalist No. 44", "The Papers of James Madison, Secretary of State Series, Vol. 2", "Letters and other writings of James Madison", "The Federalist Papers", "Letters from the Secretary of State to Messrs. Monroe and Pinkney, on subjects committed to their joint negotiations", "The Papers of James Madison, Presidential Series Vol. 3", "The complete Madison", "Letters from the Secretary of State to Mr. Monroe, on the subject of impressments, &c", "Papers of James Madison Volume 10: May 27, 1787-March 3, 1788", "Federalist No. 53", "The forging of American federalism", "Federalist No. 52", "Religious freedom", "Federalist No. 48", "The Papers of James Madison Retirement Series, Volume 1: 4 March 1817-31 January 1820", "An address delivered before the Agricultural Society of Albemarle, on Tuesday, May 12, 1818", "The Papers of James Madison, Secretary of State Series, Vol. 7", "Equal religious liberty stated and defended", "The mind of the founder", "Federalist No. 14", "Federalist No. 38", "Federalist No. 58", "The Papers of James Madison, Presidential Series Vol. 4", "The Papers of James Madison, Secretary of State Series, Vol. 6", "Federalist No. 56"], "ans_acc": 0.0684931506849315, "ans_hit": 1, "ans_f1": 0.07850467289719626, "ans_precission": 0.875, "ans_recall": 0.0410958904109589, "path_f1": 0.07850467289719626, "path_precision": 0.875, "path_recall": 0.0410958904109589, "path_ans_f1": 0.12704174228675136, "path_ans_precision": 0.875, "path_ans_recall": 0.0684931506849315}
{"id": "WebQTest-1680", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> time.event.locations -> Memorial Stadium\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> time.event.locations -> Riverfront Stadium\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.season -> 1983 Major League Baseball Season\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> time.event.locations -> Veterans Stadium\n# Answer:\n1983 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.venue -> m.0wz1z2b -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["1983 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1682", "prediction": ["# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.065q5w_ -> base.popstra.dated.participant -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz1h -> base.popstra.dated.participant -> Christine Taylor\n# Answer:\nChristine Taylor", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.celebrity -> Jeanne Tripplehorn\n# Answer:\nJeanne Tripplehorn", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.friendship -> m.065pfz3 -> base.popstra.friendship.participant -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz0t -> base.popstra.dated.participant -> Amanda Peet\n# Answer:\nAmanda Peet", "# Reasoning Path:\nBen Stiller -> base.popstra.celebrity.dated -> m.063fz14 -> base.popstra.dated.participant -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cq -> celebrities.romantic_relationship.celebrity -> Janeane Garofalo\n# Answer:\nJaneane Garofalo", "# Reasoning Path:\nBen Stiller -> celebrities.celebrity.sexual_relationships -> m.04dq6cg -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated"], "ground_truth": ["Christine Taylor", "Janeane Garofalo", "Calista Flockhart", "Claire Forlani", "Jeanne Tripplehorn", "Rhea Durham", "Amanda Peet"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.691358024691358, "ans_precission": 0.875, "ans_recall": 0.5714285714285714, "path_f1": 0.5185185185185185, "path_precision": 0.875, "path_recall": 0.3684210526315789, "path_ans_f1": 0.691358024691358, "path_ans_precision": 0.875, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1683", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> common.topic.image -> Norwegianmalforms\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.language_family -> Indo-European languages\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.region -> Europe\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.gdp_nominal_per_capita -> g.11b60vk8pf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Norwegian Language", "Bokm\u00e5l", "Nynorsk"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1684", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Menander\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced_by -> Homer\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Christopher Marlowe -> influence.influence_node.influenced_by -> Virgil\n# Answer:\nChristopher Marlowe", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Geoffrey Chaucer -> influence.influence_node.influenced_by -> Ovid\n# Answer:\nGeoffrey Chaucer", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> common.topic.notable_types -> Author\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Virgil -> influence.influence_node.influenced_by -> Homer\n# Answer:\nVirgil", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> influence.influence_node.influenced -> Baltasar Graci\u00e1n\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> influence.influence_node.influenced_by -> Lucian -> people.person.gender -> Male\n# Answer:\nLucian", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form"], "ground_truth": ["Seneca the Younger", "Plautus", "Geoffrey Chaucer", "Thomas More", "Ovid", "Virgil", "Lucian", "Christopher Marlowe", "Michel de Montaigne", "John Pory", "Thomas Kyd", "Edmund Spenser", "Terence", "Plutarch"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.43243243243243246, "ans_precission": 0.8888888888888888, "ans_recall": 0.2857142857142857, "path_f1": 0.43243243243243246, "path_precision": 0.8888888888888888, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5095541401273885, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-1685", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nBoston Latin School", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nInstitution", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.profession -> Diplomat -> fictional_universe.character_occupation.characters_with_this_occupation -> Jeffrey Sinclair\n# Answer:\nDiplomat", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1686", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_holder -> Ernest McFarland\n# Answer:\nErnest McFarland", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.office_holder -> Paul Fannin\n# Answer:\nPaul Fannin", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0hz834l -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nJan Brewer", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.office_position_or_title -> Governor of Arizona\n# Answer:\nGovernor of Arizona", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.010f0rgh -> government.government_position_held.office_holder -> Jan Brewer\n# Answer:\nJan Brewer", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y42 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.0108gfng -> government.government_position_held.office_holder -> Keith Brown\n# Answer:\nKeith Brown", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.governing_officials -> m.04j8y46 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["Jan Brewer", "Janet Napolitano"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.25, "path_ans_recall": 0.5}
{"id": "WebQTest-1687", "prediction": ["# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.directed_by -> Gina Prince-Bythewood\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.film_casting_director -> Aisha Coley\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> film.film.subjects -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> media_common.netflix_title.netflix_genres -> Basketball\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.director.film -> 25th Hour -> film.film.edited_by -> Barry Alexander Brown\n# Answer:\n25th Hour", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Love & Basketball -> common.topic.notable_types -> Film\n# Answer:\nLove & Basketball", "# Reasoning Path:\nSpike Lee -> film.producer.film -> Clockers -> film.film.genre -> Drama\n# Answer:\nClockers", "# Reasoning Path:\nSpike Lee -> film.producer.film -> He Got Game -> media_common.netflix_title.netflix_genres -> Basketball\n# Answer:\nHe Got Game"], "ground_truth": ["Bamboozled", "25th Hour", "Lovers & Haters", "Freak", "Last Hustle in Brooklyn", "Sucker Free City", "If God Is Willing and da Creek Don't Rise", "Malcolm X", "Jungle Fever", "And Ya Don't Stop: Hip Hop's Greatest Videos, Vol. 1", "Joe's Bed-Stuy Barbershop: We Cut Heads", "M.O.N.Y.", "HIStory on Film, Volume II", "Mike Tyson: Undisputed Truth", "Mo' Better Blues", "Saint John of Las Vegas", "Crooklyn", "Passing Strange", "Chiraq", "Lumi\u00e8re and Company", "When the Levees Broke: A Requiem in Four Acts", "New Jersey Drive", "The Best Man", "She Hate Me", "Love & Basketball", "Bad 25", "Ten Minutes Older: The Trumpet", "Jim Brown: All-American", "4 Little Girls", "Miracle at St. Anna", "The Original Kings of Comedy", "Concert For New York City", "Summer of Sam", "Get on the Bus", "All the Invisible Children", "She's Gotta Have It", "Kobe Doin' Work", "A Huey P. Newton Story", "Inside Man", "Oldboy", "Clockers", "School Daze", "Da Sweet Blood of Jesus", "Girl 6", "Do the Right Thing", "He Got Game", "Red Hook Summer"], "ans_acc": 0.0851063829787234, "ans_hit": 1, "ans_f1": 0.1568627450980392, "ans_precission": 1.0, "ans_recall": 0.0851063829787234, "path_f1": 0.0411522633744856, "path_precision": 0.625, "path_recall": 0.02127659574468085, "path_ans_f1": 0.1568627450980392, "path_ans_precision": 1.0, "path_ans_recall": 0.0851063829787234}
{"id": "WebQTest-1688", "prediction": ["# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Weight loss\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Cutaneous manifestations of sarcoidosis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Garland's triad\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> medicine.disease.symptoms -> Uveoparotitis\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> Del Sorbo Antonio\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.notable_for -> g.1254zhcvf\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> people.cause_of_death.people -> Joseph Scoren\n# Answer:\nSarcoidosis", "# Reasoning Path:\nBernie Mac -> people.deceased_person.cause_of_death -> Sarcoidosis -> common.topic.subject_of -> JAMA Dermatology\n# Answer:\nSarcoidosis"], "ground_truth": ["Sarcoidosis", "Complications from pneumonia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-169", "prediction": ["# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.countries_spoken_in -> South Africa\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Dutch Language -> language.human_language.main_country -> Netherlands\n# Answer:\nDutch Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.main_country -> Germany\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> Picard Language -> language.human_language.main_country -> France\n# Answer:\nPicard Language", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nGerman Language", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nBelgium -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Second Polish Republic\n# Answer:\nGerman Language"], "ground_truth": ["German Language", "French", "Dutch Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.380952380952381, "path_precision": 0.4444444444444444, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1690", "prediction": ["# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> location.mailing_address.citytown -> Stockholm\n# Answer:\nStockholm", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> common.topic.notable_for -> g.1257hvmfs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> location.mailing_address.state_province_region -> Stockholm County\n# Answer:\nStockholm County", "# Reasoning Path:\nElectrolux -> business.business_operation.net_profit -> m.04j7kvv -> measurement_unit.dated_money_value.currency -> Swedish krona\n# Answer:\nSwedish krona", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nStreet Address 3", "# Reasoning Path:\nElectrolux -> organization.organization.headquarters -> m.03ll14c -> freebase.valuenotation.has_no_value -> Street Address 4\n# Answer:\nStreet Address 4"], "ground_truth": ["Stockholm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1691", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07cvhy0 -> measurement_unit.dated_integer.source -> United States Department of Housing and Urban Development\n# Answer:\nUnited States Department of Housing and Urban Development", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> common.topic.notable_types -> Neighborhood\n# Answer:\nManchester"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1694", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.parents -> Anne Hathaway\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Judith Quiney -> people.person.parents -> Anne Hathaway\n# Answer:\nJudith Quiney", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.spouse_s -> m.02wtqtm -> people.marriage.spouse -> Anne Hathaway\n# Answer:\nAnne Hathaway", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Susanna Hall -> people.person.children -> Elizabeth Barnard\n# Answer:\nSusanna Hall", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.sibling_s -> m.02_5694\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> people.person.nationality -> United Kingdom\n# Answer:\nHamnet Shakespeare", "# Reasoning Path:\nWilliam Shakespeare -> people.person.children -> Hamnet Shakespeare -> freebase.valuenotation.has_no_value -> Profession\n# Answer:\nHamnet Shakespeare"], "ground_truth": ["Anne Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1695", "prediction": ["# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> time.event.locations -> White House\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> government.us_president.vice_president -> Alben W. Barkley -> people.person.nationality -> United States of America\n# Answer:\nAlben W. Barkley", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1949 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nHarry S. Truman 1949 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> base.culturalevent.event.entity_involved -> Harlan F. Stone\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> base.inaugurations.inauguration_speaker.inauguration -> Harry S. Truman 1945 presidential inauguration -> common.topic.image -> Harry 1.jpg\n# Answer:\nHarry S. Truman 1945 presidential inauguration", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarry S. Truman -> book.written_work.subjects -> Family\n# Answer:\nFamily"], "ground_truth": ["1945-04-12"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1696", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> common.topic.notable_types -> Profession\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Sailor -> common.topic.notable_types -> Profession\n# Answer:\nSailor", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Abel Tasman\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> common.topic.article -> m.023ns2\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> base.descriptive_names.names.descriptive_name -> m.0106708r\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Explorer -> common.topic.notable_types -> Profession\n# Answer:\nExplorer", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.people_with_this_profession -> Ahmad ibn M\u0101jid\n# Answer:\nNavigator", "# Reasoning Path:\nFerdinand Magellan -> people.person.profession -> Navigator -> people.profession.specializations -> Sailor\n# Answer:\nNavigator"], "ground_truth": ["Navigator", "Sailor", "Explorer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1698", "prediction": ["# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.capital -> Bangkok\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Southeast Asia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.official_language -> Thai Language\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Eurasia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.form_of_government -> Unitary state\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> common.topic.article -> m.01mvl0\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.country.form_of_government -> Parliamentary system\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> common.topic.notable_for -> g.125cb1nnv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Thailand -> location.location.containedby -> Asia\n# Answer:\nThailand", "# Reasoning Path:\nBurmese cat -> biology.animal_breed.place_of_origin -> Myanmar -> common.topic.notable_types -> Country\n# Answer:\nMyanmar"], "ground_truth": ["Thailand", "Myanmar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1699", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0tln5xz -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> tv.tv_character.appeared_in_tv_program -> m.0v_cdgd -> tv.regular_tv_appearance.actor -> Jim Carrey\n# Answer:\nJim Carrey", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.012vszbf -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0tln5xz -> film.performance.film -> Coneheads\n# Answer:\nConeheads"], "ground_truth": ["Chris Pine", "William Shatner", "Jimmy Bennett"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6451612903225806, "ans_precission": 0.625, "ans_recall": 0.6666666666666666, "path_f1": 0.43478260869565216, "path_precision": 0.625, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6451612903225806, "path_ans_precision": 0.625, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-170", "prediction": ["# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0gwn5hf -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0glw4lg -> film.performance.film -> The Hobbit: An Unexpected Journey\n# Answer:\nThe Hobbit: An Unexpected Journey", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.03l6qv_ -> film.performance.actor -> Ian Holm\n# Answer:\nIan Holm", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.0112mlwm -> film.performance.actor -> Mikhail Danilov\n# Answer:\nMikhail Danilov", "# Reasoning Path:\nOld Bilbo -> film.film_character.portrayed_in_films -> m.02vc8t6 -> film.performance.film -> The Lord of the Rings: The Fellowship of the Ring\n# Answer:\nThe Lord of the Rings: The Fellowship of the Ring", "# Reasoning Path:\nOld Bilbo -> fictional_universe.fictional_character.powers_or_abilities -> Invisibility -> book.book_subject.works -> Memoirs of an Invisible Man\n# Answer:\nInvisibility"], "ground_truth": ["Norman Bird"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1700", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Skamania\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.partially_contains -> White Salmon River\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.contains -> Mount St. Helens National Volcanic Monument\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.03qtqgr\n# Answer:\ncommon.image.size"], "ground_truth": ["Skamania County", "North America", "United States of America", "Washington", "Cascade Range"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6857142857142857, "ans_precission": 0.8, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1701", "prediction": ["# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minnesota\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Minnesota\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> West North Central States\n# Answer:\nMinnesota", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Ramsey County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nRamsey County", "# Reasoning Path:\nSaint Paul -> location.location.containedby -> Area code 651 -> location.location.containedby -> Minnesota\n# Answer:\nArea code 651", "# Reasoning Path:\nSaint Paul -> location.citytown.postal_codes -> 55101 -> location.location.containedby -> United States of America\n# Answer:\n55101", "# Reasoning Path:\nSaint Paul -> location.statistical_region.population -> g.11b66dwnhr\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Ramsey County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1702", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.parents -> Elizabeth Blount\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Mary I of England -> people.person.parents -> Catherine of Aragon\n# Answer:\nMary I of England"], "ground_truth": ["Henry, Duke of Cornwall", "Edward VI of England", "Henry FitzRoy, 1st Duke of Richmond and Somerset"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.7058823529411765, "path_precision": 0.75, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1705", "prediction": ["# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.award -> NME Award for Best Music Video\n# Answer:\nNME Award for Best Music Video", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.nominated_for -> Do You Want To\n# Answer:\nDo You Want To", "# Reasoning Path:\nFranz Ferdinand -> music.featured_artist.recordings -> The Dark of the Matinee -> music.recording.tracks -> The Dark of the Matin\u00e9e\n# Answer:\nThe Dark of the Matinee", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l3qm4 -> award.award_nomination.ceremony -> NME Awards 2006\n# Answer:\nNME Awards 2006", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l54ck -> award.award_nomination.award -> NME Award for Best Live Band\n# Answer:\nNME Award for Best Live Band", "# Reasoning Path:\nFranz Ferdinand -> music.artist.album -> 2004-11-15: Heineken Music Hall, Amsterdam, Netherlands -> music.album.album_content_type -> Live Album\n# Answer:\n2004-11-15: Heineken Music Hall, Amsterdam, Netherlands", "# Reasoning Path:\nFranz Ferdinand -> award.award_nominee.award_nominations -> m.0_l6jmw -> award.award_nomination.award -> NME Award for Best Album\n# Answer:\nNME Award for Best Album"], "ground_truth": ["Glasgow"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1706", "prediction": ["# Reasoning Path:\nBenedict Arnold -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor\n# Answer:\nTraitor", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_hints.included_types -> Topic\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> common.topic.article -> m.025_0m\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> type.type.properties -> Military Commands\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Military Commander -> freebase.type_hints.included_types -> Person\n# Answer:\nMilitary Commander", "# Reasoning Path:\nBenedict Arnold -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nBenedict Arnold -> military.military_commander.military_commands -> m.049yl9_ -> military.military_command.military_conflict -> Battle of Quebec\n# Answer:\nBattle of Quebec"], "ground_truth": ["Traitor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1707", "prediction": ["# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.actor -> Michael Rosenbaum\n# Answer:\nMichael Rosenbaum", "# Reasoning Path:\nSmallville -> award.award_nominated_work.award_nominations -> m.0n4pm1k -> award.award_nomination.award_nominee -> Michael Rosenbaum\n# Answer:\nMichael Rosenbaum", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 9\n# Answer:\nSmallville - Season 9", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8f_n -> tv.regular_tv_appearance.actor -> Ian Somerhalder\n# Answer:\nIan Somerhalder", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.02t8q51 -> tv.regular_tv_appearance.actor -> John Glover\n# Answer:\nJohn Glover", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.seasons -> Smallville - Season 1\n# Answer:\nSmallville - Season 1", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxf_b -> tv.regular_tv_appearance.character -> Alexander Luthor\n# Answer:\nAlexander Luthor", "# Reasoning Path:\nSmallville -> tv.tv_program.regular_cast -> m.03bxc0v -> tv.regular_tv_appearance.actor -> Erica Durance\n# Answer:\nErica Durance"], "ground_truth": ["Michael Rosenbaum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1708", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.context_name.pronunciation -> g.125_rvx57\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.visual_color_extra.hsv -> m.010q1hqc\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_types -> Color\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_types -> Color\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_for -> g.125g6j40s\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.cmyk -> m.010q_4qh\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.article -> m.03vtbl\n# Answer:\nSilver"], "ground_truth": ["Royal blue", "Navy Blue", "White", "Blue", "Silver"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-171", "prediction": ["# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> aviation.airport.serves -> Fort Walton Beach\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> aviation.airport.hub_for -> Vision Airlines\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> location.location.containedby -> Eglin Air Force Base\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> aviation.airport.serves -> Valparaiso\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.article -> m.01q6d5\n# Answer:\nDestin\u2013Fort Walton Beach Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> location.location.containedby -> 32541\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin Executive Airport -> aviation.airport.hub_for -> Southern Airways Express\n# Answer:\nDestin Executive Airport", "# Reasoning Path:\nDestin -> location.location.nearby_airports -> Destin\u2013Fort Walton Beach Airport -> common.topic.image -> Eglinafb-15feb1999\n# Answer:\nDestin\u2013Fort Walton Beach Airport"], "ground_truth": ["Destin\u2013Fort Walton Beach Airport", "Destin Executive Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1710", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Czechoslovakia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.statistical_region.internet_users_percent_population -> g.11b60nhcd0\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Ukraine\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1711", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.0z1ycgx -> award.award_nomination.award -> Outstanding Team ESPY Award\n# Answer:\nOutstanding Team ESPY Award", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.roster -> g.11b76d7nzz\n# Answer:\nsports.sports_team.roster", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> time.event.locations -> Staples Center\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> common.topic.article -> m.0jt52q2\n# Answer:\n2012 Stanley Cup Finals"], "ground_truth": ["2014 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-1712", "prediction": ["# Reasoning Path:\nSpencer Pratt -> people.person.education -> m.0n1cn_d -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> periodicals.newspaper_circulation_area.newspapers -> The Tidings\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> location.location.containedby -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pyr3 -> base.popstra.dated.participant -> Heidi Montag\n# Answer:\nHeidi Montag", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> business.business_location.parent_company -> Top Pest Control of Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> tv.tv_actor.guest_roles -> m.0bmrvc1 -> tv.tv_guest_role.episodes_appeared_in -> Benefits\n# Answer:\nBenefits", "# Reasoning Path:\nSpencer Pratt -> people.person.place_of_birth -> Los Angeles -> base.arthist.helynevek.intezmeny -> Getty Center\n# Answer:\nLos Angeles", "# Reasoning Path:\nSpencer Pratt -> base.popstra.celebrity.dated -> m.065pxjn -> base.popstra.dated.participant -> Kelly Durbin\n# Answer:\nKelly Durbin"], "ground_truth": ["University of Southern California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1713", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Strabismus -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nStrabismus", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> George Washington\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> medicine.disease.notable_people_with_this_condition -> Vincent Schiavelli\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Marfan syndrome -> symbols.namesake.named_after -> Antoine Marfan\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.disease.notable_people_with_this_condition -> Andrew Jackson\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> medicine.icd_9_cm_classification.includes_classifications -> Alastrim\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> medicine.notable_person_with_medical_condition.condition -> Smallpox -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nSmallpox", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky"], "ground_truth": ["Strabismus", "Smallpox", "Marfan syndrome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1714", "prediction": ["# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.coach -> Doug Lidster\n# Answer:\nDoug Lidster", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.coach -> John Tortorella\n# Answer:\nJohn Tortorella", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> sports.sports_team_coach_tenure.position -> Assistant Coach\n# Answer:\nAssistant Coach", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0vbljnd -> sports.sports_team_coach_tenure.coach -> Alain Vigneault\n# Answer:\nAlain Vigneault", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0113n9pr -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.010yzbv3 -> sports.sports_team_coach_tenure.coach -> Willie Desjardins\n# Answer:\nWillie Desjardins", "# Reasoning Path:\nVancouver Canucks -> sports.sports_team.coaches -> m.0w1yrsm -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nVancouver Canucks -> ice_hockey.hockey_team.coach -> Willie Desjardins -> sports.sports_team_coach.teams_coached -> m.010yzbv3\n# Answer:\nWillie Desjardins"], "ground_truth": ["Alain Vigneault", "Doug Lidster", "Willie Desjardins", "John Tortorella"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1715", "prediction": ["# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.containedby -> Northern Ireland\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nC. S. Lewis -> people.person.places_lived -> m.04hm5pn -> people.place_lived.location -> Belfast\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> G. K. Chesterton -> people.person.place_of_birth -> Kensington\n# Answer:\nG. K. Chesterton", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.containedby -> United Kingdom\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> people.person.place_of_birth -> Belfast -> location.location.nearby_airports -> George Best Belfast City Airport\n# Answer:\nBelfast", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced_by -> William Morris -> people.person.profession -> Visual Artist\n# Answer:\nWilliam Morris", "# Reasoning Path:\nC. S. Lewis -> influence.influence_node.influenced -> William P. Young -> influence.influence_node.influenced_by -> S\u00f8ren Kierkegaard\n# Answer:\nWilliam P. Young"], "ground_truth": ["Belfast"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1716", "prediction": ["# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Baoruco Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> base.aareas.schema.administrative_area.administrative_children -> Azua Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.capital -> Santo Domingo\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.place_of_birth -> Santo Domingo -> base.biblioness.bibs_location.country -> Dominican Republic\n# Answer:\nSanto Domingo", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.location.primarily_containedby -> Hispaniola\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.administrative_divisions -> Barahona Province\n# Answer:\nDominican Republic", "# Reasoning Path:\nCarlos Pe\u00f1a -> people.person.nationality -> Dominican Republic -> location.country.first_level_divisions -> Azua Province\n# Answer:\nDominican Republic"], "ground_truth": ["Dominican Republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1719", "prediction": ["# Reasoning Path:\nJane Wyman -> people.person.children -> Michael Reagan -> people.person.children -> Cameron Michael Reagan\n# Answer:\nMichael Reagan", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hyh1t9 -> people.marriage.spouse -> Ronald Reagan\n# Answer:\nRonald Reagan", "# Reasoning Path:\nJane Wyman -> people.person.children -> Christine Reagan -> people.person.parents -> Ronald Reagan\n# Answer:\nChristine Reagan", "# Reasoning Path:\nJane Wyman -> people.person.children -> Michael Reagan -> people.person.children -> Ashley Reagan\n# Answer:\nMichael Reagan", "# Reasoning Path:\nJane Wyman -> people.person.children -> Michael Reagan -> people.person.spouse_s -> m.0j6dgh2\n# Answer:\nMichael Reagan", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hyh1t9 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hmv78j -> people.marriage.spouse -> Fred Karger\n# Answer:\nFred Karger", "# Reasoning Path:\nJane Wyman -> film.actor.film -> g.11b6s3lhg1\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJane Wyman -> people.person.spouse_s -> m.0hyh1t9 -> people.marriage.location_of_ceremony -> Glendale\n# Answer:\nGlendale"], "ground_truth": ["Fred Karger"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-172", "prediction": ["# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> government.government_office_or_title.governmental_body_if_any -> United States Congress\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.052jk8c\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.appears_in_topic_gallery -> United States Senator\n# Answer:\n600px-Senate_Seal.svg.png", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> fictional_universe.character_occupation.characters_with_this_occupation -> Senator Ross Garrison\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> base.descriptive_names.names.descriptive_name -> m.0100zlls\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senator -> common.topic.notable_for -> g.1259lv35m\n# Answer:\nUnited States Senator", "# Reasoning Path:\nUnited States Senate -> government.governmental_body.offices_positions -> United States Senate majority leader -> government.government_office_or_title.office_holders -> m.052jk8v\n# Answer:\nUnited States Senate majority leader", "# Reasoning Path:\nUnited States Senate -> common.topic.image -> 600px-Senate_Seal.svg.png -> common.image.size -> m.01x3k95\n# Answer:\n600px-Senate_Seal.svg.png"], "ground_truth": ["Chief of staff", "Senate majority leader", "President pro tempore", "President of the Senate", "State Senator", "Senator"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.75, "ans_recall": 0.3333333333333333, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.48275862068965514, "path_ans_precision": 0.875, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1721", "prediction": ["# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.official_language -> Quechuan languages -> language.language_family.languages -> Ayacucho Quechua\n# Answer:\nQuechuan languages", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Waorani Language -> language.human_language.language_family -> Language isolate\n# Answer:\nWaorani Language", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Argentina\n# Answer:\nSpanish Language", "# Reasoning Path:\nPeru -> location.country.languages_spoken -> Puquina Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nPuquina Language"], "ground_truth": ["Puquina Language", "Quechuan languages", "Ayacucho Quechua", "Spanish Language", "Omagua dialect", "Aymara language", "Waorani Language"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.6956521739130435, "ans_precission": 0.8888888888888888, "ans_recall": 0.5714285714285714, "path_f1": 0.6956521739130435, "path_precision": 0.8888888888888888, "path_recall": 0.5714285714285714, "path_ans_f1": 0.792079207920792, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-1723", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.location.containedby -> Oceania\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Zimbabwe -> base.locations.countries.continent -> Africa\n# Answer:\nZimbabwe"], "ground_truth": ["Honduras", "Kiribati", "Mandatory Palestine", "England", "New Zealand", "State of Palestine", "India", "Guyana", "United Kingdom", "United States of America", "Swaziland", "Antigua and Barbuda", "Lesotho", "Tanzania", "Sudan", "Kingdom of Great Britain", "Tokelau", "Sri Lanka", "Cook Islands", "Vatican City", "Barbados", "Canada", "Philippines", "Zambia", "Cura\u00e7ao", "Hong Kong", "Liberia", "Bangladesh", "South Yemen", "China", "Nauru", "Gibraltar", "Vanuatu", "Grenada", "Brunei", "Pakistan", "Kenya", "Timor-Leste", "Gazankulu", "Marshall Islands", "Guam", "Dominica", "Saint Kitts and Nevis", "Laos", "Ethiopia", "Montserrat", "Namibia", "Qatar", "Bahamas", "Japan", "Israel", "Australia", "Tuvalu", "Bonaire", "Saint Vincent and the Grenadines", "Malaysia", "Saint Lucia", "Jersey", "Sierra Leone", "Indonesia", "Republic of Ireland", "Territory of Papua and New Guinea", "Rwanda", "Samoa", "Isle of Man", "Ghana", "Malta", "Papua New Guinea", "Cyprus", "Fiji", "Belize", "Wales", "Territory of New Guinea", "Puerto Rico", "Gambia", "South Africa", "Cayman Islands", "Bermuda", "Jordan", "Cameroon", "Botswana", "Transkei", "Uganda", "Zimbabwe", "Nigeria", "Singapore", "Turks and Caicos Islands"], "ans_acc": 0.034482758620689655, "ans_hit": 1, "ans_f1": 0.06666666666666667, "ans_precission": 1.0, "ans_recall": 0.034482758620689655, "path_f1": 0.06666666666666667, "path_precision": 1.0, "path_recall": 0.034482758620689655, "path_ans_f1": 0.06666666666666667, "path_ans_precision": 1.0, "path_ans_recall": 0.034482758620689655}
{"id": "WebQTest-1724", "prediction": ["# Reasoning Path:\nArsenal F.C. -> soccer.football_team.player_statistics -> m.0w8_167 -> soccer.football_player_stats.player -> Nacer Barazite\n# Answer:\nNacer Barazite", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2005 FA Cup Final -> common.topic.notable_types -> Football Match\n# Answer:\n2005 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> sports.sports_championship_event.championship -> FA Cup Final\n# Answer:\n2014 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 1929\u201330 FA Cup -> time.event.next_in_series -> 1930\u201331 FA Cup\n# Answer:\n1929\u201330 FA Cup", "# Reasoning Path:\nArsenal F.C. -> soccer.football_team.matches -> 1971 FA Cup Final -> soccer.football_match.held_at -> Wembley Stadium\n# Answer:\n1971 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2005 FA Cup Final -> sports.sports_championship_event.season -> 2004\u201305 FA Cup\n# Answer:\n2005 FA Cup Final", "# Reasoning Path:\nArsenal F.C. -> sports.sports_team.championships -> 2014 FA Cup Final -> sports.sports_championship_event.runner_up -> Hull City A.F.C.\n# Answer:\n2014 FA Cup Final"], "ground_truth": ["2004\u201305 FA Cup", "2001\u201302 FA Cup", "2013\u201314 FA Cup", "1997\u201398 FA Cup", "1978\u201379 FA Cup", "1992\u201393 FA Cup", "1970\u201371 FA Cup", "2002\u201303 FA Cup"], "ans_acc": 0.125, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.125, "path_ans_precision": 0.125, "path_ans_recall": 0.125}
{"id": "WebQTest-1725", "prediction": ["# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> location.location.time_zones -> Mountain Time Zone -> common.topic.article -> m.02hczk\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> Ypsilanti High School\n# Answer:\nYpsilanti High School", "# Reasoning Path:\nPhoenix -> common.topic.notable_types -> Character Species\n# Answer:\nCharacter Species", "# Reasoning Path:\nPhoenix -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nPhoenix -> education.school_mascot.school -> The Sharon Academy\n# Answer:\nThe Sharon Academy", "# Reasoning Path:\nPhoenix -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Statistical region\n# Answer:\nCity/Town/Village"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1726", "prediction": ["# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Maine -> location.location.containedby -> United States, with Territories\n# Answer:\nMaine", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.hud_county_place.county -> Cumberland County -> location.location.containedby -> Maine\n# Answer:\nCumberland County", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maine\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Portland -> location.statistical_region.population -> g.11b66h2c5f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSouth Portland -> location.statistical_region.population -> g.11bykyd6y5\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["United States of America", "Cumberland County", "Maine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1727", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.location.containedby -> Africa\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.country.form_of_government -> Republic\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> common.topic.notable_types -> Country\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Seychelles -> location.country.official_language -> English Language\n# Answer:\nSeychelles", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda"], "ground_truth": ["Chad", "Switzerland", "Seychelles", "Niger", "Luxembourg", "France", "Rwanda", "Madagascar", "Senegal", "Equatorial Guinea", "Mali", "Benin", "Monaco", "Haiti", "Comoros", "Cameroon", "Vanuatu", "Burundi", "Ivory Coast", "Guinea", "Belgium", "Gabon", "Burkina Faso", "Central African Republic", "Martinique", "Djibouti", "Congo", "Jersey"], "ans_acc": 0.17857142857142858, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 1.0, "ans_recall": 0.14285714285714285, "path_f1": 0.05555555555555556, "path_precision": 0.125, "path_recall": 0.03571428571428571, "path_ans_f1": 0.30303030303030304, "path_ans_precision": 1.0, "path_ans_recall": 0.17857142857142858}
{"id": "WebQTest-1728", "prediction": ["# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> You Bastard\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> common.topic.notable_types -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Vroomfondle\n# Answer:\nPhilosopher", "# Reasoning Path:\nPtolemy -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nPtolemy -> people.person.profession -> Mathematician -> common.topic.notable_for -> g.12553cs6s\n# Answer:\nMathematician", "# Reasoning Path:\nPtolemy -> people.person.profession -> Philosopher -> common.topic.notable_types -> Profession\n# Answer:\nPhilosopher"], "ground_truth": ["Astrologer", "Philosopher", "Astronomer", "Mathematician", "Geographer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 1.0, "ans_recall": 0.6, "path_f1": 0.711864406779661, "path_precision": 0.875, "path_recall": 0.6, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-1729", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.spouse -> Catherine of Aragon\n# Answer:\nCatherine of Aragon", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.spouse -> Anne Boleyn\n# Answer:\nAnne Boleyn", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.location_of_ceremony -> Palace of Placentia\n# Answer:\nPalace of Placentia", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Elizabeth I of England -> people.person.parents -> Anne Boleyn\n# Answer:\nElizabeth I of England", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx3p -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.03l46gn -> people.marriage.spouse -> Catherine Parr\n# Answer:\nCatherine Parr", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.spouse_s -> m.0j4kx1_ -> people.marriage.location_of_ceremony -> Westminster Abbey\n# Answer:\nWestminster Abbey"], "ground_truth": ["Catherine of Aragon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-173", "prediction": ["# Reasoning Path:\nBaltic states -> common.topic.image -> The three Baltic states: Estonia, Latvia, and Lithuania -> common.image.appears_in_topic_gallery -> Baltic Air Policing\n# Answer:\nThe three Baltic states: Estonia, Latvia, and Lithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.location.containedby -> Eurasia\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.statistical_region.population -> g.11bc87rwmc\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc3cztm\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.country.official_language -> Lithuanian language\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Lithuania -> location.location.containedby -> Eurasia\n# Answer:\nLithuania", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.1hhc39sn_\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.1hhc3dz61\n# Answer:\nEstonia", "# Reasoning Path:\nBaltic states -> location.location.contains -> Estonia -> olympics.olympic_participating_country.medals_won -> m.04hdw28\n# Answer:\nEstonia"], "ground_truth": ["Estonia", "Lithuania", "Latvia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.717948717948718, "path_precision": 0.7777777777777778, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1730", "prediction": ["# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Paraguay -> location.location.containedby -> South America\n# Answer:\nParaguay", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Maricoxi\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> location.location.containedby -> South America -> base.ontologies.ontology_instance.equivalent_instances -> m.09kljx0\n# Answer:\nSouth America", "# Reasoning Path:\nParan\u00e1 River -> geography.river.basin_countries -> Paraguay -> location.location.containedby -> Americas\n# Answer:\nParaguay", "# Reasoning Path:\nParan\u00e1 River -> geography.river.cities -> Salto del Guair\u00e1 -> location.location.containedby -> Paraguay\n# Answer:\nSalto del Guair\u00e1"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1731", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.ethnicity -> White American\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> business.board_member.organization_board_memberships -> m.05nm0rl\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Alexander Richard Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> common.topic.article -> m.041chd\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.children -> Jennie Eisenhower\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower"], "ground_truth": ["Richard Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1732", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nUniversity of Bonn", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> base.kwebbase.kwtopic.connections_from -> theodor schwann anticipated by lorenz oken -> base.kwebbase.kwconnection.relation -> anticipated by\n# Answer:\ntheodor schwann anticipated by lorenz oken"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin", "University of Bonn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1733", "prediction": ["# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> William Morris -> influence.influence_node.influenced_by -> John Ruskin\n# Answer:\nWilliam Morris", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Thomas Carlyle\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Walt Whitman\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Ebenezer Howard -> influence.influence_node.influenced_by -> Edward Bellamy\n# Answer:\nEbenezer Howard", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> William Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha\n# Answer:\nWilliam Morris", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced_by -> Charles Darwin\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Henry David Thoreau -> influence.influence_node.influenced -> \u00c9mile Armand\n# Answer:\nHenry David Thoreau", "# Reasoning Path:\nFrank Lloyd Wright -> influence.influence_node.influenced_by -> Bruce Price -> influence.influence_node.influenced -> John Russell Pope\n# Answer:\nBruce Price"], "ground_truth": ["Henry David Thoreau", "Elizabeth Gordon", "Bruce Price", "Ebenezer Howard", "William Morris", "Louis Sullivan", "Georges I. Gurdjieff and Thomas De Hartmann"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.7272727272727273, "ans_precission": 1.0, "ans_recall": 0.5714285714285714, "path_f1": 0.7272727272727273, "path_precision": 1.0, "path_recall": 0.5714285714285714, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 1.0, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1734", "prediction": ["# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.runner_up -> New York Knicks\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> common.topic.article -> m.08x9cw\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1995 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1994 NBA Finals -> sports.sports_championship_event.season -> 1993\u201394 NBA season\n# Answer:\n1994 NBA Finals", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.roster -> m.010b9n9r -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nHouston Rockets -> basketball.basketball_team.division -> Southwest Division -> basketball.basketball_division.conference -> Western Conference\n# Answer:\nSouthwest Division", "# Reasoning Path:\nHouston Rockets -> sports.sports_team.championships -> 1995 NBA Finals -> sports.sports_championship_event.runner_up -> Orlando Magic\n# Answer:\n1995 NBA Finals"], "ground_truth": ["1994 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1735", "prediction": ["# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Central Federal District -> base.aareas.schema.administrative_area.administrative_children -> Moscow\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Adygea -> location.location.containedby -> North Caucasus\n# Answer:\nAdygea", "# Reasoning Path:\nRussia -> base.aareas.schema.administrative_area.administrative_children -> Central Federal District -> base.aareas.schema.administrative_area.administrative_children -> Moscow\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Central Federal District -> base.aareas.schema.administrative_area.administrative_area_type -> Russian federal district\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Altai Krai -> location.location.containedby -> Siberian Federal District\n# Answer:\nAltai Krai", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Central Federal District -> common.topic.notable_types -> Russian federal district\n# Answer:\nCentral Federal District", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Altai Republic -> location.location.containedby -> Siberian Federal District\n# Answer:\nAltai Republic", "# Reasoning Path:\nRussia -> location.country.administrative_divisions -> Adygea -> location.location.events -> Russo-Circassian War\n# Answer:\nAdygea"], "ground_truth": ["Central Federal District"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1736", "prediction": ["# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Vatican City\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> book.book_subject.works -> The Emperor Charles V -> book.written_work.subjects -> Germany\n# Answer:\nThe Emperor Charles V", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Austria\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> location.location.containedby -> Europe -> location.location.contains_major_portion_of -> Spain\n# Answer:\nEurope", "# Reasoning Path:\nHoly Roman Empire -> book.book_subject.works -> The Emperor Charles V -> book.written_work.subjects -> History\n# Answer:\nThe Emperor Charles V"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1737", "prediction": ["# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> m.07ykl2c -> film.performance.film -> The Last Song\n# Answer:\nThe Last Song", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> m.0lcw1hd -> film.performance.film -> Paranoia\n# Answer:\nParanoia", "# Reasoning Path:\nLiam Hemsworth -> people.person.place_of_birth -> Melbourne -> film.film_location.featured_in_films -> Knowing\n# Answer:\nMelbourne", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> g.11b6dw3r61\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z9fwdz -> award.award_nomination.nominated_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> award.award_winner.awards_won -> m.0z83x7v -> award.award_honor.honored_for -> The Hunger Games\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nLiam Hemsworth -> film.actor.film -> g.11b6g5qdf3\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.award -> Teen Choice Award for Choice Movie Scene Stealer: Male\n# Answer:\nTeen Choice Award for Choice Movie Scene Stealer: Male", "# Reasoning Path:\nLiam Hemsworth -> award.award_nominee.award_nominations -> m.0z83xb2 -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards"], "ground_truth": ["The Expendables 2", "Empire State", "The Hunger Games: Mockingjay, Part 2", "Timeless", "Triangle", "Love and Honor", "Knowing", "The Last Song", "The Hunger Games: Catching Fire", "Cut Bank", "By Way of Helena", "The Dressmaker", "Draft:Independence Day 2", "Arabian Nights", "The Hunger Games", "Aurora Rising", "The Hunger Games: Mockingjay, Part 1", "Paranoia"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.5, "ans_recall": 0.16666666666666666, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.32432432432432434, "path_ans_precision": 0.6, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-1738", "prediction": ["# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.actor -> Amelia Heinle\n# Answer:\nAmelia Heinle", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2x3j -> tv.regular_tv_appearance.actor -> Daniel Polo\n# Answer:\nDaniel Polo", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g6jxk -> tv.regular_tv_appearance.actor -> Steve Burton\n# Answer:\nSteve Burton", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010pt1qr -> tv.regular_tv_appearance.actor -> Gina Tognoni\n# Answer:\nGina Tognoni", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.0112pnnf -> tv.regular_tv_appearance.actor -> Cady McClain\n# Answer:\nCady McClain", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010g2jfs -> tv.regular_tv_appearance.character -> Victoria Newman\n# Answer:\nVictoria Newman", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.recurring_writers -> m.0bmlttr -> tv.tv_program_writer_relationship.writer -> Jim Houghton\n# Answer:\nJim Houghton", "# Reasoning Path:\nThe Young and the Restless -> tv.tv_program.regular_cast -> m.010pt1qr -> tv.regular_tv_appearance.character -> Phyllis Summers Newman\n# Answer:\nPhyllis Summers Newman"], "ground_truth": ["Joshua Morrow"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1739", "prediction": ["# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> tv.tv_actor.starring_roles -> m.02s9hv3\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Matt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman\n# Answer:\nMatt Stone", "# Reasoning Path:\nSouth Park -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> South Park Universe -> fictional_universe.fictional_universe.works_set_here -> South Park: Bigger, Longer & Uncut\n# Answer:\nSouth Park Universe", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> music.producer.releases_produced -> Chocolate Salty Balls\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> tv.tv_actor.starring_roles -> m.02s9htm\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.program_creator -> Trey Parker -> people.person.nationality -> United States of America\n# Answer:\nTrey Parker", "# Reasoning Path:\nSouth Park -> tv.tv_program.languages -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language"], "ground_truth": ["Trey Parker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-174", "prediction": ["# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2xx -> sports.sports_team_roster.team -> Nevada Wolf Pack football\n# Answer:\nNevada Wolf Pack football", "# Reasoning Path:\nColin Kaepernick -> people.person.profession -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2xx -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2y4 -> sports.sports_team_roster.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nColin Kaepernick -> american_football.football_player.position_s -> Quarterback -> sports.sports_position.sport -> American football\n# Answer:\nQuarterback", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2xx -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nColin Kaepernick -> sports.pro_athlete.teams -> m.0hqf2y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["San Francisco 49ers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1740", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Maddox Chivan Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nMaddox Chivan Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> people.person.children -> James Haven\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Pax Thien Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nPax Thien Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> common.topic.notable_for -> g.12556r571\n# Answer:\nKnox Leon Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Shiloh Nouvel Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nShiloh Nouvel Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Vivienne Marcheline Jolie-Pitt -> people.person.parents -> Brad Pitt\n# Answer:\nVivienne Marcheline Jolie-Pitt", "# Reasoning Path:\nAngelina Jolie -> people.person.children -> Knox Leon Jolie-Pitt -> people.person.nationality -> United States of America\n# Answer:\nKnox Leon Jolie-Pitt"], "ground_truth": ["James Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1741", "prediction": ["# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.citytown -> Charlotte\n# Answer:\nCharlotte", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.state_province_region -> North Carolina\n# Answer:\nNorth Carolina", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> location.mailing_address.postal_code -> 28255\n# Answer:\n28255", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nBank of America Corporation -> organization.organization.headquarters -> m.03lfgp9 -> freebase.valuenotation.has_no_value -> Street Address 3\n# Answer:\nStreet Address 3", "# Reasoning Path:\nBank of America Corporation -> common.topic.webpage -> m.01xwxgw -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nBank of America Corporation -> business.issuer.issue -> BANK OF AMERICA CORP Common Stock -> business.issue.trading_symbol -> BOAT:BACJPY\n# Answer:\nBANK OF AMERICA CORP Common Stock"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1742", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6gg_n\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Zealand\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Unitary state", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1744", "prediction": ["# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0r66q69 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0r66mqx -> sports.sports_team_roster.team -> Cincinnati Reds\n# Answer:\nCincinnati Reds", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nJosh Hamilton -> baseball.baseball_player.batting_stats -> m.06qnhgk -> baseball.batting_statistics.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.0j2ql15 -> base.schemastaging.athlete_salary.team -> Texas Rangers\n# Answer:\nTexas Rangers", "# Reasoning Path:\nJosh Hamilton -> sports.pro_athlete.teams -> m.0r66mqx -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nJosh Hamilton -> base.schemastaging.athlete_extra.salary -> m.01064_wv -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Los Angeles Angels of Anaheim"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1746", "prediction": ["# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> location.location.time_zones -> Central Africa Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Zambia\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Uganda\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71nzld4\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nBurundi -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Angola\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Zambia\n# Answer:\nAfrica", "# Reasoning Path:\nBurundi -> location.statistical_region.gni_in_ppp_dollars -> g.11b60sz8sd\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1748", "prediction": ["# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> Hausa Language\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Sahel\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.country.languages_spoken -> French\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> location.location.partially_contains -> Yobe River\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Niger -> base.locations.countries.continent -> Africa\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> location.location.partially_containedby -> Nigeria -> location.location.adjoin_s -> m.02xj5pm\n# Answer:\nNigeria", "# Reasoning Path:\nNiger River -> geography.river.basin_countries -> Niger -> location.location.adjoin_s -> m.048z_n9\n# Answer:\nNiger", "# Reasoning Path:\nNiger River -> geography.river.basin_countries -> Niger -> location.location.partially_contains -> Yobe River\n# Answer:\nNiger"], "ground_truth": ["Niger", "Nigeria", "Mali"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.4800000000000001, "path_precision": 0.375, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-175", "prediction": ["# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.school -> University of Delaware\n# Answer:\nUniversity of Delaware", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> sports.drafted_athlete.drafted -> m.05bj8xr -> sports.sports_league_draft_pick.draft -> 2008 NFL draft\n# Answer:\n2008 NFL draft", "# Reasoning Path:\nJoe Flacco -> people.person.places_lived -> m.0wkkybd -> people.place_lived.location -> Audubon\n# Answer:\nAudubon", "# Reasoning Path:\nJoe Flacco -> sports.pro_athlete.teams -> m.07djpl8 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nJoe Flacco -> american_football.football_player.passing -> m.07914ry -> american_football.player_passing_statistics.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nJoe Flacco -> sports.pro_athlete.teams -> m.0j2jlzj -> sports.sports_team_roster.team -> Delaware Fightin' Blue Hens football\n# Answer:\nDelaware Fightin' Blue Hens football", "# Reasoning Path:\nJoe Flacco -> base.schemastaging.athlete_extra.salary -> m.0j2kdfx -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens"], "ground_truth": ["University of Delaware"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1750", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Alex Burkart\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.places_lived -> m.03pt7rq -> people.place_lived.location -> Janesville\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Aurelia Harwood\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.statistical_region.population -> g.11b66f8swj\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> common.topic.webpage -> m.059qqdm\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> periodicals.newspaper_circulation_area.newspapers -> Janesville Messenger\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.people_born_here -> Bob Strampe\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.place_of_birth -> Janesville -> location.location.contains -> The Back Bar\n# Answer:\nJanesville"], "ground_truth": ["Janesville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1751", "prediction": ["# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Kings County\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> location.location.containedby -> Canada\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.containedby -> Canada\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Nova Scotia -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nNova Scotia", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> film.film_location.featured_in_films -> Amelia\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Canada -> location.location.containedby -> North America\n# Answer:\nCanada", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0y3\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.statistical_region.population -> g.11byk_y_pp\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> location.location.containedby -> Wolfville -> location.location.geolocation -> m.02_kg9n\n# Answer:\nWolfville", "# Reasoning Path:\nAcadia University -> education.educational_institution.phone_number -> m.05sq0xy\n# Answer:\neducation.educational_institution.phone_number"], "ground_truth": ["Wolfville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1752", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> common.topic.notable_types -> Profession\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Novelist -> people.profession.corresponding_type -> Author\n# Answer:\nNovelist", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nPoet", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Writer -> people.profession.specializations -> Screenwriter\n# Answer:\nWriter", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.profession -> Poet -> common.topic.notable_types -> Profession\n# Answer:\nPoet"], "ground_truth": ["Novelist", "Writer", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1753", "prediction": ["# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.service_location -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.service_location -> France\n# Answer:\nFrance", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxcyk -> base.schemastaging.phone_sandbox.service_location -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.caller_category -> Consumer Service\n# Answer:\nConsumer Service", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxb2j -> base.schemastaging.phone_sandbox.service_location -> France\n# Answer:\nFrance", "# Reasoning Path:\nTomtom Nv -> common.topic.notable_for -> g.125dysf1w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nTechnical Support", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdxf42 -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nTomtom Nv -> base.schemastaging.organization_extra.phone_number -> m.0qdx6cj -> base.schemastaging.phone_sandbox.contact_category -> Technical Support\n# Answer:\nTechnical Support"], "ground_truth": ["Canada", "Switzerland", "Australia", "Belgium", "France", "United Kingdom", "United States of America"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.43636363636363634, "ans_precission": 0.4444444444444444, "ans_recall": 0.42857142857142855, "path_f1": 0.39999999999999997, "path_precision": 0.4444444444444444, "path_recall": 0.36363636363636365, "path_ans_f1": 0.43636363636363634, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1754", "prediction": ["# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.region -> Asia\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nKurdish languages", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwyx0z8\n# Answer:\nlocation.statistical_region.gdp_nominal"], "ground_truth": ["Persian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1755", "prediction": ["# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.contains -> Bellewaerde\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.geolocation -> m.0239wqc\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Adolph, Landgrave of Hesse-Philippsthal-Barchfeld\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.containedby -> Belgium\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> common.topic.notable_for -> g.1255_4z5q\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Albert Dev\u00e8ze\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.events -> Siege of Ypres\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> time.event.locations -> Ypres -> location.location.people_born_here -> Andreas Hyperius\n# Answer:\nYpres", "# Reasoning Path:\nSecond Battle of Ypres -> military.military_conflict.military_personnel_involved -> Oswald Mosley -> military.military_person.participated_in_conflicts -> World War I\n# Answer:\nOswald Mosley"], "ground_truth": ["Ypres"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1756", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> France\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.region -> Europe\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1757", "prediction": ["# Reasoning Path:\nMichael Crabtree -> american_football.football_player.position_s -> Wide receiver -> sports.sports_position.sport -> American football\n# Answer:\nWide receiver", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.position_s -> Wide receiver -> common.topic.article -> m.02g_71\n# Answer:\nWide receiver", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.position_s -> Wide receiver -> common.topic.webpage -> m.09w_mj1\n# Answer:\nWide receiver", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.position_s -> Wide receiver -> common.topic.image -> Wide Receiver, aufgestellt in der I-Formation\n# Answer:\nWide receiver", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.position_s -> Wide receiver -> common.topic.notable_for -> g.12551794r\n# Answer:\nWide receiver", "# Reasoning Path:\nMichael Crabtree -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player"], "ground_truth": ["American football player", "Athlete", "Wide receiver"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1758", "prediction": ["# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.containedby -> Africa\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> common.topic.notable_for -> g.125blwt0x\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.03x68v0\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.languages_spoken -> English Language\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Zambia -> location.country.official_language -> English Language\n# Answer:\nZambia", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Transkei -> location.location.containedby -> South Africa\n# Answer:\nTranskei", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.country.administrative_divisions -> Southern District\n# Answer:\nBotswana", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Zambia -> location.country.languages_spoken -> English Language\n# Answer:\nZambia", "# Reasoning Path:\nAfrikaans Language -> language.human_language.countries_spoken_in -> Botswana -> location.location.adjoin_s -> m.0441kl4\n# Answer:\nBotswana"], "ground_truth": ["Namibia", "Malawi", "South Africa", "Botswana", "Zambia", "Gazankulu", "Transkei"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5783132530120482, "ans_precission": 0.8888888888888888, "ans_recall": 0.42857142857142855, "path_f1": 0.5783132530120482, "path_precision": 0.8888888888888888, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6956521739130435, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-1759", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Choctawhatchee River -> location.location.partially_containedby -> Alabama\n# Answer:\nChoctawhatchee River", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> California\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Colorado\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> common.topic.subjects -> Automation\n# Answer:\nEPIC Systems, Inc"], "ground_truth": ["Georgia", "Alabama"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2162162162162162, "path_precision": 0.5, "path_recall": 0.13793103448275862, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-176", "prediction": ["# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.parents -> Conrad Ostberg Stone\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> common.topic.notable_for -> g.1258vxmmk\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.spouse_s -> m.0j4jwqj\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.children -> Spencer Stone\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> freebase.valuenotation.has_value -> Parents\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> people.person.profession -> General contractor\n# Answer:\nJeff Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Krista Stone -> people.person.profession -> Homemaking\n# Answer:\nKrista Stone", "# Reasoning Path:\nEmma Stone -> people.person.parents -> Jeff Stone -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nJeff Stone"], "ground_truth": ["Jeff Stone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1760", "prediction": ["# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.actor -> Cote de Pablo\n# Answer:\nCote de Pablo", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 10\n# Answer:\nNCIS - Season 10", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.series -> NCIS\n# Answer:\nNCIS", "# Reasoning Path:\nMossad Agent Ziva David -> common.topic.notable_for -> g.1258gnrh5\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 11\n# Answer:\nNCIS - Season 11", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 3\n# Answer:\nNCIS - Season 3", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_program -> m.043zbc6 -> tv.regular_tv_appearance.seasons -> NCIS - Season 4\n# Answer:\nNCIS - Season 4", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.03ly823 -> tv.tv_guest_role.actor -> Cote de Pablo\n# Answer:\nCote de Pablo", "# Reasoning Path:\nMossad Agent Ziva David -> tv.tv_character.appeared_in_tv_episodes -> m.09nj77s -> tv.tv_guest_role.episodes_appeared_in -> Kill Ari - Part 2\n# Answer:\nKill Ari - Part 2"], "ground_truth": ["Cote de Pablo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.2222222222222222, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1761", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.parents -> Elizabeth Blount\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> royalty.noble_person.titles -> m.05yv3wm\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset"], "ground_truth": ["Edward VI of England"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1763", "prediction": ["# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.baseball_coach -> Dave Duncan\n# Answer:\nDave Duncan", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0p -> baseball.current_coaching_tenure.baseball_coach -> Jos\u00e9 Oquendo\n# Answer:\nJos\u00e9 Oquendo", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> people.person.gender -> Male\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q0y -> baseball.current_coaching_tenure.baseball_coach -> Joe Pettini\n# Answer:\nJoe Pettini", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_manager -> Mike Matheny -> baseball.baseball_player.batting_stats -> m.06rgj_g\n# Answer:\nMike Matheny", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1p -> baseball.current_coaching_tenure.baseball_coach -> Hal McRae\n# Answer:\nHal McRae", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0669jb5 -> baseball.current_coaching_tenure.baseball_coach -> Dave McKay\n# Answer:\nDave McKay", "# Reasoning Path:\nSt. Louis Cardinals -> baseball.baseball_team.current_coaches -> m.0514q1f -> baseball.current_coaching_tenure.coaching_position -> Pitching Coach\n# Answer:\nPitching Coach"], "ground_truth": ["Dave Duncan", "Joe Pettini", "Hal McRae", "Jos\u00e9 Oquendo", "Dave McKay", "Marty Mason"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.625, "ans_recall": 0.8333333333333334, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.625, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1764", "prediction": ["# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Konkan\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> common.topic.notable_for -> g.1256k4jqk\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.contains -> Ratnagiri\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.containedby -> Maharashtra\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> common.topic.notable_for -> g.1254zzphk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> common.topic.article -> m.034n1y\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> base.crime.convicted_criminal.convictions -> m.05h1g7c -> base.crime.criminal_conviction.guilty_of -> Illegal drug trade\n# Answer:\nIllegal drug trade", "# Reasoning Path:\nDawood Ibrahim -> people.person.place_of_birth -> Ratnagiri district -> location.location.people_born_here -> Purushottam Shivaram Rege\n# Answer:\nRatnagiri district", "# Reasoning Path:\nDawood Ibrahim -> people.person.spouse_s -> m.0n91dwt -> freebase.valuenotation.has_value -> From\n# Answer:\nFrom"], "ground_truth": ["Maharashtra"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.1111111111111111, "path_recall": 0.3333333333333333, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1765", "prediction": ["# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Adana Province -> location.location.containedby -> Mediterranean Region, Turkey\n# Answer:\nAdana Province", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Adana Province -> common.topic.notable_types -> Administrative Division\n# Answer:\nAdana Province", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Ad\u0131yaman Province -> location.location.containedby -> Southeastern Anatolia Region\n# Answer:\nAd\u0131yaman Province", "# Reasoning Path:\nTurkey -> location.country.administrative_divisions -> Afyonkarahisar Province -> location.location.containedby -> Aegean Region\n# Answer:\nAfyonkarahisar Province", "# Reasoning Path:\nTurkey -> military.military_combatant.military_conflicts -> m.065sxsl -> military.military_combatant_group.conflict -> Korean War\n# Answer:\nKorean War"], "ground_truth": ["Eurasia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1766", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> L -> fictional_universe.fictional_character.ethnicity -> English people\n# Answer:\nL", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.main_country -> United States of America\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussian -> people.ethnicity.languages_spoken -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language"], "ground_truth": ["Tsakhur Language", "Ukrainian Language", "Adyghe Language", "Karachay-Balkar Language", "Khakas Language", "Abaza Language", "Tabassaran Language", "Ingush Language", "Dargwa Language", "Yakut Language", "Buryat language", "Udmurt Language", "Mari language", "Altai language", "Russian Language", "Lezgi Language", "Kabardian Language", "Lak Language", "Tatar Language", "Kalmyk-Oirat Language", "Crimean Turkish Language", "Rutul language", "Chechen Language", "Nogai Language", "Komi language", "Azerbaijani language", "Erzya Language", "Avar Language", "Moksha Language", "Yiddish Language", "Bashkir Language", "Aghul language", "Kumyk Language", "Tuvin Language", "Osetin Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15613382899628253, "ans_precission": 0.875, "ans_recall": 0.08571428571428572, "path_f1": 0.07650273224043716, "path_precision": 0.875, "path_recall": 0.04, "path_ans_f1": 0.15613382899628253, "path_ans_precision": 0.875, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-1767", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Mojola Agbebi\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ann Nixon Cooper\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06_41f8\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> base.usnris.nris_listing.significance_level -> National\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_burial -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Benjamin F. Ward\n# Answer:\nMartin Luther King, Jr. National Historic Site"], "ground_truth": ["Martin Luther King, Jr. National Historic Site"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1768", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Paseana -> base.thoroughbredracing.thoroughbred_racehorse.color -> Bay\n# Answer:\nPaseana", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1769", "prediction": ["# Reasoning Path:\nWales -> base.biblioness.bibs_location.country -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> United Kingdom\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.administrative_division.first_level_division_of -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> location.country.first_level_divisions -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom, with Dependencies and Territories -> common.topic.notable_types -> Country\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nWales -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom", "United Kingdom, with Dependencies and Territories"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-177", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.article -> m.05230r2\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> common.topic.notable_for -> g.1255mq7kb\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Edith Roosevelt\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> location.location.geolocation -> m.0cvdkt2\n# Answer:\nYoungs Memorial Cemetery", "# Reasoning Path:\nTheodore Roosevelt -> people.person.place_of_birth -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nTheodore Roosevelt -> people.deceased_person.place_of_burial -> Youngs Memorial Cemetery -> people.place_of_interment.interred_here -> Margaret Woodrow Wilson\n# Answer:\nYoungs Memorial Cemetery"], "ground_truth": ["Youngs Memorial Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1770", "prediction": ["# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> common.topic.notable_types -> Country\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> sports.sport_country.athletic_performances -> m.0fp9_m_\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> location.location.time_zones -> Central European Time Zone\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> people.person.nationality -> Spain -> location.country.official_language -> Spanish Language\n# Answer:\nSpain", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.match_format -> Men's singles\n# Answer:\nMen's singles", "# Reasoning Path:\nRafael Nadal -> award.award_nominee.award_nominations -> m.010_zj6l -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nRafael Nadal -> tennis.tennis_player.matches_won -> m.010h70y7 -> tennis.tennis_match.event -> Miami Open\n# Answer:\nMiami Open"], "ground_truth": ["Spain"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1771", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_for -> g.1q6hmhsk5\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.article -> m.0hr6vbt\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Moses and Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101 -> common.topic.notable_types -> Belief\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Zoroastrianism\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic view of angels -> common.topic.article -> m.0698fy\n# Answer:\nIslamic view of angels"], "ground_truth": ["Monotheism", "Tawhid", "Prophets in Islam", "God in Islam", "Predestination in Islam", "Mahdi", "Islamic holy books", "Masih ad-Dajjal", "Islamic view of angels", "Qiyamah", "\u1e6c\u016bb\u0101", "Sharia", "Entering Heaven alive"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.45528455284552855, "ans_precission": 0.875, "ans_recall": 0.3076923076923077, "path_f1": 0.45528455284552855, "path_precision": 0.875, "path_recall": 0.3076923076923077, "path_ans_f1": 0.45528455284552855, "path_ans_precision": 0.875, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-1772", "prediction": ["# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nMariano Rajoy", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.office_holder -> Mariano Rajoy\n# Answer:\nMariano Rajoy", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0cnvd2z -> government.government_position_held.office_holder -> Jos\u00e9 Luis Rodr\u00edguez Zapatero\n# Answer:\nJos\u00e9 Luis Rodr\u00edguez Zapatero", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrj1t -> government.government_position_held.basic_title -> Deputy prime minister\n# Answer:\nDeputy prime minister", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.010wrg7c -> government.government_position_held.office_holder -> Alejandro Rodr\u00edguez de Valc\u00e1rcel\n# Answer:\nAlejandro Rodr\u00edguez de Valc\u00e1rcel", "# Reasoning Path:\nSpain -> government.governmental_jurisdiction.governing_officials -> m.0hnsn5h -> government.government_position_held.office_position_or_title -> Prime Minister of Spain\n# Answer:\nPrime Minister of Spain", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc55416\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Jos\u00e9 Luis Rodr\u00edguez Zapatero", "Mariano Rajoy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-1773", "prediction": ["# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.parents -> Gorgo, Queen of Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.place_of_birth -> Sparta\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> common.topic.notable_for -> g.12578hrb8\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> people.person.nationality -> Greece\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Cause of death\n# Answer:\nPleistarchus", "# Reasoning Path:\nLeonidas I -> people.person.place_of_birth -> Sparta -> military.military_combatant.military_conflicts -> m.03z96bp\n# Answer:\nSparta", "# Reasoning Path:\nLeonidas I -> people.person.place_of_birth -> Sparta -> location.location.people_born_here -> Cleomenes III\n# Answer:\nSparta", "# Reasoning Path:\nLeonidas I -> people.person.children -> Pleistarchus -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nPleistarchus"], "ground_truth": ["Pleistarchus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1774", "prediction": ["# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.spouse -> Michael Williams\n# Answer:\nMichael Williams", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nJudi Dench -> people.person.spouse_s -> m.0hyl98h -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nJudi Dench -> people.person.profession -> Actor -> award.award_discipline.awards_in_this_discipline -> Golden Globe Award for Best Actress \u2013 Motion Picture \u2013 Drama\n# Answer:\nActor", "# Reasoning Path:\nJudi Dench -> film.actor.film -> m.0113rwfg -> film.performance.character -> Evelyn Greenslade\n# Answer:\nEvelyn Greenslade", "# Reasoning Path:\nJudi Dench -> award.award_winner.awards_won -> m.04ktfzn -> award.award_honor.ceremony -> 71st Academy Awards\n# Answer:\n71st Academy Awards"], "ground_truth": ["Michael Williams"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1775", "prediction": ["# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> Andorra\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.countries_spoken_in -> France\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Catalan language -> language.human_language.region -> Europe\n# Answer:\nCatalan language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Basque Language", "Catalan language", "Occitan language", "Spanish Language", "Galician Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.6774193548387097, "path_precision": 0.7777777777777778, "path_recall": 0.6, "path_ans_f1": 0.7164179104477612, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.6}
{"id": "WebQTest-1776", "prediction": ["# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.containedby -> Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> book.written_work.previous_in_series -> Hemingway Adventure\n# Answer:\nHemingway Adventure", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Al Sharqia Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.partiallycontains -> m.0wg920z\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> geography.geographical_feature_category.features -> Atacama Desert\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.contains -> Alexandria Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> book.book_subject.works -> A Tragedy of Arms\n# Answer:\nNorth Africa", "# Reasoning Path:\nSahara -> geography.geographical_feature.category -> Desert -> fictional_universe.type_of_fictional_setting.settings -> Sandia\n# Answer:\nDesert", "# Reasoning Path:\nSahara -> location.location.containedby -> North Africa -> location.location.events -> African campaigns of World War II\n# Answer:\nNorth Africa"], "ground_truth": ["North Africa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1777", "prediction": ["# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.children -> Rorrey Fenty\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Parents\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.nationality -> Barbados\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.children -> Rajad Fenty\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> people.person.gender -> Female\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Monica Braithwaite -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nMonica Braithwaite", "# Reasoning Path:\nRihanna -> people.person.parents -> Ronald Fenty -> celebrities.celebrity.substance_abuse_problems -> m.0k8hg6n\n# Answer:\nRonald Fenty"], "ground_truth": ["Monica Braithwaite"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1779", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Theodore Roosevelt Inaugural National Historic Site\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Theodore Roosevelt Inaugural National Historic Site\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Buffalo\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> common.topic.article -> m.05c4h5h\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> location.statistical_region.population -> g.11bc88l5f2\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> symbols.name_source.namesakes -> Roosevelt -> common.topic.article -> m.02gt_g\n# Answer:\nRoosevelt", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nPresident"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-178", "prediction": ["# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.speaker_s -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rpn5h -> event.speech_or_presentation.speaker_s -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.presented_work -> Arsenal of Democracy\n# Answer:\nArsenal of Democracy", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rm8xf -> event.speech_or_presentation.speaker_s -> Winston Churchill\n# Answer:\nWinston Churchill", "# Reasoning Path:\nWorld War II -> base.skosbase.vocabulary_equivalent_topic.narrower_concept -> World War, 1939-1945--Hostages -> base.skosbase.skos_concept.broader_topic -> Hostage\n# Answer:\nWorld War, 1939-1945--Hostages", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rln07 -> event.speech_or_presentation.type_or_format_of_presentation -> Radio address\n# Answer:\nRadio address", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.010nmtv6 -> event.speech_or_presentation.speaker_s -> Y\u014dhei K\u014dno\n# Answer:\nY\u014dhei K\u014dno", "# Reasoning Path:\nWorld War II -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rmf82 -> event.speech_or_presentation.speaker_s -> Winston Churchill\n# Answer:\nWinston Churchill"], "ground_truth": ["Franklin D. Roosevelt", "Harry S. Truman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1780", "prediction": ["# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9d4 -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9d4 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cx -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> american_football.football_player.receiving -> m.07sgvjq -> american_football.player_receiving_statistics.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9dd -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> common.image.size -> m.0cph215\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0zt0n9f -> base.schemastaging.athlete_salary.team -> Baltimore Ravens\n# Answer:\nBaltimore Ravens", "# Reasoning Path:\nMichael Oher -> base.schemastaging.athlete_extra.salary -> m.0j2l9cn -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Baltimore Ravens"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.6666666666666666, "path_recall": 0.15384615384615385, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1781", "prediction": ["# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> Immanuel Kant\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> John Locke\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Sigmund Freud\n# Answer:\nBaruch Spinoza", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced -> Sigmund Freud\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Mahatma Gandhi -> influence.influence_node.influenced_by -> Gautama Buddha\n# Answer:\nMahatma Gandhi", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> influence.influence_node.influenced_by -> Buddhism\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Arthur Schopenhauer -> people.person.religion -> Atheism\n# Answer:\nArthur Schopenhauer", "# Reasoning Path:\nAlbert Einstein -> influence.influence_node.influenced_by -> Baruch Spinoza -> influence.influence_node.influenced -> Ludwig Wittgenstein\n# Answer:\nBaruch Spinoza"], "ground_truth": ["Hermann Minkowski", "Ernst Mach", "James Clerk Maxwell", "Fyodor Dostoyevsky", "George Bernard Shaw", "Paul Val\u00e9ry", "Arthur Schopenhauer", "Bernhard Riemann", "Henry George", "Riazuddin", "David Hume", "Moritz Schlick", "Baruch Spinoza", "Hendrik Lorentz", "Isaac Newton", "Thomas Young", "Karl Pearson", "Mahatma Gandhi"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 1.0, "ans_recall": 0.16666666666666666, "path_f1": 0.2857142857142857, "path_precision": 1.0, "path_recall": 0.16666666666666666, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 1.0, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-1782", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.places_exported_to -> m.048prww -> location.imports_and_exports.exported_to -> Sudan\n# Answer:\nSudan", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.main_country -> Saudi Arabia\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.countries_spoken_in -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Saudi Arabia\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.main_country -> Sudan\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic Language", "# Reasoning Path:\nEgypt -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> France\n# Answer:\nAdventures by Disney"], "ground_truth": ["Sudan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.26086956521739135, "path_precision": 0.375, "path_recall": 0.2, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1784", "prediction": ["# Reasoning Path:\nAfrican Union -> organization.organization.founders -> Muammar Gaddafi -> people.deceased_person.place_of_death -> Sirte\n# Answer:\nMuammar Gaddafi", "# Reasoning Path:\nAfrican Union -> organization.organization.founders -> Muammar Gaddafi -> people.person.nationality -> Libya\n# Answer:\nMuammar Gaddafi", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> location.mailing_address.citytown -> Addis Ababa\n# Answer:\nAddis Ababa", "# Reasoning Path:\nAfrican Union -> organization.organization.founders -> Muammar Gaddafi -> people.person.place_of_birth -> Qasr Abu Hadi\n# Answer:\nMuammar Gaddafi", "# Reasoning Path:\nAfrican Union -> base.schemastaging.context_name.pronunciation -> g.125_l8y1s\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nAfrican Union -> organization.organization.headquarters -> m.0pcb32w -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAfrican Union -> organization.organization.founders -> Muammar Gaddafi -> people.person.spouse_s -> m.0hykj61\n# Answer:\nMuammar Gaddafi", "# Reasoning Path:\nAfrican Union -> organization.organization.parent -> m.0ccc11c -> organization.organization_relationship.parent -> Organisation of African Unity\n# Answer:\nOrganisation of African Unity", "# Reasoning Path:\nAfrican Union -> organization.organization.founders -> Muammar Gaddafi -> people.person.children -> Saif al-Arab Gaddafi\n# Answer:\nMuammar Gaddafi"], "ground_truth": ["Addis Ababa"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1785", "prediction": ["# Reasoning Path:\nKim Richards -> film.actor.film -> m.0131hj6c -> film.performance.film -> Nanny and the Professor\n# Answer:\nNanny and the Professor", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.02vblh0 -> film.performance.film -> Devil Dog: The Hound of Hell\n# Answer:\nDevil Dog: The Hound of Hell", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngb3y -> tv.regular_tv_appearance.series -> Hello, Larry\n# Answer:\nHello, Larry", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.03jrr4j -> film.performance.film -> No Deposit, No Return\n# Answer:\nNo Deposit, No Return", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.children -> Kyle Richards\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.03kzv77 -> film.performance.film -> Tuff Turf\n# Answer:\nTuff Turf", "# Reasoning Path:\nKim Richards -> tv.tv_actor.starring_roles -> m.0bngq3l -> tv.regular_tv_appearance.series -> James at 15\n# Answer:\nJames at 15", "# Reasoning Path:\nKim Richards -> film.actor.film -> m.03lbqgb -> film.performance.film -> Meatballs Part II\n# Answer:\nMeatballs Part II"], "ground_truth": ["Nanny and the Professor", "Devil Dog: The Hound of Hell", "Tuff Turf", "Black Snake Moan", "Race to Witch Mountain", "The Car", "No Deposit, No Return", "Meatballs Part II", "Escape to Witch Mountain", "The Whiz Kid and the Mystery at Riverton", "Raid on Entebbe", "Return from Witch Mountain"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.625, "ans_recall": 0.4166666666666667, "path_f1": 0.4761904761904762, "path_precision": 0.625, "path_recall": 0.38461538461538464, "path_ans_f1": 0.5, "path_ans_precision": 0.625, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-1788", "prediction": ["# Reasoning Path:\nHenri Rousseau -> common.topic.article -> m.02rdfj\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.image -> Pink Bulls\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Origins, Imitation, Conventions\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.webpage -> m.0b46487\n# Answer:\nModern art", "# Reasoning Path:\nHenri Rousseau -> common.topic.notable_for -> g.125bpgqyn\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Post-Impressionism", "Modern art", "Na\u00efve art", "Primitivism"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.8, "ans_recall": 0.25, "path_f1": 0.38095238095238093, "path_precision": 0.8, "path_recall": 0.25, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.8, "path_ans_recall": 0.25}
{"id": "WebQTest-1789", "prediction": ["# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.religion -> United Methodist Church -> religion.religion.founding_figures -> John Wesley\n# Answer:\nUnited Methodist Church", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> common.topic.notable_for -> g.1257qk0k_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Mayor\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Edwards -> people.person.profession -> Politician -> base.schemastaging.context_name.pronunciation -> g.125_lw10d\n# Answer:\nPolitician"], "ground_truth": ["Political corruption"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-179", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Th\u00e9o van Rysselberghe\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Albert Dubois-Pillet\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artworks -> Sunday\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> common.topic.article -> m.0gcv9\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Alfred William Finch\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.image -> Georges Seurat - Un dimanche apr\u00c3\u00a8s-midi \u00c3\u00a0 l'\u00c3\u008ele de la Grande Jatte v2\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Fauvism -> common.topic.notable_for -> g.1257w3_9g\n# Answer:\nFauvism"], "ground_truth": ["Impressionism", "Modern art", "Neo-impressionism", "Modernism", "Fauvism"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6774193548387097, "ans_precission": 0.7777777777777778, "ans_recall": 0.6, "path_f1": 0.5283018867924528, "path_precision": 0.7777777777777778, "path_recall": 0.4, "path_ans_f1": 0.6774193548387097, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.6}
{"id": "WebQTest-1790", "prediction": ["# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> A View to a Kill\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> common.topic.article -> m.018y12\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Who Framed Roger Rabbit\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Adams Point\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Arena\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> government.governmental_jurisdiction.governing_officials -> m.04cpv8p\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> film.film_location.featured_in_films -> Baraka\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.deceased_person.place_of_death -> Oakland -> location.place_with_neighborhoods.neighborhoods -> Bella Vista\n# Answer:\nOakland", "# Reasoning Path:\nHuey P. Newton -> people.person.sibling_s -> m.0yqb6x_ -> people.sibling_relationship.sibling -> Lee Edward Newton\n# Answer:\nLee Edward Newton"], "ground_truth": ["Oakland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1791", "prediction": ["# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> people.person.place_of_birth -> Mainz\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.argumentmaps.innovator.original_ideas -> Movable type\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> common.topic.article -> m.05sy2\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPrinting press -> base.ontologies.ontology_instance.equivalent_instances -> m.09dvt_4 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nPrinting press -> common.topic.notable_for -> g.1255pjwdy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.kwebbase.kwtopic.has_sentences -> After Fust's death, Schoeffer continued the printing business and expanded it until it covered virtually the whole of western Europe and included also publishing and bookselling.\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> common.topic.webpage -> m.09xsh6z -> common.webpage.resource -> From Our Staffers: 'Gutenberg! The Musical!'\n# Answer:\nFrom Our Staffers: 'Gutenberg! The Musical!'", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.kwebbase.kwtopic.connections_from -> johannes gutenberg invention improved by willem blaeu\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.kwebbase.kwtopic.has_sentences -> By the late 1430s he had apparently begun experimenting with printing.\n# Answer:\nJohannes Gutenberg", "# Reasoning Path:\nPrinting press -> law.invention.inventor -> Johannes Gutenberg -> base.kwebbase.kwtopic.connections_from -> johannes gutenberg invention user was aldus manutius\n# Answer:\nJohannes Gutenberg"], "ground_truth": ["1440"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1792", "prediction": ["# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.location.containedby -> United Kingdom\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.geolocation -> m.02_h136\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.country.first_level_divisions -> Scotland\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.administrative_division.country -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> Scotland -> location.administrative_division.country -> United Kingdom\n# Answer:\nScotland", "# Reasoning Path:\nMidlothian -> location.administrative_division.country -> United Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nMidlothian -> location.location.containedby -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom"], "ground_truth": ["United Kingdom", "United Kingdom, with Dependencies and Territories", "Scotland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.761904761904762, "ans_precission": 0.8888888888888888, "ans_recall": 0.6666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1793", "prediction": ["# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.team -> Ume\u00e5 IK\n# Answer:\nUme\u00e5 IK", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vl6r -> sports.sports_team_roster.team -> Los Angeles Sol\n# Answer:\nLos Angeles Sol", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z3vp21 -> sports.sports_team_roster.team -> Tyres\u00f6 FF\n# Answer:\nTyres\u00f6 FF", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z9nl0h -> sports.sports_team_roster.team -> Brazil women's national football team\n# Answer:\nBrazil women's national football team", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.0z9p26m -> sports.sports_team_roster.team -> Santos FC\n# Answer:\nSantos FC", "# Reasoning Path:\nMarta -> sports.pro_athlete.teams -> m.04gd4np -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nMarta -> people.person.places_lived -> m.0wkn5gz -> people.place_lived.location -> Dois Riachos\n# Answer:\nDois Riachos", "# Reasoning Path:\nMarta -> people.person.sibling_s -> m.0kfs8d6 -> people.sibling_relationship.sibling -> Jos\u00e9 da Silva\n# Answer:\nJos\u00e9 da Silva"], "ground_truth": ["Brazil women's national football team", "Tyres\u00f6 FF"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1794", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> United States of America\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> base.wikipedia_infobox.settlement.area_code -> Area code 313\n# Answer:\nDetroit", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.place_of_birth -> Colonia Dubl\u00e1n\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.place_of_birth -> Detroit -> location.location.containedby -> Wayne County\n# Answer:\nDetroit"], "ground_truth": ["Detroit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1795", "prediction": ["# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.parents -> Toni Terry\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> common.topic.notable_types -> Person\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.sibling_s -> m.0k01v5f\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> common.topic.article -> m.0lv3tly\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> people.person.parents -> Toni Terry\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Georgie John Terry -> people.person.nationality -> England\n# Answer:\nGeorgie John Terry", "# Reasoning Path:\nJohn Terry -> people.person.children -> Summer Rose Terry -> common.topic.notable_types -> Person\n# Answer:\nSummer Rose Terry", "# Reasoning Path:\nJohn Terry -> common.topic.image -> John-Terry2 -> common.image.size -> m.0292hlm\n# Answer:\nJohn-Terry2"], "ground_truth": ["Summer Rose Terry", "Georgie John Terry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1796", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.film -> Live at Wembley July 16, 1988\n# Answer:\nLive at Wembley July 16, 1988", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> music.producer.releases_produced -> Yours Faithfully -> common.topic.notable_types -> Musical Release\n# Answer:\nYours Faithfully", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WBAV -> broadcast.content.location -> Charlotte\n# Answer:\nWBAV", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.artist -> Janet Jackson\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WSJT -> broadcast.content.artist -> Janet Jackson\n# Answer:\nWSJT"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1797", "prediction": ["# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.048z_8v\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> book.book_subject.works -> An Honorable Defeat: The Last Days of the Confederate Government\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> common.topic.notable_types -> Country\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> William F. Draper -> military.military_person.participated_in_conflicts -> Siege of Petersburg\n# Answer:\nWilliam F. Draper", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> military.military_combatant.military_commanders -> m.049y2m5\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> Confederate States of America -> location.country.capital -> Richmond\n# Answer:\nConfederate States of America", "# Reasoning Path:\nSiege of Vicksburg -> military.military_conflict.military_personnel_involved -> William F. Draper -> people.person.nationality -> United States of America\n# Answer:\nWilliam F. Draper", "# Reasoning Path:\nSiege of Vicksburg -> base.culturalevent.event.entity_involved -> John C. Pemberton -> military.military_person.participated_in_conflicts -> Mexican\u2013American War\n# Answer:\nJohn C. Pemberton"], "ground_truth": ["Ulysses S. Grant", "John C. Pemberton", "Confederate States of America", "Union", "United States of America"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5217391304347827, "ans_precission": 0.75, "ans_recall": 0.4, "path_f1": 0.5217391304347827, "path_precision": 0.75, "path_recall": 0.4, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-1799", "prediction": ["# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066t9b\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> education.field_of_study.students_majoring -> m.012nj6qx\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> book.author.book_editions_published -> Dead Souls -> book.book.genre -> Fiction\n# Answer:\nDead Souls", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> base.descriptive_names.names.descriptive_name -> m.01066tcy\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Playwright -> common.topic.article -> m.09sfv\n# Answer:\nPlaywright", "# Reasoning Path:\nNikolai Gogol -> people.person.profession -> Novelist -> people.profession.specialization_of -> Writer\n# Answer:\nNovelist"], "ground_truth": ["Novelist", "Writer", "Playwright"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-180", "prediction": ["# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> geography.river.mouth -> Gulf of Mexico\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> United States of America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> geography.river.mouth -> Mississippi River -> location.location.containedby -> North America\n# Answer:\nMississippi River", "# Reasoning Path:\nMissouri River -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMissouri River -> common.topic.notable_types -> River -> type.type.domain -> Physical Geography\n# Answer:\nRiver", "# Reasoning Path:\nMissouri River -> common.topic.notable_types -> River -> type.type.properties -> Mouth\n# Answer:\nRiver", "# Reasoning Path:\nMissouri River -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi\n# Answer:\nUnited States of America"], "ground_truth": ["Mississippi River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1800", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 2012 Major League Baseball season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.seasons -> 1974 Major League Baseball Season\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2ds6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt479\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> book.book_subject.works -> The Gashouse Gang: How Dizzy Dean, Leo Durocher, Branch Rickey, Pepper Martin, and Their Colorful, Come-from-Behind Ball Club Won the World Series - and America's Heart - During the Great Depression -> book.written_work.subjects -> St. Louis Cardinals\n# Answer:\nThe Gashouse Gang: How Dizzy Dean, Leo Durocher, Branch Rickey, Pepper Martin, and Their Colorful, Come-from-Behind Ball Club Won the World Series - and America's Heart - During the Great Depression", "# Reasoning Path:\nWorld Series -> common.topic.notable_for -> g.125f97994\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> business.employer.employees -> m.010vsyz7\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt4fq\n# Answer:\nMajor League Baseball"], "ground_truth": ["1903 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1801", "prediction": ["# Reasoning Path:\nSimon Birch -> film.film_character.portrayed_in_films -> m.0k39m5 -> film.performance.actor -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.award_nominee -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.0k39m5 -> film.performance.actor -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.award -> Critics' Choice Movie Award for Best Young Performer\n# Answer:\nCritics' Choice Movie Award for Best Young Performer", "# Reasoning Path:\nSimon Birch -> award.award_winning_work.awards_won -> m.09tr1z6 -> award.award_honor.award_winner -> Ian Michael Smith\n# Answer:\nIan Michael Smith", "# Reasoning Path:\nSimon Birch -> award.award_nominated_work.award_nominations -> m.0n4xs9w -> award.award_nomination.ceremony -> 4th Critics' Choice Awards\n# Answer:\n4th Critics' Choice Awards", "# Reasoning Path:\nSimon Birch -> award.award_winning_work.awards_won -> m.09tr1z6 -> award.award_honor.ceremony -> 4th Critics' Choice Awards\n# Answer:\n4th Critics' Choice Awards", "# Reasoning Path:\nSimon Birch -> film.film.starring -> m.04m3px3 -> film.performance.actor -> Jim Carrey\n# Answer:\nJim Carrey"], "ground_truth": ["Ian Michael Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1802", "prediction": ["# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Hawaii-Aleutian Time Zone\n# Answer:\nHawaii-Aleutian Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> American Samoa\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Alaska Time Zone\n# Answer:\nAlaska Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Atlantic Time Zone\n# Answer:\nAtlantic Time Zone", "# Reasoning Path:\nUnited States of America -> location.location.time_zones -> Samoa Time Zone -> time.time_zone.locations_in_this_time_zone -> Palmyra Atoll\n# Answer:\nSamoa Time Zone", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71srh9q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Chamorro Time Zone", "Central Time Zone", "Samoa Time Zone", "Alaska Time Zone", "Hawaii-Aleutian Time Zone", "Atlantic Time Zone", "Pacific Time Zone", "Eastern Time Zone", "Mountain Time Zone"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.606060606060606, "ans_precission": 0.6666666666666666, "ans_recall": 0.5555555555555556, "path_f1": 0.606060606060606, "path_precision": 0.6666666666666666, "path_recall": 0.5555555555555556, "path_ans_f1": 0.606060606060606, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-1803", "prediction": ["# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Oakland Motor Car Company\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> Cadillac\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> common.topic.notable_types -> Industry\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.child_industry -> Motor Vehicles and Passenger Car Bodies (automobiles)\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> common.topic.notable_for -> g.125gnq786\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.parent_industry -> Transportation Equipment Manufacturing\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> business.business_operation.industry -> Motor Vehicles and Passenger Car Bodies -> business.industry.companies -> American Motors\n# Answer:\nMotor Vehicles and Passenger Car Bodies", "# Reasoning Path:\nGeneral Motors -> common.topic.subjects -> GMC -> common.topic.subjects -> GMC Yukon\n# Answer:\nGMC"], "ground_truth": ["Automotive industry", "Motor Vehicles and Passenger Car Bodies"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-1804", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Sierra Leone\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language"], "ground_truth": ["Irish", "Esperanto Language", "Cornish Language", "English Language", "British Sign Language", "Scots Language", "Scottish Gaelic", "Welsh Language"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.125, "path_precision": 0.125, "path_recall": 0.125, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-1805", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_for -> g.1255wtfxx\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> base.schemastaging.context_name.pronunciation -> g.125_kkdgj\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1806", "prediction": ["# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Power forward -> sports.sports_position.sport -> Basketball\n# Answer:\nPower forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> g.11byb3s7cc\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.sports_played_professionally -> m.0c3p_f6 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> John Salmons\n# Answer:\nSmall forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> sports.sports_position.players -> m.010fwg1m\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> common.topic.image -> Basketball half-court\n# Answer:\nPoint forward", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.position_s -> Point forward -> common.topic.notable_types -> Basketball Position\n# Answer:\nPoint forward"], "ground_truth": ["Point forward", "Shooting guard", "Power forward", "Small forward", "Forward"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.75, "ans_recall": 0.8, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.75, "path_ans_recall": 0.8}
{"id": "WebQTest-1807", "prediction": ["# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.team -> Edmonton Oilers\n# Answer:\nEdmonton Oilers", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.07vkdw8 -> sports.sports_team_roster.position -> Centerman\n# Answer:\nCenterman", "# Reasoning Path:\nWayne Gretzky -> sports.pro_athlete.teams -> m.0125clwk -> sports.sports_team_roster.position -> Centerman\n# Answer:\nCenterman", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Wayne Gretzky Sports Centre\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> government.governmental_jurisdiction.government_bodies -> Brantford City Council\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> sports.sports_team_location.teams -> Brantford Smoke\n# Answer:\nBrantford", "# Reasoning Path:\nWayne Gretzky -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nWayne Gretzky -> people.person.place_of_birth -> Brantford -> travel.travel_destination.tourist_attractions -> Arnold Anderson Stadium at Cockshutt Park\n# Answer:\nBrantford"], "ground_truth": ["Edmonton Oilers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1808", "prediction": ["# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nOllanta Humala", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.appointed_by -> Ollanta Humala\n# Answer:\nOllanta Humala", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_holder -> Juan Jim\u00e9nez Mayor\n# Answer:\nJuan Jim\u00e9nez Mayor", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> government.government_position_held.office_holder -> Ana Jara Vel\u00e1squez\n# Answer:\nAna Jara Vel\u00e1squez", "# Reasoning Path:\nPeru -> common.topic.article -> m.016w_5\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.011c1dkp -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> government.governmental_jurisdiction.governing_officials -> m.0yp_d4w -> government.government_position_held.office_position_or_title -> Prime Minister of Peru\n# Answer:\nPrime Minister of Peru"], "ground_truth": ["Ollanta Humala"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1809", "prediction": ["# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> The Edge\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> business.product_line.producer_s -> m.0p62thw\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> common.topic.image -> SG LPbody3\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson Les Paul -> music.guitar.brand -> Gibson Guitar Corporation\n# Answer:\nGibson Les Paul", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> music.guitar.guitarists -> Angus Young\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> common.topic.article -> m.01jydg\n# Answer:\nGibson SG", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJerry Garcia -> music.guitarist.guitars_played -> Gibson SG -> common.topic.notable_types -> Guitar\n# Answer:\nGibson SG"], "ground_truth": ["Fender Stratocaster", "Gibson Les Paul", "Gibson SG"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-181", "prediction": ["# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.11b60tqlwz\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60t8r6s\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_nominal_per_capita -> g.11b60gm1cf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.1245_1qhj\n# Answer:\nlocation.statistical_region.gdp_growth_rate"], "ground_truth": ["Thai baht"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1810", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.corresponding_type -> Visual Artist\n# Answer:\nSculptor", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.specialization_of -> Artist\n# Answer:\nSculptor", "# Reasoning Path:\nLeonardo da Vinci -> book.book_subject.works -> Leonardo Da Vinci -> common.topic.notable_for -> g.1255j1dx2\n# Answer:\nLeonardo Da Vinci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting"], "ground_truth": ["Drawing", "Sculpture", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1811", "prediction": ["# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.office_holder -> Cory Booker\n# Answer:\nCory Booker", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcsw0 -> government.government_position_held.office_holder -> Bob Menendez\n# Answer:\nBob Menendez", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.05kcjc9 -> government.government_position_held.office_holder -> Frank Lautenberg\n# Answer:\nFrank Lautenberg", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.07yrp98 -> government.government_position_held.office_holder -> Jon Corzine\n# Answer:\nJon Corzine", "# Reasoning Path:\nNew Jersey -> government.governmental_jurisdiction.governing_officials -> m.010f1q8l -> government.government_position_held.office_holder -> Kim Guadagno\n# Answer:\nKim Guadagno", "# Reasoning Path:\nNew Jersey -> government.political_district.representatives -> m.0y7nq48 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Cory Booker", "Bob Menendez"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1812", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1813", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\n2014 Argentina vs Slovenia friendly match", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.statistics -> m.0w9hr2y -> soccer.football_player_stats.team -> FC Barcelona B\n# Answer:\nFC Barcelona B", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.statistics -> m.0w8wsdx -> soccer.football_player_stats.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro"], "ground_truth": ["FC Barcelona", "Argentina national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.37037037037037035, "path_precision": 0.625, "path_recall": 0.2631578947368421, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1815", "prediction": ["# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zgr -> education.education.institution -> Lafayette Elementary School\n# Answer:\nLafayette Elementary School", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.institution -> University of Phoenix\n# Answer:\nUniversity of Phoenix", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zh9 -> education.education.institution -> Mcmain Magnet Secondary School\n# Answer:\nMcmain Magnet Secondary School", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zgy -> education.education.institution -> Eleanor McMain Secondary School\n# Answer:\nEleanor McMain Secondary School", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpj -> education.education.institution -> University of Houston\n# Answer:\nUniversity of Houston", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.major_field_of_study -> Psychology\n# Answer:\nPsychology", "# Reasoning Path:\nLil Wayne -> people.person.education -> m.0h2_zpr -> education.education.degree -> Bachelor of Science\n# Answer:\nBachelor of Science", "# Reasoning Path:\nLil Wayne -> music.artist.contribution -> m.0rg70rk -> music.recording_contribution.album -> BedRock\n# Answer:\nBedRock"], "ground_truth": ["Lafayette Elementary School", "University of Houston", "University of Phoenix", "Mcmain Magnet Secondary School", "Eleanor McMain Secondary School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1817", "prediction": ["# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bvz -> education.education.institution -> King Edward's School, Birmingham\n# Answer:\nKing Edward's School, Birmingham", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.02wmyw7 -> education.education.degree -> First Class Honours\n# Answer:\nFirst Class Honours", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0w48bx3 -> education.education.institution -> St. Philip's School\n# Answer:\nSt. Philip's School", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.employment_history -> m.02ht_8b -> business.employment_tenure.company -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nJ. R. R. Tolkien -> people.person.education -> m.0n1m7cd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date"], "ground_truth": ["University of Oxford", "St. Philip's School", "Exeter College, Oxford", "King Edward's School, Birmingham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1818", "prediction": ["# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> St. Louis -> location.location.containedby -> Missouri\n# Answer:\nSt. Louis", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> Missouri -> base.aareas.schema.administrative_area.capital -> Jefferson City\n# Answer:\nMissouri", "# Reasoning Path:\nSaint Louis University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nSaint Louis University -> education.educational_institution.subsidiary_or_constituent_schools -> Saint Louis University School of Public Health -> organization.organization.headquarters -> m.0wytzz_\n# Answer:\nSaint Louis University School of Public Health", "# Reasoning Path:\nSaint Louis University -> common.topic.article -> m.02tzb3\n# Answer:\ncommon.topic.article"], "ground_truth": ["Missouri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1819", "prediction": ["# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvl -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzv4 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> The Catholic Register -> book.periodical.subjects -> Catholicism\n# Answer:\nThe Catholic Register", "# Reasoning Path:\nCanada -> location.statistical_region.religions -> m.05spzvc -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.religion -> Protestantism\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.nationality -> United Kingdom\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> The Catholic Register -> common.topic.notable_types -> Newspaper\n# Answer:\nThe Catholic Register", "# Reasoning Path:\nCanada -> periodicals.newspaper_circulation_area.newspapers -> Anglican Journal -> book.periodical.subjects -> Anglicanism\n# Answer:\nAnglican Journal"], "ground_truth": ["Protestantism", "Catholicism", "Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-182", "prediction": ["# Reasoning Path:\nKhlo\u00e9 Kardashian -> celebrities.celebrity.sexual_relationships -> m.010bqp30 -> celebrities.romantic_relationship.celebrity -> French Montana\n# Answer:\nFrench Montana", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.spouse_s -> m.07y7d6c -> people.marriage.spouse -> Lamar Odom\n# Answer:\nLamar Odom", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> celebrities.celebrity.sexual_relationships -> m.010bqp30 -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> celebrities.celebrity.sexual_relationships -> m.010bqp30 -> freebase.valuenotation.has_value -> Start date\n# Answer:\nStart date", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.spouse_s -> m.07y7d6c -> people.marriage.location_of_ceremony -> Beverly Hills\n# Answer:\nBeverly Hills", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.children -> Rob Kardashian\n# Answer:\nRobert Kardashian", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> celebrities.celebrity.sexual_relationships -> m.010bqp30 -> freebase.valuenotation.has_no_value -> End date\n# Answer:\nEnd date", "# Reasoning Path:\nKhlo\u00e9 Kardashian -> people.person.parents -> Robert Kardashian -> people.person.gender -> Male\n# Answer:\nRobert Kardashian"], "ground_truth": ["Lamar Odom"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1821", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.location.containedby -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Unitary state\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.languages_spoken -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> base.locations.countries.continent -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Presidential system\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Argentina -> location.country.languages_spoken -> Italian Language\n# Answer:\nArgentina", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Venezuela -> location.location.containedby -> South America\n# Answer:\nVenezuela"], "ground_truth": ["Honduras", "Cuba", "Venezuela", "Colombia", "Chile", "Argentina", "Equatorial Guinea", "Bolivia", "Nicaragua", "Costa Rica", "El Salvador", "Dominican Republic", "Ecuador", "Puerto Rico", "Peru", "Guatemala", "Spain", "Uruguay", "Panama", "Paraguay"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2608695652173913, "ans_precission": 1.0, "ans_recall": 0.15, "path_f1": 0.2608695652173913, "path_precision": 1.0, "path_recall": 0.15, "path_ans_f1": 0.2608695652173913, "path_ans_precision": 1.0, "path_ans_recall": 0.15}
{"id": "WebQTest-1822", "prediction": ["# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.person.nationality -> United States of America\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.person.gender -> Male\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Paul Murray Ryan -> people.deceased_person.place_of_death -> Janesville\n# Answer:\nPaul Murray Ryan", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> people.person.gender -> Female\n# Answer:\nElizabeth A. Ryan", "# Reasoning Path:\nPaul Ryan -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPaul Ryan -> people.person.places_lived -> m.03pt7rq -> people.place_lived.location -> Janesville\n# Answer:\nJanesville", "# Reasoning Path:\nPaul Ryan -> people.person.parents -> Elizabeth A. Ryan -> common.topic.notable_for -> g.126tl4m_x\n# Answer:\nElizabeth A. Ryan"], "ground_truth": ["Samuel Ryan", "Elizabeth Ryan", "Charles Ryan", "Elizabeth A. Ryan", "Paul Murray Ryan", "Janna Ryan"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.34285714285714286, "path_precision": 0.75, "path_recall": 0.2222222222222222, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-1823", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraqi Kurdistan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Jordan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> book.book_subject.works -> A reed shaken by the wind -> book.written_work.subjects -> Middle East\n# Answer:\nA reed shaken by the wind", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.statistical_region.consumer_price_index -> g.11b6c_z3xh\n# Answer:\nlocation.statistical_region.consumer_price_index"], "ground_truth": ["Republic", "Parliamentary republic", "Parliamentary system", "Federal republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1824", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Portugal\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Germany\n# Answer:\nEuro"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1825", "prediction": ["# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_types -> Book\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.book.editions -> The shame of the cities\n# Answer:\nThe Shame of the Cities", "# Reasoning Path:\nLincoln Steffens -> common.topic.notable_for -> g.1255gcgx2\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The autobiography of Lincoln Steffens -> common.topic.notable_for -> g.1255m88ck\n# Answer:\nThe autobiography of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The world of Lincoln Steffens -> common.topic.notable_types -> Book\n# Answer:\nThe world of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The world of Lincoln Steffens -> common.topic.notable_for -> g.1258skx01\n# Answer:\nThe world of Lincoln Steffens", "# Reasoning Path:\nLincoln Steffens -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nLincoln Steffens -> people.deceased_person.place_of_death -> Carmel-by-the-Sea -> location.location.containedby -> United States of America\n# Answer:\nCarmel-by-the-Sea", "# Reasoning Path:\nLincoln Steffens -> book.author.works_written -> The Shame of the Cities -> book.written_work.original_language -> English Language\n# Answer:\nThe Shame of the Cities"], "ground_truth": ["The autobiography of Lincoln Steffens", "Moses in red", "The world of Lincoln Steffens", "John Reed", "The Shame of the Cities", "The struggle for self-government", "Into Mexico and out", "Boy on horseback", "Die Geschichte meines Lebens", "The Least Of These", "Upbuilders", "Lincoln Steffens speaking", "The Old Jim Horse"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.3428571428571429, "ans_precission": 0.6666666666666666, "ans_recall": 0.23076923076923078, "path_f1": 0.3428571428571429, "path_precision": 0.6666666666666666, "path_recall": 0.23076923076923078, "path_ans_f1": 0.3428571428571429, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-1826", "prediction": ["# Reasoning Path:\nAnders Celsius -> people.person.education -> m.02wp1_z -> education.education.institution -> Uppsala University\n# Answer:\nUppsala University", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.article -> m.04v_v4\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> base.kwebbase.kwtopic.has_sentences -> Celsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer. -> base.kwebbase.kwsentence.previous_sentence -> The Celsius temperature scale, also called the Centigrade temperature scale because of the 100-degree interval between the defined points, is now based on 0\u00b0 for the freezing point of water and 100\u00b0 for the boiling point of water.\n# Answer:\nCelsius read his famous paper on his thermometer: \\\"Observations  Concerning the Two Constant Degrees on a Thermometer\\\"  to the Swedish  Academy of Sciences in 1742, and it was long known as the \\\"Swedish  thermometer\\\", only around 1800 becoming known as the Celsius thermometer.", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> astronomy.extraterrestrial_location.type_of_planetographic_feature -> Impact crater\n# Answer:\nCelsius", "# Reasoning Path:\nAnders Celsius -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Degree Celsius -> common.topic.article -> m.0216d\n# Answer:\nDegree Celsius", "# Reasoning Path:\nAnders Celsius -> symbols.name_source.namesakes -> Celsius -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCelsius"], "ground_truth": ["Uppsala University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1827", "prediction": ["# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.structure.architectural_style -> Modern architecture\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nMuseum of Modern Art", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.building.building_function -> Museum\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Frick Collection -> travel.tourist_attraction.near_travel_destination -> New York\n# Answer:\nFrick Collection", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> location.location.containedby -> United States of America\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Solomon R. Guggenheim Museum -> architecture.museum.type_of_museum -> Art Gallery\n# Answer:\nSolomon R. Guggenheim Museum", "# Reasoning Path:\nNew York City -> travel.travel_destination.tourist_attractions -> Museum of Modern Art -> architecture.building.building_function -> Museum\n# Answer:\nMuseum of Modern Art"], "ground_truth": ["American Museum of Natural History", "Crocheron Park", "Empire State Building", "Museum of Modern Art Department of Film", "Museum of Modern Art", "Japan Society of New York", "National Academy Museum and School", "New York Public Library for the Performing Arts", "Museum of Sex", "Chelsea Art Museum", "Madison Square Garden", "Grand Central Terminal", "Imagination Playground at Burling Slip", "Freedomland U.S.A.", "Flatiron Building", "St. Patrick's Cathedral", "Chinatown", "New York Aquarium", "United Nations Headquarters", "Felix M. Warburg House", "Wave Hill", "Statue of Liberty", "Gavin Brown's Enterprise", "Staten Island Ferry", "Metropolitan Museum of Art", "Peking", "Times Square", "Travefy", "Solomon R. Guggenheim Museum", "High Line", "Museum of Arts and Design", "Rockefeller Center", "UAE Healthy Kidney 10K", "Henry Clay Frick House", "Darien Lake", "Central Park", "Little Italy", "The Cloisters", "Headless Horseman Hayrides", "Battery Park", "Theodore Roosevelt Birthplace National Historic Site", "FusionArts Museum", "Brooklyn Bridge", "Chrysler Building", "Morgan Library & Museum", "Brooklyn Botanic Garden", "Louis Armstrong House", "New York Mini 10K", "New York City Half Marathon", "Tesla Science Center at Wardenclyffe", "Andrew Carnegie Mansion", "Frick Collection", "Broadway Theatre", "Central Park Zoo", "Franklin D. Roosevelt Presidential Library and Museum", "International Center of Photography", "New York International Fringe Festival", "American Folk Art Museum", "Museum of Mathematics", "Statue of Liberty National Monument", "George Gustav Heye Center", "A.I.R. Gallery"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09230769230769231, "ans_precission": 1.0, "ans_recall": 0.04838709677419355, "path_f1": 0.11940298507462686, "path_precision": 1.0, "path_recall": 0.06349206349206349, "path_ans_f1": 0.09230769230769231, "path_ans_precision": 1.0, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-1828", "prediction": ["# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> French Polynesia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> French Polynesia\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Andorra\n# Answer:\nFrench franc", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> New Caledonia\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.currency_used -> CFP franc -> finance.currency.countries_used -> Wallis and Futuna\n# Answer:\nCFP franc", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Spain\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nFrance -> location.country.currency_formerly_used -> French franc -> finance.currency.countries_formerly_used -> Monaco\n# Answer:\nFrench franc"], "ground_truth": ["Euro", "CFP franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1829", "prediction": ["# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404v_ -> location.religion_percentage.religion -> Jehovah's Witnesses\n# Answer:\nJehovah's Witnesses", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04405bz -> location.religion_percentage.religion -> Christianity\n# Answer:\nChristianity", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04405j8 -> location.religion_percentage.religion -> Buddhism\n# Answer:\nBuddhism", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04404p3 -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nMassachusetts -> location.statistical_region.religions -> m.04403kn -> location.religion_percentage.religion -> Churches of Christ\n# Answer:\nChurches of Christ", "# Reasoning Path:\nMassachusetts -> location.administrative_division.capital -> m.0jvw4t4 -> location.administrative_division_capital_relationship.capital -> Boston\n# Answer:\nBoston", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> USS Massachusetts (BB-59) -> location.location.geolocation -> m.06_4sm9\n# Answer:\nUSS Massachusetts (BB-59)", "# Reasoning Path:\nMassachusetts -> symbols.name_source.namesakes -> 4547 Massachusetts -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\n4547 Massachusetts"], "ground_truth": ["Jehovah's Witnesses", "Christianity", "Presbyterianism", "Pentecostalism", "Episcopal Church", "Judaism", "Lutheranism", "Methodism", "Baptists", "Buddhism", "United Church of Christ", "Catholicism", "Churches of Christ", "Protestantism"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.45454545454545453, "ans_precission": 0.625, "ans_recall": 0.35714285714285715, "path_f1": 0.45454545454545453, "path_precision": 0.625, "path_recall": 0.35714285714285715, "path_ans_f1": 0.45454545454545453, "path_ans_precision": 0.625, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-1830", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Kogi Language -> language.human_language.language_family -> Chibchan languages\n# Answer:\nKogi Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language"], "ground_truth": ["Wayuu Language", "Macuna Language", "P\u00e1ez language", "Piapoco Language", "Playero language", "Siona Language", "Inga, Jungle Language", "Piaroa Language", "Tunebo, Angosturas Language", "Achawa language", "Tucano Language", "Providencia Sign Language", "Omejes Language", "Waimaj\u00e3 Language", "Koreguaje Language", "Cof\u00e1n Language", "Macagu\u00e1n Language", "Cumeral Language", "Guahibo language", "Guambiano Language", "Cams\u00e1 Language", "Minica Huitoto", "Hupd\u00eb Language", "Carijona Language", "Baudo language", "Arhuaco Language", "Cuiba language", "Nukak language", "Piratapuyo Language", "Cabiyar\u00ed Language", "Puinave Language", "Spanish Language", "Awa-Cuaiquer Language", "Totoro Language", "Cagua Language", "Andoque Language", "Uwa language", "Romani, Vlax Language", "Siriano Language", "Tunebo, Barro Negro Language", "Murui Huitoto language", "Coxima Language", "Bora Language", "Curripaco Language", "Ponares Language", "Tanimuca-Retuar\u00e3 Language", "Ticuna language", "Runa Language", "Islander Creole English", "Kuna, Border Language", "Tunebo, Central Language", "Tuyuca language", "Palenquero Language", "Carabayo Language", "Bar\u00ed Language", "Natagaimas Language", "Ember\u00e1, Northern Language", "Tunebo, Western Language", "Tomedes Language", "Tinigua language", "Yucuna Language", "Macaguaje Language", "Inga Language", "Yukpa Language", "Andaqui Language", "Tama Language", "Barasana Language", "Ocaina Language", "Quechua, Napo Lowland Language", "Kogi Language", "Catio language", "Anserma Language", "Cocama language", "Cubeo Language", "Nonuya language", "Malayo Language", "Guanano Language", "Desano Language", "Nheengatu language", "Coyaima Language", "S\u00e1liba Language", "Chipiajes Language", "Pijao Language", "Muinane Language", "Guayabero Language", "Colombian Sign Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04582651391162029, "path_precision": 0.875, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-1833", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1835", "prediction": ["# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.containedby -> Czech Republic\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.containedby -> Czech Republic\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.contains -> AZ Tower\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> common.topic.article -> m.01h10g\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.containedby -> Brno-City District\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.deceased_person.place_of_death -> Brno -> location.location.events -> 1993 Women's European Volleyball Championship\n# Answer:\nBrno", "# Reasoning Path:\nGregor Mendel -> people.person.place_of_birth -> Hyn\u010dice -> location.location.geolocation -> m.0wmmvdg\n# Answer:\nHyn\u010dice", "# Reasoning Path:\nGregor Mendel -> people.person.nationality -> Austria-Hungary -> location.location.containedby -> Europe\n# Answer:\nAustria-Hungary"], "ground_truth": ["Austrian Silesia", "Brno"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-1836", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> people.place_lived.location -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjyl -> people.place_lived.location -> De Smet\n# Answer:\nDe Smet", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_gghr5 -> people.place_lived.location -> Burr Oak\n# Answer:\nBurr Oak", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> End date\n# Answer:\nEnd date", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_gghtv -> people.place_lived.location -> Mansfield\n# Answer:\nMansfield", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghk_x -> people.place_lived.location -> Walnut Grove\n# Answer:\nWalnut Grove", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> United States of America\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> people.person.places_lived -> m.0_ghjwl -> freebase.valuenotation.has_value -> Start date\n# Answer:\nStart date"], "ground_truth": ["Kansas", "Mansfield", "Burr Oak", "Walnut Grove", "De Smet", "Pepin"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.625, "ans_recall": 0.8333333333333334, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.625, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-1838", "prediction": ["# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Alba Berlin -> sports.sports_team.location -> Berlin\n# Answer:\nAlba Berlin", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany men's national volleyball team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany men's national volleyball team", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany men's national water polo team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany men's national water polo team", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> sports.tournament_team.tournaments_competed_in -> m.09t9_3v\n# Answer:\nGermany national handball team", "# Reasoning Path:\nGermany -> sports.sports_team_location.teams -> Germany national handball team -> common.topic.notable_types -> Sports Team\n# Answer:\nGermany national handball team"], "ground_truth": ["Bayer Giants Leverkusen", "German National Ice Hockey Team", "Germany women's national handball team", "Germany women's national field hockey team", "Germany national American football team", "Germany national handball team", "Brose Baskets", "EWE Baskets Oldenburg", "Germany national basketball team", "Skyliners Frankfurt", "Germany men's national inline hockey team", "Germany women's national volleyball team", "QTSV Quackenbr\u00fcck", "Germany men's national field hockey team", "Germany national baseball team", "Germany national korfball team", "Germany U20 women's volleyball team", "Germany men's national volleyball team", "Germany women's national basketball team", "Germany national badminton team", "Germany national rugby league team", "Germany national rugby union team", "Germany women's national football team", "FC Bayern Munich", "Germany national speedway team", "Germany women's national beach handball team", "Germany men's national water polo team", "Telekom Baskets Bonn", "Germany national beach handball team", "Germany Davis Cup team", "Germany women's national rugby union team", "Germany national football team", "Germany women's national under-18 ice hockey team", "Germany men's national pitch and putt team", "BG G\u00f6ttingen", "Germany Fed Cup team", "Germany women's national ice hockey team", "K\u00f6ln 99ers", "Alba Berlin", "Germany national cricket team", "Germany national beach soccer team"], "ans_acc": 0.0975609756097561, "ans_hit": 1, "ans_f1": 0.16877637130801687, "ans_precission": 0.625, "ans_recall": 0.0975609756097561, "path_f1": 0.1652892561983471, "path_precision": 0.625, "path_recall": 0.09523809523809523, "path_ans_f1": 0.16877637130801687, "path_ans_precision": 0.625, "path_ans_recall": 0.0975609756097561}
{"id": "WebQTest-1839", "prediction": ["# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nOklahoma -> government.governmental_jurisdiction.governing_officials -> m.0100vzfb -> government.government_position_held.office_holder -> John R. Bennett\n# Answer:\nJohn R. Bennett", "# Reasoning Path:\nOklahoma -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Ozark Howler -> base.mystery.cryptid.area_of_occurrence -> Texas\n# Answer:\nOzark Howler"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-184", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.places_lived -> m.03pvr9m -> people.place_lived.location -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.jurisdiction_of_office -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Senator\n# Answer:\nUnited States of America", "# Reasoning Path:\nMitt Romney -> government.politician.government_positions_held -> m.04stpgl -> government.government_position_held.office_position_or_title -> Governor of Massachusetts\n# Answer:\nGovernor of Massachusetts"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1840", "prediction": ["# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> common.topic.image -> Norwegianmalforms\n# Answer:\nNynorsk", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.image -> Norwegianmalforms\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> common.topic.notable_types -> Human Language\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.statistical_region.net_migration -> g.1q5jhf535\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> language.human_language.countries_spoken_in -> Finland\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.country.languages_spoken -> Saami, North Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSaami, North Language", "# Reasoning Path:\nNorway -> location.statistical_region.gdp_nominal_per_capita -> g.11b60vk8pf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nNorway -> location.country.official_language -> Norwegian Language -> language.human_language.writing_system -> Danish and Norwegian alphabet\n# Answer:\nNorwegian Language", "# Reasoning Path:\nNorway -> location.country.official_language -> Nynorsk -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nNynorsk"], "ground_truth": ["Saami, North Language", "Nynorsk", "Norwegian Language", "Saami, South Language", "Bokm\u00e5l", "Finnish, Kven Language", "Saami, Lule Language"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.8, "ans_recall": 0.42857142857142855, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.8, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1842", "prediction": ["# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.article -> m.02f32r\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.11b71rkqjy\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> finance.currency.target_of_exchange -> m.03x91lj\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.notable_types -> Currency\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.webpage -> m.04m19t9\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12cp_j7mk\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2y\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nSingapore -> location.statistical_region.deposit_interest_rate -> g.12tb6f6tz\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSingapore -> location.country.currency_used -> Singapore dollar -> common.topic.webpage -> m.04m19tg\n# Answer:\nSingapore dollar", "# Reasoning Path:\nSingapore -> olympics.olympic_participating_country.athletes -> m.0k8qc9m -> olympics.olympic_athlete_affiliation.sport -> Sailing\n# Answer:\nSailing"], "ground_truth": ["Singapore dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1843", "prediction": ["# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> language.human_language.main_country -> Canada\n# Answer:\nBlackfoot Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> South Africa\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> common.topic.notable_for -> g.125bpgfw7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> English Language -> language.human_language.dialects -> Scottish English\n# Answer:\nEnglish Language", "# Reasoning Path:\nBlackfoot Confederacy -> people.ethnicity.languages_spoken -> Blackfoot Language -> common.topic.image -> Traditional territory in green, reserves and reservations in orange.\n# Answer:\nBlackfoot Language"], "ground_truth": ["English Language", "Blackfoot Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1844", "prediction": ["# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.0k2m57 -> film.performance.actor -> Alan Rickman\n# Answer:\nAlan Rickman", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> people.person.gender -> Male\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> tv.tv_producer.programs_produced -> m.0gj0gz_\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.starring -> m.0k2m57 -> film.performance.character -> The Sheriff of Nottingham\n# Answer:\nThe Sheriff of Nottingham", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> film.producer.film -> Moll Flanders\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> tv.tv_producer.tv_episodes_produced -> m.0zh513n\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> film.producer.film -> August Rush\n# Answer:\nRichard B. Lewis", "# Reasoning Path:\nRobin Hood: Prince of Thieves -> film.film.produced_by -> Richard B. Lewis -> tv.tv_producer.programs_produced -> m.0z03fvz\n# Answer:\nRichard B. Lewis"], "ground_truth": ["Kevin Costner"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1845", "prediction": ["# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Mountain Time Zone -> common.topic.article -> m.02hczk\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nTexas -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1847", "prediction": ["# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Turkey\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Iraq\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Alazani\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partiallycontains -> m.010r1xzh\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.ontologies.ontology_instance.equivalent_instances -> m.098lxkm\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> location.location.partially_contains -> Aragvi River\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.geographic_distribution -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nKurds -> people.ethnicity.people -> Abdullah Cevdet -> people.person.nationality -> Turkey\n# Answer:\nAbdullah Cevdet"], "ground_truth": ["Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1848", "prediction": ["# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.family -> Brass instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> music.instrument.family -> Brass instrument\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> common.topic.notable_types -> Musical instrument\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> W. C. Handy\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.subjects -> Trumpeter\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Cornet -> music.instrument.instrumentalists -> Bix Beiderbecke\n# Answer:\nCornet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> common.topic.notable_types -> Book\n# Answer:\nTrumpet", "# Reasoning Path:\nLouis Armstrong -> music.group_member.instruments_played -> Trumpet -> book.written_work.original_language -> English Language\n# Answer:\nTrumpet"], "ground_truth": ["Cornet", "Trumpet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1849", "prediction": ["# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0y9pvhm -> film.performance.character -> Back Up Dancer\n# Answer:\nBack Up Dancer", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0y9pvhm -> film.performance.film -> A Cinderella Story: Once Upon a Song\n# Answer:\nA Cinderella Story: Once Upon a Song", "# Reasoning Path:\nCamp Rock -> film.film.starring -> m.0h5rcmk -> film.performance.character -> Brown Cesario\n# Answer:\nBrown Cesario", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0jzt3lm -> film.performance.character -> Holly Hobbie\n# Answer:\nHolly Hobbie", "# Reasoning Path:\nCamp Rock -> film.film.cinematography -> Dean Cundey -> film.actor.film -> m.0y54l69\n# Answer:\nDean Cundey", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.010r560c -> film.performance.character -> Camille Gage\n# Answer:\nCamille Gage", "# Reasoning Path:\nAlyson Stoner -> film.actor.film -> m.0y9pvhm -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nCamp Rock -> media_common.netflix_title.netflix_genres -> The Walt Disney Company -> organization.organization.founders -> Roy O. Disney\n# Answer:\nThe Walt Disney Company"], "ground_truth": ["Caitlyn Geller"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-185", "prediction": ["# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Martinique\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nFrance -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Martinique\n# Answer:\nConstitutional republic", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71tyl21\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Semi-presidential system", "Constitutional republic", "Unitary state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1850", "prediction": ["# Reasoning Path:\nMadonna -> people.person.religion -> Jewish Renewal\n# Answer:\nJewish Renewal", "# Reasoning Path:\nMadonna -> people.person.religion -> Catholicism -> media_common.quotation_subject.quotations_about_this_subject -> Catholicism is not a soothing religion. It's a painful religion. We're all gluttons for punishment.\n# Answer:\nCatholicism", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064_jxy -> base.popstra.dated.participant -> Chris Rock\n# Answer:\nChris Rock", "# Reasoning Path:\nMadonna -> celebrities.celebrity.sexual_relationships -> m.030qm3x -> celebrities.romantic_relationship.celebrity -> Warren Beatty\n# Answer:\nWarren Beatty", "# Reasoning Path:\nMadonna -> celebrities.celebrity.sexual_relationships -> m.04c__4z -> celebrities.romantic_relationship.celebrity -> Jean-Michel Basquiat\n# Answer:\nJean-Michel Basquiat", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> common.topic.image -> x.jpg\n# Answer:\n.977 The '80s Channel", "# Reasoning Path:\nMadonna -> celebrities.celebrity.sexual_relationships -> m.04c__56 -> celebrities.romantic_relationship.celebrity -> Mark Kamins\n# Answer:\nMark Kamins", "# Reasoning Path:\nMadonna -> base.popstra.celebrity.dated -> m.064f3ln -> base.popstra.dated.participant -> Dennis Rodman\n# Answer:\nDennis Rodman", "# Reasoning Path:\nMadonna -> broadcast.artist.content -> .977 The '80s Channel -> broadcast.content.producer -> 977 Music\n# Answer:\n.977 The '80s Channel"], "ground_truth": ["Jewish Renewal", "Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1851", "prediction": ["# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Arab world\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.containedby -> Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.contains -> Suez Governorate\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.events -> Middle East Theatre of World War II\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.location.adjoin_s -> m.04_7vd8 -> location.adjoining_relationship.adjoins -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> North Africa -> location.location.partially_contains -> Middle East\n# Answer:\nNorth Africa", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.partially_contains -> Arab world\n# Answer:\nWestern Asia", "# Reasoning Path:\nEgypt -> location.location.partially_containedby -> Western Asia -> location.location.contains -> Israel\n# Answer:\nWestern Asia"], "ground_truth": ["Israel", "Libya", "Sudan"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.125, "ans_recall": 0.3333333333333333, "path_f1": 0.08, "path_precision": 0.25, "path_recall": 0.047619047619047616, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1853", "prediction": ["# Reasoning Path:\nRichard Hammond -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nRichard Hammond -> tv.tv_personality.tv_regular_appearances -> g.11bv36x_sm\n# Answer:\ntv.tv_personality.tv_regular_appearances", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Toby Isaacs\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Harmony Kendall\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> base.descriptive_names.names.descriptive_name -> m.010543ty\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> people.profession.specialization_of -> Broadcaster\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> common.topic.notable_for -> g.125bfnthb\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Jackie Burkhart\n# Answer:\nPresenter", "# Reasoning Path:\nRichard Hammond -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor"], "ground_truth": ["Journalist", "Screenwriter", "Voice Actor", "Author", "Writer", "Presenter", "Stunt Performer", "Television producer", "Radio personality"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3555555555555555, "ans_precission": 0.8888888888888888, "ans_recall": 0.2222222222222222, "path_f1": 0.3555555555555555, "path_precision": 0.8888888888888888, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3555555555555555, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-1854", "prediction": ["# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_episodes -> g.11byb56_yy\n# Answer:\ntv.tv_character.appeared_in_tv_episodes", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0h28yx8 -> film.performance.actor -> Alex Borstein\n# Answer:\nAlex Borstein", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nLois Griffin -> film.film_character.portrayed_in_films -> m.0k3ssk -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLois Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntr0s -> tv.regular_tv_appearance.seasons -> Family Guy - Season 1\n# Answer:\nFamily Guy - Season 1"], "ground_truth": ["Alex Borstein"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.46153846153846156, "path_precision": 0.3333333333333333, "path_recall": 0.75, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-1855", "prediction": ["# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> travel.transport_terminus.travel_destinations_served -> m.051qtw8\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.hub_for -> Air France\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> film.film_location.featured_in_films -> La Jet\u00e9e\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> aviation.airport.serves -> Essonne\n# Answer:\nParis Orly Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.hub_for -> Vueling\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Charles de Gaulle Airport -> aviation.airport.serves -> Tour Triangle\n# Answer:\nCharles de Gaulle Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Beauvais\u2013Till\u00e9 Airport -> aviation.airport.serves -> Beauvais\n# Answer:\nBeauvais\u2013Till\u00e9 Airport", "# Reasoning Path:\nParis -> location.location.nearby_airports -> Paris Orly Airport -> aviation.airport.hub_for -> Air France\n# Answer:\nParis Orly Airport"], "ground_truth": ["Paris Orly Airport", "Charles de Gaulle Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1857", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.album -> g.121_5zdm\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> music.album.genre -> Pop rock\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> common.topic.article -> m.0nfvxvd\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.album -> Towards the Sun -> music.composition.composer -> Gary Go\n# Answer:\nTowards the Sun", "# Reasoning Path:\nRihanna -> music.artist.album -> Unapologetic -> music.album.release_type -> Album\n# Answer:\nUnapologetic", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.album -> Loud Tour Live at the O2 -> music.album.genre -> Contemporary R&B\n# Answer:\nLoud Tour Live at the O2", "# Reasoning Path:\nRihanna -> music.artist.album -> Towards the Sun -> music.album.album_content_type -> Soundtrack\n# Answer:\nTowards the Sun"], "ground_truth": ["Cockiness (Love It) Remix", "Loud Tour Live at the O2", "g.121n8wcw", "Diamonds", "Rockstar 101: The Remixes", "Talk That Talk", "Diamonds (remix)", "Jump (Club Cheval Rap Remix)", "Towards the Sun", "Unapologetic", "Devil in a Blue Dress", "Birthday Cake", "g.121xjjw_", "g.121_5zdm"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.3157894736842105, "ans_precission": 0.6, "ans_recall": 0.21428571428571427, "path_f1": 0.4274809160305344, "path_precision": 0.7, "path_recall": 0.3076923076923077, "path_ans_f1": 0.40579710144927533, "path_ans_precision": 0.7, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1858", "prediction": ["# Reasoning Path:\nDave Ramsey -> people.person.places_lived -> m.03phzfq -> people.place_lived.location -> Nashville\n# Answer:\nNashville", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Casey Atwood\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> common.topic.article -> m.03gf9f\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Jim Turner\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.citytown.postal_codes -> 37011\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.geolocation -> m.0kgzpb\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.containedby -> Tennessee\n# Answer:\nAntioch", "# Reasoning Path:\nDave Ramsey -> people.person.place_of_birth -> Antioch -> location.location.people_born_here -> Keenan Reynolds\n# Answer:\nAntioch"], "ground_truth": ["Nashville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-186", "prediction": ["# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> royalty.monarch.royal_line -> House of Gl\u00fccksburg\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> people.family_member.family -> House of Gl\u00fccksburg\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> people.person.parents -> Frederick IX of Denmark\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> government.governmental_jurisdiction.governing_officials -> m.0_g55l8 -> government.government_position_held.appointed_by -> Helle Thorning-Schmidt\n# Answer:\nHelle Thorning-Schmidt", "# Reasoning Path:\nDenmark -> periodicals.newspaper_circulation_area.newspapers -> Dagbladet B\u00f8rsen -> book.newspaper.circulation_areas -> Copenhagen\n# Answer:\nDagbladet B\u00f8rsen", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Margrethe II of Denmark -> government.political_appointer.appointees -> m.0h5z38_\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nDenmark -> royalty.kingdom.rulers -> Christian III of Denmark -> common.topic.notable_types -> Monarch\n# Answer:\nChristian III of Denmark", "# Reasoning Path:\nDenmark -> government.governmental_jurisdiction.governing_officials -> m.0h5z38_ -> government.government_position_held.office_holder -> Helle Thorning-Schmidt\n# Answer:\nHelle Thorning-Schmidt"], "ground_truth": ["Margrethe II of Denmark", "Margrethe Vestager", "Helle Thorning-Schmidt"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666665, "path_precision": 0.75, "path_recall": 0.6, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1860", "prediction": ["# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> common.topic.image -> True-color image taken by the Galileo probe.\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.image -> Ganymede g1 true 2\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> common.topic.notable_types -> Moon\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Io -> astronomy.celestial_object.locations -> Volcanology of Io\n# Answer:\nIo", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.notable_types -> Moon\n# Answer:\nGanymede", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Callisto -> common.topic.image -> Callisto from space.  In \u201cThe Callistan Menace\u201d Callisto has an atmosphere and native life forms\n# Answer:\nCallisto", "# Reasoning Path:\nGalileo Galilei -> astronomy.astronomer.astronomical_objects_discovered -> Ganymede -> common.topic.image -> Galilean moon Laplace resonance animation\n# Answer:\nGanymede"], "ground_truth": ["Europa", "Io", "Callisto", "Ganymede"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-1861", "prediction": ["# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60xj9xc\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Yemen\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> government.form_of_government.countries -> United Arab Emirates\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Absolute monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nAbsolute monarchy", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.net_migration -> g.1q5jfl07n\n# Answer:\nlocation.statistical_region.net_migration", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Brunei\n# Answer:\nUnitary state", "# Reasoning Path:\nSaudi Arabia -> location.country.form_of_government -> Islamic state -> common.topic.notable_types -> Form of Government\n# Answer:\nIslamic state", "# Reasoning Path:\nSaudi Arabia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_6n2_\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Islamic state", "Unitary state", "Absolute monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1864", "prediction": ["# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.09l2mf7 -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nMichael Weatherly", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.actor -> Michael Weatherly\n# Answer:\nMichael Weatherly", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_for -> g.125g474d5\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.species -> Homo sapiens -> fictional_universe.character_species.found_in_fictional_universe -> Anitaverse\n# Answer:\nHomo sapiens", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> fictional_universe.fictional_character.occupation -> Special Agent -> common.topic.article -> m.02lml8\n# Answer:\nSpecial Agent", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> tv.regular_tv_appearance.seasons -> NCIS - Season 9\n# Answer:\nNCIS - Season 9", "# Reasoning Path:\nSpecial Agent Anthony DiNozzo -> tv.tv_character.appeared_in_tv_program -> m.03jrblb -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Michael Weatherly"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1865", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.specialization_of -> Religious Leader\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> fictional_universe.character_occupation.characters_with_this_occupation -> Joseph\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> people.profession.specialization_of -> Spiritual leader\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> base.descriptive_names.names.descriptive_name -> m.01064z57\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> book.book_subject.works -> Prophets of doom\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> fictional_universe.character_occupation.characters_with_this_occupation -> Jesus - The Christ\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Carpentry -> common.topic.notable_types -> Profession\n# Answer:\nCarpentry", "# Reasoning Path:\nJesus Christ -> people.person.profession -> Prophet -> fictional_universe.character_occupation.characters_with_this_occupation -> Tiresias\n# Answer:\nProphet", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.1255sm1pr\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Carpentry", "Prophet", "Preacher"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7272727272727272, "ans_precission": 0.8, "ans_recall": 0.6666666666666666, "path_f1": 0.7272727272727272, "path_precision": 0.8, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7272727272727272, "path_ans_precision": 0.8, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1866", "prediction": ["# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNew York"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1868", "prediction": ["# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.actor -> Josh Charles\n# Answer:\nJosh Charles", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_for -> g.125fszykz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKnox Overstreet -> film.film_character.portrayed_in_films -> m.0jx21d -> film.performance.film -> Dead Poets Society\n# Answer:\nDead Poets Society", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character using Wham-O product\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> common.topic.article -> m.02_96_z\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> type.type.expected_by -> Killed by\n# Answer:\nFilm character", "# Reasoning Path:\nKnox Overstreet -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Annotation\n# Answer:\nFilm character"], "ground_truth": ["Josh Charles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-187", "prediction": ["# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.major_field_of_study -> Physics\n# Answer:\nPhysics", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.institution -> Trinity Hall, Cambridge\n# Answer:\nTrinity Hall, Cambridge", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> education.education.institution -> St Albans School\n# Answer:\nSt Albans School", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.02kq1zs -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_y -> education.education.institution -> St Albans High School for Girls\n# Answer:\nSt Albans High School for Girls", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.03p8dsf -> education.education.degree -> Doctorate\n# Answer:\nDoctorate", "# Reasoning Path:\nStephen Hawking -> people.person.education -> m.05svw_r -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date"], "ground_truth": ["Physics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.125, "path_recall": 0.16666666666666666, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1870", "prediction": ["# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Vermont\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> film.person_or_entity_appearing_in_film.films -> m.0_xwg9m -> film.personal_film_appearance.film -> Jonas in the Jungle\n# Answer:\nJonas in the Jungle", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nDavid Carradine -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["David Carradine's AM & PM Tai Chi Workout for Beginners", "Oceans of Fire", "Americana", "I Saw What You Did", "Hair High", "UnConventional", "Fuego", "Last Stand at Saber River", "Future Force", "On the Line", "Bala Perdida", "Bird on a Wire", "Double Trouble", "Taggart", "Midnight Fear", "Archie's Final Project", "The Long Riders", "Evil Toons", "Lone Wolf McQuade", "Code Name Jaguar", "Night of the Templar", "Six Days in Paradise", "Big Stan", "Last Goodbye", "Trick or Treats", "Kung Fu Killer", "Being Michael Madsen", "Behind Enemy Lines", "Capital Punishment", "Wizards of the Lost Kingdom 2", "Fall Down Dead", "The Rage", "Thunder and Lightning", "Last Hour", "Homo Erectus", "Death Race 2000", "Final Move", "Kill Zone", "Safari 3000", "Cybercity", "Camille", "Crank: High Voltage", "Roadside Prophets", "The Golden Boys", "Money to Burn", "Road of No Return", "The Misfit Brigade", "Permanent Vacation", "Kill Bill: The Whole Bloody Affair", "Guaranteed on Delivery", "Brothers in Arms", "Try This One for Size", "An American Tail: The Treasure of Manhattan Island", "Blackout", "Richard III", "Six Against the Rock", "Tropical Snow", "Martial Law", "Sundown: The Vampire in Retreat", "Kill Bill Volume 1", "The Outsider", "Dune Warriors", "Out of the Wilderness", "The Warrior and the Sorceress", "The Last Sect", "Boxcar Bertha", "Deathsport", "Bad Cop", "Children of the Corn V: Fields of Terror", "Gray Lady Down", "Warlords", "Hell Ride", "Crime Zone", "Dangerous Curves", "Detention", "Kill Bill Volume 2", "Q", "Absolute Evil", "Animal Instincts", "Armed Response", "Future Zone", "Max Havoc: Curse of the Dragon", "Night Rhythms", "All Hell Broke Loose", "Bound for Glory", "Balto II: Wolf Quest", "Dead & Breakfast", "Kandisha", "Autumn", "The Good Guys and the Bad Guys", "High Noon, Part II: The Return of Will Kane", "How to Rob a Bank", "Treasure Raiders", "David Carradine's Shaolin Cardio Kick Boxing Workout", "Project Eliminator", "The Serpent's Egg", "Maybe I'll Come Home in the Spring", "Circle of Iron", "Eldorado", "Dark Fields", "The Ultimate Enemy", "Starz Inside: Unforgettably Evil", "The Monster Hunter", "Mean Streets", "By Dawn's Early Light", "Fast Charlie... the Moonbeam Rider", "Young Billy Young", "Stretch", "Son of the Dragon", "Miracle at Sage Creek", "Macho Callahan", "Knocking On Death's Door", "Kiss of a Stranger", "Sonny Boy", "Brotherhood of the Gun", "Criminal Desire", "Waxwork II: Lost in Time", "Jealousy", "David Carradine Kung Fu Action Masters", "The New Swiss Family Robinson", "Cannonball", "Cloud Dancer", "True Legend", "Crime School", "Nightfall", "Dinocroc vs. Supergator", "Kung Fu: The Movie", "The Bad Seed", "Run for Your Life", "Break", "Nowhere to Run", "The Donor", "Naked Movie", "Mr. Horn", "Karate Cop", "Macon County Jail", "American Reel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1871", "prediction": ["# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> common.topic.article -> m.01q69x\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> sports.sports_championship_event.champion -> Detroit Red Wings\n# Answer:\n1954 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1967 Stanley Cup Finals -> sports.sports_championship_event.champion -> Toronto Maple Leafs\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> symbols.namesake.named_after -> Frederick Stanley, 16th Earl of Derby -> symbols.name_source.namesakes -> Stanley, Nova Scotia\n# Answer:\nFrederick Stanley, 16th Earl of Derby", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1964 Stanley Cup Finals -> sports.sports_championship_event.season -> 1963\u201364 NHL season\n# Answer:\n1964 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> time.recurring_event.current_frequency -> Yearly -> common.topic.notable_types -> Unit of frequency\n# Answer:\nYearly"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1873", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> English Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Sh\u011bn -> common.topic.notable_types -> Country\n# Answer:\nSh\u011bn", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Singapore -> location.country.official_language -> Tamil Language\n# Answer:\nSingapore", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.containedby -> Eurasia\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Sh\u011bn -> location.country.capital -> Linquan County\n# Answer:\nSh\u011bn", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.form_of_government -> Socialist state\n# Answer:\nVietnam"], "ground_truth": ["China"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1874", "prediction": ["# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nPortugal -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Algeria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Brazil\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nPortugal -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.sports -> Artistic gymnastics\n# Answer:\n1952 Summer Olympics"], "ground_truth": ["Saudi Arabia", "Mexico", "Libya", "Angola", "Morocco", "Jordan", "Mozambique", "Tunisia", "Algeria", "Egypt", "Singapore", "United Arab Emirates"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.02816901408450704, "path_precision": 0.125, "path_recall": 0.015873015873015872, "path_ans_f1": 0.1, "path_ans_precision": 0.125, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-1875", "prediction": ["# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1876", "prediction": ["# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.location.containedby -> Asia\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.writing_system -> Cuneiform\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.countries_spoken_in -> Assyria\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.location.containedby -> Middle East\n# Answer:\nIraq", "# Reasoning Path:\nBabylon -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Iraqi Kurdistan\n# Answer:\nIraq", "# Reasoning Path:\nBabylonia -> location.country.languages_spoken -> Akkadian language -> language.human_language.language_family -> East Semitic languages\n# Answer:\nAkkadian language", "# Reasoning Path:\nBabylonia -> location.location.people_born_here -> Hillel the Elder -> people.person.gender -> Male\n# Answer:\nHillel the Elder"], "ground_truth": ["Akkadian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1877", "prediction": ["# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> The Body in Pieces\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Oscar Bluemner\n# Answer:\nModern art", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.art_forms -> Painting -> base.descriptive_names.names.descriptive_name -> m.0109p390\n# Answer:\nPainting", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Der Blaue Reiter -> common.topic.notable_for -> g.12596yl82\n# Answer:\nDer Blaue Reiter", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> German Expressionism -> visual_art.art_period_movement.associated_artists -> Paul Klee\n# Answer:\nGerman Expressionism", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> German Expressionism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nGerman Expressionism", "# Reasoning Path:\nWassily Kandinsky -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.image -> Pink Bulls\n# Answer:\nModern art"], "ground_truth": ["Expressionism", "Abstract art", "German Expressionism", "Modern art", "Der Blaue Reiter"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8358208955223881, "ans_precission": 0.875, "ans_recall": 0.8, "path_f1": 0.711864406779661, "path_precision": 0.875, "path_recall": 0.6, "path_ans_f1": 0.8358208955223881, "path_ans_precision": 0.875, "path_ans_recall": 0.8}
{"id": "WebQTest-1878", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> South Korea\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1879", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.statistical_region.energy_use_per_capita -> g.1245_22wq\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.article -> m.049p35\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> book.book_subject.works -> A simple Habana melody -> book.written_work.subjects -> History\n# Answer:\nA simple Habana melody", "# Reasoning Path:\nCuba -> location.statistical_region.official_development_assistance -> g.12cp_kbnl\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Cuban convertible peso", "Cuban peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-188", "prediction": ["# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpk4bl -> education.education.institution -> West Anchorage High School\n# Answer:\nWest Anchorage High School", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0cpkbwp -> education.education.institution -> Juneau-Douglas High School\n# Answer:\nJuneau-Douglas High School", "# Reasoning Path:\nBristol Palin -> people.person.education -> m.0byl4zl -> education.education.institution -> Wasilla High School\n# Answer:\nWasilla High School", "# Reasoning Path:\nBristol Palin -> people.person.sibling_s -> m.04mvspr -> people.sibling_relationship.sibling -> Trig Palin\n# Answer:\nTrig Palin", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09ww50l -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.09wv0fg -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nBristol Palin -> common.topic.webpage -> m.0dbrf71 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic", "# Reasoning Path:\nBristol Palin -> tv.tv_personality.tv_regular_appearances -> m.0j70nj_ -> tv.tv_regular_personal_appearance.program -> Dancing with the Stars\n# Answer:\nDancing with the Stars"], "ground_truth": ["West Anchorage High School", "Wasilla High School", "Juneau-Douglas High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1880", "prediction": ["# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.office_holder -> Anna Moeller\n# Answer:\nAnna Moeller", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzs_6j -> government.government_position_held.office_holder -> John D. Anthony\n# Answer:\nJohn D. Anthony", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0_gt56s -> government.government_position_held.office_holder -> Art Turner\n# Answer:\nArt Turner", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9t2 -> government.government_position_held.office_holder -> David Harris\n# Answer:\nDavid Harris", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzq9mp -> government.government_position_held.office_holder -> David Harris\n# Answer:\nDavid Harris", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nIllinois House of Representatives -> common.image.size -> m.02dtb2t\n# Answer:\ncommon.image.size", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.010nnlj1 -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nIllinois House of Representatives -> government.governmental_body.members -> m.0wzs_6j -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives"], "ground_truth": ["Ann Williams", "Tom Cross", "John D. Anthony", "Bernard B. Wolfe", "Bruce L. Douglas", "James T. Londrigan", "Harber H. Hall", "Daniel M. Pierce", "Terry Deering", "David Harris", "Robert W. McCarthy", "Tobias Barry", "Thaddeus Jones", "Josh Harms", "Wayne Rosenthal", "Art Turner", "Adam Brown", "Ralph C. Capparelli", "Anna Moeller", "Bradley M. Glass"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.29411764705882354, "ans_precission": 0.5555555555555556, "ans_recall": 0.2, "path_f1": 0.35714285714285715, "path_precision": 0.5555555555555556, "path_recall": 0.2631578947368421, "path_ans_f1": 0.29411764705882354, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.2}
{"id": "WebQTest-1881", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> base.aareas.schema.administrative_area.pertinent_type -> Swedish municipality -> freebase.type_hints.included_types -> Location\n# Answer:\nSwedish municipality", "# Reasoning Path:\nSweden -> government.governmental_jurisdiction.agencies -> Swedish International Development Cooperation Agency -> organization.organization.parent -> m.0cm66g9\n# Answer:\nSwedish International Development Cooperation Agency", "# Reasoning Path:\nSweden -> base.aareas.schema.administrative_area.pertinent_type -> Swedish municipality -> type.type.properties -> County\n# Answer:\nSwedish municipality", "# Reasoning Path:\nSweden -> government.governmental_jurisdiction.agencies -> Swedish International Development Cooperation Agency -> common.topic.notable_types -> Government Agency\n# Answer:\nSwedish International Development Cooperation Agency"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1882", "prediction": ["# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.basic_title -> State Senator\n# Answer:\nState Senator", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.office_position_or_title -> New York State Senator\n# Answer:\nNew York State Senator", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_j -> government.government_position_held.office_position_or_title -> Assistant Secretary of the Navy\n# Answer:\nAssistant Secretary of the Navy", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.0b_p9hl -> government.government_position_held.jurisdiction_of_office -> New York\n# Answer:\nNew York", "# Reasoning Path:\nFranklin D. Roosevelt -> people.person.profession -> Politician -> type.type.domain -> Government\n# Answer:\nPolitician", "# Reasoning Path:\nFranklin D. Roosevelt -> government.politician.government_positions_held -> m.03q2h_9 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident"], "ground_truth": ["Governor of New York", "Assistant Secretary of the Navy", "New York State Senator"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1883", "prediction": ["# Reasoning Path:\nRobert Downey Jr. -> base.popstra.celebrity.canoodled -> m.0652wvp -> base.popstra.canoodled.participant -> Calista Flockhart\n# Answer:\nCalista Flockhart", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.episodes_appeared_in -> The Fat Guy Strangler\n# Answer:\nThe Fat Guy Strangler", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0lvm -> tv.tv_guest_role.episodes_appeared_in -> Paula DeAnda, Robert Downey Jr.;\n# Answer:\nPaula DeAnda, Robert Downey Jr.;", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.0126b7kf -> film.performance.film -> Captain America: Civil War\n# Answer:\nCaptain America: Civil War", "# Reasoning Path:\nRobert Downey Jr. -> celebrities.celebrity.substance_abuse_problems -> m.02_7q8_ -> celebrities.substance_abuse_problem.substance -> Methamphetamine\n# Answer:\nMethamphetamine", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.040r4ks -> tv.tv_guest_role.character -> Patrick Pewterschmidt\n# Answer:\nPatrick Pewterschmidt", "# Reasoning Path:\nRobert Downey Jr. -> film.actor.film -> m.0126b7kf -> film.performance.character -> Iron Man\n# Answer:\nIron Man", "# Reasoning Path:\nRobert Downey Jr. -> tv.tv_actor.guest_roles -> m.0bv0rg1 -> tv.tv_guest_role.episodes_appeared_in -> Show #0534\n# Answer:\nShow #0534"], "ground_truth": ["California Substance Abuse Treatment Facility and State Prison, Corcoran"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1884", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nPunahou School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nNoelani Elementary School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2, "path_ans_precision": 0.125, "path_ans_recall": 0.5}
{"id": "WebQTest-1885", "prediction": ["# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Ayyappan\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Yama\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.beliefs -> Sa\u1e43s\u0101ra\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Rama\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> Haridra Ganapati\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.beliefs -> Dharma\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.deities -> N\u0101ga\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> common.topic.notable_for -> g.125bspml8\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nShiva -> religion.deity.deity_of -> Hinduism -> religion.religion.beliefs -> Reincarnation\n# Answer:\nHinduism", "# Reasoning Path:\nShiva -> common.topic.article -> m.074lh\n# Answer:\ncommon.topic.article"], "ground_truth": ["Hinduism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1886", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Waking up early\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove"], "ground_truth": ["Bifocals", "Franklin stove", "Lightning rod", "Glass harmonica"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.75, "path_precision": 0.75, "path_recall": 0.75, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-1887", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.long_term_unemployment_rate -> g.12cp_k6s5\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> common.topic.notable_types -> War\n# Answer:\nSonderbund War", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> base.culturalevent.event.entity_involved -> Canton of Fribourg\n# Answer:\nSonderbund War", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> base.culturalevent.event.entity_involved -> Canton of Zug\n# Answer:\nSonderbund War"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1888", "prediction": ["# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt479\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt4fq\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt49c\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt4l_\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0ck2dsc\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> organization.organization.child -> m.0zs6st6\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nWorld Series -> book.book_subject.works -> The Gashouse Gang: How Dizzy Dean, Leo Durocher, Branch Rickey, Pepper Martin, and Their Colorful, Come-from-Behind Ball Club Won the World Series - and America's Heart - During the Great Depression -> book.written_work.subjects -> St. Louis Cardinals\n# Answer:\nThe Gashouse Gang: How Dizzy Dean, Leo Durocher, Branch Rickey, Pepper Martin, and Their Colorful, Come-from-Behind Ball Club Won the World Series - and America's Heart - During the Great Depression", "# Reasoning Path:\nWorld Series -> common.topic.notable_for -> g.125f97994\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWorld Series -> sports.sports_championship.league -> Major League Baseball -> sports.sports_league.teams -> m.0crt47r\n# Answer:\nMajor League Baseball"], "ground_truth": ["1982 World Series"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1889", "prediction": ["# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Baptists\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> base.popstra.celebrity.dated -> m.065pxsm -> base.popstra.dated.participant -> Dolly Kyle Browning\n# Answer:\nDolly Kyle Browning", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.gender -> Male\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> base.politicalconventions.presidential_nominee.nominated_at -> 2000 Democratic National Convention\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> people.person.religion -> Southern Baptist Convention\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nAl Gore", "# Reasoning Path:\nBill Clinton -> film.person_or_entity_appearing_in_film.films -> m.010l29t4 -> film.personal_film_appearance.film -> All About Ann: Governor Richards of the Lone Star State\n# Answer:\nAll About Ann: Governor Richards of the Lone Star State", "# Reasoning Path:\nBill Clinton -> government.us_president.vice_president -> Al Gore -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nAl Gore"], "ground_truth": ["Al Gore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-189", "prediction": ["# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> periodicals.newspaper_circulation_area.newspapers -> Al-Shorouk\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> location.location.contains -> Saudi Arabia\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> base.locations.countries.continent -> Africa -> base.locations.continents.planet -> Earth\n# Answer:\nAfrica", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Sudan\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> book.book_subject.works -> As Egypt Howls and History Tweets\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Arab world -> location.location.contains -> Saudi Arabia\n# Answer:\nArab world", "# Reasoning Path:\nEgypt -> location.location.containedby -> Middle East -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nMiddle East", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real"], "ground_truth": ["Middle East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1890", "prediction": ["# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Chamber music -> music.genre.parent_genre -> Classical music\n# Answer:\nChamber music", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.compositions -> L'adieu du cavalier\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Song\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.genre.artists -> Aaron Copland\n# Answer:\nArt song", "# Reasoning Path:\nLife of Franz Liszt -> base.kwebbase.kwtopic.connections_to -> achille-claude debussy played for franz liszt -> base.kwebbase.kwconnection.subject -> Claude Debussy\n# Answer:\nachille-claude debussy played for franz liszt", "# Reasoning Path:\nLife of Franz Liszt -> film.music_contributor.film -> The Loves of Liszt -> film.film.music -> Ferenc Farkas\n# Answer:\nThe Loves of Liszt", "# Reasoning Path:\nLife of Franz Liszt -> music.artist.genre -> Art song -> music.compositional_form.subforms -> Lied\n# Answer:\nArt song"], "ground_truth": ["Les Morts, S. 516", "Weihnachtsbaum, S. 186: Nr. 7. Schlummerlied", "Grandes \u00e9tudes de Paganini, S. 141: No. 6. Variations in A minor", "Zigeuner-Epos, S. 695b: No. 11 in A minor. Lento", "Romance, S. 169", "Vom Fels zum Meer!, S. 229", "Via Crucis, S. 504a: Station III: J\u00e9sus tombe pour la premi\u00e8re fois", "Etude in Twelve Exercises", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 389", "Bist du, S. 277/2", "Carrousel de Madame Pelet-Narbonne, S. 214a", "\u00c9tude en douze exercices, S. 136: IV. Allegro grazioso", "Glanes de Woronince", "Evocation \u00e0 la Chapelle Sixtine, S. 658", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2b. Au bord d'une source", "Drei Lieder aus Schillers Wilhelm Tell", "Historische ungarische Bildnisse, S. 205a: No. 5. V\u00f6r\u00f6smarty Mih\u00e1ly", "Magyar dalok, S. 242: No. 9 in A minor", "Ungarische Nationalmelodien, S. 243: No. 3. Pr\u00e9lude. Allegretto", "Klavierst\u00fcck aus der Bonn Beethoven-Kantate, S. 507", "Ihr Glocken von Marling, S. 328", "Helge's Treue, S. 686", "Tre sonetti di Petrarca, S. 270: I. Pace non trovo", "Soir\u00e9es de Vienne, S. 427: No. 2 in A-flat major. Poco allegro", "Zigeuner-Epos, S. 695b: No. 7 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\". Tempo di marcia", "Romance oubli\u00e9e, S. 132a", "Die Zelle in Nonnenwerth, S. 534/3", "I' vidi in terra angelici costumi", "Douze grandes \u00e9tudes, S. 137: No. 6 in G minor (Largo patetico)", "Historical Hungarian Portraits, S. 205: No. 2. E\u00f6tv\u00f6s Jozsef", "Album-Leaf: Langsam in C-sharp minor, S. 166o", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Introduction", "Pr\u00e4ludium und Fuge \u00fcber das Motiv B.A.C.H., S. 529/1", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 22 in E flat major \\\"Pester Carneval\\\"", "Fantasia quasi Concerto \\\"After Reading Dante\\\"", "R\u00e9miniscences de \\\"La Juive\\\", S. 409a", "Litanies de Marie, S. 171e", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 2. Bechlarn", "Sardanapale", "Au bord d'une source, S. 160 no. 4 bis", "Consolation in E major, S. 172 no. 2: Un poco pi\u00f9 mosso", "Album-Leaf: Braunschweig Preludio, S. 166f", "Soir\u00e9es de Vienne, S. 427: No. 9 in A-flat major. Preludio a capriccio", "Valse m\u00e9lancolique, S. 210a", "Dante Sonata", "Trois morceaux suisses, S. 156a: No. 2. Un soir dans la montagne", "Wartburglieder", "Festmarsch zur Goethe-Jubil\u00e4umfeier, S. 521", "Sancta Dorothea, S. 187", "Der traurige M\u00f6nch", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 11. (B\u00e9n\u00e9diction) (Pr\u00e9lude)", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9a. Allegretto", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 2. Der Hirt", "R\u00e9miniscences de Don Juan, S. 656", "Concerto path\u00e9tique", "Romancero espagnol, S. 695c: No. 3. Jota aragonesa and coda", "Douze grandes \u00e9tudes, S. 137: No. 11 in D-flat major (Lento assai)", "Einleitung, Fuge und Magnificat, S. 672b", "Two Concert \u00c9tudes", "Via Crucis, S. 53: Station X: Jesus wird entkleidet", "Weimars Volkslied, S. 542/2", "Paraphrase de concert sur Ernani I, S. 431a", "Sept variations brillantes sur un th\u00e8me de Rossini, op. 2, S. 149", "Historical Hungarian Portraits, S. 205: No. 3. V\u00f6r\u00f6smarty Mihaly", "Angiolin dal biondo crin (Arranged for solo piano)", "Weihnachtsbaum, S. 185a: VI. R\u00e9veille-Matin (Wecker)", "\u00dcber allen Gipfeln ist Ruh, S. 306", "Kennst du das Land, S. 531 no. 3", "Wie entgehn der Gefahr?", "Grandes \u00e9tudes de Paganini, S. 141: No. 5. La Chasse in E major", "Trauervorspiel und Trauermarsch, S. 206: No. 1. Trauervorspiel", "Ai No Yume", "Hexam\u00e9ron, S. 365b: IV. Variation II. Moderato", "Walhall aus Der Ring des Nibelungen, S. 449", "Elegy no. 2, S. 131", "In domum Domini ibimus, S. 505", "Muttergottes-Str\u00e4u\u00dflein zum Maimonate", "Comment, disaient-ils, S. 535", "Six poesies. Buch der Lieder", "Ballade no. 2, S. 170a", "Es muss ein Wunderbares sein, S. 314", "Via Crucis, S. 504a: Vexilla regis", "Urbi et orbi, S. 184", "Weihnachtsbaum, S. 186: Nr. 10. Ehemals", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7b. Lento", "Polonaise aus Tschaikowskys \\\"Eugen Onegin\\\", S. 429", "Reimar der Alte", "Galop de bal, S. 220", "Der blinde S\u00e4nger, S. 546", "Hungarian Rhapsody no. 10 in E major, S. 244 no. 10 bis", "Chor\u00e4le, S. 506a: No. 9. Vexilla regis", "Zweite Festmarsch nach Motiven von E H z S-C-G, S. 522", "Chor\u00e4le, S. 506a: No. 3. Meine Seele erhebt den Herrn (Der Kirchensegen, Psalm 67)", "Douze grandes \u00e9tudes, S. 137: No. 1 in C major (Presto)", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 3 in A-flat minor \\\"La Campanella\\\"", "Consolation in E-Dur", "Einleitung und Coda zu Smetanas Polka, S. 570a", "Album-Leaf: Tempo di marcia in E-flat major, S. 167o", "Via Crucis, S. 504a: Station VII: J\u00e9sus tombe pour la seconde fois", "Variation on the March from Bellini's I Puritani", "Grandes \u00e9tudes de Paganini, S. 141: No. 4. Arpeggio in E major", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 14 in A minor", "Korrekturblatt, S. 701k", "Salve Polonia, S. 518", "Hungarian Coronation Mass, S. 11: V. Offertorium", "Rondeau fantastique sur un th\u00e8me espagnol \\\"La Contrabandista\\\", S.252", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: I. Fantasy", "Transcendental \u00c9tude No. 6", "R\u00e1koczy March in A minor", "Eine Faust-Symphonie, S. 108: III. Mephistopheles. Allegro vivace, ironico", "L'Hymne du Pape, S. 530", "Ab irato, grande \u00e9tude de perfectionnement, S. 143", "Album-Leaf in G minor, S. 166l/2", "\u00c9tude en douze exercices, S. 136: II. Allegro con molto", "Ce qu'on entend sur la montagne", "Im Rhein, im sch\u00f6nen Strome, S. 272/1", "In Liebeslust, S. 318", "Hungarian Rhapsody no. 14 in F minor, S. 244 no. 14", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 3. Grande Marche caracteristique", "Air cosaque, S. 249c", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 8 in D-flat major \\\"M. K.\\\"", "M\u00e9lodie polonaise, S. 249a", "Sunt lacrymae rerum, S. 162c", "Illustrations du Proph\u00e8te, S. 414: No. 3: Pastorale - Appel aux armes", "Apr\u00e8s une lecture du Dante - Fantasia quasi Sonata, S. 158c", "Marche hongroise, S. 425/2e", "La lugubre gondola, S. 199a/1", "Pri\u00e8re d'un enfant \u00e0 son r\u00e9veil, S. 171c", "Five Hungarian Folksongs, S. 245: No. 5. B\u00fasongva. Lento", "Variations sur Le Carnaval de Venise, S. 700a", "Totentanz, S. 126: II. Variation I", "Weihnachtsbaum, S. 185a: IV. Adeste fideles (gleichsam als Marsch der heiligen drei K\u00f6nige)", "Album-Leaf: Andantino in E-flat major, S. 163a/2", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 15 in D minor", "Hungarian Rhapsody no. 11 in A minor, S. 244 no. 11", "Weihnachtsbaum, S. 185a: II. O heilige Nacht", "Geharnischte Lieder, S. 511: No. 2. Nicht gezagt!", "Die Lorelei, S. 532", "Tyrolean Melody, S. 385a", "g.1235wfk4", "Die Lore Ley: neue umgearbeitete Ausg.", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 2. Ave Maria", "Magyar dalok, S. 242: No. 8 in F minor", "Siegesmarsch, S. 233a", "Dante Symphony", "Apparitions, S. 155: No. 3. Fantaisie sur une valse de Fran\u00e7ois Schubert", "Grande valse di bravura, S. 209", "Album-Leaf: Introduction to the Grande \u00c9tude de Paganini no. 6, S. 141/6bis", "Lilie, S. 166m/1", "Fun\u00e9railles", "Douze grandes \u00e9tudes, S. 137: No. 10 in F minor (Presto molto agitato)", "Album-Leaf in E major (Leipzig), S. 163d", "Einsam bin ich, nicht allein, S. 453", "Totentanz, S. 126/1", "Historical Hungarian Portraits, S. 205: No. 4. Teleki Laszlo", "\u00c9tude en douze exercices, S. 136: XII. Allegro non troppo", "Totentanz, S. 126: IV. Variation III", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 7 in E major \\\"Alternative\\\"", "Hungarian Rhapsody no. 9 in E-flat major, S. 244 no. 9 \\\"Carnival in Pest\\\"", "Variations on \\\"Tisz\u00e1ntuli sz\u00e9p l\u00e9any\\\", S. 384a", "Seven brilliant variations on a theme by Rossini, S. 149", "Via Crucis, S. 53: Station XIV: Jesus wird ins grab gelegt", "Ave Maria IV in G major, S. 545", "Chapelle de Guillaume Tell, S. 160 no. 1", "Via Crucis, S. 53: Station VI: Sancta Veronica", "Album-Leaf in C minor (Pressburg), S. 163c", "Via Crucis, S. 53: Station III: Jesus f\u00e4llt zum ersten Mal", "Tannh\u00e4user Overture", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1/c", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 1 in E-flat major", "Responsorien und Antiphonen, S. 30: II. Feria V in coena Domini", "La cloche sonne, S. 238", "Album-Leaf in G major, S. 166q", "Tarantella, S. 162 no. 3", "Hungarian Rhapsody for Orchestra no. 6 in D-flat major \\\"Carnival in Pest\\\", S. 359/6", "Des toten Dichters Liebe", "Der du von dem Himmel bist, S. 279/1", "Ang\u00e9lus ! Pri\u00e8re aux anges gardiens, S. 163 no. 1", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 5. Pater noster", "Hungarian Coronation Mass, S. 11: I. Kyrie", "Spirto gentil, S. 400a", "Orpheus, S. 511b", "Cavatine de Robert le Diable, S. 412a", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 2. Der Sturm", "La Campanella : Nu Rave", "Valse de concert, S. 430", "Tre sonetti di Petrarca, S. 158: No. 2. Sonetto CIV. Pace non trovo", "Historical Hungarian Portraits, S. 205: No. 1. Sz\u00e9ch\u00e9nyi Istvan", "Hussitenlied, S. 234", "Soir\u00e9es de Vienne, S. 427: No. 7 in A major. Allegro spiritoso", "Sonata in B minor", "Stabat mater, S. 172b", "Kling leise, mein Lied, S. 301/1", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 12 in E minor \\\"Hero\u00efde \u00e9l\u00e9giaque\\\"", "Comment, disaient-ils, S. 276/2", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Berceuse", "Three Concert \u00c9tudes", "Album-Leaf: Larghetto in D-flat major, S. 167p", "Von der Wiege bis zum Grabe, S. 107", "La Lugubre gondola I, S. 200/1", "Harmonies po\u00e9tiques et religieuses", "Marche des Tcherkesses de l'op\u00e9ra Rouslan et Loudmila de Glinka, S. 406/1", "Schlummerlied im Grabe, S. 195a", "Hungarian Coronation Mass, S. 11: II. Gloria", "Au bord d'une source", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 1. Andante", "Chor\u00e4le, S. 506a: No. 5. Nun ruhen all W\u00e4lder", "Excelsior!, S. 500", "Von der Wiege bis zum Grabe, S. 107: III. Die Wiege des zukunftigen Lebens", "Weinen, Klagen, Sorgen, Zagen, Pr\u00e4ludium nach Johann Sebastian Bach, S. 179", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: I. Der n\u00e4chtliche Zug", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/1", "Es rauschen die Winde, S. 294/1", "O du mein holder Abendstern: Recitativ und Romanze aus Wagners T\u00e4nnhauser", "Orpheus, S. 672a", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: I. Allegro maestoso", "Via Crucis, S. 53: Station VII: Jesus f\u00e4llt zum zweiten Mal", "Venezia e Napoli, S. 159: No. 3. Andante placido", "La lugubre gondola, S. 134b", "Canzonetta del Salvator Rosa, S. 161 no. 3", "Scherzo und Marsch, S. 177", "Hungarian Rhapsody No. 15", "Album-Leaf: Lyon Pr\u00e9lude, S. 166d", "Hungarian Rhapsody for Orchestra no. 5 in E minor, S. 359/5", "Ann\u00e9es de p\u00e8lerinage", "Hungarian Coronation Mass, S. 11: VIII. Agnus Dei", "Die Zelle in Nonnenwerth, S. 534/2 bis", "Salve Regina, S. 669 no. 1", "Ann\u00e9es de p\u00e8lerinage : Troisi\u00e8me ann\u00e9e, S. 163", "Liebestraum As-Dur \\\"Hohe Liebe\\\", S. 541 Nr. 1", "Die Rose, S. 571", "Hexam\u00e9ron, S. 365b: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Der Fischerknabe", "Hungarian Rhapsody no. 7 in D minor, S. 244 no. 7", "Ann\u00e9es de p\u00e8lerinage : Premi\u00e8re ann\u00e9e : Suisse, S. 160", "Historical Hungarian Portraits, S. 205: No. 6. Pet\u00f6fi Sandor", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 2. Marche indienne", "Seconde marche hongroise, S. 232", "Grand solo de concert, S. 175a", "Hexam\u00e9ron, S. 392: IX. Finale. Molto vivace quasi prestissimo", "Deux marches dans le genre hongrois, S. 693: No. 1 in D minor", "Am Grabe Richard Wagners, S. 202", "Das Grab und die Rose", "2 Polonaises, S. 223: no. 1 \\\"Polonaise m\u00e9lancolique\\\" in C minor", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: III. Andante mesto - Allegro maestoso - Recitativo - Largo maestoso - Allegro fuocoso", "Liebestraum As-Dur \\\"Oh Lieb, so lang du lieben kannst\\\", S. 541 Nr. 3", "Chor\u00e4le, S. 506a: No. 1. Crux ave benedicta", "Historische ungarische Bildnisse, S. 205a: No. 6. Pet\u00f6fi S\u00e1ndor", "Valse, S. 210b", "Paralipom\u00e8nes \u00e0 la Divina Commedia, S. 158a", "Album-Leaf, S. 167c", "Mariotte \u2013 Valse de Marie, S. 212b", "Valse oubli\u00e9e no. 1, S. 215 no. 1", "R\u00e9miniscences de Norma, S. 394", "Magyar dalok, S. 242: No. 1 in C minor", "Cantico del sol di San Francisco d'Assisi", "Rondo di bravura, op. 4 no. 2, S. 152", "Weihnachtsbaum, S. 185a: X. [Ehemals]", "Dante fragment, S. 701e", "O Roma nobilis, S. 546a", "Salve Regina", "A magyarok Istene", "Berceuse, S. 174/1", "Liebestr\u00e4ume", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403", "Hungaria, S. 511e", "Marche fun\u00e8bre de Dom S\u00e9bastien, S. 402", "Hexam\u00e9ron, S. 392: IV. Variation II. Moderato", "Hungarian Rhapsody for Orchestra no. 1 in F minor, S. 359/1", "Eine Faust-Symphonie, S. 108: I. Faust. Lento assai", "Consolation in E major, S. 171a no. 2: Un poco pi\u00f9 mosso", "Zigeuner-Epos, S. 695b: No. 3 in D-flat major. Sehr langsam", "Faust Symphony", "Consolations", "Waldesrauschen (Forest Murmurs)", "Fantasie sur l'\u00f3pera hongroise Sz\u00e9p Ilonka, S. 417", "Grosse Concert-Fantasie aus der Oper Sonnambula, S. 393/3", "Konzert-Walzer \u00fcber zwei Themen aus Donizettis \\\"Lucia und Parisina\\\", S. 401", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 5. Miserere", "R\u00e1k\u00f3czi-Marsch, S. 692d", "Fantaisie sur des motifs de La pastorella dell\u2019Alpi e Li marinari des Soir\u00e9es musicales, S. 423", "Soir\u00e9es de Vienne, S. 427: No. 3 in E major. Allegro vivace", "Weihnachtsbaum, S. 186: Nr. 12. Polnisch", "Album-Leaf: Schlusschor des entfesselten Prometheus. Andante solenne in D-flat major, S. 167q", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/1", "Transcendental \u00c9tude No. 11", "Gebet, S. 265", "Hungarian Rhapsody No. 1", "P\u00e1sztor Lakodalmas, S. 405a", "Totentanz, S. 126: III. Variation II", "Schlummerlied, S. 186/7a", "Mephisto Polka", "Concerto for Piano No. 1 in E flat major, S 124: III. Allegretto vivace", "Hunnenschlacht", "L\u00e4ndler, S. 211a", "\u00c9tude en douze exercices, S. 136: V. Moderato", "Aux cypr\u00e8s de la Villa d'Este I : Thr\u00e9nodie, S. 163 no. 2", "Weil noch, Sonnenstrahl", "Polnisch, S. 701g", "Mal\u00e9diction, S. 121", "Apr\u00e9s une lecture du Dante, S. 161/7 (Ann\u00e9es de P\u00e9l\u00e8rinage II/7)", "Responsorien und Antiphonen, S. 30: IV. Sabbato sancto", "De Profundis, S. 121a", "Pastorale, S. 160 no. 3", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 2. Lass\u00fa magyar", "Beethoven Symphonies", "Transcendental \u00c9tude No. 12", "Grandes \u00e9tudes de Paganini, S. 141: No. 2. Octave in E-flat major", "Ungarische Nationalmelodien, S. 243: No. 2. Animato", "Transcendental \u00c9tude No. 4", "Grande fantaisie dramatique sur des th\u00e8mes de l'op\u00e9ra Les Huguenots, S. 412/3, R. 211 \\\"R\u00e9miniscences des Huguenots\\\"", "Festmarsch zu S\u00e4kularfeier von Goethes Geburtstag, S. 227", "Hungarian Rhapsody no. 17 in D minor, S. 244 no. 17", "Liebestraum Es-Dur \\\"Seliger Tod\\\", S. 541 Nr. 2", "Leyer und Schwert, S. 452: II. Schwertlied", "Oh! quand je dors, S. 282/1", "Transcendental \u00c9tude No. 1", "Via Crucis, S. 504a: Station VI: Sancta Veronica", "Magyar dalok, S. 242: No. 4 in C-sharp major", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 7. Hymne de l'enfant \u00e0 son r\u00e9veil", "Waltz in E-flat major, S. 209a", "Schuberts ungarische Melodien, S. 425a: No. 2. Marcia (Marche hongroise)", "Operatic aria, S. 701h/1", "La Marseillaise, S. 237", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: I. Maestoso - Quasi allegretto", "Apparitions, S. 155: No. 1. Senza lentezza quasi allegretto", "Cantico del Sol di San Francesco d'Assisi, S. 499", "Album-Leaf: Aus den Mephisto-Walzer. Episode aus Lenaus Faust - Der Tanz in der Dorfschenke, S. 167m", "Eine Symphonie zu Dantes Divina Commedia, S. 109: II. Purgatorio - Magnificat", "Weihnachtsbaum, S. 186: Nr. 11. Ungarisch", "Klavierst\u00fcck \u00fcber ein fremdes Thema, S. 387a", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 4. Litanies de Marie", "Magyar dalok, S. 242: No. 2 in C major", "Grande \u00e9tude de perfectionnement", "Transcendental \u00c9tude No. 9", "Totentanz, S. 126: V. Variation IV", "An Edlitam. Zur silbernen Hochzeit", "Grand galop chromatique, S. 219 bis", "Via Crucis, S. 53: Station VIII: Die Frauen von Jerusalem", "Piano Concerto No. 2", "La romanesca, S. 252a/1", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 1. Einleitung - Pastorale", "Biterolf und der Schmied von Ruhla", "Leyer und Schwert, S. 452: IV. Lutzows wilde Jagd", "O du mein holder Abendstern, S. 444", "Les Cloches de Gen\u00e8ve : Nocturne, S. 160 no. 9", "\u00c9tude en douze exercices, S. 136: III. Allegro sempre legato", "Die Perle", "Weihnachtslied \\\"Christus ist geboren\\\", S. 502", "Album-Leaf: Allegretto in A major, S. 167n", "Marche hongroise, S425/2b", "Hungarian Coronation Mass, S. 11: IV. Credo", "Grande fantaisie sur la tyrolienne de l'op\u00e9ra La fianc\u00e9e de Auber, S. 385/1", "\u00c9tude en douze exercices, S. 136: VIII. Allegro con spirito", "J'ai perdu ma force et ma vie, S. 327", "Via Crucis, S. 504a: Station XII: J\u00e9sus meurt sur la croix", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 1. Invocation", "Totentanz, S. 126: I. Andante - Allegro - Allegro moderato", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 1 in D major", "Elegy no. 1, S. 130b", "Ballade aus Der fliegende Holl\u00e4nder, S. 441", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 10. Cantique d'amour", "Hungarian Coronation Mass, S. 11: VII. Benedictus", "Christmas Again", "Magnificat, S. 182a", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: II. Fuge", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": I. Kyrie. Andante solenne", "Zigeuner-Epos, S. 695b: No. 9 in E-flat major. Andante cantabile quasi adagio", "Zigeuner-Epos, S. 695b: No. 6 in G minor. Lento", "Valse de bravoure, S. 214 no. 1", "An Frau Minne", "Weihnachtsbaum, S. 186: Nr. 3. Die Hirten an der Krippe", "Weihnachtsbaum, S. 185a: VIII. [Alt-provenzalische No\u00ebl]", "Paraphrase de concert sur Ernani II, S. 432", "Albumblatt in Walzerform, S. 166", "Pri\u00e8re et Berceuse de La muette de Portici d'Auber, S. 387: Pri\u00e8re", "Venezia e Napoli, S. 159: No. 1. Lento", "Isoldens Liebestod, S. 447", "Enfant, si j'\u00e9tais roi, S. 537", "Enfant, si j'\u00e9tais roi, S. 283/2", "Mazeppa, S. 138", "Weihnachtsbaum, S. 186: Nr. 5. Scherzoso", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8a. Andante con sentimento", "Am Rhein, im sch\u00f6nen Strome, S. 531 no. 2", "La notte, S. 516a", "Ave maris stella, S. 669 no. 2", "Angelus!, S. 162a/2", "Dante Symphony (arrangement for piano)", "Valse m\u00e9lancolique, S. 214 no. 2", "Ungarischer Marsch in B-flat major, S. 229a", "Pilgerchor aus Tannh\u00e4user, S. 443/1", "Ave Maria in D major, S. 504/1", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 3. Allegretto", "Album-Leaf in C major, S. 167s \\\"Lyon\\\"", "Grande fantaisie sur des th\u00e8mes de l'op\u00e9ra Niobe, S. 419", "\u00c9tude en douze exercices, S. 136: VII. Allegretto con molta espressione", "Resignazione, S. 187b", "Grand solo de concert, S. 365", "Deux \u00e9pisodes d'apres le Faust de Lenau, S. 110: II. Der Tanz in der Dorfschenke (Erster Mephisto-Walzer)", "Mephisto Waltz no. 3, S. 215a", "Fantasie \u00fcber englische Themen, S. 694", "Prol\u00e9gom\u00e8nes \u00e0 la Divina Commedia, S. 158b", "Il penseroso, S. 161 no. 2", "Marche hongroise, S. 233b", "Mephisto Waltz no. 3, S. 216", "Sunt lacrymae rerum / En mode hongrois, S. 163 no. 5", "Der Gl\u00fcckliche", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 19 in F-sharp minor", "Magyar rapsz\u00f3dia, S. 244/16/1", "Valse m\u00e9lancolique, S. 210", "Marche fun\u00e8bre, S. 226a", "Consolation in E major, S. 172 no. 1: Andante con moto", "Harmonie nach Rossini's Carit\u00e0, S. 701j", "Sz\u00f3zat und Hymnus, S. 486", "Missa solennis zur Einweihung der Basilika in Gran, S. 9 \\\"Graner Messe\\\": III. Credo. Antande maestoso, risoluto", "Allegro di bravura, op. 4 no. 1, S. 151", "Christus", "Ave Maria in D-flat major, S. 504/2", "Le Rossignol, S. 249d", "Die Zelle in Nonnenwerth, S. 382", "Illustrations du Proph\u00e8te, S. 414: No. 1. Pri\u00e8re - Hymne triomphal - Marche du sacre", "Legend no. 2: \\\"St. Francis Walking on the Waves\\\"", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7a. Allegro", "Ungarischer Marsch zur Kr\u00f6nungsfeier in Ofen-Pest am 8. Juni 1867, S. 523", "Ebony Rhapsody", "Ungarische Nationalmelodien, S. 243: No. 1. Tempo giusto", "Harmonies po\u00e9tiques et religieuses, S. 154", "Chor\u00e4le, S. 506a: No. 6. O Haupt voll Blut und Wunden", "Via Crucis, S. 53: Station IV: Jesus begegnet seiner Heiligen Mutter", "Venezia e Napoli, S. 159: No. 4. Tarantelles napolitaines", "Album-Leaf: Berlin Preludio, S. 164g", "Album-Leaf, S. 167d", "Angiolin, S. 531 no. 6", "Hexam\u00e9ron, S. 392: I. Introduction", "\u00c9tude en douze exercices, S. 136: IX. Allegro grazioso", "Chor\u00e4le, S. 506a: No. 7. O Lamm Gottes", "Il m'aimait tant, S. 533", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 2a. Le Lac de Wallenstadt", "Sz\u00f3zat und Hymnus, S. 353", "Ungarischer Geschwindmarsch, S. 233", "S'il est un charmant gazon, S. 284/2", "Weihnachtsbaum, S. 185a: XII. Polnisch", "Hungarian Rhapsody no. 3 in B-flat major, S. 244 no. 3", "Responsorien und Antiphonen, S. 30: V. In officio defunctorum", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 3 in E major", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 2 in C minor \\\"Langueur?\\\"", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: I. Fantasie", "God Save the Queen, S. 235", "Die Zelle in Nonnenwerth, S. 534/2", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9b. Allegretto", "Trauervorspiel und Trauermarsch, S. 206: No. 2. Trauermarsch", "St. Stanislaus fragment, S. 688a", "Einleitung und Coda zu Rubinsteins \u00c9tude in C-Dur, S. 554a", "Album d'un voyageur, S. 156: III. Paraphrases: 10. Ranz de vaches [de F. Huber] - Aufzug auf die Alp - Improvisata", "Ouvert\u00fcre zu Tannh\u00e4user von Richard Wagner", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/2", "Marche militaire, S. 426a", "Heinrich von Ofterdingen", "Petite Valse favorite, S. 212a", "Hungarian Rag", "Grande Fantaisie sur des th\u00e8mes de Paganini, S. 700/1", "Marche fun\u00e8bre et Cavatine de Lucia de Lammermoor, S. 398", "Fantasy and Fugue on the Theme B-A-C-H", "Weihnachtsbaum, S. 186: Nr. 4. Adeste fideles", "Ave Maria (d'Arcadelt), S. 183 no. 2", "Consolation in E major, S. 171a no. 5: Andantino", "Und sprich, S. 329", "Der du von dem Himmel bist, S. 279/3", "Hexam\u00e9ron, S. 392: VIII. Variation VI. Largo - Coda", "Hungarian Coronation Mass, S. 11: III. Graduale (Psalm 116)", "Orpheus, S. 98", "Gaudeamus igitur, S. 240/1", "Es war ein K\u00f6nig in Thule, S. 278/2", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 2. Hoffest: Marsch und Polonaise", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1a in G minor", "Illustrations de l'op\u00e9ra L'Africaine, S. 415: No. 1. Pri\u00e8re des matelots \\\"\u00d4 grand Saint Dominique\\\"", "Zigeuner-Epos, S. 695b: No. 8 in D major. Adagio sostenuto a capriccio", "H\u00e9ro\u00efde fun\u00e8bre", "Trois morceaux suisses, S. 156a: No. 3. Ranz de ch\u00e8vres", "Elegy no. 2, S. 197", "\u00c9tude en douze exercices, S. 136: I. Allegro con fuoco", "Le Rossignol, S. 250 no. 1", "Gaudeamus igitur, S. 240/2", "Zwei St\u00fccke aus dem Oratorium Christus, S. 498c: No. 2. Das Wunder", "Cs\u00e1rd\u00e1s", "Suite no. 4 in G major, op. 61 \\\"Mozartiana\\\": III. Preghiera: Andante non tanto", "San Francesco, S. 498d", "Album-Leaf: Freudvoll und leidvoll, S. 166n", "Vexilla regis prodeunt, S. 185", "Aux anges gardiens, S. 162a/1 bis", "La campanella", "Canzone napolitana, S. 248", "Grandes etudes de Paganini", "Album-Leaf: Serenade, S. 166g", "Adela\u00efde von Beethoven, S. 466: Cadenza ad libitum", "Tasso, Lamento e Trionfo", "La lugubre gondola", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: I. Benedictus", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/2", "Transcendental \u00c9tude No. 8", "Les Pr\u00e9ludes, S. 511a", "Hexam\u00e9ron, S. 392: VII. Variation V. Vivo e brillante - Fuocoso molto energico - Lento quasi recitativo", "Album-Leaf, S. 167e", "Historical Hungarian Portraits, S. 205: No. 5. De\u00e1k Ferenc", "Les Sab\u00e9ennes, berceuse de l\u2019op\u00e9ra La Reine de Saba, S. 408", "R\u00e1k\u00f3czi-Marsch, S. 244a", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8c. Allegro moderato", "Die Lorelei, S. 531 no. 1", "Wieder m\u00f6cht' ich dir begegnen", "La lugubre gondola, S. 200/2", "Hungarian Rhapsody no. 16 in A minor, S. 244 no. 16", "Hungarian Rhapsody for Orchestra no. 3 in D major, S. 359/3", "Hungarian Rhapsody no. 1 in C-sharp minor, S. 244 no. 1", "Sursum corda, S. 163 no. 7", "Via Crucis, S. 53: Station V: Simon von Kyrene hilft Jesus das Kreuz zu tragen", "R\u00e9miniscences de Don Juan", "Album-Leaf: Andante religioso, S. 166h", "Ave Maria in E major, S. 182 \\\"Die Glocken von Rom\\\"", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 16 in E major", "Totentanz, S. 126: VIII. Variation VI", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 4. Vall\u00e9e d'Obermann", "Einleitung und Schlu\u00dftakte zu Tausigs 3. Valse-Caprice, S. 571a", "L\u00e4ndler in A-flat major, S. 211", "Cadenza to the first movement of Beethoven's Piano Concerto no. 3, S. 389a", "In festo transfigurationis Domini nostri Jesu Christi, S. 188", "Ave maris stella, S. 506", "Feierlicher Marsch zum heiligen Gral aus Parsifal, S. 450", "Deux l\u00e9gendes, S. 175: II bis. St. Fran\u00e7ois de Paule marchant sur les flots", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: II. Adagio", "Paraphrases pour piano sur le th\u00e8me favori et oblig\u00e9: 1a. Pr\u00e9lude \u00e0 la Polka de Borodine, S. 207a", "Weimars Volkslied", "Ann\u00e9es de p\u00e8lerinage : Deuxi\u00e8me ann\u00e9e : Italie, S. 161", "Trois \u00e9tudes de concert, S. 144: III. \\\"Un sospiro\\\" in D-flat major", "Gaudeamus igitur, S. 509", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 6. Pater noster, d'apr\u00e8s la Psalmodie de l'\u00c9glise", "Album-Leaf: Magyar, S. 164e", "Rhapsodie espagnole", "Hexam\u00e9ron, S. 365b: IX. Finale. Molto vivace quasi prestissimo", "Album-Leaf: Preludio, S. 164j", "Responsorien und Antiphonen, S. 30: III. Feria VI in Parasceve", "Consolation in E major, S. 172 no. 6: Allegretto sempre cantabile", "Chor\u00e4le, S. 506a: No. 8. O Traurigkeit", "Magyar dalok, S. 242: No. 6 in G minor", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5a in E major \\\"La Chasse\\\"", "Klavierst\u00fcck in A-flat major, S. 192 no. 2. Lento assai", "Valse oubli\u00e9e no. 3, S. 215 no. 3a", "Den Cypressen der Villa d'Este, S. 162b", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18", "Magyar tempo, S. 241b", "Consolation in Des-Dur", "Tu es Petrus, S. 664", "Chanson boh\u00e9mienne, S. 250 no. 2", "Leyer und Schwert, S. 452: I. [Introduction]", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: II. Faust: 1. Osterhymne", "Mephisto Waltz No. 1", "Variationen \u00fcber das Motiv von Bach, S. 180", "Hexam\u00e9ron, S. 392: V. Variation III di bravura - Ritornello", "Album Leaf in E major (Vienna), S. 164a", "Il penseroso, S. 157b", "Chor\u00e4le, S. 506a: No. 10. Was Gott tut, das ist wohlgetan", "Der 18. Psalm", "Douze grandes \u00e9tudes, S. 137: No. 9 in A-flat major (Andantino)", "Rosario, S. 670: No. 3. Mysteria gloriosa", "Album d'un voyageur, S. 156: III. Paraphrases: 11. Un soir dans les montagnes [de Knop] - Nocturne pastorale", "Glanes de Woronince, S. 249: No. 1. Ballade ukraine (Dumka)", "Douze grandes \u00e9tudes, S. 137: No. 8 in C minor (Presto strepitoso)", "Puszta-Wehmut, S. 246", "La perla, S. 326/2", "Angiolin dal biondo crin", "Sunt lacrymae rerum \u2013 in ungarischen Weise, S. 162d", "A holt k\u00f6lt\u0151 szerelme, S. 349", "Schlaflos!, S. 203", "Hungarian Rhapsody no. 8 in F-sharp minor, S. 244 no. 8", "Der tugendhafte Schreiber", "Klavierst\u00fcck in E major, S. 192 no. 1. Sehr langsam", "Es war ein K\u00f6nig in Thule, S. 531 no. 4", "Aus der Ungarischen Kr\u00f6nungsmesse, S. 501: II. Offertorium", "Freisch\u00fctz Fantasie, S. 451", "Pr\u00e4ludium und Fuge \u00fcber den Namen BACH, S. 260: I. Pr\u00e4ludium", "Valse-caprice no. 6, S.427/6b", "Wie singt die Lerche sch\u00f6n", "C\u00e9l\u00e8bre m\u00e9lodie hongroise, S. 243a", "Transcendental \u00c9tudes", "Galop (in A minor), S. 218", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 2 in C major", "Feuille d'album, S.165", "Zigeuner-Epos, S. 695b: No. 10 in F major. Lento", "Krakowiak, S. 166m/4", "Schuberts ungarische Melodien, S. 425a: No. 3. Allegretto", "Li marinari", "Fantasie \u00fcber Themen aus Beethoven's Ruinen von Athen, S. 388b", "Grande Fantaisie sur des motifs de Soir\u00e9es musicales, S. 422/2", "Five Hungarian Folksongs, S. 245: No. 4. Kiss\u00e9 \u00e9l\u00e9nken. Vivace", "\u00c9tude en douze exercices, S. 136: X. Moderato", "Via Crucis, S. 504a: Station XI: J\u00e9sus est attach\u00e9 \u00e0 la croix", "Transcendental \u00c9tude No. 2", "Toccata, S. 197a", "Hungarian Rhapsody no. 18 in F-sharp minor, S. 244 no. 18/1", "Variation \u00fcber einen Walzer von Diabelli, S. 147", "Responsorien und Antiphonen, S. 30: I. In nativitate Domini", "R\u00e1k\u00f3czi March, S. 608", "Romance oubli\u00e9e, S. 527", "B\u00fclow-Marsch, S. 230", "Der alte Vagabund", "Der traurige M\u00f6nch, S. 348", "Soir\u00e9es de Vienne, S. 427: No. 4 in D-flat major. Andantino a capriccio", "Die Ideale", "Aux cypr\u00e8s de la Villa d'Este II : Thr\u00e9nodie, S. 163 no. 3", "Grosses Konzertsolo, S. 176", "Mephisto Waltz no. 2, S. 515", "Festkl\u00e4nge, S. 101", "Zigeuner-Epos, S. 695b: No. 2 in C major. Andantino", "Sospiri!, S. 192 no. 5. Andante", "Romancero espagnol, S. 695c: No. 1. Introduction and Fandango with variations", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 2. Hymne du matin", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 8b. Andante molto espressivo", "Weihnachtsbaum, S. 185a: VII. Schlummerlied", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 5 in E major \\\"La Chasse\\\"", "R\u00e1k\u00f3czy March, S. 117", "Blume und Duft", "Hungarian Rhapsody no. 12 in C-sharp minor, S. 244 no. 12", "Sposalizio", "Historische ungarische Bildnisse, S. 205a: No. 1. Sz\u00e9chenyi Istv\u00e1n", "Via Crucis, S. 53: Einleitung. Vexilla regis", "Adagio in C major, S. 158d", "Sonetto 123 del Petrarca, S. 161 no. 6", "Vive Henri IV, S. 239", "Capriccio alla turca sur des motifs de Beethoven, S. 388", "Album-Leaf in E major (Detmold), S. 164d", "Prometheus", "R\u00e1k\u00f3czi March, S. 242a", "Totentanz, S. 126: VII. Cadenza", "Die Loreley, S. 273/1", "Festvorspiel, S. 226", "Eine Symphonie zu Dantes Divina Commedia, S. 109: I. Inferno", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 21 in E minor", "Album-Leaf in A-flat, S. 166c", "Nocturne, S. 191/1", "Einleitung und Coda zu Raffs Walzer in Des-Dur, S. 551a", "Tscherkessenmarsch aus Russlan und Ludmilla, S. 406/2", "Romance oubli\u00e9e, S. 527 bis", "Klavierst\u00fcck in F-sharp major, S. 192 no. 3. Sehr langsam", "Via Crucis, S. 504a: Station VIII: Les Femmes de J\u00e9rusalem", "Totentanz, S. 525, R. 188", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 4 in D-flat major \\\"Derni\u00e8re illusion\\\"", "L'Id\u00e9e fixe, S. 395", "Rosario, S. 670: No. 2. Mysteria dolorosa", "Trois \u00e9tudes de concert, S. 144: I. \\\"Il lamento\\\" in A-flat minor", "Via Crucis, S. 53: Station XI: Jesus wird ans Kreuz geschlagen", "Consolation in E major, S. 171a no. 6: Allegretto", "B\u00e9n\u00e9diction et serment de Benvenuto Cellini, S. 396", "Album-Leaf: Agitato in G major, S. 167l", "Konzertparaphrase \u00fcber Mendelssohns Hochzeitsmarsch und Elfenreigen aus der Musik zu Shakespeares Sommernachtstraum, S. 410", "Hungarian Rhapsody no. 13 in A minor, S. 244 no. 13", "Mephisto Waltz no. 4, S. 216b", "R\u00e9miniscences de Boccanegra, S. 438", "Don Sanche", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 20 in G minor \\\"Rumanian Rhapsody\\\"", "Album-Leaf: Adagio \u2013 religioso in C major, S. 164l", "Album-Leaf: Magyar II in B-flat minor, S. 164e/2", "Feuille d'album no. 2, S. 167", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 624: III. Fugue", "Die drei Zigeuner", "Benedetto sia 'l giorno", "L'Id\u00e9e fixe, S. 470a no. 1", "Soir\u00e9es de Vienne, S. 427: No. 6 in A minor. Allegro con strepito", "Freudvoll und Leidvoll, S. 280/2", "Transcendental \u00c9tude No. 5", "Tre sonetti di Petrarca, S. 270: III. I' vidi in terra angelici costumi", "Glanes de Woronince, S. 249: No. 2. M\u00e9lodies polonaises", "Grande Fantaisie di bravura sur La Clochette de Paganini, S. 420", "Historische ungarische Bildnisse, S. 205a: No. 2. De\u00e1k Ferenc", "Album-Leaf: Magyar in D-flat major, S. 164e/3", "Geharnischte Lieder, S. 511: No. 1. Vor der Schlacht", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 3. Les Cloches de G*****", "Weihnachtsbaum, S. 185a: IX. [Abendglocken]", "Douze grandes \u00e9tudes, S. 137: No. 7 in E-flat major (Allegro deciso)", "R\u00e9miniscences des Huguenots, S. 412/1", "Le Triomphe fun\u00e8bre du Tasse, S. 517", "Album-Leaf: Andante in E-flat major, S. 167r", "Miserere du Trovatore, S. 433", "Weihnachtsbaum, S. 186: Nr. 8. Altes provenzalisches Weihnachtslied", "Romance oubli\u00e9e, S. 132c", "Grande Fantaisie Symphonique on themes from Berlioz's \\\"L\u00e9lio\\\" for Piano and Orchestra, S. 120", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 3. Interludium", "Trois \u00e9tudes de concert, S. 144: II. \\\"La leggierezza\\\" in F minor", "Album-Leaf: Moderato in D-flat major, S. 164k", "Aus der Musik von Eduard Lassen zu Hebbels Nibelungen und Goethes Faust, S. 496: I. Nibelungen: 1. Hagen und Kriemhild", "Mazeppa", "Mephisto Waltzes", "Klavierst\u00fcck in A-flat major, S. 189a", "Album-Leaf in D major, S. 164h", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 13 in A minor \\\"R\u00e1k\u00f3czi-Marsch\\\"", "19 Hungarian Rhapsodies for Piano, S 244 No. 15 \\\"R\u00e1k\u00f3czy March\\\"", "L\u00e9gende No. 1: St Fran\u00e7ois d'Assise", "Grosse Konzertfantasie \u00fcber spanische Weisen, S. 253", "Album-Leaf: Exeter Preludio, S. 164c", "Valse de l'op\u00e9ra \\\"Faust\\\", S. 407", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 3. Der Alpenj\u00e4ger", "Album-Leaf in E major, S. 167t", "Walzer in A major, S. 208a", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 3. Hymne de la nuit", "Ungarische National-Melodie, S. 242/13 bis", "Album Leaf in F-sharp minor, S. 163a/1", "Via Crucis, S. 504a: Station IX: J\u00e9sus tombe une troisi\u00e8me fois", "Five Hungarian Folksongs, S. 245: No. 2. M\u00e9rsek\u00e9lve. Allegretto", "Weihnachtsbaum, S. 186: Nr. 2. O heilige Nacht!", "Album-Leaf: Fugue chromatique. Allegro in G minor, S. 167j", "Huldigungsmarsch, S. 228/1", "Consolation in D-flat major, S. 171a no. 4: Quasi adagio", "Mazurka brilliante, S. 221", "R.W.-Venezia, S. 201", "Tr\u00fcbe Wolken (Nuages gris), S. 199, R. 78", "Das Veilchen", "Feuilles d\u2019album, S. 165", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 6. Hymne de l'enfant \u00e0 son r\u00e9veil", "Tre sonetti di Petrarca, S. 270: II. Benedetto sia 'l giorno", "Canzone, S. 162 no. 2", "Valse \u00e0 capriccio sur deux motifs de Lucia et Parisina de Donizetti, S. 401", "Valse de concert sur deux motifs de Lucia et Parisina de Donizetti, S. 214 no. 3", "Bagatelle sans tonalit\u00e9", "Der Alpenj\u00e4ger", "Wo weilt er? (Heimat)", "Recueillement, S. 204", "Festpolonaise, S. 619a", "\u00c0 la Chapelle Sixtine, S. 461/1", "Ave Maria II, S. 38", "Historical Hungarian Portraits, S. 205: No. 7. Mosonyi Mih\u00e1ly", "Soir\u00e9es de Vienne, S. 427: No. 1 in A-flat major. Allegretto malinconico", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 1 in G minor", "Lieder aus Schillers Wilhelm Tell, S. 292a: Nr. 1. Der Fischerknabe", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 2. Marsch der Kreuzritter", "Variationen \u00fcber das Motiv von Bach, S. 673", "Mephisto Waltz no. 2, S. 111", "Festkl\u00e4nge, S. 511d", "Deux marches dans le genre hongrois, S. 693: No. 2 in B-flat minor", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 1. \u00c9levez-vous, voix de mon \u00e2me", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 2. Die Schl\u00fcsselblumen", "En r\u00eave - Nocturne, S. 207", "Consolation No. 3", "Via Crucis, S. 504a: Station XIII: J\u00e9sus est d\u00e9pos\u00e9 de la croix", "Muttergottes-Str\u00e4usslein zum Mai-Monate, S. 316: Nr. 1. Das Veilchen", "Geharnischte Lieder, S. 511: No. 3. Es rufet Gott uns mahnend", "Album-Leaf: Andantino in E major, S. 163d/ii", "R\u00e9miniscences de Lucrezia Borgia, S. 400: II. Chanson \u00e0 boire (Orgie). Duo-finale", "\u00c9tude en douze exercices, S. 136: XI. Allegro grazioso", "Fanfare zur Enth\u00fcllung des Carl-Augusts Monument, S. 542b", "Weihnachtsbaum, S. 186: Nr. 9. Abendglocken", "Am stillen Herd, S. 448", "Klavierkonzert No. 1 Es-dur: III. Allegro marciale animato", "Album Leaf in G major (Dante-Symphony progression), S. 167f", "Album-Leaf: Aus dem Purgatorio des Dante Sinfonie. Lamentoso in B minor, S. 166r/2", "2 Cs\u00e1rd\u00e1s, S. 225: No. 2. Cs\u00e1rd\u00e1s obstin\u00e9", "Weihnachtsbaum, S. 185a: I. Psallite - Altes Weihnachtslied", "Schuberts ungarische Melodien, S. 425a: No. 1. Andante", "Via Crucis, S. 504a: Station X: J\u00e9sus est d\u00e9pouill\u00e9 de ses v\u00eatements", "Consolation in E major, S. 171a no. 1: Andante con moto", "Symphonisches Zwischenspiel zu \u00dcber allen Zauber Liebe, S. 497", "Cadenza, S. 695f", "Bist du", "Fantaisie sur des motifs de l'op\u00e9ra Lucrezia Borgia de G. Donizetti, S. 399a", "Drei St\u00fccke aus der Legende der heiligen Elisabeth, S. 498a: Nr. 1. Orchester Einleitung", "Album Leaf in A major, S. 166k", "Hungarian Rhapsody for Orchestra no. 4 in D minor, S. 359/4", "Fantasie und Fuge \u00fcber das Thema B-A-C-H, S. 529/2", "Schlummerlied mit Arabesken, S. 454", "Phantasiest\u00fcck \u00fcber Motive aus Rienzi, S. 439", "Feuille d\u2019album no. 1 in E major, S. 164", "Trois Chansons, S. 510a: No. 2. Avant la bataille", "Grande paraphrase de la Marche de Donizetti pour le Sultan Abdul-Medjid Khan, S. 403 bis", "Die Schl\u00fcsselblumen", "\u0421\u043b\u0463\u043f\u043e\u0439, S. 350", "Hungaria", "M\u00e9lodies hongroises d'apr\u00e8s Franz Schubert, S. 425: No. 2. Marche hongroise", "Dumka, S. 249b", "Postludium, S. 162f", "Totentanz, S. 126: X. Allegro animato", "Mephisto Waltz no. 4, S. 696", "Tre sonetti del Petrarca", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 8. Miserere d'apr\u00e8s Palestrina", "Im Rhein, im sch\u00f6nen Strome, S. 272/2", "Wer nie sein Brot mit Tr\u00e4nen a\u00df", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 1. Hirtengesang an der Krippe", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 6. Psaume", "Marche h\u00e9ro\u00efque, S. 510", "Von der Wiege bis zum Grabe, S. 107: I. Die Wiege", "Huit variations, op. 1, S. 148", "S'il est un charmant gazon, S. 538", "Grandes \u00e9tudes de Paganini, S. 141: No. 1. Tremolo in G minor", "Schwebe, schwebe, blaues Auge, S. 305/2", "Morceau de salon (\u00e9tude de perfectionnement), S. 142", "Die Macht der Musik", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/3", "Klavierst\u00fcck in A-flat major, S. 189", "Zwei St\u00fccke aus der heiligen Elisabeth, S. 693a: No. 1. Das Rosenmirakel", "Der Hirt", "Le Mal du pays, S. 160 no. 8", "Sarabande and Chaconne from Handel's Almira", "Douze grandes \u00e9tudes, S. 137: No. 5 in B-flat major (Equalmente)", "Hexam\u00e9ron, S. 365b: I. Introduction. Extr\u00eamement lent", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 1. Lyon", "Le Lac de Wallenstadt, S. 156/2a bis", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 7c. Allegro pastorale", "Symphonic poems", "Via Crucis, S. 53: Station XIII: Jesus wird vom Kreuz genommen", "Les Adieux, r\u00eaverie sur un motif de Rom\u00e9o et Juliette, S. 409", "Oh! quand je dors, S. 536", "Deux l\u00e9gendes, S. 175: St. Fran\u00e7ois d'Assise: la pr\u00e9dication aux oiseaux", "Valse-Impromptu, S. 213a", "Magyar dalok, S. 242: No. 5 in D-flat major", "Magyarische Litanei", "Un sospiro", "Album-Leaf: Vivace ma non troppo in D-flat major, S. 167g", "R\u00e9miniscences de Robert le Diable, S. 413", "Marche hongroise, S. 425/2e bis", "Trois Chansons, S. 510a: No. 1. La Consolation", "Douze grandes \u00e9tudes, S. 137: No. 3 in F major (Poco adagio)", "Mosonyis Grabgeleit, S. 194", "Zwei Orchesters\u00e4tze aus dem Oratorium Christus, S. 498b: Nr. 2. Die heiligen drei K\u00f6nige \u2013 Marsch", "Festpolonaise, S. 230a", "Douze grandes \u00e9tudes, S. 137: No. 2 in A minor (Molto vivace a capriccio)", "Ungarische National-Melodien (Im leichten Style bearbeitet), S. 243bis: No. 3 in B-flat major", "R\u00e9miniscences des Puritains, S. 390/1", "Von der Wiege bis zum Grabe, S. 512: II. Der Kampf ums Dasein", "Valse-Impromptu, S. 213 bis", "Valse oubli\u00e9e no. 4, S. 215 no. 4", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/2", "Vall\u00e9e d'Obermann, S. 160 no. 6", "K\u00fcnstlerfestzug zur Schillerfeier 1859, S. 520/1", "Sposalizio, S. 157a", "Valse-Impromptu", "Symphonic Poem no. 2 \\\"Tasso, Lament and Triumph\\\"", "Concerto path\u00e9tique, S. 365a", "Variation on a Waltz by Diabelli", "Piano Concerto No. 3", "Hungarian Rhapsody No. 2", "Canzonetta del Salvator Rosa, S. 157c", "Album d'un voyageur, S. 156: II. Fleurs m\u00e9lodiques des Alpes: 9c. Andantino con molto sentimento", "O sacrum convivium, S. 674a", "Hungarian Rhapsody no. 5 in E minor, S. 244 no. 5 \\\"H\u00e9ro\u00efde \u00e9l\u00e9giaque\\\"", "Weihnachtsbaum, S. 185a: III. Die Hirten an der Krippe (In dulci jubilo)", "Magyar dalok, S. 242: No. 7 in E-flat major", "Weihnachtsbaum, S. 185a: V. Scherzoso", "Un portrait en musique de la Marquise de Blocqueville, S. 190: III. M\u00eame mouvement mais avec incertitude", "Orpheus", "Ouvert\u00fcre zu Tannh\u00e4user, S. 442", "Hungarian Rhapsody no. 4 in E-flat major, S. 244 no. 4", "Hungarian Rhapsody for Orchestra no. 2 in C minor, S. 359/2", "Feuille morte, S. 428", "Soir\u00e9es de Vienne, S. 427: No. 5 in G-flat major. Moderato cantabile con affetto", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: II. Adagio", "Scherzo in G minor, S. 153", "Tre sonetti di Petrarca", "R\u00e1k\u00f3czi-Marsch, S. 244b", "Via Crucis, S. 504a: Station XIV: J\u00e9sus est mis dans le sepulcre", "Hyr\u0107, S. 166m/2", "Fantasie \u00fcber zwei Motive aus W. A. Mozarts Die Hochzeit des Figaro", "Concerto sans orchestre, S. 524a", "Von der Wiege bis zum Grabe, S. 107: II. Der Kampf ums Dasein", "Marie-Po\u00e8me, S. 701b", "Hungarian Rhapsody no. 2 in C-sharp minor, S. 244 no. 2 bis", "Chor\u00e4le, S. 506a: No. 4. Nun danket alle Gott", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259", "Grande fantaisie de concert, S. 393/2", "Angiolin dal biondo crin, S. 269/3", "Grand galop chromatique", "Pilgerchor aus Tannh\u00e4user, S. 443/2", "Impromptu in F-sharp major, S. 191/2, R. 59 \\\"Nocturne\\\"", "Spinnerin-Lied, Transkripition aus Wagners \\\"Der fliegende Holl\u00e4nder\\\", S. 440", "Weihnachtsbaum, S. 186: Nr. 6. Carillon", "Die stille Wasserrose", "Der du vom Himmel bist, S. 531 no. 5", "Marche hongroise, S. 425/2b", "Unstern! Sinistre, disastro, S. 208, R. 80", "Les Jeux d'eaux \u00e0 la Villa d'Este, S. 163 no. 4", "Weihnachtsbaum, S. 185a: XI. Ungarisch", "\u00c0 la Chapelle Sixtine, S. 360", "Die Zelle in Nonnenwerth, S. 534/1", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 6 in A major \\\"Attente\\\"", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 10. (Hymne)", "Rosario, S. 670: No. 1. Mysteria gaudiosa", "Valse-caprice No. 9 (Sehnsuchtswalzer), S. 427/9", "Via Crucis, S. 504a: Station I: J\u00e9sus est condamn\u00e9 \u00e0 mort", "Walther von der Vogelweide", "Consolation in C-sharp minor, S. 171a no. 3", "Winzerchor aus den entfesselten Prometheus, S. 692e", "Album d'un voyageur, S. 156: I. Impressions et po\u00e9sies: 5. La chapelle de Guillaume Tell", "R\u00e9miniscences de Lucrezia Borgia, S. 400: I. Trio du seconde Acte", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4 in E major", "Tre sonetti di Petrarca, S. 158: No. 1. Sonetto XLVII. Benedetto sia il giorno", "Glasgow fragment, S. 701f", "Morceau en fa majeur, S. 695", "Am Rhein", "R\u00e9miniscences des Huguenots, S. 412/2", "O lieb, so lang du lieben kannst!, S. 540a", "Piano Concerto No. 1", "Marche hongroise, S. 425/2c", "Danza sacra e duetto finale d\u2019Aida, S. 436", "\u00c0 la Chapelle Sixtine, S. 461/2", "Fantasia on Hungarian Folk Melodies for piano and orchestra, S. 123", "Mazurek, S. 166m/3", "Sonetto 104 del Petrarca", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 9. La Lampe du temple (Andante lagrimoso)", "2 Polonaises, S. 223: no. 2 in E major", "Hungarian Rhapsody No. 13", "Hungarian Rhapsody No. 19", "Fantasy on Themes from Mozart's Figaro and Don Giovanni", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 6 in A minor", "Introduction et Polonaise de l'op\u00e9ra I puritani, S. 391", "Album-Leaf in A major, S. 166s", "Technische Studien, S. 146: No. 62. Spr\u00fcnge mit der Tremolo-Begleitung", "Halloh!, Jagdchor und Steyrer aus der Oper Tony, S. 404", "Totentanz", "Variations de bravoure sur des th\u00e8mes de Paganini, S. 700/2", "Album-Leaf: Andante religiosamente in G major, S. 166j", "Trauerode (Die Todten), S. 268 no. 2", "Illustrations du Proph\u00e8te, S. 414: No. 2: Les Patineurs: Scherzo", "Piano Concerto in E-flat major, S. 125a", "Via Crucis, S. 504a: Station V: Simon le Cyr\u00e9n\u00e9en aide J\u00e9sus \u00e0 porter sa croix", "Fantasie und Fuge \u00fcber den Choral Ad nos, ad salutarem undam, S. 259: III. Fugue", "Gretchen aus Faust-Symphonie, S. 513", "Adagio non troppo, S. 151a", "Andante sensibilissimo, S. 701c", "Weihnachtsbaum, S. 186: Nr. 1. Altes Weihnacthslied (Psalite)", "Prozinsky Fragment for piano, S. 701v", "Via Crucis, S. 53: Station I: Jesus wird zum Tode verdammt", "Orage, S. 160 no. 5", "Gnomenreigen", "Chor\u00e4le, S. 506a: No. 2. Jesu Christe: Die f\u00fcnf Wunden", "Schnitterchor aus den entfesselten Prometheus, S. 507a", "Den Schutz-Engeln, S. 162a/1", "R\u00e9miniscences de La Scala, S. 458", "Pens\u00e9es \\\"Nocturne,\\\" S. 168b", "Romancero espagnol, S. 695c: No. 2. Elaboration of an unidentified theme", "Klavierst\u00fcck in D-flat major, S. 189b", "Album-Leaf: Pr\u00e9lude omnitonique, S. 166e", "Five Hungarian Folksongs, S. 245: No. 1. Lassan. Lento", "Trois Chansons, S. 510a: No. 3. L'Esp\u00e9rance", "Aus Lohengrin, S. 446: No. 2. Elsas Traum", "Valse oubli\u00e9e no. 3, S. 215 no. 3", "Abschied, S. 251", "Introduction des Variations sur une marche du Si\u00e8ge de Corinthe, S. 421a", "Album-Leaf: Andantino in E-flat, S. 163a", "Coro di festa e marcia funebre de Don Carlos, S. 435", "Wilde Jagd: Scherzo, S. 176a", "Album-Leaf (Premi\u00e8re Consolation), S. 171b", "Hamlet, S. 104", "Magyar dalok, S. 242: No. 10 in D major", "Rigoletto", "2 Cs\u00e1rd\u00e1s, S. 225: No. 1. Cs\u00e1rd\u00e1s", "Ich liebe dich, S. 542a", "Wiegenlied, S. 198", "Chanson du B\u00e9arn, S. 236 no. 2", "Via Crucis, S. 504a: Station II: J\u00e9sus est charg\u00e9 de sa croix", "Ungarische Zigeunerweisen", "Weimars Volkslied, S. 542/1", "Einzug der G\u00e4ste auf der Wartburg, S. 445 no. 1", "\u00c9tude en douze exercices, S. 136: VI. Molto agitato", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 1. Trauermarsch (Grande Marche fun\u00e8bre)", "Gastibelza, S. 540", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise I", "Hungarian Rhapsody No. 6", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 4b in E major", "Fantasie \u00fcber Motive aus Beethovens Ruinen von Athen, S. 122", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 3. B\u00e9n\u00e9diction de Dieu dans la solitude", "A magyarok Istene, S. 543bis", "Liebestraum No. 3 As-dur", "Album-Leaf: Purgatorio. Andante in B minor, S. 166r/1", "R\u00e9miniscences de Lucia di Lammermoor, S. 397", "Kaiser Wilhelm!, S. 197b", "Two Hungarian Recruiting Dances, S. 241 \\\"Zum Andenken\\\": No. 1. Kinizsi n\u00f3t\u00e1ja", "Ihr Auge, S. 310/2", "Impromptu brillant sur des th\u00e8mes de Rossini et Spontini, op. 3, S. 150", "\u00c9l\u00e9gie sur des motifs du Prince Louis Ferdinand de Prusse, S. 168/1", "Der n\u00e4chtliche Zug, S. 513a", "Via Crucis, S. 53: Station IX: Jesus f\u00e4llt zum dritten Mal", "Resignazione, S. 187a", "Cadenza for \\\"Un sospiro\\\", S. 144/3", "Hungarian Coronation Mass, S. 11: VI. Sanctus", "Ora pro nobis, S. 262", "Von der Wiege bis zum Grabe, S. 512: I. Die Wiege", "Slavimo slavno, Slaveni!, S. 503", "Harmonies po\u00e9tiques et religieuses, S. 172a: No. 8. Prose des morts - De profundis", "Album Leaf in E major, S. 166a", "Transcendental \u00c9tude No. 7", "\u00c9tudes d'ex\u00e9cution transcendante d'apr\u00e8s Paganini, S. 140: No. 2 in E-flat major", "Schwanengesang und Marsch aus Hunyadi L\u00e1szl\u00f3, S. 405", "Soir\u00e9es de Vienne, S. 427: No. 8 in D major. Allegro con brio", "Album-Leaf: Andantino in A-flat major, S. 166p", "Album-Leaf in A-flat major, S. 166l", "Allegro maestoso, S. 692c", "Marche fun\u00e8bre, S. 163 no. 6", "Liebestr\u00e4ume no. 2, S. 541a \\\"Gestorben war ich\\\"", "Les pr\u00e9ludes", "Comment, disaient-ils", "Fantaisie romantique sur deux m\u00e9lodies suisses", "Leyer und Schwert, S. 452: III. Gebet (vor der Schlacht)", "Douze grandes \u00e9tudes, S. 137: No. 4 in D minor (Allegro patetico)", "Pace non trovo", "Ein Fichtenbaum steht einsam, S. 309", "Album-Leaf in E-flat major, S. 167k", "Alleluia, S. 183 no. 1", "Melodie in Dorische Tonart, S. 701d", "Huldigungsmarsch, S. 228ii", "Fantaisie sur des motifs favoris de l'op\u00e9ra Somnambula de Bellini, S. 393/1", "Venezia e Napoli, S. 159: No. 2. Allegro", "Festkantate zur Enth\u00fcllung des Beethoven-Denkmals in Bonn, S. 67: II. Allegro deciso", "Die Loreley", "Vergiftet sind mein Lieder, S. 289/3", "Von der Wiege bis zum Grabe, S. 512: III. Die Wiege des zukunftigen Lebens", "Ave Maria (d'Arcadelt), S. 659", "Mazeppa, S. 511c", "Was tun?", "O lieb, so lang du lieben kannst, S. 298/2", "Chor\u00e4le, S. 506a: No. 11. Wer nur den lieben Gott l\u00e4sst walten?", "Canzone napolitana, S. 248a", "Eglogue, S. 160 no. 7", "Via Crucis, S. 53: Station XII: Jesus stirbt am Kreuze", "Requiem f\u00fcr die Orgel, S. 266: VII. Postludium", "Klavierst\u00fcck in F-sharp major, S. 193", "Album-Leaf: Quasi mazurek in C major, S. 163e", "A magyarok Istene, S. 543", "Album Leaf in E-flat (Leipzig), S. 164b", "Album-Leaf in A-flat (Portugal), S. 166b", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 9. Andante lagrimoso", "Fantasy and Fugue on the chorale 'Ad nos ad salutarem undam'", "Hungarian Battle March, S. 119", "Au lac de Wallenstadt, S. 160 no. 2", "Ballade No. 1", "Eine Faust-Symphonie, S. 108: IV. Chorus mysticus", "Tre sonetti di Petrarca, S. 158: No. 3. Sonetto CXXIII. I' vidi in terra angelici costumi", "Hexam\u00e9ron, S. 365b: VIIIb. Coda", "Transcendental \u00c9tude No. 10", "La romanesca, S. 252a/2", "Deux Polonaises de l'oratorio St. Stanislas, S. 519: Polonaise II", "Concerto for Piano and Orchestra no. 1 in E-flat major, S. 124: II. Quasi adagio", "Seconda mazurka di Tirindelli, S. 573a", "Angelus! Pri\u00e8re \u00e0 l'ange gardien, S. 162a/4", "Souvenir de la fianc\u00e9e, S. 385/3", "Historische ungarische Bildnisse, S. 205a: No. 3. Teleki L\u00e1szl\u00f3", "Gondoliera, S. 162 no. 1", "Aus Lohengrin, S. 446: No. 1. Festspiel und Brautlied", "Via Crucis, S. 53: Station II: Jesus tr\u00e4gt sein Kreuz", "Pr\u00e9ludes et Harmonies po\u00e9tiques et religieuses, S. 171d: No. 5 in G-flat major", "Hungarian Rhapsodies", "Tarantelle di bravura d\u2019apr\u00e8s la tarantelle de La muette de Portici, S. 386/2", "Zigeuner-Epos, S. 695b: No. 4 in C-sharp major. Animato", "Valse-caprice no. 6, S.427/6a", "Drei M\u00e4rsche von Franz Schubert, S. 426: No. 2. Grande Marche", "Introitus, S. 268 no. 1", "Glanes de Woronince, S. 249: No. 3. Complainte (Dumka)", "Zigeuner-Epos, S. 695b: No. 1 in C minor. Lento", "Historische ungarische Bildnisse, S. 205a: No. 4. E\u00f6tv\u00f6s J\u00f3zsef", "Venezia e Napoli, S. 159", "Jeanne d'Arc au b\u00fbcher, S. 293/3", "Five Hungarian Folksongs, S. 245: No. 3. Lassan. Andante", "Historische ungarische Bildnisse, S. 205a: No. 7. Mosonyi Mih\u00e1ly", "La Mandragore, S. 698", "La Tombe et la rose, S. 539", "Petite Valse, S. 695d", "Heroischer Marsch im ungarischem Stil, S. 231", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 18 in C-sharp minor", "Anfangs wollt ich fast verzagen", "Ungarischer Sturmmarsch, S. 524", "Valse oubli\u00e9e no. 2 in A-flat major, S. 215 no. 2", "Consolation in E major, S. 172 no. 5: Andantino", "Totentanz, S. 126: IX. Cadenza", "Album-Leaf in A minor (R\u00e1k\u00f3czi-Marsch), S. 164f", "Nuages gris", "Wir sind nicht Mumien", "Klavierst\u00fcck in F-sharp major, S. 192 no. 4. Andantino", "Ballade No. 2", "Album-Leaf, S. 167h", "Epithalam, S. 526", "Elsas Brautzug zum M\u00fcnster, S. 445 no. 2", "Harmonies po\u00e9tiques et religieuses, S. 173: No. 4. Pens\u00e9e des morts", "Dem Andenken Pet\u0151fis", "Magyar dalok, S. 242: No. 11 in B-flat major", "Die Trauer-Gondel (La lugubre gondola), S. 134", "Hexam\u00e9ron, S. 365b: V. Variation III di bravura - Ritornello", "Magyar kir\u00e1ly-dal, S. 544", "R\u00e1k\u00f3czi-Marsch, S. 244c", "Album-Leaf (Ah, vous dirai-je, maman), S. 163b", "Aus Lohengrin, S. 446: No. 3. Lohengrins Verweis an Elsa", "Douze grandes \u00e9tudes, S. 137: No. 12 in B-flat minor (Andantino)", "Via Crucis, S. 504a: Station IV: J\u00e9sus rencontre sa tr\u00e8s sainte m\u00e8re", "Pastorale, S. 508", "Magyar dalok, S. 242: No. 3 in D-flat major", "Magyar rapsz\u00f3di\u00e1k, S. 242: No. 17 in A minor", "Des Tages laute Stimmen schweigen", "Elegy no. 1, S. 196", "Slavimo Slavno Slaveni!", "Totentanz, S. 126: VI. Variation V", "Sonetto 47 del Petrarca, S. 161 no. 4", "Zigeuner-Epos, S. 695b: No. 5 in D flat major. Tempo giusto", "Transcendental \u00c9tude No. 3", "Eine Faust-Symphonie, S. 108: II. Gretchen. Andante soave", "R\u00e9miniscences des Puritains, S. 390/2", "Faribolo pastour, S. 236 no. 1", "Lenore, S. 346", "Consolation in D-flat major, S. 172 no. 4: Quasi adagio", "Entwurf der Ramann-Elegie, S. 196a", "Anfang einer Jugendsonate, S. 692b", "Apparitions, S. 155: No. 2. Vivamente", "Wolfram von Eschenbach", "Trois morceaux suisses, S. 156a: No. 1. Ranz de vaches", "Grande \u00e9tude d'apr\u00e8s Paganini no. 2 in E-flat major, BV B 70", "Album d'un voyageur, S. 156: III. Paraphrases: 12. Ranz de chevres [de F. Huber] - Allegro finale"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1891", "prediction": ["# Reasoning Path:\nRusso-Japanese War -> military.military_conflict.force_strengths -> m.011pns0q -> military.force_strength.resource -> 28 cm Howitzer L/10\n# Answer:\n28 cm Howitzer L/10", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Yellow Sea -> common.topic.article -> m.0df4r\n# Answer:\nYellow Sea", "# Reasoning Path:\nRusso-Japanese War -> time.event.includes_event -> Battle of Chemulpo Bay -> time.event.locations -> Incheon\n# Answer:\nBattle of Chemulpo Bay", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> location.location.events -> Battle of Hsimucheng\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Manchuria -> common.topic.notable_for -> g.125525748\n# Answer:\nManchuria", "# Reasoning Path:\nRusso-Japanese War -> base.culturalevent.event.entity_involved -> Aleksey Kuropatkin -> military.military_person.participated_in_conflicts -> World War I\n# Answer:\nAleksey Kuropatkin", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> location.location.events -> 2013 Korean Missile Crisis\n# Answer:\nKorean Peninsula", "# Reasoning Path:\nRusso-Japanese War -> time.event.locations -> Korean Peninsula -> common.topic.image -> Location of the Korean Peninsula\n# Answer:\nKorean Peninsula"], "ground_truth": ["Yellow Sea", "Korean Peninsula", "Manchuria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1892", "prediction": ["# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2012 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2012 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.instance_of_recurring_event -> Stanley Cup Finals\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> award.award_nominee.award_nominations -> m.0z1ycgx -> award.award_nomination.award -> Outstanding Team ESPY Award\n# Answer:\nOutstanding Team ESPY Award", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> time.event.locations -> Madison Square Garden\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nLos Angeles Kings -> ice_hockey.hockey_team.captain -> Dustin Brown -> people.person.place_of_birth -> Ithaca\n# Answer:\nDustin Brown", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.roster -> g.11b76d7nzz\n# Answer:\nsports.sports_team.roster", "# Reasoning Path:\nLos Angeles Kings -> sports.sports_team.championships -> 2014 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n2014 Stanley Cup Finals"], "ground_truth": ["2012 Stanley Cup Finals", "2014 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1893", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc38hnh\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> common.topic.notable_types -> Governmental Body\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1894", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\n1953 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpwy -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.season -> 1951 Major League Baseball season\n# Answer:\n1951 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.season -> 1954 Major League Baseball season\n# Answer:\n1954 Major League Baseball season"], "ground_truth": ["Montreal Royals", "Brooklyn Dodgers", "Kansas City Monarchs"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.43478260869565216, "ans_precission": 0.625, "ans_recall": 0.3333333333333333, "path_f1": 0.4761904761904762, "path_precision": 0.625, "path_recall": 0.38461538461538464, "path_ans_f1": 0.43478260869565216, "path_ans_precision": 0.625, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1895", "prediction": ["# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.scubadiving.ecoregion.contained_by -> Cold Temperate Northwest Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh6l\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> book.book_subject.works -> The St Lawrence\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> common.topic.notable_for -> g.1254yxqd3\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Action of 8 June 1755\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.body_of_water.bridges -> \u00cele d'Orl\u00e9ans Bridge -> location.location.geolocation -> m.0wmv3yc\n# Answer:\n\u00cele d'Orl\u00e9ans Bridge", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the Atlantic\n# Answer:\nGulf of Saint Lawrence", "# Reasoning Path:\nSaint Lawrence River -> geography.river.mouth -> Gulf of Saint Lawrence -> location.location.events -> Battle of the St. Lawrence\n# Answer:\nGulf of Saint Lawrence"], "ground_truth": ["Gulf of Saint Lawrence"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1896", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1897", "prediction": ["# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> fictional_universe.fictional_character.parents -> g.12175twq\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hp2g -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Aquata\n# Answer:\nAquata", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> film.film_character.portrayed_in_films -> m.0h5nlyf\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> fictional_universe.fictional_character.occupation -> Crown Princess\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> common.topic.notable_types -> Film character\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpxv -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Alana\n# Answer:\nAlana", "# Reasoning Path:\nAriel -> common.topic.notable_for -> g.1255ddcvy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.children -> Melody -> common.topic.notable_for -> g.125fjp_9q\n# Answer:\nMelody", "# Reasoning Path:\nAriel -> common.topic.article -> m.0cl19n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAriel -> fictional_universe.fictional_character.siblings -> m.065hpgf -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Arista\n# Answer:\nArista"], "ground_truth": ["Alana", "Adella", "Arista", "Attina", "Andrina", "Aquata"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.37499999999999994, "ans_precission": 0.3, "ans_recall": 0.5, "path_f1": 0.2608695652173913, "path_precision": 0.3, "path_recall": 0.23076923076923078, "path_ans_f1": 0.37499999999999994, "path_ans_precision": 0.3, "path_ans_recall": 0.5}
{"id": "WebQTest-1898", "prediction": ["# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Glenn Quagmire\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Mickey McFinnegan\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Stewie Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> Glenn Quagmire\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.05tw61d\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> film.film_story_contributor.film_story_credits -> Family Guy Presents: Stewie Griffin: The Untold Story\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe"], "ground_truth": ["Kevin Swanson", "Dr. Elmer Hartman", "Seamus", "Stewie Griffin", "God", "Glenn Quagmire", "Nate Griffin", "Mickey McFinnegan", "Tom Tucker", "Brian Griffin", "Peter Griffin", "Jasper", "Kool-aid Guy", "Jake Tucker", "Carter Pewterschmidt"], "ans_acc": 0.26666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2702702702702703, "path_precision": 0.625, "path_recall": 0.1724137931034483, "path_ans_f1": 0.3934426229508197, "path_ans_precision": 0.75, "path_ans_recall": 0.26666666666666666}
{"id": "WebQTest-1899", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.spouse -> Ida Saxton McKinley\n# Answer:\nIda Saxton McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.spouse_s -> m.0j4k7z3 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> people.person.nationality -> United States of America\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> people.person.parents -> Ida Saxton McKinley\n# Answer:\nIda McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> common.topic.notable_for -> g.125g47nt2\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Katherine McKinley -> common.topic.notable_types -> Deceased Person\n# Answer:\nKatherine McKinley", "# Reasoning Path:\nWilliam McKinley -> people.person.children -> Ida McKinley -> common.topic.notable_types -> Deceased Person\n# Answer:\nIda McKinley"], "ground_truth": ["Ida Saxton McKinley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.administrative_division.first_level_division_of -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Tickfaw River -> location.location.partially_contained_by -> m.0wg8_pz\n# Answer:\nTickfaw River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-190", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> base.lightweight.profession.specialization_of -> Producers and Directors\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> common.topic.article -> m.02g43\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> fictional_universe.character_occupation.characters_with_this_occupation -> Dawson Leery\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist"], "ground_truth": ["Film Producer", "Artist", "Screenwriter", "Illustrator", "Writer", "Television producer", "Cartoonist", "Visual Artist", "Songwriter", "Poet", "Animator"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.5161290322580644, "ans_precission": 0.8888888888888888, "ans_recall": 0.36363636363636365, "path_f1": 0.41739130434782606, "path_precision": 0.8888888888888888, "path_recall": 0.2727272727272727, "path_ans_f1": 0.5161290322580644, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-1900", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["David Prowse", "James Earl Jones"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5555555555555556, "ans_precission": 0.625, "ans_recall": 0.5, "path_f1": 0.5555555555555556, "path_precision": 0.625, "path_recall": 0.5, "path_ans_f1": 0.5555555555555556, "path_ans_precision": 0.625, "path_ans_recall": 0.5}
{"id": "WebQTest-1901", "prediction": ["# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cbz -> education.education.institution -> Ward Melville High School\n# Answer:\nWard Melville High School", "# Reasoning Path:\nKevin James -> people.person.education -> m.0cr6cc4 -> education.education.institution -> State University of New York at Cortland\n# Answer:\nState University of New York at Cortland", "# Reasoning Path:\nKevin James -> film.actor.film -> m.0wc78q8 -> film.performance.film -> Caf\u00e9 Flesh\n# Answer:\nCaf\u00e9 Flesh", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010_t38m -> film.performance.character -> Will Cooper\n# Answer:\nWill Cooper", "# Reasoning Path:\nKevin James -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKevin James -> tv.tv_actor.guest_roles -> m.0bv0l6h -> tv.tv_guest_role.episodes_appeared_in -> Team Nogueira vs Team Mir Finale\n# Answer:\nTeam Nogueira vs Team Mir Finale", "# Reasoning Path:\nKevin James -> film.actor.film -> m.010x46kw -> film.performance.film -> Paul Blart: Mall Cop 2\n# Answer:\nPaul Blart: Mall Cop 2", "# Reasoning Path:\nKevin James -> people.person.places_lived -> m.0gyp6fb -> people.place_lived.location -> Stony Brook\n# Answer:\nStony Brook"], "ground_truth": ["State University of New York at Cortland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1902", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Braunau am Inn District\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> common.topic.webpage -> m.03l75z9\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Angela Hitler\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.geolocation -> m.02_hljt\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.contains -> Bogenhofen Seminary\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.containedby -> Upper Austria\n# Answer:\nBraunau am Inn", "# Reasoning Path:\nAdolf Hitler -> people.person.place_of_birth -> Braunau am Inn -> location.location.people_born_here -> Daniela Raschhofer\n# Answer:\nBraunau am Inn"], "ground_truth": ["1889-04-20"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1904", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> common.topic.notable_types -> Sports Facility\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field -> sports.sports_facility.teams -> Atlanta Braves\n# Answer:\nTurner Field", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium"], "ground_truth": ["Cobb Energy Performing Arts Centre", "Atlanta Jewish Film Festival", "Margaret Mitchell House & Museum", "Four Seasons Hotel Atlanta", "World of Coca-Cola", "Masquerade", "Turner Field", "Fernbank Museum of Natural History", "Omni Coliseum", "Atlanta History Center", "Fernbank Science Center", "CNN Center", "Georgia State Capitol", "Philips Arena", "Atlanta Ballet", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Georgia World Congress Center", "Fox Theatre", "Woodruff Arts Center", "Hyatt Regency Atlanta", "Atlanta Symphony Orchestra", "Martin Luther King, Jr. National Historic Site", "Underground Atlanta", "Six Flags White Water", "Zoo Atlanta", "The Tabernacle", "Jimmy Carter Library and Museum", "Variety Playhouse", "Arbor Place Mall", "Peachtree Road Race", "Atlanta Marriott Marquis", "Georgia Aquarium", "Six Flags Over Georgia", "Center for Puppetry Arts", "Georgia Dome"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 1.0, "ans_recall": 0.1111111111111111, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 1.0, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1905", "prediction": ["# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> Lesser Poland\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Auschwitz-Birkenau State Museum -> location.location.containedby -> Poland\n# Answer:\nAuschwitz-Birkenau State Museum", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.containedby -> O\u015bwi\u0119cim County\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> sports.sports_team_location.teams -> TH Unia O\u015bwi\u0119cim\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> Poland -> location.location.containedby -> Europe\n# Answer:\nPoland", "# Reasoning Path:\nAuschwitz concentration camp -> common.topic.article -> m.0w06\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> location.location.people_born_here -> Agnieszka Wr\u00f3blewska\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> common.topic.notable_types -> City/Town/Village\n# Answer:\nO\u015bwi\u0119cim", "# Reasoning Path:\nAuschwitz concentration camp -> common.topic.notable_for -> g.125dgvrnw\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAuschwitz concentration camp -> location.location.containedby -> O\u015bwi\u0119cim -> common.topic.notable_for -> g.1258m_gdq\n# Answer:\nO\u015bwi\u0119cim"], "ground_truth": ["O\u015bwi\u0119cim", "Poland", "Auschwitz-Birkenau State Museum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-1907", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Republic\n# Answer:\nRepublic", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_types -> Currency\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_for -> g.12565x5sk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Spain\n# Answer:\nUnitary state", "# Reasoning Path:\nDominican Republic -> location.statistical_region.electricity_consumption_per_capita -> g.1245_22hm\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Martinique\n# Answer:\nUnitary state"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1909", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.language_family -> Slavic languages\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language"], "ground_truth": ["German Language", "Hungarian language", "Czech Language", "Romani language", "Slovak Language", "Ukrainian Language", "Serbian language", "Croatian language", "Polish Language", "Russian Language", "Rusyn Language", "Bulgarian Language", "Greek Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-191", "prediction": ["# Reasoning Path:\nDrew Brees -> people.person.education -> m.0j_8p_y -> education.education.institution -> Stanford Graduate School of Business\n# Answer:\nStanford Graduate School of Business", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.institution -> Purdue University\n# Answer:\nPurdue University", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.052vk4c -> education.education.institution -> Westlake High School\n# Answer:\nWestlake High School", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07nvq27 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nDrew Brees -> people.person.education -> m.03gkndb -> education.education.major_field_of_study -> Industrial management\n# Answer:\nIndustrial management", "# Reasoning Path:\nDrew Brees -> people.person.parents -> Eugene Wilson Brees II -> people.person.gender -> Male\n# Answer:\nEugene Wilson Brees II", "# Reasoning Path:\nDrew Brees -> american_football.football_player.games -> m.07sh5y9 -> american_football.player_game_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nDrew Brees -> american_football.football_player.rushing -> m.09tctcx -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Purdue University", "Stanford Graduate School of Business"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1910", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Thomas Quiney\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Gilbert Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.people_born_here -> Edmund Shakespeare\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> travel.travel_destination.tourist_attractions -> Shakespeare's Birthplace\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.deceased_person.place_of_death -> Stratford-upon-Avon -> location.location.people_born_here -> Thomas Quiney\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.contains -> Church of the Holy Trinity, Stratford-upon-Avon\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> people.person.place_of_birth -> Stratford-upon-Avon -> location.location.containedby -> United Kingdom\n# Answer:\nStratford-upon-Avon", "# Reasoning Path:\nWilliam Shakespeare -> freebase.valuenotation.is_reviewed -> Art Form\n# Answer:\nArt Form"], "ground_truth": ["Stratford-upon-Avon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1911", "prediction": ["# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> education.education.institution -> Princeton University\n# Answer:\nPrinceton University", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> education.education.institution -> St. Paul Academy and Summit School\n# Answer:\nSt. Paul Academy and Summit School", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0klvz -> education.education.institution -> Nardin Academy\n# Answer:\nNardin Academy", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.02h52nm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nF. Scott Fitzgerald -> people.person.education -> m.0n0mxgb -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nF. Scott Fitzgerald -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.parent_disease -> Cardiovascular disease\n# Answer:\nheart attack"], "ground_truth": ["Princeton University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1912", "prediction": ["# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Male\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Male\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nheart attack", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.symptoms -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Diabetes mellitus\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nJoan Crawford -> film.actor.film -> m.0115tf9q -> film.performance.character -> Della Chappell\n# Answer:\nDella Chappell"], "ground_truth": ["Pancreatic cancer", "heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1913", "prediction": ["# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> education.education.institution -> USC Marshall School of Business\n# Answer:\nUSC Marshall School of Business", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nRob Kardashian -> celebrities.celebrity.sexual_relationships -> m.0nh4s8y -> celebrities.romantic_relationship.celebrity -> Rita Ora\n# Answer:\nRita Ora", "# Reasoning Path:\nRob Kardashian -> people.person.education -> m.0n1n2gl -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nRob Kardashian -> people.person.sibling_s -> m.0pz33jr -> people.sibling_relationship.sibling -> Casey Jenner\n# Answer:\nCasey Jenner", "# Reasoning Path:\nRob Kardashian -> people.person.religion -> Catholicism -> base.popstra.religion.membership -> m.0n5mz2l\n# Answer:\nCatholicism"], "ground_truth": ["USC Marshall School of Business"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1914", "prediction": ["# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.season -> 1996\u201397 NBA season\n# Answer:\n1996\u201397 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.team -> Philadelphia 76ers\n# Answer:\nPhiladelphia 76ers", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxpm -> sports.sports_award.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.04ynxq3 -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nAllen Iverson -> basketball.basketball_player.player_statistics -> m.04qrrxn -> basketball.basketball_player_stats.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.02kbc0l -> sports.sports_award.award -> NBA Rookie of the Year Award\n# Answer:\nNBA Rookie of the Year Award", "# Reasoning Path:\nAllen Iverson -> sports.sports_award_winner.awards -> m.0tlr44k -> sports.sports_award.season -> 2000\u201301 NBA season\n# Answer:\n2000\u201301 NBA season", "# Reasoning Path:\nAllen Iverson -> film.person_or_entity_appearing_in_film.films -> m.0vb4h71 -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["2000\u201301 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1915", "prediction": ["# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.0n19ssz -> education.education.institution -> University of Missouri\u2013Kansas City School of Law\n# Answer:\nUniversity of Missouri\u2013Kansas City School of Law", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.institution -> University of Missouri\u2013Kansas City\n# Answer:\nUniversity of Missouri\u2013Kansas City", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04hc6_w -> education.education.institution -> William Chrisman High School\n# Answer:\nWilliam Chrisman High School", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nHarry S. Truman -> people.person.education -> m.04t5ls_ -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.office_holder -> J. Howard Rossbach\n# Answer:\nJ. Howard Rossbach", "# Reasoning Path:\nHarry S. Truman -> book.author.works_written -> Year of Decisions -> freebase.valuenotation.has_value -> Date written\n# Answer:\nYear of Decisions", "# Reasoning Path:\nHarry S. Truman -> government.political_appointer.appointees -> m.03p40b5 -> government.government_position_held.governmental_body -> U.S. Securities and Exchange Commission\n# Answer:\nU.S. Securities and Exchange Commission"], "ground_truth": ["University of Missouri\u2013Kansas City School of Law", "William Chrisman High School", "University of Missouri\u2013Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1916", "prediction": ["# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> Utica-Rome, NY Metropolitan Statistical Area\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.us_county.hud_county_place -> Barneveld\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.hud_county_place.county -> Oneida County -> location.location.containedby -> New York\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.statistical_region.population -> g.11b66mljjp\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nUtica -> location.location.containedby -> Oneida County -> location.statistical_region.population -> g.11b66g7msw\n# Answer:\nOneida County", "# Reasoning Path:\nUtica -> location.statistical_region.population -> g.11bymks6z8\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Oneida County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1917", "prediction": ["# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> LondonScienceMuseumsReplicaDifferenceEngine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.notable_types -> Invention\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.article -> m.029x2\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> base.argumentmaps.innovator.original_ideas -> Analytical Engine -> common.topic.article -> m.0ng8\n# Answer:\nAnalytical Engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> common.topic.image -> Babbage Difference Engine\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Difference engine -> base.schemastaging.context_name.pronunciation -> g.125_nqpp3\n# Answer:\nDifference engine", "# Reasoning Path:\nCharles Babbage -> law.inventor.inventions -> Analytical Engine -> common.topic.image -> AnalyticalMachine Babbage London\n# Answer:\nAnalytical Engine"], "ground_truth": ["Analytical Engine", "Difference engine"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1919", "prediction": ["# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.actor -> Sacha Baron Cohen\n# Answer:\nSacha Baron Cohen", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.character -> Julien\n# Answer:\nJulien", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02_ -> film.performance.actor -> Andy Richter\n# Answer:\nAndy Richter", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02p -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.03ld02v -> film.performance.actor -> Cedric the Entertainer\n# Answer:\nCedric the Entertainer", "# Reasoning Path:\nMadagascar -> film.film.rating -> PG (USA) -> film.content_rating.film_rating_system -> Motion Picture Association of America film rating system\n# Answer:\nPG (USA)", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.0cg0_wz -> film.performance.actor -> Chris Miller\n# Answer:\nChris Miller", "# Reasoning Path:\nMadagascar -> film.film.starring -> m.0cj5p02 -> film.performance.actor -> Christopher Knights\n# Answer:\nChristopher Knights", "# Reasoning Path:\nMadagascar -> common.topic.notable_for -> g.12587ywd1\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Sacha Baron Cohen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1920", "prediction": ["# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> common.topic.article -> m.0btx2m\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0101bdn_\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Businessperson -> common.topic.notable_types -> Profession\n# Answer:\nBusinessperson", "# Reasoning Path:\nRick Scott -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nRick Scott -> people.person.profession -> Investor -> common.topic.notable_types -> Profession\n# Answer:\nInvestor", "# Reasoning Path:\nRick Scott -> people.person.profession -> Politician -> base.schemastaging.context_name.pronunciation -> g.125_lw10d\n# Answer:\nPolitician"], "ground_truth": ["Investor", "Lawyer", "Businessperson", "Politician", "Executive officer"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.7164179104477612, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.6}
{"id": "WebQTest-1921", "prediction": ["# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bakhchysarai Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Bilohirsk Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> Crimean Campaign\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.administrative_division.capital -> m.0103gkw3\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> base.aareas.schema.administrative_area.administrative_children -> Chornomorske Raion\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Autonomous Republic of Crimea -> location.location.events -> Austro-Russian\u2013Turkish War\n# Answer:\nAutonomous Republic of Crimea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> geography.body_of_water.islands -> Babyn Island\n# Answer:\nBlack Sea", "# Reasoning Path:\nCrimean War -> time.event.locations -> Black Sea -> common.topic.notable_types -> Body Of Water\n# Answer:\nBlack Sea"], "ground_truth": ["Baltic Sea", "Black Sea", "Caucasus", "Crimea", "Autonomous Republic of Crimea", "Balkans", "White Sea"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.4444444444444445, "path_precision": 1.0, "path_recall": 0.2857142857142857, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-1922", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.0bqsx7p -> government.government_position_held.basic_title -> Member of the Legislative Assembly\n# Answer:\nMember of the Legislative Assembly", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.title -> President\n# Answer:\nPresident", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03q2hyl -> government.government_position_held.basic_title -> Assistant Secretary\n# Answer:\nAssistant Secretary", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.03fx899 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTheodore Roosevelt -> people.person.employment_history -> m.0w7nv3t -> business.employment_tenure.company -> Federal government of the United States\n# Answer:\nFederal government of the United States", "# Reasoning Path:\nTheodore Roosevelt -> government.politician.government_positions_held -> m.04ml2sj -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor"], "ground_truth": ["President", "Vice President", "Governor", "Member of the Legislative Assembly", "Assistant Secretary"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.75, "ans_recall": 0.8, "path_f1": 0.625, "path_precision": 0.625, "path_recall": 0.625, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.75, "path_ans_recall": 0.8}
{"id": "WebQTest-1923", "prediction": ["# Reasoning Path:\nMiley Cyrus -> award.award_winner.awards_won -> m.0z8yvqj -> award.award_honor.honored_for -> Hannah Montana: The Movie\n# Answer:\nHannah Montana: The Movie", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> LOL\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> media_common.adapted_work.adaptations -> Hannah Montana: The Movie\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.parents -> Tish Cyrus -> film.producer.film -> So Undercover\n# Answer:\nTish Cyrus", "# Reasoning Path:\nMiley Cyrus -> award.award_winner.awards_won -> m.0z8yvqj -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\n2009 Teen Choice Awards", "# Reasoning Path:\nMiley Cyrus -> award.award_winner.awards_won -> m.010lkq4r -> award.award_honor.honored_for -> Wrecking Ball\n# Answer:\nWrecking Ball", "# Reasoning Path:\nMiley Cyrus -> award.award_winner.awards_won -> m.0z8yvqj -> award.award_honor.award -> Teen Choice Award for Choice Movie Actress: Music/Dance\n# Answer:\nTeen Choice Award for Choice Movie Actress: Music/Dance", "# Reasoning Path:\nMiley Cyrus -> award.award_winner.awards_won -> m.010lkk82 -> award.award_honor.award -> Billboard Music Award for Top Streaming Artist\n# Answer:\nBillboard Music Award for Top Streaming Artist"], "ground_truth": ["LOL", "High School Musical 2", "Big Fish", "Radio Disney Party Jams: The Concert", "Wizards on Deck with Hannah Montana", "Miley Cyrus: Tongue Tied", "Rock Mafia Presents: The Big Bang", "Sonic the Hedgehog", "The Last Song", "The World According to Miley Cyrus", "Hannah Montana and Miley Cyrus: Best of Both Worlds Concert", "Family Bond", "Hannah Montana: The Movie", "Super Rhino", "Bolt", "Sex and the City 2", "So Undercover"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0.07999999999999999, "ans_precission": 0.125, "ans_recall": 0.058823529411764705, "path_f1": 0.19047619047619047, "path_precision": 0.5, "path_recall": 0.11764705882352941, "path_ans_f1": 0.2608695652173913, "path_ans_precision": 0.5, "path_ans_recall": 0.17647058823529413}
{"id": "WebQTest-1925", "prediction": ["# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award_nominee -> Alison Pill\n# Answer:\nAlison Pill", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.ceremony -> 15th Screen Actors Guild Awards\n# Answer:\n15th Screen Actors Guild Awards", "# Reasoning Path:\nMilk -> award.award_winning_work.awards_won -> m.0nj7_zd -> award.award_honor.award -> Independent Spirit Award for Best Supporting Male\n# Answer:\nIndependent Spirit Award for Best Supporting Male", "# Reasoning Path:\nMilk -> award.award_winning_work.awards_won -> m.05h0msj -> award.award_honor.ceremony -> 81st Academy Awards\n# Answer:\n81st Academy Awards", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award -> Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture", "# Reasoning Path:\nMilk -> film.film.executive_produced_by -> William Horberg -> film.producer.film -> Cold Mountain\n# Answer:\nWilliam Horberg", "# Reasoning Path:\nMilk -> award.award_nominated_work.award_nominations -> m.0b4d54c -> award.award_nomination.award_nominee -> Denis O'Hare\n# Answer:\nDenis O'Hare", "# Reasoning Path:\nMilk -> award.award_winning_work.awards_won -> m.0nj7_zd -> award.award_honor.award_winner -> James Franco\n# Answer:\nJames Franco"], "ground_truth": ["Scott Smith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1926", "prediction": ["# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.country.official_language -> Arabic Language\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> Battle of Bin Jawad\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> Libyan Civil War\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.includes_event -> Operation Unified Protector -> time.event.locations -> Mediterranean Sea\n# Answer:\nOperation Unified Protector", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> Zawiya skirmish\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> 1986 United States bombing of Libya\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> common.topic.article -> m.0gh674r\n# Answer:\ncommon.topic.article", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.location.events -> 2010 Arab League summit\n# Answer:\nLibya", "# Reasoning Path:\n2011 military intervention in Libya -> time.event.locations -> Libya -> location.statistical_region.consumer_price_index -> g.11b71rqxvx\n# Answer:\nLibya"], "ground_truth": ["2011-03-19"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1928", "prediction": ["# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.team -> UMMC Ekaterinburg\n# Answer:\nUMMC Ekaterinburg", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> sports.sports_team_roster.position -> Guard\n# Answer:\nGuard", "# Reasoning Path:\nDiana Taurasi -> base.schemastaging.athlete_extra.salary -> m.0ng6vr5 -> base.schemastaging.athlete_salary.team -> Phoenix Mercury\n# Answer:\nPhoenix Mercury", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0kc2_tr -> sports.sports_team_roster.team -> Phoenix Mercury\n# Answer:\nPhoenix Mercury", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_gj -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z3t_lb -> sports.sports_team_roster.team -> WBC Spartak Moscow Region\n# Answer:\nWBC Spartak Moscow Region", "# Reasoning Path:\nDiana Taurasi -> people.person.nationality -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nDiana Taurasi -> sports.pro_athlete.teams -> m.0z6bvmq -> sports.sports_team_roster.team -> Connecticut Huskies women's basketball\n# Answer:\nConnecticut Huskies women's basketball"], "ground_truth": ["UMMC Ekaterinburg", "Phoenix Mercury"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-1929", "prediction": ["# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> travel.tourist_attraction.near_travel_destination -> Blois\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.notable_for -> g.1256sch6w\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> architecture.structure.architectural_style -> Neoclassicism\n# Answer:\nArc de Triomphe", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.article -> m.0m6_b\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> architecture.structure.architect -> Pierre Nepveu\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Galerie Claude Bernard -> common.topic.notable_types -> Museum\n# Answer:\nGalerie Claude Bernard", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Ch\u00e2teau de Chambord -> common.topic.image -> Chateau de Chambord\n# Answer:\nCh\u00e2teau de Chambord", "# Reasoning Path:\nParis -> travel.travel_destination.tourist_attractions -> Arc de Triomphe -> location.location.containedby -> France\n# Answer:\nArc de Triomphe"], "ground_truth": ["Centre Georges Pompidou", "Mus\u00e9e d'Orsay", "Panth\u00e9on", "Place de la Concorde", "Parc Ast\u00e9rix", "Sacr\u00e9-C\u0153ur, Paris", "Mus\u00e9e du quai Branly", "Arc de Triomphe", "Grande Arche", "Mus\u00e9e Maillol", "Bois de Boulogne", "Ch\u00e2teau de Chambord", "Galerie Claude Bernard", "Caf\u00e9 Volpini", "Galerie nationale du Jeu de Paume", "Champs-\u00c9lys\u00e9es", "Jardin du Luxembourg", "Les Invalides", "Eiffel Tower", "Notre Dame de Paris", "The Louvre", "La Maison Rouge", "Petit Palais", "Disneyland Paris", "Gare d'Orsay", "Folies Berg\u00e8re", "Mus\u00e9e de l'Orangerie", "Verdon Gorge", "\u00cele de la Cit\u00e9", "Mus\u00e9e des Arts et M\u00e9tiers", "Disneyland Park"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.17647058823529413, "ans_precission": 1.0, "ans_recall": 0.0967741935483871, "path_f1": 0.17647058823529413, "path_precision": 1.0, "path_recall": 0.0967741935483871, "path_ans_f1": 0.17647058823529413, "path_ans_precision": 1.0, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-193", "prediction": ["# Reasoning Path:\nBen Roethlisberger -> people.person.education -> m.02wns4p -> education.education.institution -> Miami University\n# Answer:\nMiami University", "# Reasoning Path:\nBen Roethlisberger -> common.image.size -> m.03tbgkc\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.09tcd8t -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.09tcd5t -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nBen Roethlisberger -> common.topic.webpage -> m.09wdw9_ -> common.webpage.resource -> Ratings: 'America's Got Talent' vs. 'Big Brother'\n# Answer:\nRatings: 'America's Got Talent' vs. 'Big Brother'", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.09tcd5t -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.rushing -> m.07916fp -> american_football.player_rushing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nBen Roethlisberger -> american_football.football_player.passing -> m.07916ct -> american_football.player_passing_statistics.team -> Pittsburgh Steelers\n# Answer:\nPittsburgh Steelers"], "ground_truth": ["Miami University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1932", "prediction": ["# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> royalty.monarch.kingdom -> United Kingdom\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.religion -> Protestantism\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> people.person.nationality -> United Kingdom\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> royalty.kingdom.rulers -> Elizabeth II -> government.politician.government_positions_held -> m.04g8pm8\n# Answer:\nElizabeth II", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> government.governmental_jurisdiction.governing_officials -> m.04kc_7_ -> government.government_position_held.governmental_body -> Parliament of Canada\n# Answer:\nParliament of Canada", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> South Africa\n# Answer:\nArabic Language"], "ground_truth": ["Parliament of Canada", "House of Commons of Canada"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2, "path_ans_precision": 0.125, "path_ans_recall": 0.5}
{"id": "WebQTest-1933", "prediction": ["# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.draft -> 2006 NBA draft\n# Answer:\n2006 NBA draft", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nCharlotte Hornets", "# Reasoning Path:\nAdam Morrison -> sports.pro_athlete.sports_played_professionally -> m.0c54_3g -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nAdam Morrison -> sports.drafted_athlete.drafted -> m.04_bzh1 -> sports.sports_league_draft_pick.school -> Gonzaga University\n# Answer:\nGonzaga University", "# Reasoning Path:\nAdam Morrison -> common.topic.notable_for -> g.1255m8g5p\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nAdam Morrison -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["2006 NBA draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1934", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.origin -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> people.person.place_of_birth -> Gary -> location.location.containedby -> United States of America\n# Answer:\nGary", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> film.person_or_entity_appearing_in_film.films -> m.010wz0tg -> film.personal_film_appearance.film -> Live at Wembley July 16, 1988\n# Answer:\nLive at Wembley July 16, 1988", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> 1Club.FM: V101 -> broadcast.content.artist -> Janet Jackson\n# Answer:\n1Club.FM: V101", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WSJT -> broadcast.content.artist -> Janet Jackson\n# Answer:\nWSJT", "# Reasoning Path:\nMichael Jackson -> broadcast.artist.content -> WSJT -> common.topic.notable_types -> Broadcast Content\n# Answer:\nWSJT", "# Reasoning Path:\nMichael Jackson -> music.producer.releases_produced -> Yours Faithfully -> common.topic.notable_types -> Musical Release\n# Answer:\nYours Faithfully"], "ground_truth": ["Gary"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1935", "prediction": ["# Reasoning Path:\nMaya Moore -> people.person.education -> m.04mg02t -> education.education.institution -> University of Connecticut\n# Answer:\nUniversity of Connecticut", "# Reasoning Path:\nMaya Moore -> common.topic.notable_for -> g.125cckxj9\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaya Moore -> people.person.education -> m.040ggtj -> education.education.institution -> Collins Hill High School\n# Answer:\nCollins Hill High School", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.team -> Minnesota Lynx\n# Answer:\nMinnesota Lynx", "# Reasoning Path:\nMaya Moore -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nMaya Moore -> base.schemastaging.athlete_extra.salary -> m.0nghzz3 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMaya Moore -> sports.pro_athlete.sports_played_professionally -> m.0d68m66 -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nMaya Moore -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nMaya Moore -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["University of Connecticut"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-1936", "prediction": ["# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Dodger Stadium\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.instance_of_recurring_event -> World Series\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> common.topic.notable_for -> g.1254xttcl\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> sports.sports_championship_event.season -> 1988 Major League Baseball Season\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1988 World Series -> time.event.locations -> Oakland-Alameda County Coliseum\n# Answer:\n1988 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> sports.sports_team.championships -> 1981 World Series -> time.event.locations -> Yankee Stadium\n# Answer:\n1981 World Series", "# Reasoning Path:\nLos Angeles Dodgers -> award.award_nominee.award_nominations -> m.0sgq5v3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nLos Angeles Dodgers -> baseball.baseball_team.team_stats -> m.05n69_9 -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\n1965 Major League Baseball Season"], "ground_truth": ["1988 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-1937", "prediction": ["# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jack Aubrey\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.schemastaging.context_name.pronunciation -> g.125_nnp67\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Alfred King\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> base.descriptive_names.names.descriptive_name -> m.0105cyc4\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> people.profession.specialization_of -> Navigator\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.sibling_s -> m.0n9qh71 -> people.sibling_relationship.sibling -> William Cook\n# Answer:\nWilliam Cook", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> common.topic.notable_types -> Profession\n# Answer:\nSailor", "# Reasoning Path:\nJames Cook -> people.person.profession -> Sailor -> fictional_universe.character_occupation.characters_with_this_occupation -> Bussell\n# Answer:\nSailor"], "ground_truth": ["Sailor", "Explorer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-1938", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> people.marriage.location_of_ceremony -> St. Helena\n# Answer:\nSt. Helena", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zak Williams -> people.person.place_of_birth -> San Francisco\n# Answer:\nZak Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> people.marriage.spouse -> Susan Schneider\n# Answer:\nSusan Schneider", "# Reasoning Path:\nRobin Williams -> people.person.children -> Zelda Rae Williams -> people.person.place_of_birth -> New York City\n# Answer:\nZelda Rae Williams", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0j11z6t -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRobin Williams -> people.person.spouse_s -> m.0h2n9t4 -> people.marriage.spouse -> Marsha Garces\n# Answer:\nMarsha Garces", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> San Francisco\n# Answer:\nMrs. Doubtfire"], "ground_truth": ["St. Helena"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1939", "prediction": ["# Reasoning Path:\nCanada -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6g_94\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.1hhc38hnd\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nCanada -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wjvq3\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> film.film_location.featured_in_films -> 12 Men of Christmas -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\n12 Men of Christmas", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Belgium\n# Answer:\nParliamentary system", "# Reasoning Path:\nCanada -> location.country.form_of_government -> Federal monarchy -> government.form_of_government.countries -> Australia\n# Answer:\nFederal monarchy"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-194", "prediction": ["# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.province.capital -> Toronto\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.first_level_division_of -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> location.administrative_division.country -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.people -> Veronica Webb -> people.person.ethnicity -> African American\n# Answer:\nVeronica Webb", "# Reasoning Path:\nIroquois -> common.topic.article -> m.03x21\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nQu\u00e9bec", "# Reasoning Path:\nIroquois -> people.ethnicity.included_in_group -> Native Americans in the United States -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nNative Americans in the United States", "# Reasoning Path:\nIroquois -> people.ethnicity.geographic_distribution -> Qu\u00e9bec -> location.administrative_division.first_level_division_of -> Canada\n# Answer:\nQu\u00e9bec"], "ground_truth": ["Qu\u00e9bec", "Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1940", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.notable_for -> g.1yl5ndjfk\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> people.person.religion -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir"], "ground_truth": ["Allah", "Ramdev Pir", "God"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1941", "prediction": ["# Reasoning Path:\nPeru -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Spain\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Republic\n# Answer:\nRepublic", "# Reasoning Path:\nPeru -> location.statistical_region.external_debt_stock -> g.11b71n53cl\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Guyana\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nPeru -> common.topic.article -> m.016w_5\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPeru -> location.statistical_region.energy_use_per_capita -> g.1245_8rt3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bolivia\n# Answer:\nUnitary state", "# Reasoning Path:\nPeru -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Argentina\n# Answer:\nPresidential system"], "ground_truth": ["Republic", "Constitutional republic", "Unitary state", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-1942", "prediction": ["# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> base.crime.crime_victim.crime_type -> Assassination\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> people.person.gender -> Male\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> base.nobelprizes.nobel_prize_winner.nobel_honor -> m.065nt14\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> people.person.ethnicity -> Jewish people\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yitzhak Rabin -> people.person.nationality -> Israel\n# Answer:\nYitzhak Rabin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> people.person.profession -> Politician\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Menachem Begin -> military.military_person.service -> m.059d8ry\n# Answer:\nMenachem Begin", "# Reasoning Path:\nIsraeli\u2013Palestinian conflict -> time.event.people_involved -> Yasser Arafat -> people.person.sibling_s -> m.0jvh0yf\n# Answer:\nYasser Arafat"], "ground_truth": ["Hussein of Jordan", "Hanan Ashrawi", "Ahmed Yassin", "Abdullah of Saudi Arabia", "Yitzhak Rabin", "Mahmoud Abbas", "Anthony Zinni", "Nabil Shaath", "Dalal Mughrabi", "Menachem Begin", "Yasser Arafat", "Ahmad Shukeiri", "Shimon Peres", "Haj Amin al-Husseini", "Ahmed Qurei", "Anwar Sadat", "Marwan Barghouti", "Colin Powell", "Chaim Weizmann", "David Ben-Gurion", "Ariel Sharon"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 1.0, "ans_recall": 0.14285714285714285, "path_f1": 0.25, "path_precision": 1.0, "path_recall": 0.14285714285714285, "path_ans_f1": 0.25, "path_ans_precision": 1.0, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-1943", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.parents -> H. L. Hunt\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> common.topic.notable_for -> g.1257wg7mp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> symbols.name_source.namesakes -> Lamar Hunt U.S. Open Cup\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.education -> m.0hswbpc\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> organization.organization_founder.organizations_founded -> Adams Resources & Energy\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> common.topic.notable_types -> Sports Association -> freebase.type_profile.equivalent_topic -> Sports league\n# Answer:\nSports Association"], "ground_truth": ["Bud Adams", "Lamar Hunt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-1944", "prediction": ["# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Madrid\u2013Torrej\u00f3n Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Adolfo Su\u00e1rez Madrid\u2013Barajas Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.03l85_5\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Cuatro Vientos Airport\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> travel.travel_destination.tourist_attractions -> Buen Retiro Park\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.location.nearby_airports -> Getafe Air Base\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> common.topic.webpage -> m.052jg5b\n# Answer:\nMadrid", "# Reasoning Path:\nDiego Vel\u00e1zquez -> people.deceased_person.place_of_death -> Madrid -> location.statistical_region.population -> g.11b7vbxnhf\n# Answer:\nMadrid"], "ground_truth": ["Madrid"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1946", "prediction": ["# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Guatemala\n# Answer:\nConstitutional republic", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guatemala\n# Answer:\nPresidential system"], "ground_truth": ["mx"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1947", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.long_term_unemployment_rate -> g.12cp_k6s5\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3d66v\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_kjqq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> common.topic.notable_types -> War\n# Answer:\nSonderbund War", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> base.culturalevent.event.entity_involved -> Canton of Fribourg\n# Answer:\nSonderbund War", "# Reasoning Path:\nSwitzerland -> location.location.events -> Sonderbund War -> base.culturalevent.event.entity_involved -> Canton of Zug\n# Answer:\nSonderbund War"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-195", "prediction": ["# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Tom Petty\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.notable_types -> Guitar\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> The Edge\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.group_member.instruments_played -> Bass guitar -> common.topic.notable_types -> Musical instrument\n# Answer:\nBass guitar", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> Tom Petty\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> common.topic.image -> Rickenbacker 360-12WB 12 String\n# Answer:\nRickenbacker 360/12", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Eric Clapton\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nGeorge Harrison -> music.guitarist.guitars_played -> Rickenbacker 360/12 -> music.guitar.guitarists -> The Edge\n# Answer:\nRickenbacker 360/12"], "ground_truth": ["Fender Stratocaster", "Rickenbacker 360/12"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1950", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Virginia\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> travel.tourist_attraction.near_travel_destination -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> base.schemastaging.context_name.pronunciation -> g.125_rp7jg\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> location.location.containedby -> Arlington County\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_burial -> Arlington National Cemetery -> common.topic.image -> Tombstones at Arlington National Cemetery.\n# Answer:\nArlington National Cemetery", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nJohn F. Kennedy Jr.", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> time.event.locations -> Dealey Plaza\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Arlington National Cemetery"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1952", "prediction": ["# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> religion.place_of_worship.religion -> Islam\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.containedby -> Sharifate of Mecca\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> travel.travel_destination.tourist_attractions -> Thour Mountain\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.time_zones -> East Africa Time Zone\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> travel.travel_destination.tourist_attractions -> Al Ja\u00e1ranah Mosque\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> influence.influence_node.influenced -> Muhammad ibn Ya'qub al-Kulayni -> influence.influence_node.influenced_by -> Ibn Babawayh\n# Answer:\nMuhammad ibn Ya'qub al-Kulayni", "# Reasoning Path:\nMuhammad in Islam -> people.person.place_of_birth -> Mecca -> location.location.contains -> Umm al-Qura University\n# Answer:\nMecca", "# Reasoning Path:\nMuhammad in Islam -> book.author.works_written -> Sayings of Mohammed -> common.topic.notable_types -> Book\n# Answer:\nSayings of Mohammed"], "ground_truth": ["Mecca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-1954", "prediction": ["# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Russia\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Second Polish Republic\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.region -> Europe\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.writing_system -> Cyrillic script\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Ukrainian Language -> common.topic.notable_types -> Human Language\n# Answer:\nUkrainian Language", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Hungary\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.country.languages_spoken -> Hungarian language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nHungarian language", "# Reasoning Path:\nUkraine -> location.statistical_region.poverty_rate_2dollars_per_day -> g.12cp_kbnk\n# Answer:\nlocation.statistical_region.poverty_rate_2dollars_per_day"], "ground_truth": ["Hungarian language", "Tatar Language", "Albanian language", "Moldovan language", "Ukrainian Language", "Romanian Language", "Russian Language"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4210526315789473, "ans_precission": 0.8, "ans_recall": 0.2857142857142857, "path_f1": 0.4210526315789473, "path_precision": 0.8, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4210526315789473, "path_ans_precision": 0.8, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1955", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.author -> Stephen Breyer\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> Lucas A. Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.book.genre -> Non-fiction\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Courtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk -> book.written_work.subjects -> Law clerk\n# Answer:\nCourtiers of the Marble Palace: The Rise and Influence of the Supreme Court Law Clerk", "# Reasoning Path:\nSupreme Court of the United States -> common.topic.notable_for -> g.125dtprfx\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Sonia Sotomayor", "Anthony Kennedy", "Ruth Bader Ginsburg", "Samuel Alito", "Antonin Scalia", "John Roberts", "Stephen Breyer", "Clarence Thomas", "Elena Kagan"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-1956", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Eurasia\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Kazakhstan -> location.location.partially_contains -> Ural River\n# Answer:\nKazakhstan", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Vatican City -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nVatican City", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Kazakhstan -> location.location.partially_contains -> Caspian Sea\n# Answer:\nKazakhstan", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1957", "prediction": ["# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.0y4y083 -> tv.regular_tv_appearance.series -> First Monday\n# Answer:\nFirst Monday", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.0y4y083 -> tv.regular_tv_appearance.character -> Alyssa\n# Answer:\nAlyssa", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.seasons -> Charmed Season 8\n# Answer:\nCharmed Season 8", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 6\n# Answer:\nThe Big Bang Theory - Season 6", "# Reasoning Path:\nKaley Cuoco -> film.actor.film -> g.11b8czsxkc\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jqc1n -> tv.regular_tv_appearance.character -> Billie Jenkins\n# Answer:\nBillie Jenkins", "# Reasoning Path:\nKaley Cuoco -> award.award_nominee.award_nominations -> m.010wr05z -> award.award_nomination.award -> Teen Choice Award for Choice TV Actress: Comedy\n# Answer:\nTeen Choice Award for Choice TV Actress: Comedy", "# Reasoning Path:\nKaley Cuoco -> base.schemastaging.person_extra.net_worth -> m.0j8mgxf -> measurement_unit.dated_money_value.source -> celebritynetworth.com\n# Answer:\ncelebritynetworth.com", "# Reasoning Path:\nKaley Cuoco -> tv.tv_actor.starring_roles -> m.03jrq66 -> tv.regular_tv_appearance.seasons -> The Big Bang Theory - Season 1\n# Answer:\nThe Big Bang Theory - Season 1"], "ground_truth": ["2005-09-25"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1958", "prediction": ["# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> United States of America\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> common.topic.notable_for -> g.125gzsc2j\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11b674j202\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> base.wikipedia_infobox.settlement.area_code -> Area codes 843 and 854\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.people_born_here -> Dolphin D. Overton\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> Georgetown County\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.statistical_region.population -> g.11x1clr4y\n# Answer:\nAndrews", "# Reasoning Path:\nChris Rock -> people.person.place_of_birth -> Andrews -> location.location.containedby -> South Carolina\n# Answer:\nAndrews"], "ground_truth": ["Andrews"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-196", "prediction": ["# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> education.school_category.schools_of_this_kind -> Piers Midwinter\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Bahamas\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> Navajo Language -> base.rosetta.languoid.document -> Navajo\n# Answer:\nNavajo Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Bangladesh\n# Answer:\nEnglish Language", "# Reasoning Path:\nNavajo people -> people.ethnicity.languages_spoken -> English Language -> base.rosetta.languoid.document -> English\n# Answer:\nEnglish Language"], "ground_truth": ["Navajo Language", "English Language", "Spanish Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1960", "prediction": ["# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Montreal Canadiens\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> common.topic.notable_for -> g.1259tdqtv\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1932 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1932 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.arena_stadium -> Air Canada Centre -> sports.sports_facility.teams -> Toronto Rock\n# Answer:\nAir Canada Centre", "# Reasoning Path:\nToronto Maple Leafs -> sports.professional_sports_team.owner_s -> Maple Leaf Sports & Entertainment -> business.business_operation.industry -> Sports\n# Answer:\nMaple Leaf Sports & Entertainment", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1967 Stanley Cup Finals -> common.topic.article -> m.03by19f\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nToronto Maple Leafs -> sports.sports_team.championships -> 1942 Stanley Cup Finals -> common.topic.article -> m.03cf4pr\n# Answer:\n1942 Stanley Cup Finals"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1961", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1962", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Italian Language", "Brazilian Portuguese", "Portuguese Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.2857142857142857, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-1963", "prediction": ["# Reasoning Path:\nVince Lombardi -> sports.pro_athlete.sports_played_professionally -> m.0d5jg18 -> sports.pro_sports_played.sport -> American football\n# Answer:\nAmerican football", "# Reasoning Path:\nVince Lombardi -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done. -> media_common.quotation.subjects -> Possibilities\n# Answer:\nA man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done.", "# Reasoning Path:\nVince Lombardi -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> film.actor.film -> m.0gchg4q -> film.performance.film -> Paper Lion\n# Answer:\nPaper Lion", "# Reasoning Path:\nVince Lombardi -> common.topic.notable_types -> American football player -> fictional_universe.character_occupation.characters_with_this_occupation -> Zook\n# Answer:\nAmerican football player", "# Reasoning Path:\nVince Lombardi -> people.person.quotations -> A man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done. -> common.topic.notable_types -> Quotation\n# Answer:\nA man can be as great as he wants to be. If you believe in yourself and have the courage, the determination, the dedication, the competitive drive and if you are willing to sacrifice the little things in life and pay the price for the things that are worthwhile, it can be done."], "ground_truth": ["Right guard"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1964", "prediction": ["# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0w17hf6 -> film.performance.actor -> Dahn Ben-Amotz\n# Answer:\nDahn Ben-Amotz", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.actor -> Marlon Brando\n# Answer:\nMarlon Brando", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0w17hf6 -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.0jsxq4 -> film.performance.film -> A Streetcar Named Desire\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.0jsxq4 -> film.performance.actor -> Marlon Brando\n# Answer:\nMarlon Brando", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nAlec Baldwin", "# Reasoning Path:\nA Streetcar Named Desire -> film.film.starring -> m.03l101c -> film.performance.actor -> Alec Baldwin\n# Answer:\nAlec Baldwin", "# Reasoning Path:\nStanley Kowalski -> film.film_character.portrayed_in_films -> m.03l1032 -> film.performance.actor -> Treat Williams\n# Answer:\nTreat Williams"], "ground_truth": ["Marlon Brando"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3076923076923077, "path_precision": 0.25, "path_recall": 0.4, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1965", "prediction": ["# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.0gl1bzt -> tv.regular_tv_appearance.actor -> Kristen Wiig\n# Answer:\nKristen Wiig", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.actor -> Britt McKillip\n# Answer:\nBritt McKillip", "# Reasoning Path:\nLola Bunny -> common.topic.article -> m.0534rg\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.0gl1bzt -> tv.regular_tv_appearance.series -> The Looney Tunes Show\n# Answer:\nThe Looney Tunes Show", "# Reasoning Path:\nLola Bunny -> common.topic.notable_for -> g.125773rs8\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.actor -> Britt McKillip\n# Answer:\nBritt McKillip", "# Reasoning Path:\nLola Bunny -> tv.tv_character.appeared_in_tv_program -> m.03lyj7h -> tv.regular_tv_appearance.series -> Baby Looney Tunes\n# Answer:\nBaby Looney Tunes", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0y7rs3h -> film.performance.actor -> Kath Soucie\n# Answer:\nKath Soucie", "# Reasoning Path:\nLola Bunny -> film.film_character.portrayed_in_films -> m.0qst5_z -> film.performance.film -> Baby Looney Tunes' Eggs-traordinary Adventure\n# Answer:\nBaby Looney Tunes' Eggs-traordinary Adventure"], "ground_truth": ["Britt McKillip", "Kristen Wiig", "Kath Soucie"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1966", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nFrancis Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_types -> Literary Series\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology"], "ground_truth": ["Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Voyage d'un naturaliste autour du monde", "On Natural Selection", "On the tendency of species to form varieties", "Wu zhong qi yuan", "Del Plata a Tierra del Fuego", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin Darwin", "Darwin's notebooks on transmutation of species", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Fertilisation of Orchids", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "La facult\u00e9 motrice dans les plantes", "Evolution by natural selection", "The voyage of Charles Darwin", "Insectivorous Plants", "The foundations of the Origin of species", "ontstaan der soorten door natuurlijke teeltkeus", "Evolution", "Leben und Briefe von Charles Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Notebooks on transmutation of species", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Works", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 11: 1863", "Reise um die Welt 1831 - 36", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Life of Erasmus Darwin", "Darwin's journal", "Les mouvements et les habitudes des plantes grimpantes", "La vie et la correspondance de Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "The education of Darwin", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Formation of Vegetable Mould through the Action of Worms", "El Origin De Las Especies", "Geological Observations on South America", "Darwin en Patagonia", "Human nature, Darwin's view", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Opsht\u0323amung fun menshen", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The principal works", "The Darwin Reader Second Edition", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Variation of Animals and Plants under Domestication", "Darwin-Wallace", "Tesakneri tsagume\u030c", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Cartas de Darwin 18251859", "Charles Darwin's marginalia", "Proiskhozhdenie vidov", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin and Henslow", "The Essential Darwin", "On evolution", "Darwin for Today", "Diario del Viaje de Un Naturalista Alrededor", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 16: 1868", "From so simple a beginning", "Kleinere geologische Abhandlungen", "Diary of the voyage of H.M.S. Beagle", "The Orgin of Species", "Rejse om jorden", "To the members of the Down Friendly Club", "Motsa ha-minim", "The Darwin Reader First Edition", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin's Ornithological notes", "Origins", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Volcanic Islands", "From Darwin's unpublished notebooks", "Charles Darwin's letters", "On a remarkable bar of sandstone off Pernambuco", "The Structure and Distribution of Coral Reefs", "Resa kring jorden", "Die geschlechtliche Zuchtwahl", "The Expression of the Emotions in Man and Animals", "H.M.S. Beagle in South America", "Beagle letters", "Metaphysics, Materialism, & the evolution of mind", "red notebook of Charles Darwin", "The\u0301orie de l'e\u0301volution", "Charles Darwin's natural selection", "Evolution and natural selection", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Voyage of the Beagle", "On the Movements and Habits of Climbing Plants", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Power of Movement in Plants", "vari\u00eberen der huisdieren en cultuurplanten", "Die fundamente zur entstehung der arten", "Darwin Compendium", "Notes on the fertilization of orchids", "Charles Darwin on the routes of male humble bees", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "A student's introduction to Charles Darwin", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 10: 1862", "Memorias y epistolario i\u0301ntimo", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "monograph on the sub-class Cirripedia", "Darwin", "Darwinism stated by Darwin himself", "The collected papers of Charles Darwin", "A Darwin Selection", "The living thoughts of Darwin", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Correspondence of Charles Darwin, Volume 15: 1867", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Autobiography of Charles Darwin", "Gesammelte kleinere Schriften", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 17: 1869", "Het uitdrukken van emoties bij mens en dier", "Darwin on humus and the earthworm"], "ans_acc": 0.06338028169014084, "ans_hit": 1, "ans_f1": 0.027722772277227727, "ans_precission": 0.875, "ans_recall": 0.014084507042253521, "path_f1": 0.4117647058823529, "path_precision": 1.0, "path_recall": 0.25925925925925924, "path_ans_f1": 0.11920529801324505, "path_ans_precision": 1.0, "path_ans_recall": 0.06338028169014084}
{"id": "WebQTest-1967", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.writing_system -> Serbian Cyrillic alphabet\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.main_country -> Serbia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> people.ethnicity.languages_spoken -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbs -> common.topic.article -> m.0g6gj\n# Answer:\ncommon.topic.article"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1968", "prediction": ["# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w021v\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w03yz\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.crimes_of_this_type -> 101 California Street shooting\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> people.cause_of_death.parent_cause_of_death -> Homicide\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w1yxt\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> base.fight.crime_type.acquittals_of_this_type -> m.05n91mj\n# Answer:\nMurder", "# Reasoning Path:\nLaci Peterson -> base.popstra.celebrity.breakup -> m.0j3sj1q -> base.popstra.breakup.participant -> William Kent Gain\n# Answer:\nWilliam Kent Gain", "# Reasoning Path:\nLaci Peterson -> people.deceased_person.cause_of_death -> Murder -> common.topic.webpage -> m.09w217x\n# Answer:\nMurder"], "ground_truth": ["Murder"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1969", "prediction": ["# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.team -> Brooklyn Nets\n# Answer:\nBrooklyn Nets", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> sports.sports_team_roster.team -> Washington Wizards\n# Answer:\nWashington Wizards", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0j2kv6s -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0j2gfzd -> sports.sports_team_roster.team -> Brooklyn Nets\n# Answer:\nBrooklyn Nets", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nKris Humphries -> base.schemastaging.athlete_extra.salary -> m.0w7q6my -> base.schemastaging.athlete_salary.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nKris Humphries -> sports.pro_athlete.teams -> m.0115v9w9 -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward"], "ground_truth": ["Washington Wizards"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-197", "prediction": ["# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> United States of America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> New Hampshire -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew Hampshire", "# Reasoning Path:\nNew England -> location.location.contains -> Rhode Island -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nRhode Island", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountain National Forest -> location.location.containedby -> White Mountains\n# Answer:\nWhite Mountain National Forest", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.statistical_region.co2_emissions_commercial -> m.045jfqg\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.contains -> Rhode Island -> location.location.containedby -> United States, with Territories\n# Answer:\nRhode Island", "# Reasoning Path:\nNew England -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> North Carolina\n# Answer:\nTropical Storm Chris"], "ground_truth": ["Rhode Island", "Massachusetts", "New Hampshire", "Connecticut", "Vermont", "Maine"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.35294117647058826, "ans_precission": 0.375, "ans_recall": 0.3333333333333333, "path_f1": 0.35294117647058826, "path_precision": 0.375, "path_recall": 0.3333333333333333, "path_ans_f1": 0.35294117647058826, "path_ans_precision": 0.375, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-1971", "prediction": ["# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nThe Bounty Hunter", "# Reasoning Path:\nGerard Butler -> film.actor.film -> m.0jv821 -> film.performance.film -> Beowulf & Grendel\n# Answer:\nBeowulf & Grendel", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.award -> Razzie Award for Worst Actor\n# Answer:\nRazzie Award for Worst Actor", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Bounty Hunter\n# Answer:\nThe Bounty Hunter", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0g8kc21 -> award.award_nomination.ceremony -> 31st Golden Raspberry Awards\n# Answer:\n31st Golden Raspberry Awards", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.nominated_for -> The Ugly Truth\n# Answer:\nThe Ugly Truth", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0z8k5mj -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Romantic Comedy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Romantic Comedy", "# Reasoning Path:\nGerard Butler -> award.award_nominee.award_nominations -> m.0b3wcfs -> award.award_nomination.nominated_for -> 300\n# Answer:\n300"], "ground_truth": ["How to Train Your Dragon 2", "Movie 43", "The Game of Their Lives", "Trailer for a Remake of Gore Vidal's Caligula", "How to Train Your Dragon: Legend of the Boneknapper Dragon", "Dear Frankie", "The Bounty Hunter", "How to Train Your Dragon 3", "London Has Fallen", "The Ugly Truth", "RocknRolla", "Machine Gun Preacher", "Dynamo", "The Cherry Orchard", "Justin and the Knights of Valour", "P.S. I Love You", "Timeline", "Shooters", "The Phantom of the Opera", "Harrison's Flowers", "300", "Tomorrow Never Dies", "Dracula 2000", "Attila", "Thunder Run", "Nim's Island", "Geostorm", "Reign of Fire", "Olympus Has Fallen", "Jewel of the Sahara", "Chasing Mavericks", "Fast Food", "How to Train Your Dragon", "Coriolanus", "Mrs Brown", "Lara Croft Tomb Raider: The Cradle of Life", "Playing for Keeps", "Wrath of Gods", "Tales of the Black Freighter", "Butterfly on a Wheel", "Gods of Egypt", "Law Abiding Citizen", "Beowulf & Grendel", "Gamer", "How to Train Your Dragon: Gift of the Night Fury", "One More Kiss"], "ans_acc": 0.08695652173913043, "ans_hit": 1, "ans_f1": 0.15267175572519084, "ans_precission": 0.625, "ans_recall": 0.08695652173913043, "path_f1": 0.14705882352941174, "path_precision": 0.625, "path_recall": 0.08333333333333333, "path_ans_f1": 0.15267175572519084, "path_ans_precision": 0.625, "path_ans_recall": 0.08695652173913043}
{"id": "WebQTest-1972", "prediction": ["# Reasoning Path:\nJohn Franklin -> people.person.places_lived -> m.03pqb5g -> people.place_lived.location -> Blue Island\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.containedby -> United States of America\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11b66snzrw\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07fg9l6\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.geolocation -> m.0kbsh7\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.location.containedby -> Cook County\n# Answer:\nBlue Island", "# Reasoning Path:\nJohn Franklin -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJohn Franklin -> people.person.place_of_birth -> Blue Island -> location.statistical_region.population -> g.11bc88rzdw\n# Answer:\nBlue Island"], "ground_truth": ["Blue Island"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-1975", "prediction": ["# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Family\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.actor.film -> g.11b7sqlhrr\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.language -> Germany\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.country -> United States of America\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.films_executive_produced -> Sir Billi the Vet -> film.film.runtime -> m.0h5n801\n# Answer:\nSir Billi the Vet", "# Reasoning Path:\nSean Connery -> film.producer.films_executive_produced -> The Rock -> film.film.rating -> R (USA)\n# Answer:\nThe Rock", "# Reasoning Path:\nSean Connery -> film.producer.film -> Looney Tunes: Back in Action -> film.film.genre -> Adventure Film\n# Answer:\nLooney Tunes: Back in Action", "# Reasoning Path:\nSean Connery -> film.producer.films_executive_produced -> Sir Billi the Vet -> film.film.starring -> m.0gyk81t\n# Answer:\nSir Billi the Vet"], "ground_truth": ["Sir Billi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-1976", "prediction": ["# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0s9fqn7 -> base.schemastaging.athlete_salary.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.0ngn571 -> sports.sports_team_roster.team -> Shanghai Greenland Shenhua F.C.\n# Answer:\nShanghai Greenland Shenhua F.C.", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.0ngn571 -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nDidier Drogba -> sports.pro_athlete.teams -> m.04m2qyj -> sports.sports_team_roster.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> soccer.football_player.statistics -> m.0w8wn00 -> soccer.football_player_stats.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0s9fqn7 -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.team -> Galatasaray S.K.\n# Answer:\nGalatasaray S.K.", "# Reasoning Path:\nDidier Drogba -> base.schemastaging.athlete_extra.salary -> m.0_lykg_ -> base.schemastaging.athlete_salary.currency -> Euro\n# Answer:\nEuro"], "ground_truth": ["Le Mans FC", "En Avant de Guingamp", "Ivory Coast national football team", "Galatasaray S.K.", "Shanghai Greenland Shenhua F.C.", "Olympique de Marseille", "Chelsea F.C."], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.392156862745098, "ans_precission": 0.625, "ans_recall": 0.2857142857142857, "path_f1": 0.2857142857142857, "path_precision": 0.625, "path_recall": 0.18518518518518517, "path_ans_f1": 0.392156862745098, "path_ans_precision": 0.625, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-1977", "prediction": ["# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.040l5fn -> education.education.institution -> David W. Carter High School\n# Answer:\nDavid W. Carter High School", "# Reasoning Path:\nMichael Crabtree -> people.person.education -> m.04m7z4n -> education.education.institution -> Texas Tech University\n# Answer:\nTexas Tech University", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Crabtree -> people.person.sibling_s -> m.0j5wq8m -> people.sibling_relationship.sibling -> Todd Crabtree\n# Answer:\nTodd Crabtree", "# Reasoning Path:\nMichael Crabtree -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07ych9z -> american_football.player_game_statistics.team -> San Francisco 49ers\n# Answer:\nSan Francisco 49ers", "# Reasoning Path:\nMichael Crabtree -> american_football.football_player.games -> m.07z2fhs -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Crabtree -> sports.pro_athlete.teams -> m.07dk0r3 -> sports.sports_team_roster.position -> Wide receiver\n# Answer:\nWide receiver"], "ground_truth": ["Texas Tech University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1978", "prediction": ["# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> H\u00e4ssleholm Municipality\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Scania\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.containedby -> Sk\u00e5ne County\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.geolocation -> m.0d33bvv\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Albin Tingsvall\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> common.topic.article -> m.05p2z9g\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> common.topic.notable_for -> g.1257cwfx5\n# Answer:\nVittsj\u00f6", "# Reasoning Path:\nFredrik Ljungberg -> people.person.place_of_birth -> Vittsj\u00f6 -> location.location.people_born_here -> Gustav Fridolin\n# Answer:\nVittsj\u00f6"], "ground_truth": ["Vittsj\u00f6"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-1979", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crthld -> sports.sports_league_participation.league -> American Football League\n# Answer:\nAmerican Football League", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.0tk1ttp -> sports.sports_league_draft_pick.player -> Dion Jordan\n# Answer:\nDion Jordan", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt4zr -> sports.sports_league_participation.league -> National Football League\n# Answer:\nNational Football League", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> freebase.valuenotation.is_reviewed -> Language\n# Answer:\nLanguage", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.language -> English\n# Answer:\nEnglish", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crtfcl -> sports.sports_league_participation.league -> AFC East\n# Answer:\nAFC East", "# Reasoning Path:\nMiami Dolphins -> base.schemastaging.organization_extra.contact_webpages -> m.010g8ggp -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nMiami Dolphins -> sports.sports_team.league -> m.0crt9k9 -> sports.sports_league_participation.league -> American Football Conference\n# Answer:\nAmerican Football Conference"], "ground_truth": ["Jamar Taylor"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1981", "prediction": ["# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.district_represented -> Province of Massachusetts Bay\n# Answer:\nProvince of Massachusetts Bay", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.governmental_body -> Continental Congress\n# Answer:\nContinental Congress", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.04466xb -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.legislative_sessions -> First Continental Congress\n# Answer:\nFirst Continental Congress", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nJohn Adams -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Adams -> people.person.place_of_birth -> Braintree -> location.location.containedby -> Massachusetts\n# Answer:\nBraintree"], "ground_truth": ["Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.125, "path_precision": 0.125, "path_recall": 0.125, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1982", "prediction": ["# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> Classica\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> ORSRADIO.COM Classical\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> common.topic.notable_for -> g.125dwc37n\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> radio.radio_subject.programs_with_this_subject -> Adventures in Good Music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.content.broadcast -> BR-Klassik\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Classical music -> base.musiteca.musicalstyle.concerts -> Concerto per Beethoven\n# Answer:\nClassical music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> music.genre.parent_genre -> Classical music\n# Answer:\nBaroque music", "# Reasoning Path:\nAntonio Vivaldi -> music.artist.genre -> Baroque music -> common.topic.image -> Baschenis - Musical Instruments\n# Answer:\nBaroque music"], "ground_truth": ["Classical music", "Baroque music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-1985", "prediction": ["# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0j2c6py -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> sports.pro_athlete.teams -> m.0j20pg1 -> sports.sports_team_roster.team -> Ottawa Senators\n# Answer:\nOttawa Senators", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0j2c6py -> base.schemastaging.athlete_salary.team -> Nashville Predators\n# Answer:\nNashville Predators", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0t5x4wm -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0ygmwhn -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nMike Fisher -> base.popstra.celebrity.canoodled -> m.063hchs -> base.popstra.canoodled.participant -> Carrie Underwood\n# Answer:\nCarrie Underwood", "# Reasoning Path:\nMike Fisher -> ice_hockey.hockey_player.shoots -> Right-handed -> common.topic.article -> m.02bn04\n# Answer:\nRight-handed", "# Reasoning Path:\nMike Fisher -> base.schemastaging.athlete_extra.salary -> m.0t5x4wm -> base.schemastaging.athlete_salary.team -> Nashville Predators\n# Answer:\nNashville Predators"], "ground_truth": ["Nashville Predators"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1986", "prediction": ["# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.nominated_for -> One Life\n# Answer:\nOne Life", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.07zmwhv -> award.award_nomination.nominated_for -> Casino Royale\n# Answer:\nCasino Royale", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.09zcl3b -> award.award_nomination.nominated_for -> Infamous\n# Answer:\nInfamous", "# Reasoning Path:\nDaniel Craig -> tv.tv_personality.tv_regular_appearances -> m.0zvr46v -> tv.tv_regular_personal_appearance.program -> MTV Europe Music Awards 2006\n# Answer:\nMTV Europe Music Awards 2006", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.0fq6441 -> award.award_nomination.nominated_for -> GoldenEye 007\n# Answer:\nGoldenEye 007", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.0glzddx -> award.award_nomination.nominated_for -> Casino Royale\n# Answer:\nCasino Royale", "# Reasoning Path:\nDaniel Craig -> award.award_nominee.award_nominations -> m.01140w77 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Narrator\n# Answer:\nPrimetime Emmy Award for Outstanding Narrator", "# Reasoning Path:\nDaniel Craig -> film.actor.film -> m.02h7bc8 -> film.performance.film -> The Invasion\n# Answer:\nThe Invasion"], "ground_truth": ["I Dreamed of Africa", "The Organ Grinder's Monkey", "Saint-Ex", "Dream House", "Skyfall", "Obsession", "The Trench", "The Adventures of Tintin: The Secret of the Unicorn", "Sylvia", "The Golden Compass", "Road to Perdition", "Sharpe's Eagle", "James Bond Supports International Women's Day", "Lara Croft: Tomb Raider", "Flashbacks of a Fool", "The Power of One", "Love and Rage", "The Girl Who Played with Fire", "Munich", "Hotel Splendide", "A Kid in King Arthur's Court", "The Girl with the Dragon Tattoo", "Copenhagen", "Love Is the Devil: Study for a Portrait of Francis Bacon", "Sword of Honour", "Fateless", "Infamous", "Kiss and Tell", "Bond 25", "Cowboys & Aliens", "Defiance", "Spectre", "The Jacket", "Casino Royale", "Renaissance", "Elizabeth", "The Invasion", "The Mother", "The Adventures of Young Indiana Jones: Daredevils of the Desert", "Some Voices", "Archangel", "Quantum of Solace", "Layer Cake", "Enduring Love"], "ans_acc": 0.06818181818181818, "ans_hit": 1, "ans_f1": 0.12000000000000001, "ans_precission": 0.5, "ans_recall": 0.06818181818181818, "path_f1": 0.10389610389610389, "path_precision": 0.5, "path_recall": 0.057971014492753624, "path_ans_f1": 0.12000000000000001, "path_ans_precision": 0.5, "path_ans_recall": 0.06818181818181818}
{"id": "WebQTest-1987", "prediction": ["# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Elemental: The Power of Illuminated Love\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Flayed Image\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Golden\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Hotel Lobby\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> book.book_subject.works -> Eyewitness: Reports from an Art World in Crisis\n# Answer:\nModern art", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.art_forms -> Sculpture -> award.award_discipline.awards_in_this_discipline -> Wolf Prize in Arts\n# Answer:\nSculpture", "# Reasoning Path:\nEduardo Paolozzi -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Andy Warhol\n# Answer:\nPop art"], "ground_truth": ["Bronze"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1988", "prediction": ["# Reasoning Path:\nJoe Biden -> people.person.education -> g.11b8cfq9yc\n# Answer:\npeople.person.education", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.institution -> Syracuse University\n# Answer:\nSyracuse University", "# Reasoning Path:\nJoe Biden -> common.topic.notable_for -> g.1255qxl7c\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.04hzwkj -> education.education.institution -> Archmere Academy\n# Answer:\nArchmere Academy", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnr -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nJoe Biden -> people.person.education -> m.03l4lnx -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nJoe Biden -> government.politician.government_positions_held -> m.02_x98m -> freebase.valuenotation.has_no_value -> Appointed By (if Position is Appointed)\n# Answer:\nAppointed By (if Position is Appointed)"], "ground_truth": ["History", "Political Science"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-199", "prediction": ["# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iran\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.11b71vj1zg\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Iraqi Kurdistan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Jordan\n# Answer:\nParliamentary system", "# Reasoning Path:\nIraq -> location.country.form_of_government -> Federal republic -> common.topic.notable_types -> Form of Government\n# Answer:\nFederal republic", "# Reasoning Path:\nIraq -> book.book_subject.works -> A reed shaken by the wind -> book.written_work.subjects -> Middle East\n# Answer:\nA reed shaken by the wind", "# Reasoning Path:\nIraq -> location.statistical_region.labor_participation_rate -> g.12cp_j7p6\n# Answer:\nlocation.statistical_region.labor_participation_rate", "# Reasoning Path:\nIraq -> location.statistical_region.consumer_price_index -> g.11b6c_z3xh\n# Answer:\nlocation.statistical_region.consumer_price_index"], "ground_truth": ["Republic", "Parliamentary republic", "Parliamentary system", "Federal republic"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6666666666666665, "ans_precission": 0.6, "ans_recall": 0.75, "path_f1": 0.5454545454545454, "path_precision": 0.6, "path_recall": 0.5, "path_ans_f1": 0.6666666666666665, "path_ans_precision": 0.6, "path_ans_recall": 0.75}
{"id": "WebQTest-1990", "prediction": ["# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.actor -> Nathan Lane\n# Answer:\nNathan Lane", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnb -> film.performance.actor -> Matthew Broderick\n# Answer:\nMatthew Broderick", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.character -> Timon\n# Answer:\nTimon", "# Reasoning Path:\nThe Lion King -> film.film.starring -> g.11b82bfkm7\n# Answer:\nfilm.film.starring", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnz -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Super Nintendo Entertainment System\n# Answer:\nSuper Nintendo Entertainment System", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.02vc53y -> film.performance.actor -> Madge Sinclair\n# Answer:\nMadge Sinclair", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.060084x -> film.performance.actor -> Niketa Calame\n# Answer:\nNiketa Calame", "# Reasoning Path:\nThe Lion King -> base.wikipedia_infobox.video_game.platforms -> Amiga\n# Answer:\nAmiga", "# Reasoning Path:\nThe Lion King -> film.film.starring -> m.0k5qnb -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Nathan Lane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-1991", "prediction": ["# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.containedby -> United States of America\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> book.book_subject.works -> Caught in the Middle: America's Heartland in the Age of Globalism\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nOhio -> location.location.containedby -> East North Central States -> location.location.containedby -> Midwestern United States\n# Answer:\nEast North Central States", "# Reasoning Path:\nOhio -> location.location.containedby -> Midwestern United States -> location.location.contains -> East North Central States\n# Answer:\nMidwestern United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Contiguous United States -> location.location.containedby -> United States of America\n# Answer:\nContiguous United States", "# Reasoning Path:\nOhio -> location.location.containedby -> Eastern United States -> location.location.containedby -> United States of America\n# Answer:\nEastern United States"], "ground_truth": ["Midwestern United States", "East North Central States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1992", "prediction": ["# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> United States, with Territories\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nKailua -> location.hud_foreclosure_area.ofheo_price_change -> m.07gfw9c\n# Answer:\nlocation.hud_foreclosure_area.ofheo_price_change", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> common.topic.notable_for -> g.125f1gfkx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii -> location.location.containedby -> Western United States\n# Answer:\nHawaii", "# Reasoning Path:\nKailua -> location.location.containedby -> Hawaii County -> location.location.containedby -> Hawaiian Islands\n# Answer:\nHawaii County", "# Reasoning Path:\nKailua -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Oahu"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1993", "prediction": ["# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Bolivia\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> common.topic.notable_types -> Human Language\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Quechuan languages -> language.language_family.geographic_distribution -> Peru\n# Answer:\nQuechuan languages", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Guaran\u00ed language -> common.topic.notable_types -> Human Language\n# Answer:\nGuaran\u00ed language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nArgentina -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Bolivia\n# Answer:\nSpanish Language"], "ground_truth": ["Guaran\u00ed language", "Quechuan languages", "Yiddish Language", "Spanish Language", "Italian Language"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.888888888888889, "path_precision": 1.0, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-1994", "prediction": ["# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.honored_for -> Whoopi Goldberg - Original Broadway Show Recording\n# Answer:\nWhoopi Goldberg - Original Broadway Show Recording", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.ceremony -> 28th Annual Grammy Awards\n# Answer:\n28th Annual Grammy Awards", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0k2vr3p -> award.award_honor.award -> Grammy Award for Best Comedy Album\n# Answer:\nGrammy Award for Best Comedy Album", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.honored_for -> Ghost\n# Answer:\nGhost", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.07_2z3p -> award.award_honor.honored_for -> Ghost\n# Answer:\nGhost", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.07yny9v -> award.award_honor.honored_for -> The Color Purple\n# Answer:\nThe Color Purple", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.0459r_b -> award.award_honor.award -> Mark Twain Prize for American Humor\n# Answer:\nMark Twain Prize for American Humor", "# Reasoning Path:\nWhoopi Goldberg -> award.award_winner.awards_won -> m.04ktfys -> award.award_honor.ceremony -> 63rd Academy Awards\n# Answer:\n63rd Academy Awards"], "ground_truth": ["Grammy Award for Best Comedy Album"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-1996", "prediction": ["# Reasoning Path:\nMaggie Simpson -> common.topic.notable_for -> g.1256glm_8\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> fictional_universe.fictional_character.character_created_by -> Matt Groening\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons -> fictional_universe.fictional_universe.characters -> Moe Szyslak\n# Answer:\nThe Simpsons", "# Reasoning Path:\nMaggie Simpson -> common.topic.article -> m.0h67z\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons -> tv.tv_program.spun_off_from -> The Tracey Ullman Show Simpsons Shorts\n# Answer:\nThe Simpsons", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Marge Simpson -> tv.tv_character.appeared_in_tv_program -> m.01z0f47\n# Answer:\nMarge Simpson", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons -> fictional_universe.fictional_universe.characters -> Apu Nahasapeemapetilon\n# Answer:\nThe Simpsons", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons -> fictional_universe.fictional_universe.organizations -> Kwik-E-Mart\n# Answer:\nThe Simpsons", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons -> fictional_universe.fictional_universe.characters -> Barney Gumble\n# Answer:\nThe Simpsons", "# Reasoning Path:\nMaggie Simpson -> fictional_universe.fictional_character.parents -> Homer Simpson -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The Simpsons\n# Answer:\nHomer Simpson"], "ground_truth": ["Nancy Cartwright"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-1997", "prediction": ["# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.09tcxk8 -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.rushing -> m.0791jj8 -> american_football.player_rushing_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.0791jrl -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> sports.pro_athlete.teams -> m.0n__d97 -> sports.sports_team_roster.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.09tcxk8 -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07sh8x9 -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> common.topic.notable_for -> g.1256s0v84\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07tfchw -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks", "# Reasoning Path:\nOwen Schmitt -> american_football.football_player.receiving -> m.07vzt52 -> american_football.player_receiving_statistics.team -> Seattle Seahawks\n# Answer:\nSeattle Seahawks"], "ground_truth": ["Seattle Seahawks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.26086956521739135, "path_precision": 0.6666666666666666, "path_recall": 0.16216216216216217, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1998", "prediction": ["# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> government.political_party_tenure.party -> Democratic-Republican Party\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.image -> TJeffersonrpeale\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> organization.organization.founders -> James Madison\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nDemocratic Party", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> common.topic.article -> m.07wfk\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> organization.organization_founder.organizations_founded -> Democratic-Republican Party -> government.political_party.ideology -> Republicanism\n# Answer:\nDemocratic-Republican Party", "# Reasoning Path:\nThomas Jefferson -> government.politician.party -> m.03gjg7h -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician"], "ground_truth": ["Democratic-Republican Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-1999", "prediction": ["# Reasoning Path:\nFrance -> location.statistical_region.religions -> m.03q9tjt -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nFrance -> symbols.flag_referent.flag -> m.0619w1_ -> symbols.flag_use.flag -> Flag of France\n# Answer:\nFlag of France", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71tyl21\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.12tb6gn9g\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.1245_0_b7\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc37cwf\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita"], "ground_truth": ["Catholicism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> common.topic.notable_types -> Religious Leader -> type.type.properties -> Religious Leadership\n# Answer:\nReligious Leader", "# Reasoning Path:\nAugustine of Hippo -> book.author.works_written -> Augustine -> freebase.valuenotation.is_reviewed -> Date of first publication\n# Answer:\nAugustine", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.corresponding_type -> Author\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> common.topic.notable_types -> Religious Leader -> type.type.expected_by -> Leader\n# Answer:\nReligious Leader", "# Reasoning Path:\nAugustine of Hippo -> common.topic.notable_types -> Religious Leader -> freebase.type_hints.included_types -> Person\n# Answer:\nReligious Leader", "# Reasoning Path:\nAugustine of Hippo -> book.author.works_written -> Augustine -> book.book.editions -> Augustine: On the Trinity Books 8-15 (Cambridge Texts in the History of Philosophy)\n# Answer:\nAugustine", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer -> people.profession.specializations -> Novelist\n# Answer:\nWriter"], "ground_truth": ["Writer", "Philosopher", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-200", "prediction": ["# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> book.written_work.subjects -> United States of America\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.locations -> Vietnam -> book.written_work.subjects -> United States of America\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> location.location.containedby -> Eurasia\n# Answer:\nCambodia", "# Reasoning Path:\nVietnam War -> book.written_work.subjects -> Vietnam -> book.written_work.subjects -> Asia\n# Answer:\nVietnam", "# Reasoning Path:\nVietnam War -> time.event.locations -> Cambodia -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nCambodia"], "ground_truth": ["Cambodia", "Laos", "South Vietnam", "Vietnam", "Southeast Asia", "North Vietnam"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.43478260869565216, "ans_precission": 0.625, "ans_recall": 0.3333333333333333, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-2000", "prediction": ["# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.actor -> Ron Livingston\n# Answer:\nRon Livingston", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.actor -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.0w2d85v -> tv.regular_tv_appearance.character -> Jack Berger\n# Answer:\nJack Berger", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dghk -> tv.regular_tv_appearance.actor -> Kim Cattrall\n# Answer:\nKim Cattrall", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgj0 -> tv.regular_tv_appearance.actor -> Kristin Davis\n# Answer:\nKristin Davis", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgjd -> tv.regular_tv_appearance.actor -> Cynthia Nixon\n# Answer:\nCynthia Nixon", "# Reasoning Path:\nSex and the City -> film.film.produced_by -> Sarah Jessica Parker -> film.producer.film -> Sex and the City 2\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nSex and the City -> tv.tv_program.regular_cast -> m.025dgg3 -> tv.regular_tv_appearance.seasons -> Sex and the City - Season 6\n# Answer:\nSex and the City - Season 6"], "ground_truth": ["Ron Livingston"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-2001", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmws -> sports.sports_award.award_winner -> Portsmouth F.C.\n# Answer:\nPortsmouth F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\n1988\u201389 FA Cup", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkh7k -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmnq -> sports.sports_award.award_winner -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkmws -> sports.sports_award.season -> 2007\u201308 FA Cup\n# Answer:\n2007\u201308 FA Cup", "# Reasoning Path:\nFA Cup -> common.image.appears_in_topic_gallery -> FA Cup Final referees\n# Answer:\nFA Cup Final referees", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.award_winner -> Arsenal F.C.\n# Answer:\nArsenal F.C."], "ground_truth": ["Chelsea F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2002", "prediction": ["# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.gender -> Male\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.parents -> Debbie Rowe\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.nationality -> United States of America\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> Actor\n# Answer:\nMichael Joseph Jackson, Jr.", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> people.person.parents -> Debbie Rowe\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.article -> m.0j38d_7\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Paris-Michael Katherine Jackson -> common.topic.subject_of -> Diamond Ranch Academy\n# Answer:\nParis-Michael Katherine Jackson", "# Reasoning Path:\nMichael Jackson -> people.person.children -> Michael Joseph Jackson, Jr. -> people.person.profession -> TV Personality\n# Answer:\nMichael Joseph Jackson, Jr."], "ground_truth": ["Michael Joseph Jackson, Jr.", "Paris-Michael Katherine Jackson", "Prince Michael Jackson II"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-2003", "prediction": ["# Reasoning Path:\nRobert Pattinson -> tv.tv_actor.starring_roles -> m.0gfv1f7 -> tv.regular_tv_appearance.series -> Dark Kingdom: The Dragon King\n# Answer:\nDark Kingdom: The Dragon King", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> tv.tv_actor.starring_roles -> m.0gfv1f7 -> tv.regular_tv_appearance.character -> Giselher\n# Answer:\nGiselher", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83n09 -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z8jm72 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Kristen Stewart\n# Answer:\nKristen Stewart", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0b3w_ry -> award.award_nomination.nominated_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nRobert Pattinson -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards"], "ground_truth": ["Maps to the Stars", "The Childhood of a Leader", "Love & Distrust", "Bel Ami", "Hold on to Me", "Dark Kingdom: The Dragon King", "The Haunted Airman", "Harry Potter and the Goblet of Fire", "The Twilight Saga: New Moon", "Queen of the Desert", "Cosmopolis", "Water for Elephants", "Twilight", "Vanity Fair", "Remember Me", "The Bad Mother's Handbook", "The Twilight Saga: Breaking Dawn - Part 1", "Unbound Captives", "Mission: Blacklist", "Harry Potter and the Order of the Phoenix", "How to Be", "Little Ashes", "Life", "The Twilight Saga: Breaking Dawn - Part 2", "The Summer House", "The Rover", "Eclipse"], "ans_acc": 0.14814814814814814, "ans_hit": 1, "ans_f1": 0.23952095808383234, "ans_precission": 0.625, "ans_recall": 0.14814814814814814, "path_f1": 0.09999999999999999, "path_precision": 0.625, "path_recall": 0.05434782608695652, "path_ans_f1": 0.23952095808383234, "path_ans_precision": 0.625, "path_ans_recall": 0.14814814814814814}
{"id": "WebQTest-2004", "prediction": ["# Reasoning Path:\nJapan -> location.location.time_zones -> Japan Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nJapan Time Zone", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6gg_n\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJapan -> location.statistical_region.gni_in_ppp_dollars -> g.11b60zmbbm\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Japan Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-2005", "prediction": ["# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.actor -> Ken Jenkins\n# Answer:\nKen Jenkins", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.series -> Scrubs\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 1\n# Answer:\nScrubs - Season 1", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 2\n# Answer:\nScrubs - Season 2", "# Reasoning Path:\nBob Kelso -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Scrubs -> award.award_nominated_work.award_nominations -> m.0lv4677\n# Answer:\nScrubs", "# Reasoning Path:\nBob Kelso -> tv.tv_character.appeared_in_tv_program -> m.02567sb -> tv.regular_tv_appearance.seasons -> Scrubs - Season 3\n# Answer:\nScrubs - Season 3", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nBob Kelso -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character"], "ground_truth": ["Ken Jenkins"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-2006", "prediction": ["# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> Europeans\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> L\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> music.artist.album -> g.1ywpnw1r_\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.included_in_group -> East Slavs\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Yasha Mousekewitz\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> common.topic.notable_types -> Ethnicity\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Crimson Dynamo\n# Answer:\nRussian", "# Reasoning Path:\nVanessa Carlton -> people.person.ethnicity -> Russian -> common.topic.image -> Kuban Cossack Dance\n# Answer:\nRussian"], "ground_truth": ["Scandinavians", "Russian"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.64, "path_precision": 0.8888888888888888, "path_recall": 0.5, "path_ans_f1": 0.64, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5}
{"id": "WebQTest-2007", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> Adelheid\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Romania\n# Answer:\nUkrainian Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> media_common.netflix_genre.titles -> All My Loved Ones\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.language_family -> Slavic languages\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nHungarian language", "# Reasoning Path:\nCzech Republic -> location.country.languages_spoken -> Hungarian language -> common.topic.notable_types -> Human Language\n# Answer:\nHungarian language"], "ground_truth": ["German Language", "Hungarian language", "Czech Language", "Romani language", "Slovak Language", "Ukrainian Language", "Serbian language", "Croatian language", "Polish Language", "Russian Language", "Rusyn Language", "Bulgarian Language", "Greek Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.375, "ans_precission": 1.0, "ans_recall": 0.23076923076923078, "path_f1": 0.375, "path_precision": 1.0, "path_recall": 0.23076923076923078, "path_ans_f1": 0.375, "path_ans_precision": 1.0, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-2008", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Western Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Djibouti -> location.country.languages_spoken -> French\n# Answer:\nDjibouti", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Near East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.capital -> Ray\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> common.topic.notable_types -> Country\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.form_of_government -> Monarchy\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Central Asia\n# Answer:\nSeljuk Empire"], "ground_truth": ["Saudi Arabia", "Yemen", "Bahrain", "Mandatory Palestine", "Libya", "Syria", "Mauritania", "Tunisia", "Canada", "Qatar", "Iran", "Egypt", "Israel", "South Yemen", "South Africa", "Lebanon", "Oman", "Jordan", "Kuwait", "Algeria", "Iraq", "United Arab Emirates", "Tanzania", "Morocco", "Sudan", "Turkey", "Djibouti", "Seljuk Empire"], "ans_acc": 0.07142857142857142, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 1.0, "ans_recall": 0.07142857142857142, "path_f1": 0.13333333333333333, "path_precision": 1.0, "path_recall": 0.07142857142857142, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.07142857142857142}
{"id": "WebQTest-2009", "prediction": ["# Reasoning Path:\nTim Cook -> people.person.nationality -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.employment_history -> m.04gcn1r -> business.employment_tenure.company -> Intelligent Electronics\n# Answer:\nIntelligent Electronics", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.total_residential_addresses -> m.07ghzrr\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nTim Cook -> common.topic.notable_for -> g.1255jj0n6\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Apple Inc."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-201", "prediction": ["# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> US Vice President\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.parents -> John D. Rockefeller Jr.\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfg4c\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05hlvsz\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.parents -> Abby Aldrich Rockefeller\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> common.topic.notable_types -> Film character\n# Answer:\nNelson Rockefeller", "# Reasoning Path:\nGerald Ford -> government.us_vice_president.to_president -> Richard Nixon -> people.person.nationality -> United States of America\n# Answer:\nRichard Nixon", "# Reasoning Path:\nGerald Ford -> government.us_president.vice_president -> Nelson Rockefeller -> people.person.sibling_s -> m.05px29t\n# Answer:\nNelson Rockefeller"], "ground_truth": ["Nelson Rockefeller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-2010", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkfc -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree"], "ground_truth": ["MBA", "Juris Doctor", "Bachelor of Arts"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-2011", "prediction": ["# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> travel.tourist_attraction.near_travel_destination -> Jamaica\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> common.topic.notable_types -> Tourist attraction\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> location.location.geolocation -> m.0ggh7k4\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> people.deceased_person.place_of_burial -> Bob Marley Mausoleum -> common.topic.notable_for -> g.1258t_ms1\n# Answer:\nBob Marley Mausoleum", "# Reasoning Path:\nBob Marley -> common.topic.webpage -> m.0n4vdzx -> common.webpage.resource -> Reggae Guitar Lessons\n# Answer:\nReggae Guitar Lessons", "# Reasoning Path:\nBob Marley -> film.person_or_entity_appearing_in_film.films -> m.0yk0kgr -> film.personal_film_appearance.film -> La deuxi\u00e8me femme\n# Answer:\nLa deuxi\u00e8me femme", "# Reasoning Path:\nBob Marley -> symbols.name_source.namesakes -> Tuff Gong -> music.record_label.artist -> Ziggy Marley & The Melody Makers\n# Answer:\nTuff Gong", "# Reasoning Path:\nBob Marley -> symbols.name_source.namesakes -> Tuff Gong -> common.topic.webpage -> m.04m4bfc\n# Answer:\nTuff Gong"], "ground_truth": ["Bob Marley Mausoleum"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-2012", "prediction": ["# Reasoning Path:\nLondon -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> United Kingdom\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.periodical.frequency_or_issues_per_year -> m.0jw2n9_\n# Answer:\ni", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.containedby -> England\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_containedby -> Kent\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> location.location.partially_contains -> River Thames -> location.location.partially_contained_by -> m.0wg9mlk\n# Answer:\nRiver Thames", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> common.topic.article -> m.0dk0p1s\n# Answer:\ni", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> The Guardian -> book.newspaper.circulation_areas -> United Kingdom\n# Answer:\nThe Guardian", "# Reasoning Path:\nLondon -> periodicals.newspaper_circulation_area.newspapers -> i -> book.newspaper.headquarters -> m.0j0yzj7\n# Answer:\ni"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-2013", "prediction": ["# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_for -> g.1256hnvy5\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> travel.accommodation.accommodation_type -> Boutique hotel\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.notable_types -> Accommodation\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> location.location.street_address -> m.0ldv7q7\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> travel.travel_destination.accommodation -> Hotel Sax Chicago -> common.topic.webpage -> m.0ldv4d_\n# Answer:\nHotel Sax Chicago", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11x1cvd__\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News"], "ground_truth": ["Hotel Sax Chicago"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-2014", "prediction": ["# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> base.schemastaging.context_name.pronunciation -> m.011032cv\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> people.cause_of_death.people -> Francis Wynne Masters\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> common.topic.image -> Contrast enhanced meningioma\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> medicine.notable_person_with_medical_condition.condition -> Meningioma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nMeningioma", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> award.award_nomination.award -> MTV Europe Music Award for Best Female\n# Answer:\nMTV Europe Music Award for Best Female", "# Reasoning Path:\nSheryl Crow -> people.person.profession -> Voice Actor -> common.topic.notable_types -> Profession\n# Answer:\nVoice Actor", "# Reasoning Path:\nSheryl Crow -> music.artist.track_contributions -> m.010_wmqq -> music.track_contribution.track -> Love Is All There Is\n# Answer:\nLove Is All There Is", "# Reasoning Path:\nSheryl Crow -> award.award_nominee.award_nominations -> m.0105szp3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Meningioma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-2015", "prediction": ["# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Bolivia\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Representative democracy -> government.form_of_government.countries -> Paraguay\n# Answer:\nRepresentative democracy", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Chile\n# Answer:\nConstitutional republic", "# Reasoning Path:\nArgentina -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Brazil\n# Answer:\nPresidential system", "# Reasoning Path:\nArgentina -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60prvn4\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Constitutional republic", "Representative democracy", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-2016", "prediction": ["# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Friuli-Venezia Giulia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> common.topic.article -> m.030_x3\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Italy\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.article -> m.0fgn_w\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.containedby -> Province of Gorizia\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.people_born_here -> Lucio Bertogna\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.statistical_region.population -> m.04mb7b1\n# Answer:\nSan Canzian d'Isonzo", "# Reasoning Path:\nFabio Capello -> people.person.place_of_birth -> San Canzian d'Isonzo -> location.location.people_born_here -> Gastone Bean\n# Answer:\nSan Canzian d'Isonzo"], "ground_truth": ["San Canzian d'Isonzo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-2017", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.geographic_scope -> United States of America\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Conservatism in the United States\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party -> common.topic.article -> m.0g7ly9\n# Answer:\nIllinois Republican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> organization.organization.founders -> Thomas Jefferson\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> United States Secret Service -> organization.organization.founders -> Allan Pinkerton\n# Answer:\nUnited States Secret Service", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> common.topic.notable_types -> Political party\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> United States Secret Service -> base.ontologies.ontology_instance.equivalent_instances -> m.09km072\n# Answer:\nUnited States Secret Service", "# Reasoning Path:\nAbraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party -> government.political_party.ideology -> Economic liberalism\n# Answer:\nRepublican Party"], "ground_truth": ["Republican Party", "National Union Party", "Whig Party", "Illinois Republican Party"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.5217391304347827, "path_precision": 0.75, "path_recall": 0.4, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-2018", "prediction": ["# Reasoning Path:\nJack London -> people.person.profession -> Sailor -> film.film_character.portrayed_in_films -> m.0nggq45\n# Answer:\nSailor", "# Reasoning Path:\nJack London -> people.person.profession -> Sailor -> common.topic.notable_types -> Profession\n# Answer:\nSailor", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Edmund Grey\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Novelist -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nNovelist", "# Reasoning Path:\nJack London -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> common.topic.notable_types -> Profession\n# Answer:\nScreenwriter", "# Reasoning Path:\nJack London -> people.person.profession -> Screenwriter -> freebase.equivalent_topic.equivalent_type -> TV Writer\n# Answer:\nScreenwriter"], "ground_truth": ["Essayist", "Screenwriter", "Journalist", "Author", "Novelist", "Sailor"], "ans_acc": 0.8333333333333334, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.9090909090909091, "path_ans_precision": 1.0, "path_ans_recall": 0.8333333333333334}
{"id": "WebQTest-2019", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.spouse -> Myrna Colley-Lee\n# Answer:\nMyrna Colley-Lee", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.spouse -> Jeanette Adair Bradshaw\n# Answer:\nJeanette Adair Bradshaw", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.children -> Alfonso Rene Freeman II\n# Answer:\nAlfonso Freeman", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dmb -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nMorgan Freeman -> people.person.spouse_s -> m.0hj6dq1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMorgan Freeman -> people.person.children -> Alfonso Freeman -> people.person.children -> Donovan Lee Freeman\n# Answer:\nAlfonso Freeman"], "ground_truth": ["Myrna Colley-Lee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-202", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.broadband_penetration_rate -> g.1245_6n11\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-2020", "prediction": ["# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Futa Pass -> common.topic.notable_for -> g.125brlwwv\n# Answer:\nFuta Pass", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> location.country.official_language -> Italian Language\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.mountains -> Alpe di Succiso -> location.location.containedby -> Province of Reggio Emilia\n# Answer:\nAlpe di Succiso", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_for -> g.125bhhqyn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nApennine Mountains -> location.location.containedby -> Italy -> base.aareas.schema.administrative_area.administrative_children -> Tuscany\n# Answer:\nItaly", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Cisa Pass -> location.location.containedby -> Italy\n# Answer:\nCisa Pass", "# Reasoning Path:\nApennine Mountains -> common.topic.notable_types -> Mountain range -> freebase.type_hints.included_types -> Geographical Feature\n# Answer:\nMountain range", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Futa Pass -> common.topic.article -> m.02qrc4s\n# Answer:\nFuta Pass", "# Reasoning Path:\nApennine Mountains -> geography.mountain_range.passes -> Futa Pass -> common.topic.image -> Flaminia militare\n# Answer:\nFuta Pass"], "ground_truth": ["Apennine Mountains"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2021", "prediction": ["# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Charles Dickens\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced -> Toni Morrison\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Thomas Malory -> influence.influence_node.influenced -> Mark Twain\n# Answer:\nThomas Malory", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Thomas Wolfe\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Edgar Allan Poe\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> people.person.gender -> Male\n# Answer:\nWilliam Faulkner", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> Robert Burns -> influence.influence_node.influenced -> J. D. Salinger\n# Answer:\nRobert Burns", "# Reasoning Path:\nJohn Steinbeck -> influence.influence_node.influenced_by -> William Faulkner -> influence.influence_node.influenced_by -> Herman Melville\n# Answer:\nWilliam Faulkner"], "ground_truth": ["Thomas Malory", "Robert Burns", "Sherwood Anderson", "William Faulkner"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-2022", "prediction": ["# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> common.topic.notable_for -> g.12589h52q\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> people.ethnicity.people -> Afa Anoa\u02bbi Jr.\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> common.topic.notable_types -> Ethnicity\n# Answer:\nSamoan American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> African American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nAfrican American", "# Reasoning Path:\nDwayne Johnson -> people.person.ethnicity -> Samoan American -> common.topic.article -> m.07j80j\n# Answer:\nSamoan American"], "ground_truth": ["African American", "Samoan American"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-2023", "prediction": ["# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> location.mailing_address.citytown -> Brussels\n# Answer:\nBrussels", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> common.topic.notable_for -> g.12571zl_t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> location.mailing_address.country -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> Belgium -> location.country.capital -> Brussels\n# Answer:\nBelgium", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.headquarters -> m.04300hm -> freebase.valuenotation.has_no_value -> State/Province/Region\n# Answer:\nState/Province/Region", "# Reasoning Path:\nNorth Atlantic Treaty Organization (NATO) -> organization.organization.founders -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics\n# Answer:\nUnited States of America"], "ground_truth": ["Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-2024", "prediction": ["# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> United States of America\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.hud_foreclosure_area.ofheo_price_change -> m.07gzwmg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> Washington County\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.hud_foreclosure_area.total_residential_addresses -> m.07gzwm1\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.location.containedby -> Maryland\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Sharpsburg -> location.statistical_region.population -> g.11b66jxpbg\n# Answer:\nSharpsburg", "# Reasoning Path:\nBattle of Antietam -> base.culturalevent.event.entity_involved -> George B. McClellan -> people.person.nationality -> United States of America\n# Answer:\nGeorge B. McClellan", "# Reasoning Path:\nBattle of Antietam -> time.event.locations -> Maryland -> location.location.adjoin_s -> m.02thq3m\n# Answer:\nMaryland"], "ground_truth": ["Maryland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-2025", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.containedby -> World Ocean\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> geography.geographical_feature.category -> Ocean\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> Somewheres east of Suez\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Indian Ocean in World War II\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> book.book_subject.works -> A fish caught in time: the search for the coelacanth\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.basin_countries -> Democratic Republic of the Congo -> location.location.containedby -> Africa\n# Answer:\nDemocratic Republic of the Congo", "# Reasoning Path:\nZambezi -> geography.river.mouth -> Indian Ocean -> location.location.events -> Action of 11 January 1944\n# Answer:\nIndian Ocean", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Zambia\n# Answer:\nVictoria Falls"], "ground_truth": ["Indian Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-2027", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hj3 -> soccer.football_player_stats.team -> Preston North End F.C.\n# Answer:\nPreston North End F.C."], "ground_truth": ["Manchester United F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-2028", "prediction": ["# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.office_holder -> Michel Sapin\n# Answer:\nMichel Sapin", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0ngn_cw -> government.government_position_held.office_holder -> Nicolas Sarkozy\n# Answer:\nNicolas Sarkozy", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_2fw -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_7kh -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_8wl -> government.government_position_held.appointed_by -> Fran\u00e7ois Hollande\n# Answer:\nFran\u00e7ois Hollande", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> freebase.valuenotation.has_no_value -> District represented (if position is district-related)\n# Answer:\nDistrict represented (if position is district-related)", "# Reasoning Path:\nFrance -> location.statistical_region.gdp_nominal_per_capita -> g.11b60tv_69\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nFrance -> government.governmental_jurisdiction.governing_officials -> m.0102_81j -> government.government_position_held.governmental_body -> Government of France\n# Answer:\nGovernment of France", "# Reasoning Path:\nFrance -> location.statistical_region.gni_in_ppp_dollars -> g.11b60wv0lq\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Nicolas Sarkozy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.125, "path_precision": 0.1, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-2029", "prediction": ["# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Southeast Europe\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> base.aareas.schema.administrative_area.administrative_children -> Ionian Islands\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Crete -> location.location.containedby -> Greece\n# Answer:\nCrete", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.country.capital -> Athens\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> location.location.containedby -> Eurasia\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> location.location.containedby -> Greece -> base.aareas.schema.administrative_area.administrative_children -> Crete\n# Answer:\nGreece", "# Reasoning Path:\nKnossos -> travel.tourist_attraction.near_travel_destination -> Malia, Crete -> location.location.containedby -> Greece\n# Answer:\nMalia, Crete", "# Reasoning Path:\nKnossos -> common.topic.notable_for -> g.125bqvqht\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKnossos -> common.topic.article -> m.0ksnz\n# Answer:\ncommon.topic.article"], "ground_truth": ["Greece", "Crete"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-2030", "prediction": ["# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> United States, with Territories\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Lincoln County -> location.location.containedby -> Nevada\n# Answer:\nLincoln County", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> Lincoln County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> White Pine County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.location.containedby -> United States of America\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.geolocation -> m.043260z\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> Churchill County\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> aviation.airport.serves -> Nevada -> location.statistical_region.religions -> m.04403h4\n# Answer:\nNevada", "# Reasoning Path:\nArea 51 -> location.location.containedby -> Lincoln County -> location.us_county.hud_county_place -> Lincoln County / County Place 3201799999\n# Answer:\nLincoln County"], "ground_truth": ["Lincoln County", "Nevada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2222222222222222, "path_recall": 0.5, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-2031", "prediction": ["# Reasoning Path:\nChris Riley -> common.topic.notable_for -> g.1258z_d26\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Australia\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.people_born_here -> Andrew Walker\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> common.topic.article -> m.03d8_df\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.article -> m.0h3p6\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> travel.travel_destination.tourist_attractions -> Billabong Ranch\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.containedby -> Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> common.topic.image -> Echuca location map in Victoria\n# Answer:\nEchuca", "# Reasoning Path:\nChris Riley -> people.person.place_of_birth -> Echuca -> location.location.people_born_here -> Jack Quinn\n# Answer:\nEchuca"], "ground_truth": ["Echuca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-204", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> location.location.containedby -> North Africa\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-205", "prediction": ["# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.0n1fbly -> education.education.institution -> University of New Zealand\n# Answer:\nUniversity of New Zealand", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq21s -> education.education.institution -> University of Cambridge\n# Answer:\nUniversity of Cambridge", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02kq20k -> education.education.institution -> University of Canterbury\n# Answer:\nUniversity of Canterbury", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> Trinity College, Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.02wntpq -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nErnest Rutherford -> people.person.education -> m.0kv984f -> education.education.institution -> Nelson College\n# Answer:\nNelson College", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> location.location.contains -> University of Cambridge\n# Answer:\nCambridge", "# Reasoning Path:\nErnest Rutherford -> people.deceased_person.place_of_death -> Cambridge -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCambridge"], "ground_truth": ["University of Canterbury", "University of New Zealand", "Trinity College, Cambridge", "Nelson College", "University of Cambridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-206", "prediction": ["# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> sports.sports_position.sport -> Basketball\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> sports.pro_athlete.sports_played_professionally -> m.0c53rsf -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> sports.sports_position.sport -> Basketball\n# Answer:\nPoint guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.notable_for -> g.1255g6p7n\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.image -> Jordan by Lipofsky 16577\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.notable_types -> Basketball Position\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Shooting guard -> common.topic.article -> m.03558w\n# Answer:\nShooting guard", "# Reasoning Path:\nBrandon Roy -> basketball.basketball_player.position_s -> Point guard -> common.topic.notable_for -> g.125c79ttb\n# Answer:\nPoint guard"], "ground_truth": ["Small forward", "Shooting guard", "Point guard"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-207", "prediction": ["# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> location.location.containedby -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Massachusetts\n# Answer:\nUnited States of America", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Cambridge -> location.location.containedby -> Massachusetts\n# Answer:\nCambridge", "# Reasoning Path:\nHarvard University -> location.location.containedby -> Massachusetts -> location.us_state.capital -> Boston\n# Answer:\nMassachusetts", "# Reasoning Path:\nHarvard University -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Massachusetts\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America", "Cambridge", "Massachusetts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-209", "prediction": ["# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.containedby -> Asia\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> base.locations.countries.continent -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> book.written_work.subjects -> Southeast Asia -> location.location.containedby -> Asia\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.events -> Vietnam War\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Asia -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nAsia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> location.location.contains -> Thailand\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> location.location.containedby -> Southeast Asia -> organization.organization_scope.organizations_with_this_scope -> Association of Southeast Asian Nations\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nLaos -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nLaos -> location.statistical_region.debt_service_as_percent_of_trade_volume -> g.1hhc39wvj\n# Answer:\nlocation.statistical_region.debt_service_as_percent_of_trade_volume"], "ground_truth": ["Southeast Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_types -> Person\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons"], "ground_truth": ["Sal Gibson", "Leon Cole", "Francine Lons"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4800000000000001, "ans_precission": 0.375, "ans_recall": 0.6666666666666666, "path_f1": 0.4800000000000001, "path_precision": 0.375, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4800000000000001, "path_ans_precision": 0.375, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-210", "prediction": ["# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Los Angeles\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Los Angeles County\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.statistical_region.population -> g.11bc88q_gs\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> California\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> common.topic.image -> Watts in 1912\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.people_born_here -> Wanda Coleman\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nWatts", "# Reasoning Path:\nTyrese Gibson -> people.person.place_of_birth -> Watts -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nWatts"], "ground_truth": ["Watts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-211", "prediction": ["# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.sports -> Ice Hockey\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Austria\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nItaly -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Albania\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Austria", "Switzerland", "France", "Slovenia"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.013333333333333332, "path_precision": 0.1, "path_recall": 0.007142857142857143, "path_ans_f1": 0.14285714285714288, "path_ans_precision": 0.1, "path_ans_recall": 0.25}
{"id": "WebQTest-212", "prediction": ["# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> geography.river.basin_countries -> United States of America -> location.location.partially_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.country.administrative_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> base.locations.countries.states_provinces_within -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> geography.river.basin_countries -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> location.location.partially_containedby -> United States of America -> location.country.first_level_divisions -> New Mexico\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado River -> geography.river.cities -> Tucson -> location.location.containedby -> United States of America\n# Answer:\nTucson"], "ground_truth": ["Arizona", "California", "Nevada", "Colorado"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-213", "prediction": ["# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.065q6ym -> base.popstra.dated.participant -> Ryan Reynolds\n# Answer:\nRyan Reynolds", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.065q9sh -> base.popstra.dated.participant -> Josh Hartnett\n# Answer:\nJosh Hartnett", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064fp5j -> base.popstra.dated.participant -> nm1157013\n# Answer:\nnm1157013", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.friendship -> m.065pntb -> base.popstra.friendship.participant -> Pen\u00e9lope Cruz\n# Answer:\nPen\u00e9lope Cruz", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064jrnt -> base.popstra.dated.participant -> Josh Hartnett\n# Answer:\nJosh Hartnett", "# Reasoning Path:\nScarlett Johansson -> base.popstra.celebrity.dated -> m.064tt90 -> base.popstra.dated.participant -> Justin Timberlake\n# Answer:\nJustin Timberlake", "# Reasoning Path:\nScarlett Johansson -> celebrities.celebrity.sexual_relationships -> m.04xwldp -> celebrities.romantic_relationship.celebrity -> Josh Hartnett\n# Answer:\nJosh Hartnett"], "ground_truth": ["Topher Grace", "Patrick Wilson", "Ryan Reynolds", "Jared Leto", "Benicio del Toro", "Josh Hartnett", "nm1157013", "Justin Timberlake", "Romain Dauriac"], "ans_acc": 0.4444444444444444, "ans_hit": 1, "ans_f1": 0.5581395348837209, "ans_precission": 0.75, "ans_recall": 0.4444444444444444, "path_f1": 0.4137931034482759, "path_precision": 0.75, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5581395348837209, "path_ans_precision": 0.75, "path_ans_recall": 0.4444444444444444}
{"id": "WebQTest-214", "prediction": ["# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w7_ -> location.partial_containment_relationship.partially_contained_by -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8wk6 -> location.partial_containment_relationship.partially_contained_by -> Middle East\n# Answer:\nMiddle East", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.region -> Europe\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> location.location.partially_contained_by -> m.0ws8w38 -> location.partial_containment_relationship.partially_contained_by -> North Africa\n# Answer:\nNorth Africa", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Republic of Genoa\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> book.written_work.subjects -> Germanic peoples\n# Answer:\nThe Roman Empire and Its Germanic Peoples"], "ground_truth": ["Europe", "North Africa", "Middle East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-215", "prediction": ["# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> London\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> people.person.place_of_birth -> Paddington -> location.location.containedby -> United Kingdom\n# Answer:\nPaddington", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.0kb152x -> tv.tv_guest_role.episodes_appeared_in -> John C. Reilly\n# Answer:\nJohn C. Reilly", "# Reasoning Path:\nElvis Costello -> tv.tv_actor.guest_roles -> m.040p_0y -> tv.tv_guest_role.episodes_appeared_in -> How I Spent My Strummer Vacation\n# Answer:\nHow I Spent My Strummer Vacation", "# Reasoning Path:\nElvis Costello -> music.artist.genre -> Rock music -> music.genre.parent_genre -> Folk music\n# Answer:\nRock music", "# Reasoning Path:\nElvis Costello -> common.topic.webpage -> Discogs page -> common.webpage.category -> Topic Webpage\n# Answer:\nDiscogs page", "# Reasoning Path:\nElvis Costello -> people.person.sibling_s -> m.0jxyy78 -> people.sibling_relationship.sibling -> Ruari MacManus\n# Answer:\nRuari MacManus", "# Reasoning Path:\nElvis Costello -> music.artist.genre -> Rock music -> common.topic.notable_types -> Musical genre\n# Answer:\nRock music"], "ground_truth": ["Paddington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-216", "prediction": ["# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Eurasia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.location.containedby -> East Asia\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> common.topic.article -> m.0ff0q\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61fb\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.external_debt_stock -> g.11b71px2lk\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> government.governmental_jurisdiction.governing_officials -> m.010g61kh\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_h_gf\n# Answer:\nMongolia", "# Reasoning Path:\nGobi Desert -> location.location.containedby -> Mongolia -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71s_gmj\n# Answer:\nMongolia"], "ground_truth": ["Mongolia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-217", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Mexico\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-218", "prediction": ["# Reasoning Path:\nGermany -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Western Europe -> location.location.containedby -> Europe\n# Answer:\nWestern Europe", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> base.locations.countries.continent -> Europe -> base.locations.continents.planet -> Earth\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.location.containedby -> Europe -> organization.organization_scope.organizations_with_this_scope -> European Union\n# Answer:\nEurope", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Alps -> location.location.containedby -> Europe\n# Answer:\nAlps"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc38hnh\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-220", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Armenia\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> common.topic.notable_types -> Human Language\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> language.human_language.region -> Asia\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Syria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Algeria\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Kurdish languages", "Zaza language", "Turkish Language", "Arabic Language", "Ladino Language"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.7164179104477612, "path_precision": 0.8888888888888888, "path_recall": 0.6, "path_ans_f1": 0.7164179104477612, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.6}
{"id": "WebQTest-221", "prediction": ["# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0ng9xr6 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.team -> New York Knicks\n# Answer:\nNew York Knicks", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0_qrbd1 -> base.schemastaging.athlete_salary.team -> Houston Rockets\n# Answer:\nHouston Rockets", "# Reasoning Path:\nJeremy Lin -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nJeremy Lin -> base.schemastaging.athlete_extra.salary -> m.0j2mvjf -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Lin -> sports.pro_athlete.teams -> m.0k6s01p -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard"], "ground_truth": ["Los Angeles Lakers"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-222", "prediction": ["# Reasoning Path:\nSeychelles -> location.location.containedby -> Southern Sun Hotel Group -> location.location.containedby -> g.1pxpr66mx\n# Answer:\nSouthern Sun Hotel Group", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> organization.organization_scope.organizations_with_this_scope -> Southern African Development Community\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.1hhc3_l38\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Zimbabwe\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> base.locations.continents.countries_within -> Zimbabwe\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> base.locations.countries.continent -> Africa -> location.location.time_zones -> Seychelles Time Zone\n# Answer:\nAfrica", "# Reasoning Path:\nSeychelles -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qygcz\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nSeychelles -> location.location.containedby -> Africa -> base.locations.continents.countries_within -> Algeria\n# Answer:\nAfrica"], "ground_truth": ["Africa", "Southern Sun Hotel Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-224", "prediction": ["# Reasoning Path:\nWilliam Morris -> people.person.education -> m.0n1jtn3 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.02wmrt8 -> education.education.institution -> Exeter College, Oxford\n# Answer:\nExeter College, Oxford", "# Reasoning Path:\nWilliam Morris -> people.person.education -> m.04q1gj3 -> education.education.institution -> Marlborough College\n# Answer:\nMarlborough College", "# Reasoning Path:\nWilliam Morris -> influence.influence_node.influenced_by -> John Ruskin -> people.person.education -> m.0gy59l1\n# Answer:\nJohn Ruskin", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.has_sentences -> After marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish. -> base.kwebbase.kwsentence.next_sentence -> The Red House was the first of this type of domestic architecture in nineteenth century England.\n# Answer:\nAfter marrying, Morris commissioned an architect friend to build him a home at Bexleyheath in Kent which became known as the Red House  because it was built of red bricks with no stucco finish.", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of alphonse mucha -> base.kwebbase.kwconnection.relation -> a pal of\n# Answer:\nwilliam morris a pal of alphonse mucha", "# Reasoning Path:\nWilliam Morris -> influence.influence_node.influenced_by -> John Ruskin -> people.person.education -> m.02wn8vd\n# Answer:\nJohn Ruskin", "# Reasoning Path:\nWilliam Morris -> base.kwebbase.kwtopic.connections_from -> william morris a pal of edward coley burne-jones -> base.kwebbase.kwconnection.sentence -> At university Morris met the future artist Burne-Jones who became a lifelong friend.\n# Answer:\nwilliam morris a pal of edward coley burne-jones"], "ground_truth": ["University of Oxford", "Exeter College, Oxford"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-225", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> United States of America\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> United States, with Territories\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> base.aareas.schema.administrative_area.capital -> Olympia\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size"], "ground_truth": ["Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-226", "prediction": ["# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.010260w1\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> fictional_universe.character_occupation.characters_with_this_occupation -> Frasier Crane\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Disc jockey -> people.profession.specialization_of -> Entertainer\n# Answer:\nDisc jockey", "# Reasoning Path:\nJimmy Savile -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> common.topic.subjects -> Chris Free\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Presenter -> fictional_universe.character_occupation.characters_with_this_occupation -> Toby Isaacs\n# Answer:\nPresenter", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> base.descriptive_names.names.descriptive_name -> m.0102636p\n# Answer:\nRadio personality", "# Reasoning Path:\nJimmy Savile -> people.person.profession -> Radio personality -> people.profession.specialization_of -> Presenter\n# Answer:\nRadio personality"], "ground_truth": ["Disc jockey", "Presenter", "Radio personality"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-227", "prediction": ["# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.province.capital -> Toronto\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> periodicals.newspaper_circulation_area.newspapers -> Toronto Star\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> common.topic.notable_types -> US State\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> book.book_subject.works -> All Around the Town\n# Answer:\nNew York", "# Reasoning Path:\nSeneca people -> people.ethnicity.geographic_distribution -> New York -> location.location.geolocation -> m.046vxf5\n# Answer:\nNew York"], "ground_truth": ["New York", "Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> common.topic.webpage -> m.0cqsmqk -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-231", "prediction": ["# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crtdb6 -> sports.sports_league_participation.league -> National League East\n# Answer:\nNational League East", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> sports.sports_team.championships -> 1997 World Series\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt4jw -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.league -> m.0crt79d -> sports.sports_league_participation.league -> National League\n# Answer:\nNational League", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> common.topic.notable_types -> Sports Team\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> common.topic.notable_for -> g.1255pdp5m\n# Answer:\nFlorida Marlins", "# Reasoning Path:\nMiami Marlins -> award.award_nominee.award_nominations -> m.0z6d3cn -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nMiami Marlins -> sports.sports_team.previously_known_as -> Florida Marlins -> sports.sports_team.sport -> Baseball\n# Answer:\nFlorida Marlins"], "ground_truth": ["1994 Major League Baseball Season"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-232", "prediction": ["# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.location.containedby -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.official_language -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.form_of_government -> Unitary state\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> location.country.languages_spoken -> French\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.form_of_government -> Parliamentary system\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.country.official_language -> English Language\n# Answer:\nGibraltar", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Equatorial Guinea -> base.locations.countries.continent -> Africa\n# Answer:\nEquatorial Guinea", "# Reasoning Path:\nSpanish Language -> language.human_language.countries_spoken_in -> Gibraltar -> location.administrative_division.country -> United Kingdom\n# Answer:\nGibraltar"], "ground_truth": ["Honduras", "Cuba", "Venezuela", "Colombia", "Vatican City", "Chile", "Western Sahara", "Argentina", "Andorra", "Equatorial Guinea", "Barbados", "Canada", "Bolivia", "Guyana", "Nicaragua", "Costa Rica", "El Salvador", "Dominican Republic", "Kingdom of Aragon", "Belize", "Cura\u00e7ao", "United States of America", "Ecuador", "Puerto Rico", "Peru", "Guatemala", "Spain", "Gibraltar", "Uruguay", "Panama", "Mexico", "Northern Mariana Islands", "Paraguay", "Saint Lucia"], "ans_acc": 0.058823529411764705, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 1.0, "ans_recall": 0.058823529411764705, "path_f1": 0.1111111111111111, "path_precision": 1.0, "path_recall": 0.058823529411764705, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 1.0, "path_ans_recall": 0.058823529411764705}
{"id": "WebQTest-233", "prediction": ["# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Yavapai County\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0_ly_wt\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.geolocation -> m.011_ykm1\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Ro Ho En -> common.topic.article -> m.03qmw7w\n# Answer:\nRo Ho En", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Grand Canyon -> travel.tourist_attraction.near_travel_destination -> Grand Canyon National Park\n# Answer:\nGrand Canyon", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> common.topic.notable_for -> g.1z2v7jngb\n# Answer:\nCastle Hot Springs", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Ro Ho En -> common.topic.image -> RO HO EN 2007 Dec 08 40230-1\n# Answer:\nRo Ho En", "# Reasoning Path:\nPhoenix -> travel.travel_destination.tourist_attractions -> Castle Hot Springs -> location.location.containedby -> Arizona\n# Answer:\nCastle Hot Springs"], "ground_truth": ["Heard Museum", "Arizona Science Center", "Castle Hot Springs", "Pueblo Grande Ruin", "Desert Botanical Garden", "Phoenix Art Museum", "Mesa Arts Center", "Musical Instrument Museum", "Grand Canyon", "Ro Ho En", "Phoenix Mountain Preserve"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 1.0, "ans_recall": 0.2727272727272727, "path_f1": 0.42857142857142855, "path_precision": 1.0, "path_recall": 0.2727272727272727, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 1.0, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-234", "prediction": ["# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Northern Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> North America\n# Answer:\nPanama", "# Reasoning Path:\nPanama Canal -> base.engineering.canal.major_cities -> Col\u00f3n -> location.location.containedby -> Panama\n# Answer:\nCol\u00f3n", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> location.location.containedby -> Western Hemisphere\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> base.locations.continents.countries_within -> United States of America\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> North America -> cvg.computer_game_region.versions_released_in_this_region -> 2Moons\n# Answer:\nNorth America", "# Reasoning Path:\nPanama Canal -> location.location.containedby -> Panama -> location.location.containedby -> Central America\n# Answer:\nPanama"], "ground_truth": ["Panama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-235", "prediction": ["# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Screenwriter -> common.topic.notable_types -> Profession\n# Answer:\nScreenwriter", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Comedian -> common.topic.notable_types -> Profession\n# Answer:\nComedian", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Score Composer -> common.topic.notable_types -> Profession\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Producer -> common.topic.notable_types -> Profession\n# Answer:\nFilm Producer", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film Score Composer -> people.profession.specialization_of -> Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nCharlie Chaplin -> influence.influence_node.influenced -> Benny Hill -> people.person.profession -> Comedian\n# Answer:\nBenny Hill", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film director -> common.topic.notable_types -> Profession\n# Answer:\nFilm director", "# Reasoning Path:\nCharlie Chaplin -> people.person.profession -> Film director -> freebase.type_hints.included_types -> Person\n# Answer:\nFilm director"], "ground_truth": ["Film Producer", "Actor", "Film Editor", "Screenwriter", "Composer", "Film Score Composer", "Comedian", "Film director"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.7291666666666666, "path_precision": 0.875, "path_recall": 0.625, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-237", "prediction": ["# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> George Harrison\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> common.topic.notable_types -> Profession\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> common.topic.subjects -> Steve Blevins\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Guitarist -> people.profession.specialization_of -> Musician\n# Answer:\nGuitarist", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nFred Durst -> people.person.profession -> Musician -> fictional_universe.character_occupation.characters_with_this_occupation -> John Lennon\n# Answer:\nMusician"], "ground_truth": ["Rapper", "Actor", "Singer", "Guitarist", "Businessperson", "Songwriter", "Film director", "Musician"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 1.0, "ans_recall": 0.375, "path_f1": 0.5454545454545454, "path_precision": 1.0, "path_recall": 0.375, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 1.0, "path_ans_recall": 0.375}
{"id": "WebQTest-239", "prediction": ["# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Mississippi\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> location.country.administrative_divisions -> Alabama\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nAmerican Bulldog -> biology.animal_breed.place_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> Mississippi\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground -> location.location.containedby -> Baltimore\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore -> location.location.containedby -> Maryland\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground -> people.place_of_interment.interred_here -> David Poe, Jr.\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground -> location.location.containedby -> University of Maryland School of Law\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground -> common.topic.image -> WestminsterChurch1857\n# Answer:\nWestminster Hall and Burying Ground"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-240", "prediction": ["# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> business.employment_tenure.company -> University of Zurich\n# Answer:\nUniversity of Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq4ch -> business.employment_tenure.company -> Leiden University\n# Answer:\nLeiden University", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3h6 -> business.employment_tenure.company -> Charles University in Prague\n# Answer:\nCharles University in Prague", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.04hdfs2 -> business.employment_tenure.company -> ETH Zurich\n# Answer:\nETH Zurich", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.02kq3y4 -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo", "# Reasoning Path:\nAlbert Einstein -> people.person.employment_history -> m.03q49lz -> business.employment_tenure.company -> Institute for Advanced Study\n# Answer:\nInstitute for Advanced Study", "# Reasoning Path:\nAlbert Einstein -> people.person.education -> m.0k05ps4 -> education.education.institution -> University of Zurich\n# Answer:\nUniversity of Zurich"], "ground_truth": ["ETH Zurich", "Leiden University", "University of Zurich", "Charles University in Prague", "Institute for Advanced Study"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.7058823529411765, "path_precision": 0.75, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-241", "prediction": ["# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> film.film_story_contributor.film_story_credits -> Family Guy Presents: Stewie Griffin: The Untold Story\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02kk65p\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> award.award_winner.awards_won -> m.0ly_pz4\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.characters -> God\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> common.topic.notable_for -> g.1255sxsvl\n# Answer:\nFamily Guy Universe", "# Reasoning Path:\nFamily Guy -> tv.tv_program.program_creator -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02s9hdf\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nFamily Guy -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Family Guy Universe -> fictional_universe.fictional_universe.works_set_here -> Family Guy Video Game!\n# Answer:\nFamily Guy Universe"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-243", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> base.biblioness.bibs_location.state -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> base.ndbcd.buoy.region -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> event.public_speaker.speeches_or_presentations -> m.05_fz1k -> event.speech_or_presentation.type_or_format_of_presentation -> State of the Union\n# Answer:\nState of the Union", "# Reasoning Path:\nGeorge W. Bush -> people.person.place_of_birth -> New Haven -> location.location.containedby -> Connecticut\n# Answer:\nNew Haven", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["New Haven"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-245", "prediction": ["# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> time.event.locations -> Atlantic City\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_for -> g.125bn860g\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_for -> g.125ft33p8\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1928 Miss America -> award.competition.type_of_competition -> Beauty Pageant\n# Answer:\n1928 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.current_frequency -> Yearly -> base.events.festival_event.part_of_series -> National Poetry Month\n# Answer:\nYearly", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1950 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1950 Miss America", "# Reasoning Path:\nMiss America -> time.recurring_event.instances -> 1929 Miss America -> common.topic.notable_types -> Competition\n# Answer:\n1929 Miss America"], "ground_truth": ["1921-09"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-246", "prediction": ["# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Jozef Murga\u0161\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.notable_types -> Invention\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.notable_for -> g.125d8rdgm\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.image -> Hertz schematic0\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> law.invention.inventor -> Heinrich Hertz\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> common.topic.notable_for -> g.1256brrvw\n# Answer:\nWireless telegraphy", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Alternating current -> common.topic.image -> City lights viewed in a motion blurred exposure. The AC blinking causes the lines to be dotted rather than continuous\n# Answer:\nAlternating current", "# Reasoning Path:\nNikola Tesla -> law.inventor.inventions -> Wireless telegraphy -> book.book_subject.works -> Recent Advances in Wireless Telegraphy\n# Answer:\nWireless telegraphy"], "ground_truth": ["Tesla coil", "Neon lamp", "Three-phase electric power", "Induction motor", "Alternating current", "Wireless telegraphy", "Remote control"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 1.0, "ans_recall": 0.2857142857142857, "path_f1": 0.4444444444444445, "path_precision": 1.0, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 1.0, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-247", "prediction": ["# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Austrians\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.geographic_distribution -> Northern Europe\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Danes\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> British people\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.people -> \u00c9mil Goeldi\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> Dutch people\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples\n# Answer:\nGermanic peoples", "# Reasoning Path:\nFlemish people -> common.topic.article -> m.018hm2\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFlemish people -> people.ethnicity.included_in_group -> Germanic peoples -> people.ethnicity.includes_groups -> English people\n# Answer:\nGermanic peoples"], "ground_truth": ["Canada", "Australia", "Belgium", "Brazil", "South Africa", "France", "United States of America"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-248", "prediction": ["# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.biblioness.bibs_location.state -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> base.aareas.schema.administrative_area.administrative_parent -> New York -> location.location.containedby -> United States of America\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> Area code 917 -> location.location.containedby -> New York\n# Answer:\nArea code 917", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> government.governmental_jurisdiction.government_bodies -> United States Bankruptcy Court for the Southern District of New York\n# Answer:\nNew York", "# Reasoning Path:\nNew York City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew York City -> location.location.containedby -> New York -> travel.travel_destination.tourist_attractions -> Ellis Island\n# Answer:\nNew York"], "ground_truth": ["New York"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-249", "prediction": ["# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Richard Nixon\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> medicine.disease_cause.diseases -> Global aphasia\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.article -> m.0219c4\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Scott Paul\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> medicine.disease.parent_disease -> Nervous system disease\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> Alan Ladd\n# Answer:\nCerebral edema", "# Reasoning Path:\nBruce Lee -> people.deceased_person.cause_of_death -> Cerebral edema -> people.cause_of_death.people -> David Sharp\n# Answer:\nCerebral edema"], "ground_truth": ["Cerebral edema"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-250", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> South Korea\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> American Samoa\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Germany\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Northern Mariana Islands\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mexico\n# Answer:\nConstitutional republic", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guam\n# Answer:\nPresidential system", "# Reasoning Path:\nUnited States of America -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Germany\n# Answer:\nFederal republic", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> m.010r7f0z\n# Answer:\nbase.schemastaging.context_name.pronunciation"], "ground_truth": ["Constitutional republic", "Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-251", "prediction": ["# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Northern Ireland\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Wales\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> common.topic.notable_types -> Country -> type.type.properties -> Official Language\n# Answer:\nCountry", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> United Kingdom\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> common.topic.notable_types -> Country -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of England\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> common.topic.notable_types -> Country -> freebase.type_hints.included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nEngland -> location.country.currency_used -> Pound sterling -> finance.currency.countries_used -> Kingdom of Great Britain\n# Answer:\nPound sterling", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Pound sterling"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-252", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> education.education.institution -> Boston Latin School\n# Answer:\nBoston Latin School", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.is_reviewed -> Institution\n# Answer:\nInstitution", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBenjamin Franklin -> people.person.profession -> Diplomat -> fictional_universe.character_occupation.characters_with_this_occupation -> Jeffrey Sinclair\n# Answer:\nDiplomat", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.education -> m.040vjzw -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor"], "ground_truth": ["Boston Latin School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-253", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Line Digital Ltd\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-254", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.location.containedby -> Eurasia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.location.containedby -> Eurasia\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.location.containedby -> Eurasia\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.languages_spoken -> Nepali Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.form_of_government -> Communist state\n# Answer:\nVietnam", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.languages_spoken -> Esperanto Language\n# Answer:\nVietnam"], "ground_truth": ["Canada", "China", "Malaysia", "Sh\u011bn", "Vietnam", "Singapore", "Brunei"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.5753424657534246, "path_precision": 0.875, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-255", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> education.education.institution -> Noelani Elementary School\n# Answer:\nNoelani Elementary School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgp6 -> education.education.institution -> Punahou School\n# Answer:\nPunahou School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.0nbyhqc -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> 47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares -> book.written_work.subjects -> Guerrilla Decontextualization\n# Answer:\n47 Percenters and Guerrilla Decontextualization (Part 1): Dreams and Nightmares"], "ground_truth": ["Columbia University", "Harvard Law School", "St. Francis of Assisi Catholic School", "State Elementary School Menteng 01", "Noelani Elementary School", "Punahou School", "Occidental College"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.625, "ans_recall": 0.7142857142857143, "path_f1": 0.6666666666666666, "path_precision": 0.625, "path_recall": 0.7142857142857143, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.625, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-256", "prediction": ["# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.language_dialect.language -> Chinese language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.statistical_region.official_development_assistance -> g.12cp_kgkf\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.breed_of -> Dog\n# Answer:\nTibetan Terrier"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-257", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zj -> government.government_position_held.office_holder -> Leonid Brezhnev\n# Answer:\nLeonid Brezhnev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_x -> government.government_position_held.office_holder -> Mikhail Sergeyevich Gorbachev\n# Answer:\nMikhail Sergeyevich Gorbachev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko"], "ground_truth": ["Leonid Brezhnev", "Nikita Khrushchev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-258", "prediction": ["# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp9 -> location.imports_exports_by_industry.industry -> Agriculture\n# Answer:\nAgriculture", "# Reasoning Path:\nAustralia -> location.statistical_region.major_exports -> m.0cnqyp5 -> location.imports_exports_by_industry.industry -> Energy industry\n# Answer:\nEnergy industry", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.diesel_price_liter -> g.1hhc3z1j5\n# Answer:\nlocation.statistical_region.diesel_price_liter", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.main_country -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language"], "ground_truth": ["Agriculture", "Energy industry"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> sports.sports_facility.teams -> Atlanta Falcons\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> location.location.containedby -> Fulton County\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.0ccdpz0\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> common.topic.notable_types -> Sports Facility\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola -> common.topic.notable_types -> Museum\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium"], "ground_truth": ["Cobb Energy Performing Arts Centre", "Atlanta Jewish Film Festival", "Margaret Mitchell House & Museum", "Four Seasons Hotel Atlanta", "World of Coca-Cola", "Masquerade", "Turner Field", "Fernbank Museum of Natural History", "Omni Coliseum", "Atlanta History Center", "Fernbank Science Center", "CNN Center", "Georgia State Capitol", "Philips Arena", "Atlanta Ballet", "Centennial Olympic Park", "Atlanta Cyclorama & Civil War Museum", "Georgia World Congress Center", "Fox Theatre", "Woodruff Arts Center", "Hyatt Regency Atlanta", "Atlanta Symphony Orchestra", "Martin Luther King, Jr. National Historic Site", "Underground Atlanta", "Six Flags White Water", "Zoo Atlanta", "The Tabernacle", "Jimmy Carter Library and Museum", "Variety Playhouse", "Arbor Place Mall", "Peachtree Road Race", "Atlanta Marriott Marquis", "Georgia Aquarium", "Six Flags Over Georgia", "Center for Puppetry Arts", "Georgia Dome"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 1.0, "ans_recall": 0.1111111111111111, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 1.0, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-260", "prediction": ["# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> base.arthist.helynevek.intezmeny -> Church of the Holy Sepulchre\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Dominion of Melchizedek\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Mutasarrifate of Jerusalem\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Jerusalem District\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> travel.travel_destination.tourist_attractions -> Yad Vashem\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.geolocation -> m.0cmh6pq\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> Mandatory Palestine\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> location.location.containedby -> State of Palestine\n# Answer:\nJerusalem", "# Reasoning Path:\nCity of David -> location.location.containedby -> Jerusalem -> travel.travel_destination.tourist_attractions -> Bethphage\n# Answer:\nJerusalem"], "ground_truth": ["Jerusalem"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-261", "prediction": ["# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> type.type.expected_by -> Artist\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> common.topic.subject_of -> Michael Godard\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> freebase.documented_object.documentation -> m.01z0r7h\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Artist -> common.topic.notable_types -> Profession\n# Answer:\nArtist", "# Reasoning Path:\nCindy Sherman -> common.topic.notable_types -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> type.type.expected_by -> producer\n# Answer:\nVisual Artist", "# Reasoning Path:\nCindy Sherman -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist"], "ground_truth": ["Photographer", "Film director", "Visual Artist", "Artist"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-262", "prediction": ["# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.0j9dzw6 -> tv.regular_tv_appearance.series -> The Stand\n# Answer:\nThe Stand", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.0bg_048 -> tv.regular_tv_appearance.series -> The Tom Show\n# Answer:\nThe Tom Show", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.film -> The Stand\n# Answer:\nThe Stand", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.05cmxdm -> tv.regular_tv_appearance.series -> Scream Queens\n# Answer:\nScream Queens", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.0j_kmjv -> tv.regular_tv_appearance.series -> Anger Management\n# Answer:\nAnger Management", "# Reasoning Path:\nShawnee Smith -> tv.tv_actor.starring_roles -> m.03jq10c -> tv.regular_tv_appearance.series -> Becker\n# Answer:\nBecker", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.098d405 -> film.performance.character -> Julie Lawry\n# Answer:\nJulie Lawry", "# Reasoning Path:\nShawnee Smith -> film.actor.film -> m.02vc307 -> film.performance.film -> The Blob\n# Answer:\nThe Blob"], "ground_truth": ["Anger Management", "30 Days of Night: Dust to Dust", "The Tom Show", "The Stand", "Becker", "Scream Queens", "Arsenio", "Brand New Life", "All is Forgiven"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.6382978723404256, "ans_precission": 0.75, "ans_recall": 0.5555555555555556, "path_f1": 0.6666666666666665, "path_precision": 0.75, "path_recall": 0.6, "path_ans_f1": 0.6382978723404256, "path_ans_precision": 0.75, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-264", "prediction": ["# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Tuvalu\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> common.topic.notable_types -> Currency\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Christmas Island\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.diesel_price_liter -> g.12cp_h_97\n# Answer:\nlocation.statistical_region.diesel_price_liter", "# Reasoning Path:\nAustralia -> location.country.currency_used -> Australian dollar -> finance.currency.countries_used -> Kiribati\n# Answer:\nAustralian dollar", "# Reasoning Path:\nAustralia -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71l32x_\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Australian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-265", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> location.location.containedby -> Italy\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> location.location.geolocation -> m.04fngwt\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> aviation.airport.hub_for -> Ala Littoria\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> common.topic.notable_for -> g.125frds3n\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> location.location.containedby -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport"], "ground_truth": ["Leonardo da Vinci\u2013Fiumicino Airport", "Ciampino\u2013G. B. Pastine International Airport"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-266", "prediction": ["# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> location.location.time_zones -> Central Time Zone\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> amusement_parks.park.rides -> Zambezi Zinger\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> location.location.geolocation -> m.04ksmhv\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Kauffman Center for the Performing Arts -> architecture.structure.architectural_style -> High-tech architecture\n# Answer:\nKauffman Center for the Performing Arts", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Worlds of Fun -> projects.project_focus.projects -> Construction of Worlds of Fun\n# Answer:\nWorlds of Fun", "# Reasoning Path:\nKansas City -> location.statistical_region.population -> g.11b66slpck\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Kauffman Center for the Performing Arts -> common.topic.notable_types -> Venue\n# Answer:\nKauffman Center for the Performing Arts", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> amusement_parks.park.rides -> Typhoon\n# Answer:\nOceans of Fun", "# Reasoning Path:\nKansas City -> travel.travel_destination.tourist_attractions -> Oceans of Fun -> common.topic.article -> m.08c_19\n# Answer:\nOceans of Fun"], "ground_truth": ["Negro Leagues Baseball Museum", "Crown Center", "Kauffman Center for the Performing Arts", "18th and Vine-Downtown East", "Boulevard Brewing Company", "Deer Creek Golf Course", "Ewing and Muriel Kauffman Memorial Garden", "Oceans of Fun", "Worlds of Fun", "Kansas City Zoo", "Sprint Center", "Kemper Museum of Contemporary Art", "Hotel Phillips", "Liberty Memorial", "Harry S. Truman Presidential Library and Museum"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.326530612244898, "ans_precission": 0.8888888888888888, "ans_recall": 0.2, "path_f1": 0.326530612244898, "path_precision": 0.8888888888888888, "path_recall": 0.2, "path_ans_f1": 0.326530612244898, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.2}
{"id": "WebQTest-267", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> travel.travel_destination.tourist_attractions -> Martin Luther King, Jr. National Historic Site\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> base.biblioness.bibs_location.state -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.place_of_birth -> Atlanta -> location.location.containedby -> Georgia\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nDexter Avenue Baptist Church", "# Reasoning Path:\nMartin Luther King, Jr. -> common.topic.webpage -> m.0cqsmqk -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America"], "ground_truth": ["Atlanta"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-268", "prediction": ["# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.0w0h79l -> sports.sports_award.season -> 2012\u201313 NBA season\n# Answer:\n2012\u201313 NBA season", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxp7 -> sports.sports_award.season -> 2007\u201308 NBA season\n# Answer:\n2007\u201308 NBA season", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.02kbccr -> sports.sports_award.award -> NBA Rookie of the Year Award\n# Answer:\nNBA Rookie of the Year Award", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.sports_award_winner.awards -> m.04ynxph -> sports.sports_award.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season"], "ground_truth": ["2003\u201304 NBA season"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-269", "prediction": ["# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> geography.river.origin -> Schoharie County\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> common.topic.article -> m.036r_d\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.geolocation -> m.02_f418\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> location.location.containedby -> United States of America\n# Answer:\nWest Branch Delaware River", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> location.location.containedby -> New York\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> common.topic.notable_types -> Mountain\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> common.topic.article -> m.0dtzn\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDelaware River -> geography.river.origin -> Mount Jefferson -> common.topic.notable_for -> g.1257280d8\n# Answer:\nMount Jefferson", "# Reasoning Path:\nDelaware River -> geography.river.origin -> West Branch Delaware River -> common.topic.notable_for -> g.1258hkxlv\n# Answer:\nWest Branch Delaware River"], "ground_truth": ["Mount Jefferson", "West Branch Delaware River"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-270", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Tuvalu\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6gg_n\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Unitary state", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-271", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Anna Amelia Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.place_of_birth -> Colonia Dubl\u00e1n\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.parents -> Harold A. Lafount\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.parents -> Gaskell Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.profession -> Politician\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> Lenore Romney -> people.person.place_of_birth -> Logan\n# Answer:\nLenore Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> people.person.children -> Jane Romney\n# Answer:\nGeorge W. Romney", "# Reasoning Path:\nMitt Romney -> people.person.parents -> George W. Romney -> base.politicalconventions.primary_candidate.pres_delegate_vote_tally -> m.04j779b\n# Answer:\nGeorge W. Romney"], "ground_truth": ["Detroit"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-273", "prediction": ["# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Oscar Robertson Trophy\n# Answer:\nOscar Robertson Trophy", "# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Atlantic Coast Conference Men's Basketball Player of the Year\n# Answer:\nAtlantic Coast Conference Men's Basketball Player of the Year", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.draft -> 1984 NBA draft\n# Answer:\n1984 NBA draft", "# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Award share\n# Answer:\nAward share", "# Reasoning Path:\nMichael Jordan -> common.image.appears_in_topic_gallery -> Head shaving\n# Answer:\nHead shaving", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.school -> University of North Carolina at Chapel Hill\n# Answer:\nUniversity of North Carolina at Chapel Hill", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Esophageal cancer\n# Answer:\nMale", "# Reasoning Path:\nMichael Jordan -> film.person_or_entity_appearing_in_film.films -> m.010qqf1x -> film.personal_film_appearance.film -> Marathon\n# Answer:\nMarathon", "# Reasoning Path:\nMichael Jordan -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Pancreatic cancer\n# Answer:\nMale"], "ground_truth": ["1984"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-276", "prediction": ["# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> San Francisco Bay Area\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> San Francisco-San Mateo-Redwood City, CA Metropolitan Division\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> California\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> 94901 -> location.location.containedby -> Marin County\n# Answer:\n94901", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> common.topic.notable_for -> g.125d4sc6y\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.location.containedby -> North Bay\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> base.localfood.food_producing_region.seasonal_availability -> m.04_jf_1\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07h9_f5\n# Answer:\nMarin County", "# Reasoning Path:\nMission San Rafael Arc\u00e1ngel -> location.location.containedby -> Marin County -> location.statistical_region.population -> g.11b674q3py\n# Answer:\nMarin County"], "ground_truth": ["94901", "Marin County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-277", "prediction": ["# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.spouse -> Xerxes I\n# Answer:\nXerxes I", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.0130wv8p -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.spouse -> Ahasuerus\n# Answer:\nAhasuerus", "# Reasoning Path:\nEsther -> common.topic.image -> Esthermillais -> common.image.appears_in_topic_gallery -> Book of Esther\n# Answer:\nEsthermillais", "# Reasoning Path:\nEsther -> people.person.spouse_s -> m.012zbnxz -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nEsther -> book.book_subject.works -> Daily Inspiration for Finding Favor With the King -> book.written_work.subjects -> The Bible\n# Answer:\nDaily Inspiration for Finding Favor With the King", "# Reasoning Path:\nEsther -> people.person.ethnicity -> Jewish people -> people.ethnicity.included_in_group -> Semitic people\n# Answer:\nJewish people", "# Reasoning Path:\nEsther -> people.person.religion -> Judaism -> religion.religion.notable_figures -> Theodor Herzl\n# Answer:\nJudaism"], "ground_truth": ["Xerxes I", "Ahasuerus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3076923076923077, "path_precision": 0.25, "path_recall": 0.4, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-278", "prediction": ["# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.065q9gy -> base.popstra.dated.participant -> Billy Crudup\n# Answer:\nBilly Crudup", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t679 -> base.popstra.dated.participant -> Matt Damon\n# Answer:\nMatt Damon", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t67_ -> base.popstra.dated.participant -> Billy Crudup\n# Answer:\nBilly Crudup", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.05cw3vz -> celebrities.romantic_relationship.celebrity -> Hugh Dancy\n# Answer:\nHugh Dancy", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t67n -> base.popstra.dated.participant -> Hugh Dancy\n# Answer:\nHugh Dancy", "# Reasoning Path:\nClaire Danes -> base.popstra.celebrity.dated -> m.063t68b -> base.popstra.dated.participant -> Ben Lee\n# Answer:\nBen Lee", "# Reasoning Path:\nClaire Danes -> celebrities.celebrity.sexual_relationships -> m.05cw3vz -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nClaire Danes -> film.actor.film -> m.012w8_c9 -> film.performance.film -> Princess Mononoke\n# Answer:\nPrincess Mononoke"], "ground_truth": ["Hugh Dancy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-279", "prediction": ["# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> Virginia\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> Virginia\n# Answer:\nWise County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.location.containedby -> United States of America\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.hud_foreclosure_area.hhuniv -> m.07cj61p\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.citytown.postal_codes -> 24283 -> location.location.containedby -> Wise County\n# Answer:\n24283", "# Reasoning Path:\nSt. Paul -> location.statistical_region.population -> g.11b66hhtgq\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Russell County -> location.administrative_division.capital -> m.0jvvsmf\n# Answer:\nRussell County", "# Reasoning Path:\nSt. Paul -> location.location.containedby -> Wise County -> location.location.containedby -> United States of America\n# Answer:\nWise County"], "ground_truth": ["Wise County", "Russell County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> common.topic.notable_types -> City/Town/Village\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.office_position_or_title -> Member of the Queensland Legislative Assembly\n# Answer:\nMember of the Queensland Legislative Assembly", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.people_born_here -> Adrienne Pickering\n# Answer:\nWarwick", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-281", "prediction": ["# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> sports.sports_team.sport -> Cycling\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Japan women's national volleyball team -> sports.sports_team.sport -> Volleyball\n# Answer:\nJapan women's national volleyball team", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> common.topic.notable_for -> g.1yl5j5_sh\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Japan men's national volleyball team -> sports.sports_team.sport -> Volleyball\n# Answer:\nJapan men's national volleyball team", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> common.topic.notable_for -> g.1ypjsqnlh\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> common.topic.notable_types -> Sports Team\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Burning -> sports.sports_team.sport -> Professional wrestling\n# Answer:\nBurning", "# Reasoning Path:\nJapan -> sports.sports_team_location.teams -> Bridgestone-Anchor -> common.topic.article -> m.0qfps1b\n# Answer:\nBridgestone-Anchor", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Japan women's national volleyball team", "Japan national baseball team", "Japan national handball team", "Japan women's national handball team", "Japan men's national volleyball team", "Japan national football team", "Burning", "Bridgestone-Anchor"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-282", "prediction": ["# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Mongolia\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> freebase.valuenotation.is_reviewed -> End date\n# Answer:\n2010 Winter Olympics"], "ground_truth": ["China", "North Korea", "Mongolia", "Lithuania", "Finland", "Azerbaijan", "Latvia", "Belarus", "Georgia", "Poland", "Ukraine", "Norway", "Kazakhstan", "Estonia"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.025531914893617023, "path_precision": 0.3, "path_recall": 0.013333333333333334, "path_ans_f1": 0.19354838709677416, "path_ans_precision": 0.3, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-284", "prediction": ["# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.image -> Iron Age Italy\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_for -> g.125d12rwn\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> base.rosetta.languoid.parent -> Latino-Faliscan Group -> base.rosetta.languoid.parent -> Italic Group\n# Answer:\nLatino-Faliscan Group", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Aequian language\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> base.ontologies.ontology_instance.equivalent_instances -> m.09dvm1q\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Gallo-Italic of Sicily\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> common.topic.notable_types -> Language Family\n# Answer:\nItalic languages", "# Reasoning Path:\nLatin Language -> language.human_language.language_family -> Italic languages -> language.language_family.languages -> Angevin dialect\n# Answer:\nItalic languages"], "ground_truth": ["Indo-European languages", "Italic languages"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-286", "prediction": ["# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nKarlsruhe Institute of Technology", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.0n0nxj5 -> education.education.institution -> Karlsruhe Institute of Technology\n# Answer:\nKarlsruhe Institute of Technology", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.degree -> Bachelor of Engineering\n# Answer:\nBachelor of Engineering", "# Reasoning Path:\nKarl Benz -> people.person.education -> m.04hddz6 -> education.education.major_field_of_study -> Mechanical Engineering\n# Answer:\nMechanical Engineering", "# Reasoning Path:\nKarl Benz -> freebase.valuenotation.is_reviewed -> Children -> rdf-schema#range -> Person\n# Answer:\nChildren", "# Reasoning Path:\nKarl Benz -> common.topic.notable_types -> Automotive Designer -> type.type.expected_by -> Designer\n# Answer:\nAutomotive Designer", "# Reasoning Path:\nKarl Benz -> common.topic.notable_types -> Automotive Designer -> freebase.type_profile.published -> Published\n# Answer:\nAutomotive Designer", "# Reasoning Path:\nKarl Benz -> people.deceased_person.place_of_death -> Ladenburg -> common.topic.article -> m.0272gd6\n# Answer:\nLadenburg"], "ground_truth": ["Karlsruhe Institute of Technology"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-287", "prediction": ["# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.film -> Dragula\n# Answer:\nDragula", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.012zh0pk -> film.performance.film -> Chocolate City\n# Answer:\nChocolate City", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.film -> Full of It\n# Answer:\nFull of It", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.0115j6sp -> film.performance.character -> Jaime\n# Answer:\nJaime", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02vc2zv -> film.performance.film -> I Want Candy\n# Answer:\nI Want Candy", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.03l9zsy -> film.performance.film -> Scary Movie 4\n# Answer:\nScary Movie 4", "# Reasoning Path:\nCarmen Electra -> award.award_nominee.award_nominations -> m.059phxs -> award.award_nomination.nominated_for -> Starsky & Hutch\n# Answer:\nStarsky & Hutch", "# Reasoning Path:\nCarmen Electra -> film.actor.film -> m.02tb13p -> film.performance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["30 Days Until I'm Famous", "Full of It", "Baywatch: Hawaiian Wedding", "Sol Goode", "The Back Nine", "Epic Movie", "2-Headed Shark Attack", "Good Burger", "Uptown Girls", "Book of Fire", "BThere Dvd Magazine", "Carmen Electra's Fit to Strip", "The Mating Habits of the Earthbound Human", "Lap Dance", "Perfume", "Monster Island", "Hot Tamale", "Whacked!", "American Dreamz", "My Boss's Daughter", "Cheaper by the Dozen 2", "Max Havoc: Curse of the Dragon", "Christmas in Wonderland", "Starstruck", "Scary Movie", "Getting Played", "Starsky & Hutch", "The Chosen One: Legend of the Raven", "Scary Movie 4", "American Vampire", "Disaster Movie", "Oy Vey! My Son Is Gay!!", "National Lampoon's Pledge This!", "Dragula", "The Axe Boat", "Get Over It", "Naked Movie", "Date Movie", "Mardi Gras: Spring Break", "I Want Candy", "Barry Munday", "Chocolate City", "Rent Control", "Lolo's Cafe", "We Are Family", "Dirty Love", "Bedtime Stories", "Lil' Pimp", "Mr. 3000", "Searching for Bobby D", "Meet the Spartans"], "ans_acc": 0.13725490196078433, "ans_hit": 1, "ans_f1": 0.23204419889502764, "ans_precission": 0.75, "ans_recall": 0.13725490196078433, "path_f1": 0.16, "path_precision": 0.75, "path_recall": 0.08955223880597014, "path_ans_f1": 0.23204419889502764, "path_ans_precision": 0.75, "path_ans_recall": 0.13725490196078433}
{"id": "WebQTest-289", "prediction": ["# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.contains_major_portion_of -> Kazakhstan\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Philippines\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Africa -> base.locations.continents.countries_within -> South Africa\n# Answer:\nAfrica", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.partially_contains -> Soviet Union\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> base.locations.continents.countries_within -> Brunei\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.events -> World War I\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Asia -> location.location.time_zones -> Singapore Time Zone\n# Answer:\nAsia", "# Reasoning Path:\nWorld War II -> time.event.locations -> Europe -> base.locations.continents.countries_within -> Bulgaria\n# Answer:\nEurope"], "ground_truth": ["1945-09-02"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-291", "prediction": ["# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.0n10623 -> education.education.institution -> Los Angeles City College\n# Answer:\nLos Angeles City College", "# Reasoning Path:\nMorgan Freeman -> people.person.education -> m.010665q2 -> education.education.institution -> Pasadena Playhouse\n# Answer:\nPasadena Playhouse", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093_l_6 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMorgan Freeman -> business.board_member.organization_board_memberships -> m.03xbg4g -> organization.organization_board_membership.organization -> Evolution Fuels, Inc.\n# Answer:\nEvolution Fuels, Inc.", "# Reasoning Path:\nMorgan Freeman -> tv.tv_personality.tv_regular_appearances -> m.0gdl5zl -> tv.tv_regular_personal_appearance.appearance_type -> Narrator\n# Answer:\nNarrator", "# Reasoning Path:\nMorgan Freeman -> tv.tv_producer.tv_episodes_produced -> m.0j764wf -> tv.tv_producer_episode_credit.episode -> Beyond the Darkness\n# Answer:\nBeyond the Darkness", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093zz6z -> common.webpage.resource -> Robert Iler pleads guilty to mugging charge\n# Answer:\nRobert Iler pleads guilty to mugging charge", "# Reasoning Path:\nMorgan Freeman -> common.topic.webpage -> m.093z_bp -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Pasadena Playhouse", "Los Angeles City College"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-294", "prediction": ["# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> people.profession.specialization_of -> Scientist\n# Answer:\nBiologist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> people.profession.specialization_of -> Scientist\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> common.topic.notable_types -> Profession\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> common.topic.notable_for -> g.12568993_\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.03yw74x\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nBotanist", "# Reasoning Path:\nCarl Linnaeus -> people.person.profession -> Biologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Professor Archimedes Q. Porter\n# Answer:\nBiologist"], "ground_truth": ["Scientist", "Biologist", "Botanist", "Physician"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.8571428571428571, "path_precision": 1.0, "path_recall": 0.75, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-295", "prediction": ["# Reasoning Path:\nGermany -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> Austria\n# Answer:\nFederal republic", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Czech Republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Austria\n# Answer:\nParliamentary republic", "# Reasoning Path:\nGermany -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71xcxgk\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nGermany -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Poland\n# Answer:\nParliamentary republic"], "ground_truth": ["Parliamentary republic", "Representative democracy", "Constitutional republic", "Federal republic"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-296", "prediction": ["# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Egypt\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.article -> m.02tz44\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.notable_types -> Form of Government\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> State of Palestine\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Provisional government -> common.topic.notable_for -> g.12578zvtz\n# Answer:\nProvisional government", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Italy\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Republic of Kosovo\n# Answer:\nParliamentary republic", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxq\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Parliamentary republic", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-297", "prediction": ["# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> common.topic.image -> Pst\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> travel.travel_destination.tourist_attractions -> Downtown Disney -> location.location.containedby -> United States of America\n# Answer:\nDowntown Disney", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> travel.travel_destination.tourist_attractions -> Downtown Disney -> location.location.containedby -> California\n# Answer:\nDowntown Disney", "# Reasoning Path:\nAnaheim -> travel.travel_destination.tourist_attractions -> Downtown Disney -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nDowntown Disney", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nAnaheim -> travel.travel_destination.tourist_attractions -> Disney California Adventure -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nDisney California Adventure", "# Reasoning Path:\nAnaheim -> location.statistical_region.population -> g.11b66d_m1v\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Pacific Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-298", "prediction": ["# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> time.event.locations -> Riverfront Stadium\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.runner_up -> Cincinnati Reds\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> sports.sports_championship_event.season -> 1970 Major League Baseball Season\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> sports.sports_championship_event.season -> 1966 Major League Baseball Season\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1970 World Series -> common.topic.article -> m.04jg82\n# Answer:\n1970 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1966 World Series -> common.topic.notable_for -> g.125br8lv_\n# Answer:\n1966 World Series", "# Reasoning Path:\nBaltimore Orioles -> sports.sports_team.championships -> 1983 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n1983 World Series"], "ground_truth": ["1970 World Series", "1966 World Series", "1983 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> David Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Albert Tatlock\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> film.actor.film -> m.0h0_mvx\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Alf Roberts\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> tv.tv_writer.episodes_written -> Fri 9 Dec, 1960\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Annie Walker\n# Answer:\nTony Warren"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-300", "prediction": ["# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> book.author.works_written -> Bacon\n# Answer:\nBacon", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Philosopher -> fictional_universe.character_occupation.characters_with_this_occupation -> Heraclitus\n# Answer:\nPhilosopher", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.subject_of -> Autobiography\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> base.descriptive_names.names.descriptive_name -> m.01053kvm\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Scientist -> common.topic.notable_types -> Profession\n# Answer:\nScientist", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> fictional_universe.character_occupation.characters_with_this_occupation -> Thomas Jefferson\n# Answer:\nStatesman", "# Reasoning Path:\nFrancis Bacon -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman"], "ground_truth": ["Philosopher", "Spy", "Author", "Statesman", "Scientist"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.8135593220338982, "path_precision": 0.8888888888888888, "path_recall": 0.75, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-301", "prediction": ["# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Aerobatic Pilot\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Airman\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.firsts.achievement.firsts -> m.0g8qg3r\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specializations -> Pilot trainer\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> fictional_universe.character_occupation.characters_with_this_occupation -> Wrongway Feldman\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> people.profession.specialization_of -> Navigator\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> base.descriptive_names.names.descriptive_name -> m.0100tslg\n# Answer:\nPilot", "# Reasoning Path:\nAmelia Earhart -> fictional_universe.fictional_character.occupation -> Pilot -> fictional_universe.character_occupation.characters_with_this_occupation -> Biggles\n# Answer:\nPilot"], "ground_truth": ["Pilot", "Writer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-302", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_types -> Color\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.notable_for -> g.1255wtr8_\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> base.schemastaging.context_name.pronunciation -> g.125_rvx57\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.article -> m.06kqt8\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Navy Blue -> base.schemastaging.visual_color_extra.srgb -> m.010q_4pn\n# Answer:\nNavy Blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Royal blue -> common.topic.webpage -> m.0b46mhm\n# Answer:\nRoyal blue", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_types -> Color\n# Answer:\nSilver", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.colors -> Silver -> common.topic.notable_for -> g.125g6j40s\n# Answer:\nSilver"], "ground_truth": ["Royal blue", "Navy Blue", "White", "Blue", "Silver"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 1.0, "ans_recall": 0.8, "path_f1": 0.7499999999999999, "path_precision": 1.0, "path_recall": 0.6, "path_ans_f1": 0.888888888888889, "path_ans_precision": 1.0, "path_ans_recall": 0.8}
{"id": "WebQTest-303", "prediction": ["# Reasoning Path:\nMargaret Hoover -> people.person.education -> m.0n1gxgt -> education.education.institution -> Davidson College\n# Answer:\nDavidson College", "# Reasoning Path:\nMargaret Hoover -> people.person.education -> g.11c3kps_b6\n# Answer:\npeople.person.education", "# Reasoning Path:\nMargaret Hoover -> common.topic.notable_for -> g.125dcyfk6\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.people_with_this_profession -> Adam Beashel\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> common.topic.notable_for -> g.1254zz38g\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> base.descriptive_names.names.descriptive_name -> m.0_z7q_s\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.people_with_this_profession -> Ann Lewis\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> common.topic.notable_types -> Profession\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.profession -> Strategist -> people.profession.specialization_of -> Businessperson\n# Answer:\nStrategist", "# Reasoning Path:\nMargaret Hoover -> people.person.nationality -> United States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nUnited States of America"], "ground_truth": ["Bryn Mawr College", "Davidson College"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-304", "prediction": ["# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.nationality -> United Kingdom\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.sibling_s -> m.0tlq4hm\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> common.topic.notable_for -> g.1258kfmtv\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> people.person.parents -> Juliette Norton\n# Answer:\nPoppy Honey Rosie Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> common.topic.notable_types -> Person\n# Answer:\nPoppy Honey Rosie Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Petal Blossom Rainbow Oliver -> people.person.place_of_birth -> United Kingdom\n# Answer:\nPetal Blossom Rainbow Oliver", "# Reasoning Path:\nJamie Oliver -> people.person.children -> Poppy Honey Rosie Oliver -> common.topic.notable_for -> g.1255bqwws\n# Answer:\nPoppy Honey Rosie Oliver"], "ground_truth": ["Buddy Bear Maurice Oliver", "Daisy Boo Pamela Oliver", "Petal Blossom Rainbow Oliver", "Poppy Honey Rosie Oliver"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-305", "prediction": ["# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzr -> people.place_lived.location -> Toronto\n# Answer:\nToronto", "# Reasoning Path:\nBradley Walsh -> people.person.places_lived -> m.0h8jzzl -> people.place_lived.location -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nBradley Walsh -> award.award_winner.awards_won -> m.0x0my5d -> award.award_honor.ceremony -> 2006 British Soap Awards\n# Answer:\n2006 British Soap Awards", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.09p0wtd -> tv.tv_guest_role.episodes_appeared_in -> Juliette Lewis, Colin McAllister, Justin Ryan, Oasis\n# Answer:\nJuliette Lewis, Colin McAllister, Justin Ryan, Oasis", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.starring_roles -> m.03jprnv -> tv.regular_tv_appearance.series -> Wheel of Fortune\n# Answer:\nWheel of Fortune", "# Reasoning Path:\nBradley Walsh -> award.award_winner.awards_won -> m.0x0my5d -> award.award_honor.honored_for -> Coronation Street\n# Answer:\nCoronation Street", "# Reasoning Path:\nBradley Walsh -> tv.tv_actor.guest_roles -> m.0gh0ff8 -> tv.tv_guest_role.episodes_appeared_in -> Episode 12\n# Answer:\nEpisode 12", "# Reasoning Path:\nBradley Walsh -> award.award_winner.awards_won -> m.0x0my5d -> award.award_honor.award -> The British Soap Award for Best Dramatic Performance\n# Answer:\nThe British Soap Award for Best Dramatic Performance"], "ground_truth": ["Toronto", "Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-306", "prediction": ["# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.manager -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> base.schemastaging.sports_team_manager_tenure.position -> Manager\n# Answer:\nManager", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0w2lsb3 -> soccer.football_team_management_tenure.manager -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> base.schemastaging.sports_team_extra.managers -> m.0w4j_k_ -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nInter Milan -> sports.sports_team.coaches -> m.0w68_yf -> sports.sports_team_coach_tenure.coach -> Walter Mazzarri\n# Answer:\nWalter Mazzarri", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0gf984m -> soccer.football_team_management_tenure.manager -> Leonardo Ara\u00fajo\n# Answer:\nLeonardo Ara\u00fajo", "# Reasoning Path:\nInter Milan -> sports.sports_team.coaches -> m.0w68_yf -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nInter Milan -> soccer.football_team.manager -> m.0bhj5cb -> soccer.football_team_management_tenure.manager -> Jos\u00e9 Mourinho\n# Answer:\nJos\u00e9 Mourinho"], "ground_truth": ["Walter Mazzarri"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-308", "prediction": ["# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.celebrity -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> celebrities.celebrity.sexual_relationships -> m.02_90w0 -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065ps9j -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.063h647 -> base.popstra.dated.participant -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.canoodled -> m.064tl5_ -> base.popstra.canoodled.participant -> Jake Gyllenhaal\n# Answer:\nJake Gyllenhaal", "# Reasoning Path:\nReese Witherspoon -> base.popstra.celebrity.dated -> m.065pts5 -> base.popstra.dated.participant -> Jeremy Sisto\n# Answer:\nJeremy Sisto", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0gxm_yb -> people.marriage.spouse -> Jim Toth\n# Answer:\nJim Toth", "# Reasoning Path:\nReese Witherspoon -> people.person.spouse_s -> m.0gxm_yb -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage"], "ground_truth": ["Jim Toth"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.second_level_division_of -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.administrative_division.country -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> United Kingdom\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow -> location.location.containedby -> Scotland\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-310", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> symbols.namesake.named_after -> Christopher Columbus\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.deposit_interest_rate -> g.11b71kjkjp\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nCosta Rica -> location.statistical_region.co2_emissions_per_capita -> g.1245_0_8r\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-311", "prediction": ["# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.citytown -> Las Vegas\n# Answer:\nLas Vegas", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> organization.organization.headquarters -> m.03dk0hf\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> location.mailing_address.state_province_region -> Nevada\n# Answer:\nNevada", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.headquarters -> m.03gbyq5 -> common.topic.notable_for -> g.125b6pk2c\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nUltimate Fighting Championship -> business.sponsor.sponsorship -> m.0_rf5gv -> business.sponsorship.sponsored_recipient -> UFC 170\n# Answer:\nUFC 170", "# Reasoning Path:\nUltimate Fighting Championship -> business.customer.supplier -> 4INFO -> business.business_operation.major_customer -> 1-800-Flowers\n# Answer:\n4INFO", "# Reasoning Path:\nUltimate Fighting Championship -> organization.organization.parent -> m.04kjp8g -> organization.organization_relationship.parent -> Zuffa\n# Answer:\nZuffa"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-312", "prediction": ["# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0tln5xz -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8ffv -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.02h8g09 -> film.performance.actor -> William Shatner\n# Answer:\nWilliam Shatner", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0hhzbby -> film.performance.film -> Star Trek Into Darkness\n# Answer:\nStar Trek Into Darkness", "# Reasoning Path:\nJames T. Kirk -> tv.tv_character.appeared_in_tv_program -> m.0v_cdgd -> tv.regular_tv_appearance.actor -> Jim Carrey\n# Answer:\nJim Carrey", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.012vszbf -> film.performance.actor -> Chris Pine\n# Answer:\nChris Pine", "# Reasoning Path:\nJames T. Kirk -> film.film_character.portrayed_in_films -> m.0tln5xz -> film.performance.film -> Coneheads\n# Answer:\nConeheads"], "ground_truth": ["Jim Carrey", "William Shatner", "Vic Mignogna"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.380952380952381, "path_precision": 0.5, "path_recall": 0.3076923076923077, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-313", "prediction": ["# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.team -> Los Angeles Angels of Anaheim\n# Answer:\nLos Angeles Angels of Anaheim", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0wdd82v -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nAlbert Pujols -> base.schemastaging.athlete_extra.salary -> m.0j2qh8m -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.0j2lc1l -> sports.sports_team_roster.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nAlbert Pujols -> sports.pro_athlete.teams -> m.0j2lc1l -> sports.sports_team_roster.position -> Infielder\n# Answer:\nInfielder", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nAlbert Pujols -> sports.drafted_athlete.drafted -> m.0461q4y -> sports.sports_league_draft_pick.school -> Maple Woods Community College\n# Answer:\nMaple Woods Community College"], "ground_truth": ["Los Angeles Angels of Anaheim", "Scottsdale Scorpions"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.25, "path_ans_recall": 0.5}
{"id": "WebQTest-314", "prediction": ["# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Denmark\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Representative democracy", "Unitary state", "Parliamentary system", "Hereditary monarchy", "Constitutional monarchy"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-315", "prediction": ["# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Texas\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> common.topic.notable_for -> g.1255l1rpv\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.geolocation -> m.0kh6hg\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.containedby -> Hardin County\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.location.time_zones -> Central Time Zone\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Saratoga -> location.citytown.postal_codes -> 77585\n# Answer:\nSaratoga", "# Reasoning Path:\nGeorge Jones -> music.artist.origin -> Vidor -> location.location.containedby -> Texas\n# Answer:\nVidor", "# Reasoning Path:\nGeorge Jones -> people.person.place_of_birth -> Saratoga -> location.location.containedby -> Texas\n# Answer:\nSaratoga"], "ground_truth": ["Saratoga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-316", "prediction": ["# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Lyricist -> people.profession.specialization_of -> Writer\n# Answer:\nLyricist", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Poet -> people.profession.specialization_of -> Writer\n# Answer:\nPoet", "# Reasoning Path:\nWilliam Shakespeare -> fictional_universe.fictional_character.occupation -> Playwright -> people.profession.specialization_of -> Writer\n# Answer:\nPlaywright", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Author -> type.type.expected_by -> Playwright\n# Answer:\nAuthor", "# Reasoning Path:\nWilliam Shakespeare -> people.person.profession -> Playwright -> rdf-schema#range -> Author\n# Answer:\nPlaywright"], "ground_truth": ["Actor", "Lyricist", "Author", "Playwright", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-317", "prediction": ["# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Ukraine\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nUkraine -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Ukraine\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nUkraine -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Russia\n# Answer:\n2010 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Ukraine\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nUkraine -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nRussia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Belarus\n# Answer:\n2014 Winter Olympics"], "ground_truth": ["Uzbekistan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-318", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.currency_used -> Swiss franc -> finance.currency.countries_used -> Liechtenstein\n# Answer:\nSwiss franc", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.statistical_region.co2_emissions_per_capita -> g.1245_dgvf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.statistical_region.long_term_unemployment_rate -> g.12cp_k6s5\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p27nwl\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.region -> Europe\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language"], "ground_truth": ["Swiss franc"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_for -> g.1256mspd6\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.languages_spoken -> Malay Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Kiribati -> location.country.currency_used -> Kiribati dollar\n# Answer:\nKiribati", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.country.form_of_government -> Federal monarchy\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-321", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "Lojban", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-322", "prediction": ["# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.film -> The Sensation of Sight\n# Answer:\nThe Sensation of Sight", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.film -> Lost City Raiders\n# Answer:\nLost City Raiders", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cs1j4x -> film.performance.film -> Life as a House\n# Answer:\nLife as a House", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0cw0xkk -> film.performance.character -> Drifter\n# Answer:\nDrifter", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.02vc21w -> film.performance.film -> In Enemy Hands\n# Answer:\nIn Enemy Hands", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crxf16 -> film.performance.film -> Changing Hearts\n# Answer:\nChanging Hearts", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.0crv_k2 -> film.performance.character -> Jack Kubiak\n# Answer:\nJack Kubiak", "# Reasoning Path:\nIan Somerhalder -> film.actor.film -> m.02vc21w -> film.performance.character -> Danny Miller\n# Answer:\nDanny Miller"], "ground_truth": ["Time Framed", "The Rules of Attraction", "Fireball", "The Sensation of Sight", "In Enemy Hands", "Caught on Tape", "Pulse", "The Old Man and the Studio", "Lost City Raiders", "Wake", "Life as a House", "The Lost Samaritan", "Marco Polo", "National Lampoon's TV: The Movie", "The Tournament", "Changing Hearts", "The Anomaly", "How to Make Love to a Woman", "Recess", "Anatomy of a Hate Crime", "Celebrity", "Fearless"], "ans_acc": 0.22727272727272727, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.625, "ans_recall": 0.22727272727272727, "path_f1": 0.3225806451612903, "path_precision": 0.625, "path_recall": 0.21739130434782608, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.625, "path_ans_recall": 0.22727272727272727}
{"id": "WebQTest-323", "prediction": ["# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.executive_produced_by -> Todd Hallowell\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Cinderella Man -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nCinderella Man", "# Reasoning Path:\nRon Howard -> film.director.film -> A Beautiful Mind -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.production_companies -> Universal Studios\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> Dr. Seuss' How the Grinch Stole Christmas -> film.film.film_casting_director -> Jane Jenkins\n# Answer:\nDr. Seuss' How the Grinch Stole Christmas", "# Reasoning Path:\nRon Howard -> film.producer.film -> A Beautiful Mind -> film.film.production_companies -> Universal Studios\n# Answer:\nA Beautiful Mind", "# Reasoning Path:\nRon Howard -> film.director.film -> Night Shift -> film.film.produced_by -> Brian Grazer\n# Answer:\nNight Shift"], "ground_truth": ["The Dilemma", "Night Shift", "Ransom", "The Lost Symbol", "Cotton Candy", "Apollo 13", "Parenthood", "Rush", "Willow", "In the Heart of the Sea", "Inferno", "A Beautiful Mind", "The Da Vinci Code", "Cinderella Man", "Backdraft", "The Paper", "Angels & Demons", "Frost/Nixon", "Dr. Seuss' How the Grinch Stole Christmas", "The Missing", "Grand Theft Auto", "Jay-Z: Made in America", "Splash", "Far and Away", "Cocoon", "The Dark Tower", "EDtv", "Gung Ho", "Presidential Reunion"], "ans_acc": 0.13793103448275862, "ans_hit": 1, "ans_f1": 0.2424242424242424, "ans_precission": 1.0, "ans_recall": 0.13793103448275862, "path_f1": 0.11650485436893203, "path_precision": 0.375, "path_recall": 0.06896551724137931, "path_ans_f1": 0.2424242424242424, "path_ans_precision": 1.0, "path_ans_recall": 0.13793103448275862}
{"id": "WebQTest-326", "prediction": ["# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.parent_disease -> Ill-defined descriptions and complications of heart disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> Hypertension\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease.medical_specialties -> Internal medicine\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> base.medical_schema_staging.medical_subspecialty.parent_medical_specialty -> Internal medicine\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> heart attack\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> medicine.disease_cause.diseases -> Transient ischemic attack\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> Myocardial Ischemia\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nGerald Ford -> people.deceased_person.cause_of_death -> Cardiovascular disease -> people.cause_of_death.includes_causes_of_death -> Angina Pectoris\n# Answer:\nCardiovascular disease"], "ground_truth": ["Cardiovascular disease"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-327", "prediction": ["# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> France\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> base.locations.countries.continent -> Eurasia -> common.topic.notable_types -> Location\n# Answer:\nEurasia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.partiallycontains -> m.0wg8kz1\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> base.locations.continents.countries_within -> Syria\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Kurd Mountains -> location.location.partiallycontains -> m.0wg9l7z\n# Answer:\nKurd Mountains", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nAsia", "# Reasoning Path:\nTurkey -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxt\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nTurkey -> location.location.partially_containedby -> Asia -> base.locations.continents.countries_within -> Iran\n# Answer:\nAsia"], "ground_truth": ["Europe", "Eurasia", "Asia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.6829268292682926, "path_precision": 0.7, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-328", "prediction": ["# Reasoning Path:\nKevin Love -> people.person.education -> m.04ftwdc -> education.education.institution -> University of California, Los Angeles\n# Answer:\nUniversity of California, Los Angeles", "# Reasoning Path:\nKevin Love -> sports.drafted_athlete.drafted -> m.04_by5l -> sports.sports_league_draft_pick.school -> University of California, Los Angeles\n# Answer:\nUniversity of California, Los Angeles", "# Reasoning Path:\nKevin Love -> people.person.education -> m.045z4gx -> education.education.institution -> Lake Oswego High School\n# Answer:\nLake Oswego High School", "# Reasoning Path:\nKevin Love -> common.topic.notable_for -> g.125c8dfyr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKevin Love -> sports.drafted_athlete.drafted -> m.04_by5l -> sports.sports_league_draft_pick.draft -> 2008 NBA draft\n# Answer:\n2008 NBA draft", "# Reasoning Path:\nKevin Love -> sports.drafted_athlete.drafted -> m.04_by5l -> sports.sports_league_draft_pick.team -> Memphis Grizzlies\n# Answer:\nMemphis Grizzlies", "# Reasoning Path:\nKevin Love -> freebase.valuenotation.is_reviewed -> Height -> type.property.unit -> Meter\n# Answer:\nHeight", "# Reasoning Path:\nKevin Love -> people.person.parents -> Karen Love -> people.person.gender -> Female\n# Answer:\nKaren Love", "# Reasoning Path:\nKevin Love -> people.person.parents -> Stan Love -> people.person.education -> m.0n1ldkz\n# Answer:\nStan Love"], "ground_truth": ["University of California, Los Angeles"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-329", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Kingdom of Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Brazil\n# Answer:\nConstitutional republic"], "ground_truth": ["Parliamentary republic", "Unitary state", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> base.aareas.schema.administrative_area.pertinent_type -> Swedish municipality -> type.type.properties -> County\n# Answer:\nSwedish municipality", "# Reasoning Path:\nSweden -> government.governmental_jurisdiction.agencies -> Swedish International Development Cooperation Agency -> organization.organization.parent -> m.0cm66g9\n# Answer:\nSwedish International Development Cooperation Agency", "# Reasoning Path:\nSweden -> base.aareas.schema.administrative_area.pertinent_type -> Swedish municipality -> freebase.type_hints.included_types -> Location\n# Answer:\nSwedish municipality", "# Reasoning Path:\nSweden -> government.governmental_jurisdiction.agencies -> Swedish International Development Cooperation Agency -> common.topic.notable_types -> Government Agency\n# Answer:\nSwedish International Development Cooperation Agency"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-331", "prediction": ["# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> location.location.containedby -> Luxor Governorate -> location.location.containedby -> Egypt\n# Answer:\nLuxor Governorate", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> location.country.administrative_divisions -> Cairo\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> travel.tourist_attraction.near_travel_destination -> Luxor -> location.location.containedby -> Egypt\n# Answer:\nLuxor", "# Reasoning Path:\nKarnak -> location.location.geolocation -> m.02_h8kg\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nEgypt", "# Reasoning Path:\nKarnak -> base.schemastaging.context_name.pronunciation -> g.125_p5b7s\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nKarnak -> location.location.containedby -> Egypt -> travel.travel_destination.tour_operators -> \u010cedok\n# Answer:\nEgypt"], "ground_truth": ["Luxor Governorate", "Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-333", "prediction": ["# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.nominated_for -> High Crimes\n# Answer:\nHigh Crimes", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_r69qt -> award.award_nomination.nominated_for -> The Magic of Belle Isle\n# Answer:\nThe Magic of Belle Isle", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv31_ -> award.award_nomination.nominated_for -> Bruce Almighty\n# Answer:\nBruce Almighty", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.ceremony -> 34th NAACP Image Awards\n# Answer:\n34th NAACP Image Awards", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_tl4p_ -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nDriving Miss Daisy", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_sv_jd -> award.award_nomination.award -> NAACP Image Award for Outstanding Actor in a Motion Picture\n# Answer:\nNAACP Image Award for Outstanding Actor in a Motion Picture", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.0_r69qt -> award.award_nomination.ceremony -> 44th NAACP Image Awards\n# Answer:\n44th NAACP Image Awards", "# Reasoning Path:\nMorgan Freeman -> award.award_nominee.award_nominations -> m.021ydzf -> award.award_nomination.nominated_for -> Driving Miss Daisy\n# Answer:\nDriving Miss Daisy"], "ground_truth": ["Nurse Betty", "The Art of Romare Bearden", "Lucy", "Johnny Handsome", "Heart Stopper", "London Has Fallen", "Wanted", "The Big Bounce", "Dolphin Tale 2", "Evan Almighty", "Batman Begins", "Seven", "Resting Place", "Lean on Me", "Ben-Hur", "Ted 2", "Now You See Me", "National Geographic: Inside the White House", "Island of Lemurs: Madagascar 3D", "The Maiden Heist", "The Power of One", "Gone Baby Gone", "m.0h0_gsx", "Unleashed", "The Contract", "Deep Impact", "The Love Guru", "Under Suspicion", "Edison", "The Lego Movie", "The Shawshank Redemption", "10 Items or Less", "Wish Wizard", "The Pawnbroker", "Marie", "Teachers", "The Dark Knight Rises", "Roll Of Thunder, Hear My Cry", "For Love of Liberty: The Story of America's Black Patriots", "War of the Worlds", "Transcendence", "Bruce Almighty", "The Long Way Home", "Thick as Thieves", "Dolphin Tale", "High Crimes", "Chain Reaction", "5 Flights Up", "Where Were You When the Lights Went Out?", "Driving Miss Daisy", "An Unfinished Life", "Dreamcatcher", "The Dark Knight", "Outbreak", "RED", "America Beyond the Color Line", "Along Came a Spider", "We the People", "The Civil War", "Invictus", "Last Knights", "Street Smart", "Death of a Prophet", "Glory", "The Bonfire of the Vanities", "Conan the Barbarian", "Last Vegas", "Hard Rain", "Moll Flanders", "Unforgiven", "Feast of Love", "The Sum of All Fears", "The Execution of Raymond Graham", "Eyewitness", "Magnificent Desolation: Walking On The Moon 3D", "All About Us", "A Raisin in the Sun", "Levity", "Attica", "The Hunting of the President", "A Man Called Adam", "Cosmic Voyage", "Soul Brothas and Sistas: Vol. 4: Quadruple Feature", "Million Dollar Baby", "The Bucket List", "m.0h0zs8c", "The Magic of Belle Isle", "Kiss the Girls", "Guilty by Association", "Now You See Me: The Second Act", "Clinton and Nadine", "Oblivion", "Olympus Has Fallen", "That Was Then... This Is Now", "Lucky Number Slevin", "Fight for Life", "Robin Hood: Prince of Thieves", "Brubaker", "Amistad", "Clean and Sober", "Harry & Son"], "ans_acc": 0.039603960396039604, "ans_hit": 1, "ans_f1": 0.074487895716946, "ans_precission": 0.625, "ans_recall": 0.039603960396039604, "path_f1": 0.1282051282051282, "path_precision": 0.625, "path_recall": 0.07142857142857142, "path_ans_f1": 0.074487895716946, "path_ans_precision": 0.625, "path_ans_recall": 0.039603960396039604}
{"id": "WebQTest-334", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pzp2 -> base.popstra.dated.participant -> Arlene Dahl\n# Answer:\nArlene Dahl", "# Reasoning Path:\nJohn F. Kennedy -> base.popstra.celebrity.dated -> m.065pt_4 -> base.popstra.dated.participant -> Betty Grable\n# Answer:\nBetty Grable", "# Reasoning Path:\nJohn F. Kennedy -> people.person.spouse_s -> m.02h_bpj -> people.marriage.spouse -> Jacqueline Kennedy Onassis\n# Answer:\nJacqueline Kennedy Onassis", "# Reasoning Path:\nJohn F. Kennedy -> people.person.spouse_s -> m.02h_bpj -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> people.person.spouse_s -> m.02h_bpj -> people.marriage.location_of_ceremony -> St. Mary's Church Complex\n# Answer:\nSt. Mary's Church Complex", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> A Nation of Immigrants -> common.topic.notable_types -> Book\n# Answer:\nA Nation of Immigrants", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Marilyn Monroe", "Judith Exner"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-335", "prediction": ["# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_for -> g.125c74q9f\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_for -> g.1257mndy1\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.article -> m.049p35\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.webpage -> m.04m29wr\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban peso -> common.topic.webpage -> m.04m29wf\n# Answer:\nCuban peso", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.article -> m.049p6m\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.energy_use_per_capita -> g.1245_22wq\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nCuba -> location.country.currency_used -> Cuban convertible peso -> common.topic.notable_types -> Currency\n# Answer:\nCuban convertible peso", "# Reasoning Path:\nCuba -> location.statistical_region.official_development_assistance -> g.12cp_kbnl\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Cuban convertible peso", "Cuban peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-336", "prediction": ["# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> finance.currency.countries_used -> Western Sahara\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.country.currency_used -> Moroccan dirham -> common.topic.notable_types -> Currency\n# Answer:\nMoroccan dirham", "# Reasoning Path:\nMorocco -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc385h5\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nMorocco -> location.statistical_region.external_debt_stock -> g.11b71x3g2t\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nMorocco -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc39ccx\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nMorocco -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Tunisia\n# Answer:\nUnitary state", "# Reasoning Path:\nMorocco -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> common.topic.article -> m.0dgr09\n# Answer:\nTaza-Al Hoceima-Taounate", "# Reasoning Path:\nMorocco -> base.aareas.schema.administrative_area.administrative_children -> Taza-Al Hoceima-Taounate -> location.location.time_zones -> Western European Time Zone\n# Answer:\nTaza-Al Hoceima-Taounate"], "ground_truth": ["Moroccan dirham"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-337", "prediction": ["# Reasoning Path:\nMary McLeod Bethune -> people.person.profession -> g.121bkpjb\n# Answer:\npeople.person.profession", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> travel.tourist_attraction.near_travel_destination -> Washington, D.C.\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> influence.influence_node.influenced -> Eleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr.\n# Answer:\nEleanor Roosevelt", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> common.topic.notable_for -> g.1258mdtk6\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> common.topic.article -> m.0bbss6\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Bethune-Cookman University -> education.educational_institution.mascot -> Wil D Cat\n# Answer:\nBethune-Cookman University", "# Reasoning Path:\nMary McLeod Bethune -> influence.influence_node.influenced -> Eleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted\n# Answer:\nEleanor Roosevelt", "# Reasoning Path:\nMary McLeod Bethune -> symbols.name_source.namesakes -> Mary McLeod Bethune Council House National Historic Site -> location.location.geolocation -> m.0ct0w22\n# Answer:\nMary McLeod Bethune Council House National Historic Site", "# Reasoning Path:\nMary McLeod Bethune -> people.person.place_of_birth -> Mayesville -> location.statistical_region.population -> g.11b66c6l9t\n# Answer:\nMayesville"], "ground_truth": ["Educator"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-338", "prediction": ["# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6gg_n\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJapan -> location.statistical_region.gni_in_ppp_dollars -> g.11b60zmbbm\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_6fj1\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.12cp_jcjg\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Japanese yen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-339", "prediction": ["# Reasoning Path:\nMother Teresa -> people.person.nationality -> Ottoman Empire -> common.topic.notable_types -> Country\n# Answer:\nOttoman Empire", "# Reasoning Path:\nMother Teresa -> people.person.place_of_birth -> Skopje -> base.biblioness.bibs_location.country -> Republic of Macedonia\n# Answer:\nSkopje", "# Reasoning Path:\nMother Teresa -> people.person.place_of_birth -> Skopje -> location.location.containedby -> Republic of Macedonia\n# Answer:\nSkopje", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_for -> g.1256fnzxx\n# Answer:\nHumanitarian", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> base.aareas.schema.administrative_area.administrative_children -> Tamil Nadu\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> common.topic.notable_types -> Country\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> people.person.nationality -> India -> location.country.languages_spoken -> English Language\n# Answer:\nIndia", "# Reasoning Path:\nMother Teresa -> base.activism.activist.area_of_activism -> Humanitarian -> common.topic.notable_types -> Profession\n# Answer:\nHumanitarian"], "ground_truth": ["Ottoman Empire", "India"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.0j37n7l -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.0j37n7l -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.010dqbnl -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton, Sr.", "# Reasoning Path:\nCam Newton -> people.person.education -> m.0kccbhk -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.010dqbnl -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> common.topic.notable_types -> American football player\n# Answer:\nCecil Newton, Sr."], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.2222222222222222, "path_recall": 0.6666666666666666, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-340", "prediction": ["# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.children -> Lotus Marie Pryor\n# Answer:\nRain Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.parents -> Patricia Price\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> common.topic.notable_for -> g.1257xht0m\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.gender -> Male\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Steven Pryor -> people.person.parents -> Flynn Belaine\n# Answer:\nSteven Pryor", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.profession -> Actor\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Richard Pryor Jr. -> people.person.sibling_s -> m.0t4tf8x\n# Answer:\nRichard Pryor Jr.", "# Reasoning Path:\nRichard Pryor -> people.person.children -> Rain Pryor -> people.person.profession -> Comedian\n# Answer:\nRain Pryor"], "ground_truth": ["Franklin Pryor", "Steven Pryor", "Richard Pryor Jr."], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-341", "prediction": ["# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.sports -> Artistic gymnastics\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> France\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics -> olympics.olympic_games.participating_countries -> Germany\n# Answer:\n1900 Summer Olympics", "# Reasoning Path:\nSpain -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> Belgium\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp"], "ground_truth": ["Morocco", "France", "Gibraltar", "Portugal", "Andorra"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.018348623853211014, "path_precision": 0.1, "path_recall": 0.010101010101010102, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.1, "path_ans_recall": 0.2}
{"id": "WebQTest-342", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> New Zealand -> location.country.languages_spoken -> Standard Chinese\n# Answer:\nNew Zealand", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.locations.countries.continent -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.country.form_of_government -> Parliamentary system\n# Answer:\nAntigua and Barbuda"], "ground_truth": ["Dominica", "Kiribati", "Sierra Leone", "Turks and Caicos Islands", "Republic of Ireland", "Territory of Papua and New Guinea", "Rwanda", "New Zealand", "Saint Kitts and Nevis", "Samoa", "India", "Barbados", "Montserrat", "Canada", "Namibia", "Bahamas", "Ghana", "Isle of Man", "Guyana", "Papua New Guinea", "Malta", "Fiji", "Philippines", "Zambia", "United Kingdom", "Hong Kong", "Belize", "Liberia", "Wales", "Territory of New Guinea", "Puerto Rico", "Gambia", "Swaziland", "South Africa", "Jersey", "Cayman Islands", "Bermuda", "Nauru", "Cameroon", "Botswana", "Tuvalu", "Gibraltar", "Antigua and Barbuda", "Vanuatu", "Grenada", "Zimbabwe", "Uganda", "Lesotho", "Saint Vincent and the Grenadines", "Tanzania", "Nigeria", "Pakistan", "Kenya", "Sudan", "Singapore", "Marshall Islands", "Guam", "Saint Lucia", "Cook Islands"], "ans_acc": 0.05084745762711865, "ans_hit": 1, "ans_f1": 0.09610983981693365, "ans_precission": 0.875, "ans_recall": 0.05084745762711865, "path_f1": 0.09610983981693365, "path_precision": 0.875, "path_recall": 0.05084745762711865, "path_ans_f1": 0.09610983981693365, "path_ans_precision": 0.875, "path_ans_recall": 0.05084745762711865}
{"id": "WebQTest-343", "prediction": ["# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kazakhstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Kyrgyzstan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Tajikistan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> olympics.olympic_participating_country.olympics_participated_in -> The London 2012 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\nThe London 2012 Summer Olympics", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.membership_organization.members -> m.04dj3ty\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> UNESCO -> organization.organization.founders -> United States of America\n# Answer:\nUNESCO", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Uzbekistan\n# Answer:\nShanghai Cooperation Organisation", "# Reasoning Path:\nChina -> organization.organization_founder.organizations_founded -> Shanghai Cooperation Organisation -> organization.organization.founders -> Russia\n# Answer:\nShanghai Cooperation Organisation"], "ground_truth": ["G-20 major economies", "World Bank", "Caribbean Development Bank", "Asian Development Bank", "United Nations", "Asia-Pacific Economic Cooperation", "Shanghai Cooperation Organisation", "African Development Bank", "UNESCO"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.3544303797468354, "ans_precission": 0.875, "ans_recall": 0.2222222222222222, "path_f1": 0.3544303797468354, "path_precision": 0.875, "path_recall": 0.2222222222222222, "path_ans_f1": 0.3544303797468354, "path_ans_precision": 0.875, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-344", "prediction": ["# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minnesota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.hud_county_place.county -> Dakota County -> location.location.containedby -> Minnesota\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Minnesota -> location.location.containedby -> West North Central States\n# Answer:\nMinnesota", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nWest St. Paul -> common.topic.notable_for -> g.12593ww_q\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.us_county.hud_county_place -> Dakota County / County Place 2703799999\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.statistical_region.co2_emissions_residential -> m.045jr02\n# Answer:\nDakota County", "# Reasoning Path:\nWest St. Paul -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07h5g3v\n# Answer:\nlocation.hud_foreclosure_area.bls_unemployment_rate", "# Reasoning Path:\nWest St. Paul -> location.location.containedby -> Dakota County -> location.hud_foreclosure_area.estimated_number_foreclosures -> m.07h4zmh\n# Answer:\nDakota County"], "ground_truth": ["Dakota County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-346", "prediction": ["# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nVincent Grey", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.character -> Vincent Grey\n# Answer:\nVincent Grey", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.0k6gz9 -> film.performance.actor -> Donnie Wahlberg\n# Answer:\nfilm.performance.actor", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jq0nf -> film.performance.character -> Kyra Collins\n# Answer:\nKyra Collins", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02tbg6_ -> film.performance.character -> Mr. Grey\n# Answer:\nMr. Grey", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.0k6gz9 -> film.performance.film -> The Sixth Sense\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nThe Sixth Sense -> film.film.starring -> m.03jrcw6 -> film.performance.character -> Sean\n# Answer:\nSean", "# Reasoning Path:\nDonnie Wahlberg -> film.actor.film -> m.02vc00j -> film.performance.character -> Danny Quinn\n# Answer:\nDanny Quinn"], "ground_truth": ["Vincent Grey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-347", "prediction": ["# Reasoning Path:\nMontesquieu -> common.topic.notable_for -> g.125fyk02t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> influence.influence_node.influenced_by -> Sextus Empiricus\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> William Stanley Jevons\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> influence.influence_node.influenced_by -> Adam Smith\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> base.kwebbase.kwtopic.connections_to -> arthur schopenhauer opposed views of georg wilhelm friedrich hegel\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> people.person.place_of_birth -> Stuttgart\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Jeremy Bentham -> influence.influence_node.influenced -> Atilla Yayla\n# Answer:\nJeremy Bentham", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Georg Wilhelm Friedrich Hegel -> people.person.education -> m.0v415j2\n# Answer:\nGeorg Wilhelm Friedrich Hegel", "# Reasoning Path:\nMontesquieu -> influence.influence_node.influenced -> Thomas Jefferson -> influence.influence_node.influenced -> Ayn Rand\n# Answer:\nThomas Jefferson"], "ground_truth": ["Paul F\u00e9val, p\u00e8re", "Edmund Burke", "Friedrich Hayek", "James Madison", "\u00c9mile Durkheim", "Alexis de Tocqueville", "Thomas Jefferson", "Jeremy Bentham", "Clive James", "Georg Wilhelm Friedrich Hegel", "Isaiah Berlin", "Thomas Paine", "John Adams", "David Hume", "Hannah Arendt", "Louis Althusser", "Edward Gibbon", "William Blackstone", "Roberto Mangabeira Unger", "Jean-Jacques Rousseau", "Adam Smith"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.24615384615384617, "ans_precission": 0.8888888888888888, "ans_recall": 0.14285714285714285, "path_f1": 0.24615384615384617, "path_precision": 0.8888888888888888, "path_recall": 0.14285714285714285, "path_ans_f1": 0.3137254901960784, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-349", "prediction": ["# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nUtah Jazz", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qht47 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nUtah Jazz", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qgd67 -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qsrc6 -> basketball.basketball_player_stats.team -> Utah Jazz\n# Answer:\nUtah Jazz", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qjzy0 -> basketball.basketball_player_stats.season -> 2005\u201306 NBA season\n# Answer:\n2005\u201306 NBA season", "# Reasoning Path:\nCarlos Boozer -> basketball.basketball_player.player_statistics -> m.04qmbl6 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nCarlos Boozer -> base.schemastaging.athlete_extra.salary -> m.0j2hs7x -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Cleveland Cavaliers", "Utah Jazz"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.location.containedby -> Maryland\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County -> location.us_county.hud_county_place -> Frederick County / County Place 2402199999\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.statistical_region.population -> g.11b66fv4wt\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrederick -> location.statistical_region.population -> g.11btww0g9h\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-351", "prediction": ["# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.image -> South Coast (Maine)\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.nearby_airports -> Portland International Jetport\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.owner -> Blethen Maine Newspapers, Inc. -> common.topic.notable_for -> g.1259lvkfc\n# Answer:\nBlethen Maine Newspapers, Inc.", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.notable_types -> Location\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> common.topic.article -> m.0852wf\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.article -> m.0261l71\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Portland -> location.location.containedby -> United States of America\n# Answer:\nPortland", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.circulation_areas -> Southern Maine Coast -> common.topic.notable_for -> g.125660zbd\n# Answer:\nSouthern Maine Coast", "# Reasoning Path:\nPortland Press Herald -> book.newspaper.owner -> Blethen Maine Newspapers, Inc. -> business.employer.employees -> m.0ywzm6d\n# Answer:\nBlethen Maine Newspapers, Inc."], "ground_truth": ["Blethen Maine Newspapers, Inc."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-352", "prediction": ["# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> location.location.containedby -> Berlin\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> architecture.structure.architectural_style -> Nazi architecture\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> architecture.structure.architect -> Werner March\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Olympiastadion -> sports.sports_facility.teams -> Hertha BSC\n# Answer:\nOlympiastadion", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> common.topic.notable_types -> Building\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> film.film_story_contributor.film_story_credits -> As from Afar -> film.film.story_by -> Simon Wiesenthal\n# Answer:\nAs from Afar", "# Reasoning Path:\nAlbert Speer -> architecture.architect.structures_designed -> Reich Chancellery -> common.topic.image -> Das Palais Schulenburg um 1830\n# Answer:\nReich Chancellery", "# Reasoning Path:\nAlbert Speer -> people.person.spouse_s -> m.0j6bbq6 -> people.marriage.spouse -> Margarete Weber\n# Answer:\nMargarete Weber"], "ground_truth": ["Reich Chancellery", "Deutsches Stadion", "Olympiastadion", "Volkshalle"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-354", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Titian\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> base.schemastaging.context_name.pronunciation -> g.125_krwf6\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artists -> Andrea del Sarto\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> visual_art.art_period_movement.associated_artworks -> Young Man with an Apple\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> High Renaissance -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nHigh Renaissance", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artists -> Rosalba Carriera\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.associated_periods_or_movements -> Pastel Art -> visual_art.art_period_movement.associated_artworks -> Young Girl Holding a Crown of Laurel\n# Answer:\nPastel Art", "# Reasoning Path:\nLeonardo da Vinci -> symbols.name_source.namesakes -> Da Vinci -> common.topic.notable_types -> Book\n# Answer:\nDa Vinci"], "ground_truth": ["High Renaissance", "Pastel Art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-356", "prediction": ["# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> aviation.airport.serves -> TownePlace Suites Arlington Near Six Flags\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> Tarrant County\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> common.topic.article -> m.02pq2dc\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b66bywm8\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.geolocation -> m.02_r44n\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> common.topic.notable_types -> Airport\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.hud_county_place.county -> Tarrant County -> location.location.nearby_airports -> Hicks Airfield\n# Answer:\nTarrant County", "# Reasoning Path:\nArlington -> location.location.nearby_airports -> Arlington Municipal Airport -> location.location.containedby -> 76018\n# Answer:\nArlington Municipal Airport", "# Reasoning Path:\nArlington -> location.statistical_region.population -> g.11b7td7_9_\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nArlington -> location.metropolitan_area_anchor.metropolitan_area -> Fort Worth-Arlington, TX Metropolitan Division -> common.topic.notable_types -> US Metropolitan Division\n# Answer:\nFort Worth-Arlington, TX Metropolitan Division"], "ground_truth": ["Arlington Municipal Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-359", "prediction": ["# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w90221 -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> base.schemastaging.athlete_extra.salary -> m.0kdmv43 -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w8wcgz -> soccer.football_player_stats.team -> Wigan Athletic F.C.\n# Answer:\nWigan Athletic F.C.", "# Reasoning Path:\nEmile Heskey -> soccer.football_player.statistics -> m.0w9mysy -> soccer.football_player_stats.team -> Newcastle Jets FC\n# Answer:\nNewcastle Jets FC", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.05ckn9m -> sports.sports_team_roster.position -> Forward\n# Answer:\nForward", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.04m0_rf -> sports.sports_team_roster.team -> Wigan Athletic F.C.\n# Answer:\nWigan Athletic F.C.", "# Reasoning Path:\nEmile Heskey -> sports.pro_athlete.teams -> m.0n3jm1r -> sports.sports_team_roster.team -> Newcastle Jets FC\n# Answer:\nNewcastle Jets FC"], "ground_truth": ["Newcastle Jets FC", "Aston Villa F.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.625, "path_recall": 0.7142857142857143, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmyl -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-360", "prediction": ["# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.containedby -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.deceased_person.place_of_death -> Tokyo -> location.location.containedby -> Japan\n# Answer:\nTokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.neighborhood.neighborhood_of -> Tokyo\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Junichi Sasai\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> base.schemastaging.context_name.pronunciation -> g.125_m3lkn\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> common.topic.article -> m.03xm9t\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.place_of_birth -> Aoyama, Minato, Tokyo -> location.location.people_born_here -> Kenichi Enomoto\n# Answer:\nAoyama, Minato, Tokyo", "# Reasoning Path:\nHirohito -> people.person.spouse_s -> m.0j4k61w -> people.marriage.spouse -> Empress K\u014djun\n# Answer:\nEmpress K\u014djun"], "ground_truth": ["Aoyama, Minato, Tokyo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-361", "prediction": ["# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> film.film_location.featured_in_films -> Ecstasy\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> common.topic.notable_for -> g.1255jtr81\n# Answer:\nSigmund Freud Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> location.location.geolocation -> m.02_s799\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sch\u00f6nbrunn Palace -> travel.travel_destination.tour_operators -> Adventures by Disney - Austria, Germany, Czech Republic Vacation\n# Answer:\nSch\u00f6nbrunn Palace", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> location.location.geolocation -> m.01225mmz\n# Answer:\nSigmund Freud Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> architecture.museum.type_of_museum -> Jewish museum\n# Answer:\nSigmund Freud Museum", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> St. Stephen's Cathedral, Vienna -> common.topic.webpage -> m.03l8gr3\n# Answer:\nSt. Stephen's Cathedral, Vienna", "# Reasoning Path:\nVienna -> travel.travel_destination.tourist_attractions -> Sigmund Freud Museum -> common.topic.image -> Berggasse Vienna March 2007 002\n# Answer:\nSigmund Freud Museum"], "ground_truth": ["Schloss Hof", "Prater", "Imperial Crypt", "Kunsthistorisches Museum", "Belvedere, Vienna", "Minoritenkirche", "Augustinian Church, Vienna", "Capuchin Church, Vienna", "Schottenkirche, Vienna", "Vienna Observatory", "Leopold Museum", "Jesuit Church, Vienna", "Rock im Park", "Museum of Military History, Vienna", "Vienna State Opera", "Sch\u00f6nbrunn Palace", "Haus der Musik", "Vienna International Centre", "Liechtenstein Museum", "Rathaus, Vienna", "Sigmund Freud Museum", "St. Peter's Church", "Millennium Tower", "Naturhistorisches Museum", "Albertina", "mumok", "Karlskirche, Vienna", "Hofburg Palace", "St. Stephen's Cathedral, Vienna"], "ans_acc": 0.10344827586206896, "ans_hit": 1, "ans_f1": 0.1875, "ans_precission": 1.0, "ans_recall": 0.10344827586206896, "path_f1": 0.19354838709677416, "path_precision": 1.0, "path_recall": 0.10714285714285714, "path_ans_f1": 0.1875, "path_ans_precision": 1.0, "path_ans_recall": 0.10344827586206896}
{"id": "WebQTest-362", "prediction": ["# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> England\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.next_in_series -> 2003 Cricket World Cup\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> common.topic.image -> Cricket World Cup trophy 2\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> cricket.cricket_tournament_event.host -> Republic of Ireland\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> common.image.appears_in_topic_gallery -> \u00c9ire\n# Answer:\n\u00c9ire", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> cricket.cricket_tournament_event.tournament -> The Cricket World Cup\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 1999 Cricket World Cup -> time.event.locations -> Netherlands\n# Answer:\n1999 Cricket World Cup", "# Reasoning Path:\nIreland -> location.location.events -> 2008 Nicky Rackard Cup -> common.topic.article -> m.03hgw2x\n# Answer:\n2008 Nicky Rackard Cup", "# Reasoning Path:\nIreland -> location.location.events -> Battle of Affane -> time.event.locations -> County Waterford\n# Answer:\nBattle of Affane"], "ground_truth": ["Irish War of Independence", "Anglo-Spanish War", "Jacobite risings", "Sack of Dun Gallimhe", "Irish Rebellion of 1798", "Second Desmond Rebellion", "Planned French invasion of Britain", "Guerrilla phase of the Irish Civil War", "Siege of Drogheda", "Williamite War in Ireland", "Desmond Rebellions", "Battle of Ros-Mhic-Thri\u00fain", "Norman invasion of Ireland", "Nine Years' War", "Irish Rebellion of 1641", "Irish Confederate Wars", "Battle of the Curragh", "Siege of Wexford", "Battle of Glentaisie", "Siege of Smerwick", "Battle of Knockdoe", "Bruce campaign in Ireland", "Wars of the Three Kingdoms", "Cromwellian conquest of Ireland", "Battle of Belahoe", "Irish Free State offensive", "Battle of Tochar Cruachain-Bri-Ele", "Siege of Waterford", "Exp\u00e9dition d'Irlande", "Battle of Affane", "Irish Civil War"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.049999999999999996, "ans_precission": 0.1111111111111111, "ans_recall": 0.03225806451612903, "path_f1": 0.04878048780487805, "path_precision": 0.1111111111111111, "path_recall": 0.03125, "path_ans_f1": 0.049999999999999996, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-365", "prediction": ["# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Peoria Zoo -> common.topic.webpage -> m.0cv3kq0\n# Answer:\nPeoria Zoo", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Peoria Zoo -> common.topic.notable_for -> g.1259xmvw4\n# Answer:\nPeoria Zoo", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> John C. Flanagan House Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h82bq\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Peoria Zoo -> zoos.zoo.exhibits -> Africa Exhibit\n# Answer:\nPeoria Zoo", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> John C. Flanagan House Museum -> location.location.containedby -> Illinois\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> travel.travel_destination.tourist_attractions -> Wildlife Prairie State Park -> location.location.geolocation -> m.0cq_w11\n# Answer:\nWildlife Prairie State Park", "# Reasoning Path:\nPeoria -> location.location.contains -> John C. Flanagan House Museum -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.05h82bq\n# Answer:\nJohn C. Flanagan House Museum", "# Reasoning Path:\nPeoria -> location.location.contains -> Peoria Civic Center -> location.location.containedby -> United States of America\n# Answer:\nPeoria Civic Center"], "ground_truth": ["Peoria Civic Center", "Lakeview Museum of Arts and Sciences", "Peoria Zoo", "George L. Luthy Memorial Botanical Garden", "WeaverRidge Golf Club", "Wildlife Prairie State Park", "Dozer Park", "Heart of Illinois Fair", "John C. Flanagan House Museum", "Par-A-Dice Hotel and Casino"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5490196078431373, "path_precision": 0.875, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-366", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> Cornish Language -> language.human_language.countries_spoken_in -> England\n# Answer:\nCornish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Wales\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nUnited Kingdom -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language"], "ground_truth": ["Irish", "Esperanto Language", "Cornish Language", "English Language", "British Sign Language", "Scots Language", "Scottish Gaelic", "Welsh Language"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.125, "path_precision": 0.125, "path_recall": 0.125, "path_ans_f1": 0.4, "path_ans_precision": 1.0, "path_ans_recall": 0.25}
{"id": "WebQTest-367", "prediction": ["# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Soldier\n# Answer:\nSoldier", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.specializations -> Singer\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> common.topic.subject_of -> Stephen Melton\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Musician -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> common.topic.subject_of -> Justin Bieber\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger", "# Reasoning Path:\nElvis Presley -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger"], "ground_truth": ["Singer", "Soldier", "Actor", "Musician"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 1.0, "ans_recall": 0.75, "path_f1": 0.5263157894736842, "path_precision": 0.5555555555555556, "path_recall": 0.5, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 1.0, "path_ans_recall": 0.75}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> common.topic.article -> m.02hczk\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-370", "prediction": ["# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.language_family -> West Germanic languages\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.human_language.region -> Friesland\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Frisian languages -> language.language_dialect.language -> Dutch Language\n# Answer:\nFrisian languages", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.dialects -> Frisian languages\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.countries_spoken_in -> Kingdom of the Netherlands\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nNetherlands -> location.country.official_language -> Dutch Language -> language.human_language.main_country -> Cura\u00e7ao\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.country.languages_spoken -> Dutch Language -> language.human_language.countries_spoken_in -> Kingdom of the Netherlands\n# Answer:\nDutch Language", "# Reasoning Path:\nNetherlands -> location.statistical_region.gni_in_ppp_dollars -> g.11b60prvm_\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nNetherlands -> location.statistical_region.internet_users_percent_population -> g.11b60rwzc0\n# Answer:\nlocation.statistical_region.internet_users_percent_population"], "ground_truth": ["Frisian languages", "Dutch Language", "West Flemish"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6829268292682926, "path_ans_precision": 0.7, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-371", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60nhckb\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nGreece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ld\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-374", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> visual_art.visual_art_form.artworks -> Dam\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> common.topic.notable_types -> Visual Art Form\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Sculpture -> common.topic.subjects -> Artist\n# Answer:\nSculpture", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> visual_art.visual_art_form.artworks -> Suite Noa Noa - Noa Noa\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> education.field_of_study.students_majoring -> m.0_w593n\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Printmaking -> book.book_subject.works -> Eric Ravilious\n# Answer:\nPrintmaking", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Eric Ravilious\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.art_forms -> Drawing -> business.consumer_product.brand -> Eyecandyair\n# Answer:\nDrawing", "# Reasoning Path:\nHenri Matisse -> common.topic.article -> m.0gcv9\n# Answer:\ncommon.topic.article"], "ground_truth": ["Collage", "Painting", "Printmaking", "Drawing", "Sculpture"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.7164179104477612, "ans_precission": 0.8888888888888888, "ans_recall": 0.6, "path_f1": 0.7164179104477612, "path_precision": 0.8888888888888888, "path_recall": 0.6, "path_ans_f1": 0.7164179104477612, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.6}
{"id": "WebQTest-375", "prediction": ["# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_for -> g.1255hfpzv\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.article -> m.0273w68\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Wilkie Collins\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_types -> Book\n# Answer:\nA House to Let", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nAldous Huxley"], "ground_truth": ["Oliver Twist"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-376", "prediction": ["# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Adhey Kangal -> film.film.genre -> Thriller\n# Answer:\nAdhey Kangal", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.runtime -> m.0ghfzm6\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.country -> United Kingdom\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> common.topic.webpage -> m.03lzz8x -> common.webpage.resource -> m.0bl41lc\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.release_date_s -> m.0glb3fq\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> film.film_story_contributor.film_story_credits -> Agatha Christie's Miss Marple: 4:50 from Paddington -> film.film.starring -> m.0nd9m31\n# Answer:\nAgatha Christie's Miss Marple: 4:50 from Paddington", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> Agatha Christie and the Eleven Missing Days -> book.book.editions -> Agatha Christie and the eleven missing days\n# Answer:\nAgatha Christie and the Eleven Missing Days", "# Reasoning Path:\nAgatha Christie -> book.book_subject.works -> The Getaway Guide to Agatha Christie's England -> book.written_work.subjects -> England\n# Answer:\nThe Getaway Guide to Agatha Christie's England"], "ground_truth": ["The Mysterious Affair at Styles (Large Print Edition)", "Sad Cypress (Hercule Poirot Mysteries)", "The murder on the links", "A Caribbean Mystery (Miss Marple Mysteries)", "Poirot Investigates", "Endless Night", "Murder On The Links (Classic Books on Cassettes Collection)", "Death on the Nile.", "Curtain (Ulverscroft Large Print Ser.)", "The Mystery of the Blue Train", "Poirot Investigates (Hercule Poirot Mysteries (Paperback))", "Cards on the Table", "They Do It With Mirrors", "They Do It with Mirrors", "Appointment with Death (Hercule Poirot)", "Five Little Pigs (Agatha Christie Mysteries Collection)", "The Hollow (Poirot)", "The Clocks (Poirot)", "Taken at the Flood (Agatha Christie Collection)", "Come tell me how you live", "Partners in crime.", "The labours of Hercules", "Murder on the Orient Express", "The Mysterious Mr.Quin", "The Sittaford Mystery (BBC Radio Presents)", "Murder in the Mews (Poirot)", "Endless night", "The Unexpected Guest", "Five little pigs (Agatha Christie Mystery Collection)", "Elephants Can Remember (Poirot)", "The Hollow (A Hercule Poirot Novel)", "Murder on the links (The Agatha Christie mystery collection)", "A Caribbean Mystery (BBC Radio Presents: An Audio Dramatization)", "The Murder of Roger Ackroyd (Hercule Poirot Mystery)", "Clocks (Hc Collection)", "Five Little Pigs.", "A Murder Is Announced (Agatha Christie Collection)", "Sad cypress.", "Murder at the vicarage.", "After the Funeral (BBC Audio Crime)", "Secret Adversary", "Five Little Pigs (Hercule Poirot Mysteries)", "A Murder Is Announced (G. K. Hall (Large Print))", "Partners in Crime (Vol. 1 Finessing the King, Vol 2 The Crackler, Vol 3 The Unbreakable Alibi)", "The Big Four", "The secret of Chimneys", "Appointment with Death (BBC Radio Collection)", "The Body in the Library (BBC Radio Collection)", "The Mysterious Mr Quin", "The Murder on the Links (Hercule Poirot)", "Murder Is Easy (Audio Editions Mystery Masters)", "Death in the Clouds", "Ordeal By Innocence", "Curtain (The Agatha Christie mystery collection)", "Body in the Library (Miss Marple Mysteries (Paperback))", "Third Girl (Agatha Christie Collection)", "They Do It with Mirrors (Agatha Christie Collection)", "Cat Among the Pigeons", "Murder is easy", "Ordeal by Innocence (Agatha Christie Mysteries Collection)", "The Regatta Mystery", "The Hollow (Ulverscroft Large Print)", "And Then There Were None", "Murder in Mesopotamia (Poirot)", "Lord Edgware dies.", "Death on the Nile (Hercule Poirot)", "Dumb witness", "Towards Zero (Agatha Christie Collection)", "Peril At End House", "Evil under the Sun (Hercule Poirot Mysteries)", "PASSENGER TO FRANKFURT.", "Mystery Of The Blue Train / The Listerdale Mystery / Murder At The Vicarage", "The Body In The Library (A Miss Marple Mystery)", "Death in the clouds", "Death In The Clouds", "Towards Zero (The Agatha Christie Mystery Collection)", "Towards zero", "The hollow", "Sittaford Mystery", "The Adventure of the Christmas Pudding", "Cards on the Table (A Hercule Poirot Mystery)", "The Burden", "Cards on the Table (Hercule Poirot)", "Murder Is Easy", "Come, tell me how you live", "The Secret of Chimneys (Agatha Christie Signature Edition)", "Secret Adversary (Unabridged Classics)", "Murder in the mews", "Elephants Can Remember (Agatha Christie Mysteries Collection)", "Elephants Can Remember", "The Pale Horse (St. Martin's Minotaur Mysteries)", "PASSENGER TO FRANKFURT", "The Man In The Brown Suit (Classic Books on Cassettes Collection)", "Problem at Pollensa Bay", "Sleeping Murder (Miss Marple)", "One, Two, Buckle My Shoe (Agatha Christie Mysteries Collection (Paperback))", "They Came to Baghdad (Dell Book)", "The Secret of Chimneys (Mystery Masters Series)", "Taken at the Flood (Radio Collection)", "Taken At The Flood (A Hercule Poirot Mystery)", "The hound of death", "Third Girl (Poirot)", "Taken at the Flood (Poirot)", "Murder at the Vicarage (G. K. Hall (Large Print))", "The Mysterious Affair at Styles (Hercule Poirot Mysteries (Audio))", "The Secret of Chimneys", "The A.B.C. Murders", "Elephants can remember.", "Burden", "Elephants Can Remember (The Agatha Christie Mystery Collection)", "A Murder Is Announced (Miss Marple Mysteries (Paperback))", "Crooked House (Agatha Christie Collection)", "Death on the Nile (The Christie Collection)", "A Caribbean Mystery (Miss Marple)", "Five Little Pigs", "Three Act Tragedy", "A Pocket Full of Rye (Miss Marple Mysteries (Audio))", "Destination Unknown (Agatha Christie Collection)", "A CARIBBEAN MYSTERY", "The pale horse.", "Sparkling cyanide.", "Elephants can remember", "Pale Horse (R)", "A Caribbean Mystery (Greenway E.)", "Secret Adversary (The Agatha Christie Mystery Collection)", "After the Funeral (Hercule Poirot)", "Labours of Hercules", "4.50 From Paddington", "Destination Unknown (St. Martin's Minotaur Mysteries)", "The A.B.C. Murders (The Christie Collection)", "After the Funeral (The Christie Collection)", "Crooked House", "Three blind mice and other stories", "Evil Under the Sun (Agatha Christie Collection)", "Evil Under the Sun (Poirot)", "A Pocket Full of Rye (A Jane Marple Murder Mystery)", "The Labours of Hercules", "The Secret Adversary (Classic Books on Cassettes Collection)", "Evil Under the Sun (BBC Radio Collection)", "4.50 from Paddington (Agatha Christie Signature Edition)", "The Mystery of the Blue Train (Hercule Poirot Mysteries)", "Murder in the Mews", "Murder On The Links", "Taken at the Flood (aka There is a Tide...)", "And Then There Were None (Agatha Christie Audio Mystery)", "Curtain (Poirot)", "Appointment with Death", "Ordeal by Innocence (Agatha Christie Collection)", "A Pocket Full of Rye", "Sittaford Mystery (St. Martin's Minotaur Mysteries)", "They Do It with Mirrors (Miss Marple)", "Murder in Mesopotamia (BBC Radio Collection)", "Double Sin and Other Stories", "Lord Edgware Dies (BBC Radio Collection)", "4.50 from Paddington (Miss Marple)", "Sad cypress", "Double sin and other stories", "The Sittaford Mystery (BBC Radio Collection)", "Star over Bethlehem and other stories by Agatha Christie Mallowan", "The Big Four (Poirot)", "The murder at the vicarage.", "The Adventure of the Christmas Pudding and Other Stories", "Five Little Pigs (Also published as Murder In Retrospect)", "And Then There Were None (The Christie Collection)", "The Adventure of the Christmas Pudding (Poirot)", "Hickory Dickory Dock (Agatha Christie Collection)", "Postern of Fate (Tommy & Tuppence Chronology)", "A Caribbean Mystery", "The Mysterious Mr. Quin (Paperback)) (Hercule Poirot Mysteries (Paperback))", "Cat among the pigeons.", "Appointment With Death", "Towards Zero (Audio Editions Mystery Masters)", "Death on the Nile (Hercule Poirot Mysteries (Paperback))", "A Murder is Announced", "Lord Edgware Dies (BBC Audio Crime)", "Hickory dickory dock", "Murder at the Vicarage", "Hickory Dickory Dock (Poirot)", "The Listerdale Mystery", "The Clocks", "Murder Is Easy (St. Martin's Minotaur Mysteries)", "The Seven Dials mystery.", "Evil Under the Sun (Hercule Poirot Mysteries)", "Destination unknown (Agatha Christie mystery collection)", "The Thirteen Problems", "The Murder of Roger Ackroyd", "They Do It with Mirrors (BBC Radio Collection)", "The Hollow.", "Sleeping murder", "They Came to Baghdad (Mystery Masters)", "The body in the library", "Peril at end house.", "The Thirteen Problems (Miss Marple Mysteries)", "A Murder Is Announced", "The Mysterious Mr. Quin", "THE MYSTERY OF THE BLUE TRAIN.", "The Big Four (Hercule Poirot Mysteries)", "The crooked house", "Rose and the Yew Tree", "The Listerdale Mystery (Agatha Christie Collection)", "Peril at End House.", "Destination Unknown (Signature Editions)", "Problem at Pollensa Bay and 7 Other Mysteries", "The Unexpected Guest (Acting Edition)", "Nemesis (Miss Marple Mysteries)", "Pale Horse", "The adventure of the Christmas pudding", "4.50 from Paddington (BBC Radio Collection)", "Seven Dials Mystery", "While the light lasts and other stories", "The A.B.C. murders", "POIROT INVESTIGATES (Hercule Poirot Mysteries (Paperback))", "Cards on the Table (Agatha Christie Mysteries Collection (Paperback))", "Murder in Mesopotamia", "The adventure of the Christmas pudding, and a selection of entr\u00e9es", "And Then There Were None (Agatha Christie Collection)", "Sparkling Cyanide (Agatha Christie Collection)", "Postern of Fate", "Lord Edgware Dies (Agatha Christie Collection S.)", "Sad Cypress", "Secret of Chimneys (St. Martin's Minotaur Mysteries)", "Peril at End House", "Come, Tell Me How You Live", "Death on the Nile (Hercule Poirot Mysteries (Audio))", "Sparkling Cyanide", "Appointment with death", "They Came to Baghdad (The Agatha Christie Mystery Collection)", "Parker Pyne investigates", "The Clocks (A Hercule Poirot Murder Mystery)", "The Rose and the Yew Tree", "And Then There Were None Book (Detective English Readers)", "Unfinished Portrait", "Mystery of the blue train", "They do it with mirrors", "Cat Among the Pigeons ( A Hercule Poirot Mystery)", "The \\\"Mysterious Mr Quin\\\"", "CURTAIN", "By the Pricking of my Thumbs", "The Body in the Library (Miss Marple)", "Postern of Fate (Agatha Christie Audio Mystery)", "Peril at End House (BBC Radio Collection)", "And Then There Were None (The Agatha Christie Mystery Collection)", "Evil Under the Sun (G. K. Hall (Large Print))", "Partners in Crime", "Death in the Clouds (Agatha Christie Mysteries Collection (Paperback))", "Postern of fate", "Poirot Investigates (Poirot)", "Death On The Nile (Hercule Poirot Mysteries (Paperback))", "Dumb Witness (Poirot)", "Murder in Mesopotamia (BBC Radio Presents)", "The secret of chimneys", "Death in the Clouds (Hercule Poirot)", "Murder at the Vicarage (Agatha Christie Audio Mystery)", "The Secret of Chimneys (Ulverscroft Mystery)", "The Murder at the Vicarage (Miss Marple Mysteries)", "The Sittaford Mystery (Mystery Masters Series)", "Secret of Chimneys", "The Mysterious Mr.Quin (Agatha Christie Signature Edition)", "Parker Pyne Investigates (Agatha Christie Collection)", "CROOKED HOUSE", "The Golden Ball and Other Stories", "Ordeal by innocence", "The Mystery of the Blue Train (Poirot)", "Man in the Brown Suit", "Appointment With Death (Hercule Poirot Mysteries)", "And Then There Were None (Audio Editions Mystery Masters)", "The adventure of the Christmas pudding and a selection of entre\u0301es.", "The Secret Adversary (Tommy & Tuppence Chronology)", "One, Two Buckle My Shoe (BBC Radio Collection)", "Endless night.", "The Big Four (G K Hall Large Print Book Series)", "Death Comes As the End (The Agatha Christie Mystery Collection)", "A Murder is Announced (Miss Marple Mysteries)", "Endless Night (Agatha Christie Collection)", "Seven Dials Mystery (Agatha Christie Audio Mystery)", "The under dog and other stories", "The Murder at the Vicarage (Acting Edition)", "The Body in the Library (BBC Audio Crime)", "The secret adversary", "The Pale Horse", "A Pocket Full of Rye (Miss Marple)", "Poems", "And Then There Were None (Ulverscroft Large Print)", "Towards Zero", "Curtain", "The Rose and the Yew Tree (Westmacott)", "The Mysterious Affair At Styles (Classic Books on Cassettes Collection) (Classic Books on Cassettes Collection)", "Murder at the Vicarage (Miss Marple Mysteries)", "A Murder Is Announced (BBC Radio Collection)", "Dumb witness.", "Nemesis (Miss Marple)", "And Then There Were None (St. Martin's True Crime Library)", "The Witness for the Prosecution and Other Stories", "The golden ball and other stories", "A Pocket Full of Rye (BBC Radio Collection: Crimes and Thrillers)", "Death in the Clouds (Agatha Christie Collection)", "A Murder Is Announced (Miss Marple Mysteries)", "The Under Dog and Other Stories", "Towards zero.", "Nemesis (BBC Radio Collection)", "Cards on the table (Agatha Christie mystery collection)", "Three Act Tragedy (Hercule Poirot Mysteries)", "Death on the Nile (Hercule Poirot Mysteries)", "Evil under the sun.", "The Hollow", "Partners in Crime (Agatha Christie Collection)", "Cat Among the Pigeons (Ulverscroft Mystery)", "Absent in the Spring", "Curtain (Hercule Poirot Mysteries)", "Appointment with death.", "Murder in the Mews (Agatha Christie Signature Edition)", "The Sittaford Mystery", "The Secret Adversary (Large Print Edition)", "The mysterious affair at Styles.", "Evil Under the Sun (Agatha Christie Audio Mystery)", "Death Comes As the End (Mystery Masters)", "The seven dials mystery", "The Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "Murder in the mews.", "Appointment with Death (Hercule Poirot Mysteries)", "Peril at End House (Hercule Poirot Mysteries)", "Murder at the Vicarage (Miss Marple Mysteries (Paperback))", "Murder of Roger Ackroyd", "The Adventure of the Christmas Pudding (The Crime Club)", "The mysterious affair at Styles", "The murder of Roger Ackroyd.", "Sleeping Murder (The Agatha Christie Mystery Collection)", "The Man in the Brown Suit (Agatha Christie Mysteries Collection)", "And Then There Were None (Mystery Masters)", "A pocket full of rye", "Hickory Dickory Dock (Hercule Poirot Mysteries (Paperback))", "Sleeping Murder (Miss Marple Mysteries)", "Five little pigs", "They Do It With Mirrors (Miss Marple Mysteries)", "Crooked House (Agatha Christie Audio Mystery)", "Big Four (Ulverscroft Large Print)", "The Murder of Roger Ackroyd (Poirot)", "The Murder on the Links", "Sleeping murder.", "The Big Four (Agatha Christie Collection)", "Passenger to Frankfurt (Winterbrook Edition)", "The Mysterious Affair at Styles (Dodo Press)", "The Clocks (Hercule Poirot Mysteries)", "Evil Under the Sun", "Poirot Investigates (G. K. Hall (Large Print))", "Towards Zero (St. Martin's Minotaur Mysteries)", "A Caribbean Mystery (Audio Editions)", "Murder in Mesopotamia (Agatha Christie Audio Mystery)", "The Man In The Brown Suit", "Death on the Nile (BBC Radio Collection)", "Death on the Nile (Agatha Christie Audio Mystery)", "The pale horse", "The Secret Adversary (Dodo Press)", "Peril at End House (Poirot)", "The Man in the Brown Suit (Mystery Masters)", "The Secret Adversary", "Ordeal by Innocence", "The Murder of Roger Ackroyd (Agatha Christie Audio Mystery)", "The Under Dog and Other Stories (G. K. Hall (Large Print))", "Towards Zero (Agatha Christie Mysteries Collection (Paperback))", "Murder Is Easy (Agatha Christie Collection)", "Passenger to Frankfurt.", "A Pocket Full of Rye (BBC Audio Crime)", "Three ACT Tragedy", "Cat among the pigeons", "4.50 from Paddington", "Lord Edgware Dies", "They do it with Mirrors", "Appointment With Death (G K Hall Large Print Book Series)", "Witness for the prosecution, and other stories", "After the Funeral (Agatha Christie Collection)", "Sparkling cyanide", "Three Act Tragedy (Poirot)", "Murder in the Mews (Ulverscroft Large Print)", "The Murder at the Vicarage (Miss Marple)", "Ordeal by Innocence (Signature Editions)", "They Came to Baghdad (G. K. Hall (Large Print))", "Destination unknown", "Partners in Crime (The Agatha Christie Mystery Collection)", "Poirot investigates", "The Sittaford Mystery (St. Martin's Minotaur Mysteries)", "The Mysterious Affair at Styles", "The Mysterious Affair at Styles, Large-Print Edition", "Three act tragedy", "Hickory Dickory Dock", "Elephants Can Remember (The Christie Collection)", "The Clocks (Agatha Christie Collection)", "The Body in the Library", "By the pricking of my thumbs", "Poirot Investigates (Hercule Poirot Mysteries)", "Murder In Mesopotamia (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Partners in Crime (Listen for Pleasure)", "Murder on the links", "The big four.", "The Secret Of Chimneys (St. Martin's Minotaur Mysteries)", "Passenger to Frankfurt (The Christie Collection)", "Crooked house.", "Partners in crime", "Murder on the Links (BBC Audio Crime)", "Cards on the Table (Hercule Poirot Mysteries)", "The Secret Adversary (Agatha Christie Audio Mystery)", "The Man in the Brown Suit (Agatha Christie Collection)", "Death Comes as the End", "Poirot investigates.", "One, two, buckle my shoe", "Murder is Easy (St. Martin's Minotaur Mysteries)", "The Murder At the Vicarage", "A Cat Among the Pigeons (Agatha Christie Collection)", "Sad Cypress (BBC Radio Collection)", "By The Pricking Of My Thumbs (Tommy and Tuppence Mysteries)", "Sparkling Cyanide (Mystery Masters Series)", "The murder of Roger Ackroyd", "a Pocket full of Rye", "Lord Edgware dies", "Death on the Nile", "Endless Night (Ulverscroft Large Print)", "The Man in the Brown Suit", "Mrs. McGinty's dead", "Five little pigs.", "The Murder of Roger Ackroyd. (Lernmaterialien)", "A Murder Is Announced (Miss Marple)", "The Pale Horse (The Agatha Christie mystery collection)", "Three blind mice and other stories (Agatha Christie Mystery Collection)", "Destination Unknown (Mystery Masters Series)", "Double Sin and other stories", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries (Paperback))", "Sleeping Murder (Miss Marple Mysteries (Paperback))", "The Secret Adversary (G K Hall's Agatha Christie Series)", "Problem at Pollensa Bay & Other Stories", "Golden Ball and Other Stories (Agatha Christie Mysteries Collection)", "They Came to Baghdad", "Taken at the Flood", "Pale horse", "Cat Among the Pigeons (Hercule Poirot Mysteries)", "The Sittaford Mystery (Agatha Christie Collection)", "Nemesis (Cover to Cover Classics)", "Dumb Witness", "Taken at the flood", "They Do It with Mirrors (Miss Marple Mysteries (Audio Partners))", "Death in the Clouds (Agatha Christie Audio Mystery)", "Come, Tell Me How You Live (Common Reader Editions: Rediscoveries: LONDON)", "The regatta mystery and other stories", "Five Little Pigs (The Christie Collection)", "The Murder at the Vicarage (BBC Full Cast Dramatization)", "The Hollow (Hercule Poirot)", "After the Funeral", "Big Four", "The witness for the prosecution and other stories", "The Listerdale mystery", "Third Girl (Hercule Poirot Mysteries (Paperback))", "Mysterious Affair at Styles/Cassettes (1362)", "Destination Unknown", "The mysterious Mr Quin", "Appointment with Death (The Christie Collection)", "Problem At Pollensa Bay and Seven Other Mysteries", "Sparkling Cyanide (Variant Title = Remembered Death)", "The Murder at the Vicarage", "Mysterious Affair at Styles", "The Hollow (Agatha Christie Collection)", "The mysterious Mr. Quin", "Peril at End House (The Agatha Christie Mystery Collection) (The Agatha Christie Mystery Collection)", "4.50 from Paddington (Miss Marple Mysteries (Audio))", "Death in the Clouds (BBC Radio Collection)", "Murder at the Vicarage (Agatha Christie Mysteries Collection)", "By the Pricking of My Thumbs (Tommy & Tuppence Chronology)", "Unexpected Guest", "Murder is easy.", "Death comes as the end", "Sparkling Cyanide (Audio Editions Mystery Masters)", "A Pocket Full of Rye (Agatha Christie Collection)", "CLOCKS", "Third girl", "Murder at the Vicarage (Miss Marple Mystery Series)", "Sad Cypress (Hercule Poirot)", "Absent in the Spring (Westmacott)", "Cards on the Table (Mystery Masters)", "Sparkling Cyanide (St. Martin's Minotaur Mysteries)", "The Pale Horse (Agatha Christie Collection)", "Curtain (G. K. Hall (Large Print))", "Crooked house", "Man in the Brown Suit (Ulverscroft Mystery)", "Body in the library", "The Witness for the Prosecution and Other Stories (Mystery Masters Series)", "Murder at the Vicarage (BBC Radio Presents)", "Taken at the Flood (Hercule Poirot)", "Taken at the Flood (Ulverscroft Large Print)", "After the funeral.", "Murder in Mesopotamia (Hercule Poirot Mysteries)", "Hercule Poirot\u2019s Christmas", "Clocks", "By the Pricking of My Thumbs (Agatha Christie Audio Mystery)", "The Body in the Library (BBC Radio Collection: Crimes and Thrillers)", "The harlequin tea set and other stories", "Partners in Crime (Tommy and Tuppence Mysteries)", "Star over Bethlehem, and other stories (The Agatha Christie mystery collection)", "After the Funeral (Hercule Poirot Mysteries (Paperback))", "By the Pricking of My Thumbs", "They came to Baghdad", "Cards on the table", "Murder in Mesopotamia.", "At Bertram's Hotel", "4.50 from Paddington (Agatha Christie Collection)", "The Sittaford mystery", "Cards on the Table (Agatha Christie Audio Mystery)", "Third Girl", "The unexpected guest", "The mysterious Mr. Quin.", "The mystery of the blue train", "Taken at the Flood (Hercule Poirot Mysteries (Paperback))", "Unfinished Portrait (Westmacott)", "Sparkling Cyanide (Agatha Christie Signature Edition)", "The Hound of Death", "Nemesis", "The Labours of Hercules (Hercule Poirot Mysteries)", "Elephants Can Remember (Hercule Poirot Mysteries)", "Poems (The Agatha Christie mystery collection)", "The Big Four.", "Clocks (Ulverscroft Large Print)", "A Caribbean Mystery (BBC Radio Collection)", "Witness for the Prosecution and Other Stories (G. K. Hall (Large Print))", "The thirteen problems", "The Sittaford Mystery (Agatha Christie Mystery Collection)", "Lord Edgware Dies (Poirot)", "The Murder on the Links (Hercule Poirot Mysteries)", "Death Comes as the End (Agatha Christie Collection)", "By the Pricking of My Thumbs (Tommy and Tuppence Mysteries)", "The secret of chimneys.", "The Seven Dials mystery", "Cat Among the Pigeons (Poirot)", "Partners in Crime (Agatha Christie Mysteries Collection)", "Murder in Mesopotamia (Hercule Poirot)", "The Hollow (Winterbrook Edition)", "Man in the Brown Suit (St. Martin's Minotaur Mysteries)", "One,two, buckle my shoe", "The murder on the links.", "The Man In The Brown Suit (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "Evil under the sun", "Murder on the Links", "The Hound of Death (Agatha Christie Collection)", "The Murder of Roger Ackroyd (Hercule Poirot Mysteries)", "After The Funeral", "The Unexpected Guest (BBC Radio Collection)", "A Pocket Full of Rye (Miss Marple Mysteries)", "The Body in the Library (Miss Marple Mysteries)", "They Came to Baghdad (Agatha Christie Collection)", "The Golden Ball and Other Stories (G. K. Hall (Large Print))", "Parker Pyne Investigates", "One, Two, Buckle My Shoe", "Cards on the Table (BBC Radio Collection)", "The Seven Dials Mystery (Agatha Christie Signature Edition)", "Cards on the table.", "A Caribbean Mystery (G. K. Hall (Large Print))", "Appointment with Death (Dell; 10246)", "4.50 from Paddington.", "After the Funeral (Hercule Poirot Mysteries)", "Sad Cypress (Poirot)", "The big four", "Crooked House (Minotaur Mysteries)", "The Thirteen Problems (Miss Marple)", "Cat Among the Pigeons (Hercule Poirot Mysteries (Paperback))", "They came to Baghdad.", "And then there were none", "Three Act Tragedy (BBC Radio Collection)", "A Pocket Full of Rye (Miss Marple Mysteries (Paperback))", "Nemesis (G. K. Hall (Large Print))", "Death Comes As the End", "Third Girl (Hercule Poirot Mysteries)", "Postern of Fate (G K Hall Large Print Book Series)", "The Listerdale mystery.", "Lord Edgware Dies (Hercule Poirot Mysteries)", "Mystery of the Blue Train", "Crooked House (Winterbrook Edition)", "The Labours of Hercules (Poirot)", "Sleeping Murder", "Thirteen Problems", "Towards Zero.", "One, Two, Buckle My Shoe (Ulverscroft Mystery)", "Cat among the Pigeons", "One, Two, Buckle My Shoe (Poirot)", "Hallowe'en party", "Murder On the Links", "The Mysterious Mr. Quin (Ulverscroft Large Print)", "Poirot Investigates (Hercule Poirot Mysteries (Audio))", "Hickory dickory dock.", "A murder is announced", "Sleeping Murder (Large Print Ed)", "They Do It With Mirrors (Agatha Christie Audio Mystery)", "Partners in Crime (Tommy & Tuppence Chronology)", "The Sittaford Mystery (Agatha Christie Signature Edition)", "Elephants Can Remember (Hercule Poirot Mysteries (Audio))", "Dead man's folly", "The Seven Dials Mystery (Agatha Christie Mystery Collection) Bantam Blue Leatherette Edition", "The A. B. C. Murders", "4.50 from Paddington (BBC Audio Crime)", "The Seven Dials Mystery", "Five Little Pigs (Poirot)", "Passenger to Frankfurt", "The Adventure of the Christmas Pudding and The Mystery of the Spanish Chest", "A Pocket Full Of Rye", "The murder at the vicarage"], "ans_acc": 0.005, "ans_hit": 1, "ans_f1": 0.009900990099009901, "ans_precission": 0.5, "ans_recall": 0.005, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.009900990099009901, "path_ans_precision": 0.5, "path_ans_recall": 0.005}
{"id": "WebQTest-377", "prediction": ["# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> education.education.institution -> Emsley A. Laney High School\n# Answer:\nEmsley A. Laney High School", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.school -> University of North Carolina at Chapel Hill\n# Answer:\nUniversity of North Carolina at Chapel Hill", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.institution -> University of North Carolina at Chapel Hill\n# Answer:\nUniversity of North Carolina at Chapel Hill", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.0jwk2zd -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMichael Jordan -> sports.drafted_athlete.drafted -> m.02h7cgv -> sports.sports_league_draft_pick.draft -> 1984 NBA draft\n# Answer:\n1984 NBA draft", "# Reasoning Path:\nMichael Jordan -> people.person.education -> m.04g36q3 -> education.education.major_field_of_study -> Cultural geography\n# Answer:\nCultural geography"], "ground_truth": ["Emsley A. Laney High School", "University of North Carolina at Chapel Hill"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-379", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> location.administrative_division.first_level_division_of -> Germany\n# Answer:\nBerlin"], "ground_truth": ["1945-04-30"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.ethnicity -> White people\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> government.politician.government_positions_held -> m.0b6b7y1\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.09k8753 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nGeorge W. Bush -> base.popstra.celebrity.insult_victim -> m.063s53c -> base.popstra.public_insult.perpetrator -> Al Gore\n# Answer:\nAl Gore", "# Reasoning Path:\nGeorge W. Bush -> government.political_appointer.appointees -> m.08v_t2j -> government.government_position_held.office_holder -> Colin Powell\n# Answer:\nColin Powell", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.profession -> Author\n# Answer:\nDick Cheney"], "ground_truth": ["Ralph Nader", "Michael Peroutka", "Gene Amondson", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-380", "prediction": ["# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Washington\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.location.containedby -> Portland-Vancouver-Beaverton, OR-WA Metropolitan Statistical Area\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.us_county.county_seat -> Stevenson\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> geography.mountain.mountain_range -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.02bkwf9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Skamania County -> location.us_county.hud_county_place -> Skamania County / County Place 5305999999\n# Answer:\nSkamania County", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Washington -> location.location.containedby -> Contiguous United States\n# Answer:\nWashington", "# Reasoning Path:\nMount St. Helens -> location.location.containedby -> Cascade Range -> location.location.containedby -> North America\n# Answer:\nCascade Range", "# Reasoning Path:\nMount St. Helens -> common.image.size -> m.03qtqgr\n# Answer:\ncommon.image.size"], "ground_truth": ["Skamania County", "North America", "United States of America", "Washington", "Cascade Range"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8000000000000002, "ans_precission": 0.8, "ans_recall": 0.8, "path_f1": 0.7466666666666666, "path_precision": 0.7, "path_recall": 0.8, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-381", "prediction": ["# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.languages -> English Language\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.featured_film_locations -> Las Vegas\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> tv.tv_program.filming_locations -> Las Vegas\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> common.topic.notable_types -> TV Program\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> Flight of the Conchords -> tv.tv_program.genre -> Comedy\n# Answer:\nFlight of the Conchords", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.country -> United States of America\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> tv.tv_location.tv_shows_filmed_here -> The Stand -> film.film.genre -> Drama\n# Answer:\nThe Stand", "# Reasoning Path:\nNew York City -> location.statistical_region.population -> g.11b674hm47\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Both Sides", "The Stand", "Flight of the Conchords", "Trial Heat"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.64, "path_precision": 0.8888888888888888, "path_recall": 0.5, "path_ans_f1": 0.64, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5}
{"id": "WebQTest-382", "prediction": ["# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.1254y3l6t\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.publisher -> Simon & Schuster Ltd\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> common.topic.notable_for -> g.125cjn5d9\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.written_work.subjects -> United States of America\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Tupac Shakur Collection -> common.topic.notable_types -> Book\n# Answer:\nThe Tupac Shakur Collection", "# Reasoning Path:\nTupac Shakur -> book.author.works_written -> The Rose That Grew from Concrete -> book.book.genre -> Music\n# Answer:\nThe Rose That Grew from Concrete", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book.genre -> Music\n# Answer:\nTupac", "# Reasoning Path:\nTupac Shakur -> common.topic.image -> Tupac -> book.book_edition.number_of_pages -> m.04xl38s\n# Answer:\nTupac"], "ground_truth": ["The Rose That Grew from Concrete", "Tupac", "The rose that grew from concrete"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8358208955223881, "path_precision": 0.875, "path_recall": 0.8, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-383", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> Yue Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Chinese, Hakka Language\n# Answer:\nMalaysia", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> China -> location.country.languages_spoken -> English Language\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Vietnam -> location.country.official_language -> Vietnamese Language\n# Answer:\nVietnam", "# Reasoning Path:\nChinese language -> language.human_language.main_country -> China -> location.country.official_language -> Standard Chinese\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Brunei -> location.country.official_language -> Malay Language\n# Answer:\nBrunei", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Malaysia -> location.country.languages_spoken -> Tamil Language\n# Answer:\nMalaysia"], "ground_truth": ["N\u00fcshu script", "Simplified Chinese character", "'Phags-pa script", "Traditional Chinese characters", "Chinese characters"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-384", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> common.topic.notable_types -> Profession\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Bard\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.notable_types -> Profession\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.article -> m.0knj9\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> people.profession.specializations -> Poet\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard -> common.topic.image -> The Bard\n# Answer:\nBard"], "ground_truth": ["Bard", "Writer", "Poet", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-385", "prediction": ["# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> base.biblioness.bibs_location.state -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> United States of America\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> base.wikipedia_infobox.settlement.area_code -> Area code 716\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> base.ndbcd.buoy.region -> New York\n# Answer:\nBuffalo", "# Reasoning Path:\nWilliam McKinley -> people.deceased_person.place_of_death -> Buffalo -> location.location.containedby -> Area code 716\n# Answer:\nBuffalo"], "ground_truth": ["1901-09-14"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-386", "prediction": ["# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Kentucky\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.administrative_divisions -> Tennessee\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> geography.lake.basin_countries -> United States of America -> base.locations.countries.states_provinces_within -> Nebraska\n# Answer:\nUnited States of America", "# Reasoning Path:\nLake Merritt -> location.location.containedby -> California -> location.location.partially_contains -> West Walker River\n# Answer:\nCalifornia"], "ground_truth": ["3.048"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-388", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.holidays -> Lailat al Mi'raj -> common.topic.notable_for -> g.1q6h0__29\n# Answer:\nLailat al Mi'raj", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Lailat al Mi'raj -> base.schemastaging.holiday_extra.observance_rule -> Lailat al Mi'raj observance rule (Earth, 2000 - now)\n# Answer:\nLailat al Mi'raj", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Day of Ashura -> time.holiday.featured_in_religions -> Shia Islam\n# Answer:\nDay of Ashura", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Lailat al Mi'raj -> common.topic.notable_types -> Holiday\n# Answer:\nLailat al Mi'raj", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr Eve -> common.topic.notable_types -> Holiday\n# Answer:\nEid al-Fitr Eve", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Fitr Eve -> base.schemastaging.holiday_extra.observance_rule -> Eid al-Fitr Eve observance rule (Turkey, 2000 -now)\n# Answer:\nEid al-Fitr Eve", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Barat Night -> common.topic.notable_types -> Holiday\n# Answer:\nBarat Night", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions"], "ground_truth": ["Laylat al-Qadr", "Mourning of Muharram", "Eid al-Fitr Eve", "Nuzul Al-Quran", "Ramadan", "Islamic New Year", "Eid-Ul-Adha Eve", "Eid al-Fitr", "Lailat al Mi'raj", "Day of Ashura", "Barat Night", "Night of Wishes", "Mawlid", "Day of Arafat", "Eid al-Adha"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48275862068965514, "ans_precission": 0.875, "ans_recall": 0.3333333333333333, "path_f1": 0.48275862068965514, "path_precision": 0.875, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48275862068965514, "path_ans_precision": 0.875, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-389", "prediction": ["# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Olivier Messiaen\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.notable_types -> Field of study\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> Art song -> music.compositional_form.superforms -> Vocal music\n# Answer:\nArt song", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.webpage -> m.09wjdym\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> music.genre.artists -> Francis Poulenc\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.image -> Paris Comedie-Francaise\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> people.person.profession -> Pianist -> music.musician_profession.instrument -> Piano\n# Answer:\nPianist", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> 20th-century classical music -> music.genre.parent_genre -> Classical music\n# Answer:\n20th-century classical music"], "ground_truth": ["Art song", "French opera", "Ballet", "Incidental music", "Classical music", "20th-century classical music"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.5753424657534246, "path_precision": 0.875, "path_recall": 0.42857142857142855, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> common.topic.notable_for -> g.126tnwph1\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_for -> g.125f25_6q\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> common.topic.notable_types -> Person\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-390", "prediction": ["# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_for -> g.1257xv6yy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nStanley Cup Finals -> sports.sports_championship.events -> 2014 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> New York Rangers\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.article -> m.0cc8ttf\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.champion -> Los Angeles Kings\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> time.recurring_event.instances -> 2014 Stanley Cup Finals -> sports.sports_championship_event.season -> 2013\u201314 NHL season\n# Answer:\n2014 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup Finals -> common.topic.notable_types -> Sports League Championship -> type.type.properties -> Events\n# Answer:\nSports League Championship"], "ground_truth": ["1967 Stanley Cup Finals"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-392", "prediction": ["# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09rm09r -> american_football.player_receiving_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nHank Baskett -> american_football.football_player.games -> m.07nvn3n -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.games -> m.07sh23c -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09grk6x -> american_football.player_receiving_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nHank Baskett -> american_football.football_player.games -> m.07yc3s3 -> american_football.player_game_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nHank Baskett -> american_football.football_player.games -> m.07nvn3n -> american_football.player_game_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nHank Baskett -> american_football.football_player.receiving -> m.09tcnhz -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Indianapolis Colts", "Philadelphia Eagles", "Minnesota Vikings"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.16216216216216214, "path_precision": 0.75, "path_recall": 0.09090909090909091, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-393", "prediction": ["# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> language.human_language.main_country -> Ukraine\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> common.topic.notable_types -> Human Language\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.languages_spoken -> Ukrainian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nUkrainian Language", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> language.human_language.region -> Americas\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.statistical_region.cpi_inflation_rate -> g.11b60sd4vw\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> base.rosetta.languoid.parent -> Guarani I Group\n# Answer:\nParaguayan Guaran\u00ed", "# Reasoning Path:\nParaguay -> location.statistical_region.gdp_nominal -> g.11bbwz2m1q\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nParaguay -> location.country.official_language -> Paraguayan Guaran\u00ed -> base.rosetta.languoid.local_name -> Guaran\u00ed, Paraguayan\n# Answer:\nParaguayan Guaran\u00ed"], "ground_truth": ["Spanish Language", "Paraguayan Guaran\u00ed"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-395", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> United States of America\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.citytown.postal_codes -> 15213\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Allegheny County\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> people.person.place_of_birth -> Hampton\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> organization.organization_scope.organizations_with_this_scope -> Kerr Communications\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.location -> Pittsburgh -> location.location.containedby -> Area code 412\n# Answer:\nPittsburgh", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.arena_stadium -> Pitt Stadium -> location.location.containedby -> 15213\n# Answer:\nPitt Stadium"], "ground_truth": ["Pittsburgh"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-396", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nThe King's School, Grantham", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts"], "ground_truth": ["Trinity College, Cambridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-397", "prediction": ["# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.image -> Carminic acid structure\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.notable_types -> Color\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.article -> m.01jnf9\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> common.topic.notable_for -> g.12588_wr9\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> education.educational_institution.colors -> Crimson -> base.roses.rose_color.roses_of_this_color -> Rosa 'Crimson Glory'\n# Answer:\nCrimson", "# Reasoning Path:\nHarvard University -> location.location.contains -> Gore Hall -> common.topic.notable_for -> g.1yw9k9gf0\n# Answer:\nGore Hall", "# Reasoning Path:\nHarvard University -> location.location.contains -> Massachusetts Hall -> location.location.containedby -> Cambridge\n# Answer:\nMassachusetts Hall", "# Reasoning Path:\nHarvard University -> education.educational_institution.newspaper -> The Harvard Crimson -> education.school_newspaper.school -> Harvard College\n# Answer:\nThe Harvard Crimson"], "ground_truth": ["Crimson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-398", "prediction": ["# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado -> location.location.containedby -> United States, with Territories\n# Answer:\nColorado", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> Colorado Springs -> location.location.containedby -> United States of America\n# Answer:\nColorado Springs", "# Reasoning Path:\nUniversity of the Rockies -> location.location.geolocation -> m.0ztn3fy\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nUniversity of the Rockies -> education.university.domestic_tuition -> m.0nhfxbb -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nUniversity of the Rockies -> education.educational_institution.phone_number -> m.0ng96hw\n# Answer:\neducation.educational_institution.phone_number", "# Reasoning Path:\nUniversity of the Rockies -> location.location.containedby -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America", "Colorado", "Colorado Springs"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-399", "prediction": ["# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> Metastatic breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Trisha Goddard\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.notable_people_with_this_condition -> Amanda Mealing\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> medicine.disease.includes_diseases -> Male breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> common.topic.webpage -> m.04mwg6d\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> medicine.disease.medical_specialties -> Hematology\n# Answer:\nMyelodysplastic syndrome", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Breast cancer -> common.topic.image -> Histopathologic image from ductal cell carcinoma in situ (DCIS) of breast. Hematoxylin-eosin stain\n# Answer:\nBreast cancer", "# Reasoning Path:\nRobin Roberts -> medicine.notable_person_with_medical_condition.condition -> Myelodysplastic syndrome -> base.schemastaging.context_name.pronunciation -> g.125_pcw3q\n# Answer:\nMyelodysplastic syndrome"], "ground_truth": ["Myelodysplastic syndrome", "Breast cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-400", "prediction": ["# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Third Crusade\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Southern Europe -> location.location.containedby -> Europe\n# Answer:\nSouthern Europe", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.events -> Sixth Crusade\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> common.topic.image -> NearEast2\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> location.location.contains -> Seljuk Empire\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Arabic political memoirs and other studies\n# Answer:\nNear East", "# Reasoning Path:\nByzantine Empire -> location.location.events -> Siege of Constantinople -> time.event.locations -> Constantinople\n# Answer:\nSiege of Constantinople"], "ground_truth": ["Constantinople"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-401", "prediction": ["# Reasoning Path:\nEdward Scissorhands -> film.film_character.portrayed_in_films -> m.0jxqvs -> film.performance.actor -> Johnny Depp\n# Answer:\nJohnny Depp", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p17sp -> film.film_crew_gig.crewmember -> Selena Evans-Miller\n# Answer:\nSelena Evans-Miller", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p1801 -> film.film_crew_gig.crewmember -> Matthew W. Mungle\n# Answer:\nMatthew W. Mungle", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p17sp -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nMakeup Artist", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p181x -> film.film_crew_gig.crewmember -> Rick Provenzano\n# Answer:\nRick Provenzano", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p184h -> film.film_crew_gig.film_crew_role -> Hair Stylist\n# Answer:\nHair Stylist", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p185r -> film.film_crew_gig.crewmember -> Rick Stratton\n# Answer:\nRick Stratton", "# Reasoning Path:\nEdward Scissorhands -> film.film.other_crew -> m.010p1801 -> film.film_crew_gig.film_crew_role -> Makeup Artist\n# Answer:\nMakeup Artist"], "ground_truth": ["Johnny Depp"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-402", "prediction": ["# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> music.group_membership.role -> Guitar\n# Answer:\nGuitar", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nDrums", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> music.group_membership.member -> Josh Brainard\n# Answer:\nJosh Brainard", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> g.11b7w8xg0_\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.0l1wzcp -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.0_yyf5f -> music.group_membership.group -> Scar the Martyr\n# Answer:\nScar the Martyr", "# Reasoning Path:\nJoey Jordison -> music.group_member.membership -> m.010ky_w5 -> music.group_membership.role -> Drums\n# Answer:\nDrums", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> g.11btvgynpj\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nSlipknot -> music.musical_group.member -> m.010ky_w5 -> music.group_membership.member -> Joey Jordison\n# Answer:\nmusic.group_membership.member", "# Reasoning Path:\nJoey Jordison -> music.group_member.instruments_played -> Drums -> common.topic.image -> Bass drum made from wood, rope, and cowskin\n# Answer:\nDrums"], "ground_truth": ["Drums"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-403", "prediction": ["# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Armenia\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> common.topic.notable_types -> Human Language\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Ladino Language -> language.human_language.region -> Asia\n# Answer:\nLadino Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iran\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> common.topic.notable_types -> Human Language\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.official_language -> Turkish Language -> common.topic.notable_types -> Human Language\n# Answer:\nTurkish Language", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Kurdish languages -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nKurdish languages", "# Reasoning Path:\nTurkey -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Kurdish languages", "Zaza language", "Turkish Language", "Arabic Language", "Ladino Language"], "ans_acc": 0.8, "ans_hit": 1, "ans_f1": 0.8421052631578948, "ans_precission": 0.8888888888888888, "ans_recall": 0.8, "path_f1": 0.8421052631578948, "path_precision": 0.8888888888888888, "path_recall": 0.8, "path_ans_f1": 0.8421052631578948, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.8}
{"id": "WebQTest-404", "prediction": ["# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qp97c -> basketball.basketball_player_stats.season -> 2004\u201305 NBA season\n# Answer:\n2004\u201305 NBA season", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qcjh2 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.pro_athlete.teams -> m.011461z_ -> sports.sports_team_roster.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04ql_x2 -> basketball.basketball_player_stats.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> sports.drafted_athlete.drafted -> m.04fl3dj -> sports.sports_league_draft_pick.team -> Cleveland Cavaliers\n# Answer:\nCleveland Cavaliers", "# Reasoning Path:\nLeBron James -> basketball.basketball_player.player_statistics -> m.04qmsr4 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season"], "ground_truth": ["Cleveland Cavaliers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.7058823529411765, "path_precision": 0.75, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-405", "prediction": ["# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.article -> m.012pmt\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.image -> George G. Meade Standing\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> Appomattox Campaign\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George Meade -> common.topic.image -> George Gordon Meade\n# Answer:\nGeorge Meade", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> Adolph von Steinwehr -> military.military_person.participated_in_conflicts -> Second Battle of Bull Run\n# Answer:\nAdolph von Steinwehr", "# Reasoning Path:\nBattle of Gettysburg -> military.military_conflict.military_personnel_involved -> George P. Doles -> military.military_person.participated_in_conflicts -> Battle of Antietam\n# Answer:\nGeorge P. Doles"], "ground_truth": ["Confederate States of America", "Robert E. Lee", "George Meade", "Union", "United States of America"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.31578947368421056, "ans_precission": 0.75, "ans_recall": 0.2, "path_f1": 0.019736842105263157, "path_precision": 0.75, "path_recall": 0.01, "path_ans_f1": 0.31578947368421056, "path_ans_precision": 0.75, "path_ans_recall": 0.2}
{"id": "WebQTest-406", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_manager.former_teams_managed -> m.0j7zl02\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> United States of America\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_player.position_s -> Manager\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_coaches -> m.0527d1b -> baseball.current_coaching_tenure.baseball_coach -> Dave Righetti\n# Answer:\nDave Righetti", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> baseball.baseball_player.position_s -> Catcher\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.team_stats -> m.05n69_j -> baseball.baseball_team_stats.season -> 1965 Major League Baseball Season\n# Answer:\n1965 Major League Baseball Season", "# Reasoning Path:\nSan Francisco Giants -> baseball.baseball_team.current_manager -> Bruce Bochy -> people.person.nationality -> France\n# Answer:\nBruce Bochy", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crtd80 -> sports.sports_league_participation.league -> National League West\n# Answer:\nNational League West"], "ground_truth": ["Ron Wotus", "Roberto Kelly", "Mark Gardner", "Tim Flannery", "Dave Righetti", "Bill Hayes", "Carney Lansford"], "ans_acc": 0.14285714285714285, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.125, "ans_recall": 0.14285714285714285, "path_f1": 0.13333333333333333, "path_precision": 0.125, "path_recall": 0.14285714285714285, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.125, "path_ans_recall": 0.14285714285714285}
{"id": "WebQTest-408", "prediction": ["# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> people.person.parents -> H. L. Hunt\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> sports.pro_athlete.teams -> m.0hsx4r3\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> common.topic.notable_types -> American football player\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> organization.organization_founder.organizations_founded -> Adams Resources & Energy\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> award.award_winner.awards_won -> m.0_qxnkc\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> common.topic.notable_for -> g.1257wg7mp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Lamar Hunt -> symbols.name_source.namesakes -> Lamar Hunt U.S. Open Cup\n# Answer:\nLamar Hunt", "# Reasoning Path:\nAmerican Football League -> organization.organization.founders -> Bud Adams -> people.deceased_person.cause_of_death -> Natural causes\n# Answer:\nBud Adams", "# Reasoning Path:\nAmerican Football League -> sports.sports_league.teams -> m.0crthm0 -> sports.sports_league_participation.team -> Oakland Raiders\n# Answer:\nOakland Raiders"], "ground_truth": ["Bud Adams", "Lamar Hunt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-409", "prediction": ["# Reasoning Path:\nGiuliana Rancic -> base.popstra.celebrity.dated -> m.065p_0q -> base.popstra.dated.participant -> Jerry O'Connell\n# Answer:\nJerry O'Connell", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.containedby -> Italy\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.time_zones -> Central European Time Zone\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Villa Comunale\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.location.events -> 1997 Napoli Film Festival\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> film.person_or_entity_appearing_in_film.films -> m.0w17pwb -> film.personal_film_appearance.type_of_appearance -> Host\n# Answer:\nHost", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> travel.travel_destination.tourist_attractions -> Cappella Sansevero\n# Answer:\nNaples", "# Reasoning Path:\nGiuliana Rancic -> people.person.place_of_birth -> Naples -> location.statistical_region.population -> g.11b7t8558d\n# Answer:\nNaples"], "ground_truth": ["Naples"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> base.aareas.schema.administrative_area.administrative_parent -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.administrative_division.country -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> base.aareas.schema.administrative_area.administrative_children -> Gal\u00e1pagos Province\n# Answer:\nEcuador"], "ground_truth": ["Pacific Ocean", "Gal\u00e1pagos Province", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-410", "prediction": ["# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> St Andrew's Church, Kiev\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> book.book_subject.works -> Ukraine: A History -> book.written_work.subjects -> Russia\n# Answer:\nUkraine: A History", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> location.country.capital -> Kiev -> travel.travel_destination.tourist_attractions -> Kiev Pechersk Lavra\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> travel.travel_destination.tourist_attractions -> St Andrew's Church, Kiev\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Kiev -> sports.sports_team_location.teams -> BC Budivelnyk\n# Answer:\nKiev", "# Reasoning Path:\nUkraine -> base.aareas.schema.administrative_area.administrative_children -> Cherkasy Oblast -> location.location.contains -> Kaniv\n# Answer:\nCherkasy Oblast", "# Reasoning Path:\nUkraine -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6dc92bn\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Kiev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-414", "prediction": ["# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nBrian Dawkins -> base.schemastaging.athlete_extra.salary -> m.0j3nt3p -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nBrian Dawkins -> common.image.size -> m.09hv3d2\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nBrian Dawkins -> common.topic.webpage -> m.0cvdvr0 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> award.award_nomination.award -> Walter Payton NFL Man of the Year Award\n# Answer:\nWalter Payton NFL Man of the Year Award", "# Reasoning Path:\nBrian Dawkins -> award.award_nominee.award_nominations -> m.0_qyjj3 -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nBrian Dawkins -> american_football.football_player.games -> m.07nvn5n -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nBrian Dawkins -> common.topic.webpage -> m.0cvdvr0 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.07407407407407407, "path_precision": 0.1111111111111111, "path_recall": 0.05555555555555555, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-415", "prediction": ["# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.film -> Juice\n# Answer:\nJuice", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcykh -> film.performance.film -> Nothing but Trouble\n# Answer:\nNothing but Trouble", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0j_81z -> film.performance.character -> Bishop\n# Answer:\nBishop", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vb3h0 -> film.performance.film -> Poetic Justice\n# Answer:\nPoetic Justice", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcpnk -> film.performance.film -> Bullet\n# Answer:\nBullet", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.0js_kj -> film.performance.film -> Above the Rim\n# Answer:\nAbove the Rim", "# Reasoning Path:\nTupac Shakur -> music.featured_artist.recordings -> Wanted Dead or Alive -> music.recording.releases -> The Prophet: The Best of the Works\n# Answer:\nWanted Dead or Alive", "# Reasoning Path:\nTupac Shakur -> film.actor.film -> m.02vcykh -> film.performance.character -> Digital Underground member\n# Answer:\nDigital Underground member"], "ground_truth": ["Murder Was the Case", "Poetic Justice", "Gang Related", "Bullet", "Gridlock'd", "Nothing but Trouble", "Above the Rim", "Juice"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.625, "ans_precission": 0.625, "ans_recall": 0.625, "path_f1": 0.4444444444444445, "path_precision": 0.5, "path_recall": 0.4, "path_ans_f1": 0.625, "path_ans_precision": 0.625, "path_ans_recall": 0.625}
{"id": "WebQTest-416", "prediction": ["# Reasoning Path:\nChina -> location.country.currency_used -> Renminbi -> common.topic.notable_types -> Currency\n# Answer:\nRenminbi", "# Reasoning Path:\nChina -> location.statistical_region.official_development_assistance -> g.12cp_kgkf\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.breed_of -> Dog\n# Answer:\nTibetan Terrier", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> location.location.partially_containedby -> Xinjiang\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> location.location.partially_contains -> Amur River -> location.location.partially_containedby -> Russia\n# Answer:\nAmur River", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> base.petbreeds.dog_breed.color -> Black\n# Answer:\nTibetan Terrier", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.coloring -> Black\n# Answer:\nTibetan Terrier"], "ground_truth": ["Renminbi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-417", "prediction": ["# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.actor -> Jack McBrayer\n# Answer:\nJack McBrayer", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Jack Donaghy\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> common.topic.notable_for -> g.125h4qvzs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 1\n# Answer:\n30 Rock - Season 1", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.series -> 30 Rock\n# Answer:\n30 Rock", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> people.person.gender -> Female\n# Answer:\nTina Fey", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nKenneth Parcell -> tv.tv_character.appeared_in_tv_program -> m.03hbzrb -> tv.regular_tv_appearance.seasons -> 30 Rock - Season 2\n# Answer:\n30 Rock - Season 2", "# Reasoning Path:\nKenneth Parcell -> fictional_universe.fictional_character.character_created_by -> Tina Fey -> fictional_universe.fictional_character_creator.fictional_characters_created -> Cerie\n# Answer:\nTina Fey"], "ground_truth": ["Jack McBrayer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-418", "prediction": ["# Reasoning Path:\nIndianapolis Colts -> sports.sports_team.league -> m.0crtfbg -> sports.sports_league_participation.league -> AFC South\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Tennessee Titans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> common.topic.article -> m.068g_7\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> organization.organization.parent -> m.0ck2dxw\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> common.topic.notable_types -> American football division\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> american_football.football_division.teams -> Houston Texans\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> american_football.football_team.division -> AFC South -> sports.sports_league.teams -> m.0crtf9v\n# Answer:\nAFC South", "# Reasoning Path:\nIndianapolis Colts -> sports.sports_team.league -> m.0crt9j5 -> sports.sports_league_participation.league -> American Football Conference\n# Answer:\nAmerican Football Conference"], "ground_truth": ["AFC South", "American Football Conference", "National Football League"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.375, "path_precision": 0.75, "path_recall": 0.25, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-419", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Maria da Silva\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Coimbra\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.composer -> Nasri\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.album.releases -> As Long As You Love Me (remixes)\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My Worlds: The Collection -> music.album.genre -> Pop music\n# Answer:\nMy Worlds: The Collection", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.1257jn46q\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.composition.recordings -> As Long As You Love Me\n# Answer:\nAs Long as You Love Me"], "ground_truth": ["Right Here", "Eenie Meenie", "Live My Life", "Bad Day", "PYD", "Lolly", "Never Say Never", "Recovery", "Roller Coaster", "All That Matters", "Home to Mama", "Die in Your Arms", "Never Let You Go", "Boyfriend", "Wait for a Minute", "#thatPower", "First Dance", "All Around The World", "All Bad", "Thought Of You", "Pray", "Hold Tight", "As Long as You Love Me", "Bigger", "Baby", "Heartbreaker", "Somebody to Love", "Beauty And A Beat", "Turn to You (Mother's Day Dedication)", "Confident", "Change Me"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.11695906432748539, "ans_precission": 0.625, "ans_recall": 0.06451612903225806, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.11695906432748539, "path_ans_precision": 0.625, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-420", "prediction": ["# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Metropolitan France\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.people_born_here -> Miguel Lluch\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Languedoc-Roussillon -> location.location.containedby -> Pyrenees\u2013Mediterranean Euroregion\n# Answer:\nLanguedoc-Roussillon", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> location.location.containedby -> Languedoc-Roussillon-Midi-Pyr\u00e9n\u00e9es\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.containedby -> Pyr\u00e9n\u00e9es-Orientales -> common.topic.image -> Location of Pyr\u00e9n\u00e9es-Orientales in France\n# Answer:\nPyr\u00e9n\u00e9es-Orientales", "# Reasoning Path:\nPerpignan -> location.location.nearby_airports -> Perpignan\u2013Rivesaltes Airport -> location.location.containedby -> France\n# Answer:\nPerpignan\u2013Rivesaltes Airport"], "ground_truth": ["Pyr\u00e9n\u00e9es-Orientales", "France", "Languedoc-Roussillon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-421", "prediction": ["# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.spouse -> G. Monty Brinson\n# Answer:\nG. Monty Brinson", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.spouse -> John Jackson\n# Answer:\nJohn Jackson", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0j4kh7g -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0swszf2 -> people.marriage.spouse -> Gregg Davis\n# Answer:\nGregg Davis", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0wd34pp -> people.marriage.spouse -> John J. Collett\n# Answer:\nJohn J. Collett", "# Reasoning Path:\nKim Richards -> people.person.parents -> Kathleen Richards -> people.person.spouse_s -> m.0j4js1t\n# Answer:\nKathleen Richards", "# Reasoning Path:\nKim Richards -> people.person.spouse_s -> m.0kv3rtm -> people.marriage.type_of_union -> Domestic partnership\n# Answer:\nDomestic partnership"], "ground_truth": ["Gregg Davis", "G. Monty Brinson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3076923076923077, "path_precision": 0.25, "path_recall": 0.4, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-422", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Ahatovi\u0107i massacre\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Battle of Kupres\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> military.military_conflict.military_personnel_involved -> Ratko Mladi\u0107\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Croat\u2013Bosniak War\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> base.culturalevent.event.entity_involved -> Serbia and Montenegro\n# Answer:\nBosnian War", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Operation Deliberate Force -> military.military_conflict.military_personnel_involved -> Paul Ray Smith\n# Answer:\nOperation Deliberate Force", "# Reasoning Path:\nBosnia and Herzegovina -> location.location.events -> Bosnian War -> time.event.includes_event -> Glogova massacre\n# Answer:\nBosnian War"], "ground_truth": ["Operation Miracle", "Operation Una", "Ahatovi\u0107i massacre", "Croat\u2013Bosniak War", "\u010cemerno massacre", "Operation Neretva '93", "Vi\u0161egrad massacres", "Operation Corridor 92", "Operation Storm", "Operation Mistral 2", "Operation Winter '94", "Bosnian War", "Massacre in Grabovica", "Glogova massacre", "Fo\u010da massacres", "Operation Tiger", "Sovi\u0107i massacre", "Yugoslav Wars", "Operation Jackal", "Stupni Do massacre", "Operation Summer '95", "Operation Southern Move", "NATO intervention in Bosnia and Herzegovina", "\u0160trpci massacre", "Sjeverin massacre", "Operation Spider", "Ahmi\u0107i massacre", "Battle of Hasselt", "La\u0161va Valley ethnic cleansing", "Operation Deny Flight", "Battle of Kupres", "Operation Deliberate Force", "Bosnian Genocide", "Operation Sana"], "ans_acc": 0.17647058823529413, "ans_hit": 1, "ans_f1": 0.11023622047244094, "ans_precission": 0.875, "ans_recall": 0.058823529411764705, "path_f1": 0.10727969348659003, "path_precision": 0.875, "path_recall": 0.05714285714285714, "path_ans_f1": 0.2937062937062937, "path_ans_precision": 0.875, "path_ans_recall": 0.17647058823529413}
{"id": "WebQTest-423", "prediction": ["# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> Super Bowl XLIII -> sports.sports_championship_event.runner_up -> Arizona Cardinals\n# Answer:\nSuper Bowl XLIII", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 1996 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1996 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> sports.sports_team.championships -> 2006 AFC Championship Game -> sports.sports_championship_event.season -> 2005 NFL season\n# Answer:\n2006 AFC Championship Game", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> american_football.football_coach.coaching_history -> m.05cvv1h\n# Answer:\nMike Tomlin", "# Reasoning Path:\nPittsburgh Steelers -> common.topic.webpage -> m.0gw54r6 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nPittsburgh Steelers -> american_football.football_team.current_head_coach -> Mike Tomlin -> sports.pro_athlete.teams -> m.05cvv_4\n# Answer:\nMike Tomlin"], "ground_truth": ["Super Bowl XLIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-425", "prediction": ["# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.country.languages_spoken -> Spanish Language\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> location.location.containedby -> North America\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.languages_spoken -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> language.human_language.main_country -> United States of America -> base.militaryinfiction.location_in_fiction.contains -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Antigua and Barbuda -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAntigua and Barbuda", "# Reasoning Path:\nEnglish Language -> language.human_language.countries_spoken_in -> Papua New Guinea -> location.country.official_language -> Tok Pisin Language\n# Answer:\nPapua New Guinea", "# Reasoning Path:\nEnglish Language -> education.school_category.schools_of_this_kind -> Piers Midwinter -> people.person.quotations -> Learn with a NEST. Please be my GUEST. I will give you my BEST,\n# Answer:\nPiers Midwinter"], "ground_truth": ["Honduras", "Kiribati", "Mandatory Palestine", "England", "New Zealand", "State of Palestine", "India", "Guyana", "United Kingdom", "United States of America", "Swaziland", "Antigua and Barbuda", "Lesotho", "Tanzania", "Sudan", "Kingdom of Great Britain", "Tokelau", "Sri Lanka", "Cook Islands", "Vatican City", "Barbados", "Canada", "Philippines", "Zambia", "Cura\u00e7ao", "Hong Kong", "Liberia", "Bangladesh", "South Yemen", "China", "Nauru", "Gibraltar", "Vanuatu", "Grenada", "Brunei", "Pakistan", "Kenya", "Timor-Leste", "Gazankulu", "Marshall Islands", "Guam", "Dominica", "Saint Kitts and Nevis", "Laos", "Ethiopia", "Montserrat", "Namibia", "Qatar", "Bahamas", "Japan", "Israel", "Australia", "Tuvalu", "Bonaire", "Saint Vincent and the Grenadines", "Malaysia", "Saint Lucia", "Jersey", "Sierra Leone", "Indonesia", "Republic of Ireland", "Territory of Papua and New Guinea", "Rwanda", "Samoa", "Isle of Man", "Ghana", "Malta", "Papua New Guinea", "Cyprus", "Fiji", "Belize", "Wales", "Territory of New Guinea", "Puerto Rico", "Gambia", "South Africa", "Cayman Islands", "Bermuda", "Jordan", "Cameroon", "Botswana", "Transkei", "Uganda", "Zimbabwe", "Nigeria", "Singapore", "Turks and Caicos Islands"], "ans_acc": 0.034482758620689655, "ans_hit": 1, "ans_f1": 0.06635071090047394, "ans_precission": 0.875, "ans_recall": 0.034482758620689655, "path_f1": 0.06635071090047394, "path_precision": 0.875, "path_recall": 0.034482758620689655, "path_ans_f1": 0.06635071090047394, "path_ans_precision": 0.875, "path_ans_recall": 0.034482758620689655}
{"id": "WebQTest-426", "prediction": ["# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Middle East\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Egypt\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> location.location.containedby -> Arab world\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> location.location.containedby -> Giza Governorate\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Giza -> travel.travel_destination.tourist_attractions -> Giza Plateau\n# Answer:\nGiza", "# Reasoning Path:\nGiza Necropolis -> location.location.containedby -> Egypt -> government.governmental_jurisdiction.government_bodies -> Shura Council\n# Answer:\nEgypt", "# Reasoning Path:\nGiza Necropolis -> travel.tourist_attraction.near_travel_destination -> Cairo -> location.location.containedby -> Egypt\n# Answer:\nCairo", "# Reasoning Path:\nGiza Necropolis -> film.film_location.featured_in_films -> Samsara -> film.film.featured_film_locations -> Egypt\n# Answer:\nSamsara", "# Reasoning Path:\nGiza Necropolis -> common.topic.notable_for -> g.12557nppf\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Giza", "Egypt"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-427", "prediction": ["# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Bank of America Building\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_for -> g.1256hz0nz\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> common.topic.notable_for -> g.1257xzgff\n# Answer:\n79702", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> common.topic.notable_types -> Postal Code\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79703 -> common.topic.notable_types -> Postal Code\n# Answer:\n79703", "# Reasoning Path:\nMidland -> location.statistical_region.population -> g.11b66cwzp7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.location.contains -> Centennial Tower\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79701 -> location.postal_code.country -> United States of America\n# Answer:\n79701", "# Reasoning Path:\nMidland -> location.citytown.postal_codes -> 79702 -> common.topic.notable_types -> Postal Code\n# Answer:\n79702"], "ground_truth": ["79710", "79712", "79711", "79702", "79708", "79705", "79703", "79706", "79704", "79707", "79701"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.41739130434782606, "ans_precission": 0.8888888888888888, "ans_recall": 0.2727272727272727, "path_f1": 0.41739130434782606, "path_precision": 0.8888888888888888, "path_recall": 0.2727272727272727, "path_ans_f1": 0.41739130434782606, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-428", "prediction": ["# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> United States of America\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Kansas\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Seth Hays House\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Nebraska -> location.location.containedby -> United States, with Territories\n# Answer:\nNebraska", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Indiana -> location.location.containedby -> United States, with Territories\n# Answer:\nIndiana", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.containedby -> Morris County\n# Answer:\nCouncil Grove", "# Reasoning Path:\nMidwestern United States -> location.location.contains -> Council Grove -> location.location.contains -> Butler Community College, Council Grove\n# Answer:\nCouncil Grove"], "ground_truth": ["Avenue of the Saints", "Kansas", "Missouri", "Illinois", "East North Central States", "Council Grove", "Ohio", "North Dakota", "Indiana", "Minnesota", "West North Central States", "Iowa", "Wisconsin", "South Dakota", "Nebraska", "Michigan"], "ans_acc": 0.3125, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 1.0, "ans_recall": 0.25, "path_f1": 0.4, "path_precision": 1.0, "path_recall": 0.25, "path_ans_f1": 0.47619047619047616, "path_ans_precision": 1.0, "path_ans_recall": 0.3125}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.specialization_of -> Writer\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher -> common.topic.notable_types -> Editor title\n# Answer:\nPublisher", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> type.type.expected_by -> politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician"], "ground_truth": ["Journalist", "Publisher", "Writer", "Statesman", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7017543859649122, "ans_precission": 0.625, "ans_recall": 0.8, "path_f1": 0.7017543859649122, "path_precision": 0.625, "path_recall": 0.8, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-430", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> tv.tv_program_guest.appeared_on -> m.0_grm8q -> tv.tv_guest_personal_appearance.episode -> Finalists Chosen #1\n# Answer:\nFinalists Chosen #1", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.executive_produced_by -> Allison Kaye Scarinzi\n# Answer:\nJustin Bieber's Believe"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-431", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "Lojban", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-432", "prediction": ["# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.spouse -> Frank E. Butler\n# Answer:\nFrank E. Butler", "# Reasoning Path:\nAnnie Oakley -> people.person.spouse_s -> m.0j4kg9x -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nAnnie Oakley -> film.film.cinematography -> J. Roy Hunt\n# Answer:\nJ. Roy Hunt", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Action/Adventure\n# Answer:\nAction/Adventure", "# Reasoning Path:\nAnnie Oakley -> common.topic.notable_for -> g.1259w1_jg\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Classic Action & Adventure\n# Answer:\nClassic Action & Adventure", "# Reasoning Path:\nAnnie Oakley -> common.topic.notable_for -> g.125d8f35t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Classic Movies\n# Answer:\nClassic Movies", "# Reasoning Path:\nAnnie Oakley -> media_common.netflix_title.netflix_genres -> Classic Westerns\n# Answer:\nClassic Westerns"], "ground_truth": ["Frank E. Butler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-433", "prediction": ["# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.actor -> Kevin Spacey\n# Answer:\nKevin Spacey", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.04j2wpn -> film.performance.actor -> Clancy Brown\n# Answer:\nClancy Brown", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.06wj_km -> film.performance.actor -> Clancy Brown\n# Answer:\nClancy Brown", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.02vbrr0 -> film.performance.actor -> Gene Hackman\n# Answer:\nGene Hackman", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3z7_ -> film.performance.actor -> Gene Hackman\n# Answer:\nGene Hackman", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.0k3zb8 -> film.performance.film -> Superman Returns\n# Answer:\nSuperman Returns", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.04j2wpn -> film.performance.film -> The Batman Superman Movie: World's Finest\n# Answer:\nThe Batman Superman Movie: World's Finest", "# Reasoning Path:\nAlexander Luthor -> film.film_character.portrayed_in_films -> m.06wj_km -> film.performance.film -> Superman/Batman: Public Enemies\n# Answer:\nSuperman/Batman: Public Enemies"], "ground_truth": ["Anthony LaPaglia", "Kevin Spacey", "Chris Noth", "Clancy Brown", "Gene Hackman", "Jesse Eisenberg", "Powers Boothe", "William Hootkins", "James Marsters"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.43478260869565216, "ans_precission": 0.625, "ans_recall": 0.3333333333333333, "path_f1": 0.4761904761904762, "path_precision": 0.625, "path_recall": 0.38461538461538464, "path_ans_f1": 0.43478260869565216, "path_ans_precision": 0.625, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-434", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.image -> Location of Mwinilunga in Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_for -> g.1258hl5bb\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> travel.tourist_attraction.near_travel_destination -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> location.location.containedby -> Zambia\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> common.topic.notable_for -> g.1258_gg1d\n# Answer:\nVictoria Falls"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-436", "prediction": ["# Reasoning Path:\nNigeria -> location.location.time_zones -> West Africa Time Zone\n# Answer:\nWest Africa Time Zone", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Cameroon\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> common.topic.notable_types -> Human Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> Liberia\n# Answer:\nEnglish Language", "# Reasoning Path:\nNigeria -> location.country.official_language -> English Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nEnglish Language"], "ground_truth": ["West Africa Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-437", "prediction": ["# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.location.containedby -> Nuevo Le\u00f3n\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.location.geolocation -> m.0wmwfp5\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> common.topic.notable_for -> g.126t0vkgr\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> location.administrative_division.second_level_division_of -> Mexico\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> people.deceased_person.place_of_death -> Iturbide -> common.topic.article -> m.0p3q30d\n# Answer:\nIturbide", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> common.topic.notable_for -> g.125b9dwrj\n# Answer:\nJenni", "# Reasoning Path:\nJenni Rivera -> award.award_winner.awards_won -> m.0v90q1c -> award.award_honor.ceremony -> 2013 Billboard Music Awards\n# Answer:\n2013 Billboard Music Awards", "# Reasoning Path:\nJenni Rivera -> music.artist.album -> Jenni -> common.topic.article -> m.04mymgn\n# Answer:\nJenni"], "ground_truth": ["Iturbide"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-438", "prediction": ["# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.institution -> Harvard Law School\n# Answer:\nHarvard Law School", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkf4 -> education.education.institution -> Stanford University\n# Answer:\nStanford University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.0125cyb9 -> education.education.institution -> Harvard University\n# Answer:\nHarvard University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkfc -> education.education.institution -> Brigham Young University\n# Answer:\nBrigham Young University", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.major_field_of_study -> Law\n# Answer:\nLaw", "# Reasoning Path:\nMitt Romney -> people.person.education -> m.02kvkg9 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nMitt Romney -> film.person_or_entity_appearing_in_film.films -> m.0tk1pyb -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMitt Romney -> people.person.religion -> Mormonism -> religion.religion.organizations -> Brigham Young University\n# Answer:\nMormonism"], "ground_truth": ["Brigham Young University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-439", "prediction": ["# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> location.location.time_zones -> Mountain Time Zone -> common.topic.article -> m.02hczk\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nSouth Dakota -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nUnited States of America"], "ground_truth": ["Central Time Zone", "Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st830 -> location.location_symbol_relationship.symbol -> Cactus wren\n# Answer:\nCactus wren", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04stk7b -> location.location_symbol_relationship.symbol -> Apache trout\n# Answer:\nApache trout", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-440", "prediction": ["# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.geolocation -> m.0khpj5\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Virginia\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.hud_foreclosure_area.total_residential_addresses -> m.07cjr3p\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> common.topic.notable_for -> g.1257s1xm2\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.containedby -> Hanover County\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> Mechanicsville -> location.location.people_born_here -> Wayne Grubb\n# Answer:\nMechanicsville", "# Reasoning Path:\nJason Mraz -> music.artist.origin -> California -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nCalifornia"], "ground_truth": ["California", "Mechanicsville"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-441", "prediction": ["# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Tommy Dorsey\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.parent_cause_of_death -> Pulmonary aspiration\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> common.topic.notable_for -> g.1255jz6dx\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Jimi Hendrix\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> common.topic.notable_types -> Cause Of Death\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Bon Scott\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> common.topic.article -> m.023qmcf\n# Answer:\nInhalation of vomit", "# Reasoning Path:\nJohn Bonham -> people.deceased_person.cause_of_death -> Inhalation of vomit -> people.cause_of_death.people -> Crash Holly\n# Answer:\nInhalation of vomit"], "ground_truth": ["Inhalation of vomit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-442", "prediction": ["# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Beer, Wine, and Liquor Stores\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Building Material and Garden Equipment and Supplies Dealers\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.competitive_space.company -> m.010ddb_m\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.article -> m.0191_h\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> business.industry.child_industry -> Clothing and Clothing Accessories Stores\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Retail -> common.topic.subjects -> Chemist Warehouse\n# Answer:\nRetail", "# Reasoning Path:\nWalmart -> business.business_operation.industry -> Department store -> business.industry.child_industry -> Department Stores (except Discount Department Stores)\n# Answer:\nDepartment store", "# Reasoning Path:\nWalmart -> organization.organization.sectors -> Retail -> business.industry.child_industry -> Beer, Wine, and Liquor Stores\n# Answer:\nRetail"], "ground_truth": ["Retail", "Department store", "Variety Stores"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-443", "prediction": ["# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Chicago\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.hud_county_place.county -> Miller County -> location.location.containedby -> Arkansas\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Arkansas -> location.location.containedby -> United States of America\n# Answer:\nArkansas", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Arkansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nTexarkana, Arkansas -> location.location.containedby -> Miller County -> location.location.containedby -> Arkansas\n# Answer:\nMiller County", "# Reasoning Path:\nTexarkana, Arkansas -> location.statistical_region.population -> g.11b66f8sth\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Miller County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-446", "prediction": ["# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's  Baroque music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> common.topic.subjects -> ABRSM\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Opera -> broadcast.genre.content -> 1.FM Otto's Opera House\n# Answer:\nOpera", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> organization.organization_type.organizations_of_this_type -> ABRSM\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> broadcast.genre.content -> 1.FM Otto's Classical channel\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> book.book_subject.works -> The Classical Style: Haydn, Mozart, Beethoven\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Pop music\n# Answer:\nClassical music", "# Reasoning Path:\nLudwig van Beethoven -> music.artist.genre -> Classical music -> music.genre.subgenre -> Baroque music\n# Answer:\nClassical music"], "ground_truth": ["Classical music", "Opera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-447", "prediction": ["# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> book.written_work.previous_in_series -> By the Shores of Silver Lake\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> fictional_universe.work_of_fiction.setting -> De Smet\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> book.written_work.subjects -> United States of America\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> The Long Winter -> common.topic.image -> Train stuck in snow\n# Answer:\nThe Long Winter", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> My Little House 123 -> common.topic.notable_for -> g.1z2552vs4\n# Answer:\nMy Little House 123", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Little House on the Prairie -> book.written_work.next_in_series -> On the Banks of Plum Creek\n# Answer:\nLittle House on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> Hard Times on the Prairie -> common.topic.notable_for -> g.1255l13vx\n# Answer:\nHard Times on the Prairie", "# Reasoning Path:\nLaura Ingalls Wilder -> book.author.works_written -> My Little House 123 -> common.topic.notable_types -> Written Work\n# Answer:\nMy Little House 123"], "ground_truth": ["Writings to young women from Laura Ingalls Wilder", "These Happy Golden Years (Laura Years)", "First Four Years (Little House (Original Series Paperback))", "By the Shores of Silver Lake CD (Little House)", "Little House (9 Books, Boxed Set)", "By the Shores of Silver Lake (Little House Books)", "Little House In The Big Woods Unabr CD Low Price (Little House the Laura Years)", "Little House in the Big Woods.", "Little House Friends (Little House Chapter Books/the Laura Years, 9)", "Hard Times on the Prairie", "Orillas del r\u00edo Plum", "Little House", "Little House on the Prairie (Little House (HarperTrophy))", "The Deer in the Wood (Little House)", "These Happy Golden Years CD", "These Happy Golden Years (Little House on the Prairie)", "Farmer Boy (Little House Books)", "Going West", "A Little House Traveler", "A Little House Collection", "A Little House Birthday", "Santa comes to little house", "Little House in the Big Woods (Little House Books)", "Laura & Mr. Edwards", "Christmas in the Big Woods (My First Little House Books)", "The Little House Collection", "A Farmer Boy Birthday (My First Little House Books)", "Christmas Stories (Little House Chapter Book)", "My Little House 123", "School Days (Little House Chapter Books/the Laura Years, 4)", "On the Banks of Plum Creek (Classic Mammoth)", "A FAMILY COLLECTION", "Pioneer Sisters (Little House Chapter Books/the Laura Years, 2)", "The long winter", "Little House on the Prairie (Little House)", "On the Banks of Plum Creek (Little House the Laura Years)", "Prairie Day (My First Little House)", "Little House the Laura Years Boxed Set", "Little House on the Prairie (Little Brown Notebook Series)", "These Happy Golden Years (Little House)", "Little town on the prairie", "By the shores of Silver Lake", "School Days", "These Happy Golden Years (Little House (HarperTrophy))", "Bedtime for Laura", "Laura & Nellie", "Little House Friends", "My Little House Chapter Book Collection", "By the Shores of Silver Lake (Little House)", "Farmer Boy Days (Little House Chapter Books/the Laura Years, 6)", "Sugar Snow (My First Little House Books)", "My Little House Songbook (My First Little House Books)", "The Complete Little House on the Prairie", "Animal Adventures (Little House Chapter Books)", "Caroline and Her Sister", "A Little Prairie House", "On the Banks of Plum Creek (Little House (Original Series Paperback))", "Christmas Stories (Little House Chapter Books/the Laura Years, 10)", "Pioneer Sisters", "Little House in the Big Woods (Isis Large Print for Children Windrush)", "Farmer Boy Days (Little House Chapter Books)", "Going to town", "Dance at Grandpa's", "Long Winter", "A little house sampler", "Little House Farm Days (Little House Chapter Books/the Laura Years, 7)", "Little House in the Ozarks: The Rediscovered Writings", "The First Four Years (Little House)", "Little House Parties (Little House Chapter Books)", "The Little House Collection Box Set (Full Color) (Little House)", "Little House on the Prairie (Little House Books)", "School Days (Laura (Econo-Clad))", "Little Town on the Prairie (Little House Books)", "Little House on the Prairie (Classic Mammoth)", "A Day on the Prairie", "These Happy Golden Years (Little House Books)", "Little house in the big woods", "These Happy Golden Years (Little House (Original Series Paperback))", "Little House Friends (Little House Chapter Book)", "Laura's Christmas", "Little House On The Prairie CD (Little House the Laura Years)", "My Little House Diary", "Little House On The Prairie", "By the Shores of Silver Lake", "Long Winter (Little House (HarperTrophy))", "Long Winter (Little House (Original Series Paperback))", "Largo Invierno", "Winter on the Farm (My First Little House Books (Sagebrush))", "The Deer in the Wood (My First Little House Books)", "Laura's Pa", "The Little House on the Prairie", "Little House On The Prairie (Little House (Original Series Paperback))", "School days (Little house chapter book)", "Winter on the Farm (My First Little House)", "A Little Prairie House (My First Little House Books)", "My Book of Little House Paper Dolls", "Santa Comes to Little House", "On the banks of Plum Creek", "My Book of Little House Christmas Paper Dolls: Christmas on the Prairie", "Little House Parties (Little House Chapter Books/the Laura Years, 14)", "The first four years", "West from home", "The Deer in the Wood (My First Little House Books (Sagebrush))", "My Little House Songbook (My First Little House Books, No 1)", "A Little house traveler", "Christmas Stories", "Laura & Mr. Edwards (Little House Chapter Book)", "Little House in the Big Woods (Little House)", "The Deer in the Wood", "The Long Winter (Little House Books)", "A Little House Reader", "A Little house reader", "g.11b60fcs_w", "Farmer Boy Days (Little House Chapter Book)", "Hello, Laura!", "Going West (My First Little House Books)", "The First Four Years (Little House Books)", "On the Banks of Plum Creek (One Cassette)", "Going to Town (My First Little House)", "These Happy Golden Years", "On the Banks of Plum Creek (Little House)", "Dear Laura: Letters from Children to Laura Ingalls Wilder", "Dance at Grandpa's (My First Little House)", "Laura Helps Pa", "West from Home", "Christmas in the Big Woods", "The Long Winter (Little House (Original Series Paperback))", "Pioneer Sisters (Little House Chapter Book)", "The First Four Years (Little House the Laura Years)", "Little House in the Big Woods (Classic Mammoth)", "Saving graces", "A little house treasury", "Dear Laura", "The First Four Years (Little House (Original Series Library))", "Sugar Snow (My First Little House Books (Sagebrush))", "Farmer Boy (Little House)", "Going West (My First Little House Books (Sagebrush))", "A Farmer Boy", "Little House in the Big Woods Book and Charm (Charming Classics)", "Farmer boy", "Summertime in the Big Woods", "Little House in the Big Woods", "A Farmer Boy Birthday", "Little House in the Big Woods 75th Anniversary Edition (Little House)", "Little House on the Prairie", "Little House on the Prairie NW 247", "Pioneer Sisters (Laura (Econo-Clad))", "Deer in the Wood", "Little house on the prairie.", "A Little Prairie House (Little House)", "First Four Years (Little House (HarperTrophy))", "My Little House Songbook", "County Fair (My First Little House)", "On the way home", "Animal Adventures", "Little House Parties (Little House Chapter Book)", "The Long Winter", "The First Four Years CD (Little House the Laura Years)", "The first four years.", "Sugar Snow", "Little House Farm Days", "Little Town on the Prairie", "Farmer Boy (Little House (Original Series Paperback))", "Little House In The Big Woods CD (Little House the Laura Years)", "Going West (My First Little Houe Books)", "Little House on the Prairie Book and Charm (Charming Classics)", "The Little House Baby Book", "Little House Friends (Little House Chapter Books)", "1998 Laura Ingalls Wilder Country Engagement Book", "Laura Ingalls Wilder's prairie wisdom", "The Adventures of Laura and Jack (A Little House Chapter Book) (A Little house chapter book)", "On the Banks of Plum Creek CD (Little House the Laura Years)", "Little House Farm Days (Little House Chapter Books)", "Words from a fearless heart", "Little Town On The Prairie (Little House (Original Series Paperback))", "A Little House Sampler", "Prairie Day", "My Little House Book of Animals", "Going to Town (My First Little House Books)", "Little House in the Big Woods (Little House (Original Series Paperback))", "Laura & Nellie (Little House Chapter Book)", "Farmer Boy Days", "Little House On The Prairie (Little House the Laura Years)", "My Little House Songbook (My First Little House Books Series)", "Winter Days in the Big Woods", "Winter on the Farm", "On the Way Home", "Christmas in the Big Woods (Little House)", "By the Shores of Silver Lake (Little House (Original Series Paperback))", "Little House on the Prairie Tie-in Edition (Little House)", "Country Fair", "My Little House Book of Family", "Going to Town", "Little house on the prairie", "Hard Times on the Prairie (Little House Chapter Books/the Laura Years, 8)", "My Little House Book of Memories", "Little House Parties", "School Days (Little House Chapter Book)", "These happy golden years", "On the Banks of Plum Creek", "Christmas Stories (Little House Chapter Books)", "Sugar Snow (My First Little House)", "Animal Adventures (Little House Chapter Books/the Laura Years, 3)", "County Fair (My First Little House Books (Sagebrush))", "Going West (My First Little House)", "The deer in the wood (My first Little house books)", "My Little House Birthday Book", "Farmer Boy", "The adventures of Laura and Jack", "Little House Sisters", "Hard Times on the Prairie (Little House Chapter Books)", "Laura's Little House", "Winter on the Farm (My First Little House Books)", "The First Four Years", "Little Town on the Prairie CD", "Little House on the Prairie Boxed Set ((9 Books) Little House On the Prairie; Farmer Boy; On the Banks of Plum Creek; the Long Winter; These Happy Golden Years; the First Four Years; By the Shores of Silver Lake; Little House In the Big Woods; Little Town On the Prairie)", "Little House On The Prairie Low Price CD", "Laura's Garden", "Laura's Early Years Collection", "g.122chq7m", "Little Town on the Prairie (Little House)", "The Long Winter (Little House)"], "ans_acc": 0.06756756756756757, "ans_hit": 1, "ans_f1": 0.0944206008583691, "ans_precission": 1.0, "ans_recall": 0.04954954954954955, "path_f1": 0.09340659340659341, "path_precision": 1.0, "path_recall": 0.04899135446685879, "path_ans_f1": 0.12658227848101267, "path_ans_precision": 1.0, "path_ans_recall": 0.06756756756756757}
{"id": "WebQTest-448", "prediction": ["# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Japan\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Kingdom of Italy\n# Answer:\nUnitary state", "# Reasoning Path:\nItaly -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Albania\n# Answer:\nParliamentary republic", "# Reasoning Path:\nItaly -> location.statistical_region.broadband_penetration_rate -> g.1245_6n11\n# Answer:\nlocation.statistical_region.broadband_penetration_rate"], "ground_truth": ["Parliamentary republic", "Unitary state", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-449", "prediction": ["# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_for -> g.125925tg1\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> In One Person -> book.written_work.previous_in_series -> Last Night in Twisted River\n# Answer:\nIn One Person", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.written_work.next_in_series -> A Son of the Circus\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.book_editions_published -> A Prayer for Owen Meany -> book.written_work.previous_in_series -> The Cider House Rules\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Tabby Wheelwright\n# Answer:\nA Prayer for Owen Meany", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> The imaginary girlfriend -> common.topic.notable_types -> Book\n# Answer:\nThe imaginary girlfriend", "# Reasoning Path:\nJohn Irving -> book.author.works_written -> A Prayer for Owen Meany -> book.book.characters -> Owen Meany\n# Answer:\nA Prayer for Owen Meany"], "ground_truth": ["The Cider House Rules", "A Son of the Circus", "Water Method Man", "Last Night in Twisted River", "A Sound Like Someone Trying Not to Make a Sound", "Setting Free the Bears", "The Hotel New Hampshire", "The imaginary girlfriend", "Until I Find You", "A Prayer for Owen Meany", "The World According to Garp", "Trying to Save Piggy Sneed", "3 by Irving", "The Fourth Hand", "The 158-Pound Marriage", "A Widow for One Year", "In One Person"], "ans_acc": 0.35294117647058826, "ans_hit": 1, "ans_f1": 0.3, "ans_precission": 1.0, "ans_recall": 0.17647058823529413, "path_f1": 0.2608695652173913, "path_precision": 0.5, "path_recall": 0.17647058823529413, "path_ans_f1": 0.5217391304347826, "path_ans_precision": 1.0, "path_ans_recall": 0.35294117647058826}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-450", "prediction": ["# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.symbol -> Nene\n# Answer:\nNene", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.symbol -> Reef triggerfish\n# Answer:\nReef triggerfish", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stpzt -> location.location_symbol_relationship.symbol -> Candlenut\n# Answer:\nCandlenut", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04strb6 -> location.location_symbol_relationship.symbol -> Gold dust day gecko\n# Answer:\nGold dust day gecko", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.0hz9k44 -> location.location_symbol_relationship.symbol -> Hawaiian hibiscus\n# Answer:\nHawaiian hibiscus", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04st848 -> location.location_symbol_relationship.Kind_of_symbol -> State bird\n# Answer:\nState bird", "# Reasoning Path:\nHawaii -> meteorology.cyclone_affected_area.cyclones -> Typhoon Dolphin -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTyphoon Dolphin", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.official_symbols -> m.04stk8g -> location.location_symbol_relationship.Kind_of_symbol -> State Fish\n# Answer:\nState Fish"], "ground_truth": ["Hawaiian hibiscus"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-451", "prediction": ["# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> The argument over women's rights in Victoria was lampooned in this Melbourne Punch cartoon of 1887\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> tv.tv_subject.tv_episodes -> Sufferin' Until Sufferage\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Ted Cole\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> common.topic.image -> Feminist Suffrage Parade in New York City, 1912\n# Answer:\nWomen's suffrage", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Edgar Allan Poe\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> book.book_subject.works -> Authors Frequently Mentioned on the Web\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> people.person.profession -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nAlice Stokes Paul -> base.activism.activist.area_of_activism -> Women's suffrage -> event.speech_topic.speeches_or_presentations_on_this_topic -> m.05rkjr0\n# Answer:\nWomen's suffrage"], "ground_truth": ["Organization founder"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-453", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Jamaica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.main_country -> Jamaica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Venezuela\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Colombia\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nCosta Rica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Jamaican Creole English Language", "Spanish Language", "Bribri Language", "Limonese Creole"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6153846153846154, "ans_precission": 0.8, "ans_recall": 0.5, "path_f1": 0.6153846153846154, "path_precision": 0.8, "path_recall": 0.5, "path_ans_f1": 0.6153846153846154, "path_ans_precision": 0.8, "path_ans_recall": 0.5}
{"id": "WebQTest-454", "prediction": ["# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.geolocation -> m.0239t8w\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> base.ontologies.ontology_instance.equivalent_instances -> m.09kmh29\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> North East England\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> United Kingdom\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.events -> Great fire of Newcastle and Gateshead\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> music.lyricist.lyrics_written -> Throw Down a Line -> music.composition.recordings -> Throw Down A Line\n# Answer:\nThrow Down a Line", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nHank Marvin -> people.person.place_of_birth -> Newcastle upon Tyne -> organization.organization_scope.organizations_with_this_scope -> The TEFL Academy Stoke-on-Trent\n# Answer:\nNewcastle upon Tyne"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-455", "prediction": ["# Reasoning Path:\nChipper Jones -> sports.drafted_athlete.drafted -> m.04vy30g -> sports.sports_league_draft_pick.draft -> 1990 Major League Baseball Draft\n# Answer:\n1990 Major League Baseball Draft", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0l_7 -> baseball.batting_statistics.season -> 2004 Major League Baseball season\n# Answer:\n2004 Major League Baseball season", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lq6 -> baseball.batting_statistics.season -> 1998 Major League Baseball Season\n# Answer:\n1998 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lj8 -> baseball.batting_statistics.season -> 1993 Major League Baseball Season\n# Answer:\n1993 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> sports.drafted_athlete.drafted -> m.04vy30g -> sports.sports_league_draft_pick.team -> Atlanta Braves\n# Answer:\nAtlanta Braves", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lky -> baseball.batting_statistics.season -> 1995 Major League Baseball Season\n# Answer:\n1995 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> baseball.baseball_player.batting_stats -> m.06r0lmd -> baseball.batting_statistics.season -> 1996 Major League Baseball Season\n# Answer:\n1996 Major League Baseball Season", "# Reasoning Path:\nChipper Jones -> award.award_nominee.award_nominations -> m.0z86p0x -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["1990 Major League Baseball Draft"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-456", "prediction": ["# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> tv.tv_program.theme_song -> Theme from Star Trek\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nWinona Ryder -> common.image.size -> m.04sl_lr\n# Answer:\ncommon.image.size", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> media_common.netflix_title.netflix_genres -> Television\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> tv.tv_program.program_creator -> Gene Roddenberry\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.prequel -> Star Trek V: The Final Frontier\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek: The Original Series -> tv.tv_program.genre -> Science Fiction\n# Answer:\nStar Trek: The Original Series", "# Reasoning Path:\nStar Trek -> fictional_universe.fictional_universe.works_set_here -> Star Trek VI: The Undiscovered Country -> film.film.written_by -> Gene Roddenberry\n# Answer:\nStar Trek VI: The Undiscovered Country", "# Reasoning Path:\nWinona Ryder -> film.actor.film -> m.010fw41j -> film.performance.film -> Turks & Caicos\n# Answer:\nTurks & Caicos", "# Reasoning Path:\nStar Trek -> base.reviews.reviewed_topic.reviews -> Review of Star Trek at blog.oregonlive.com -> base.reviews.review.mentions -> Zoe Saldana\n# Answer:\nReview of Star Trek at blog.oregonlive.com"], "ground_truth": ["Amanda Grayson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-459", "prediction": ["# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.hud_county_place.county -> Platte County\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> 18th and Vine-Downtown East\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.citytown.postal_codes -> 64101\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Brookside Park\n# Answer:\nKansas City", "# Reasoning Path:\nJovan Belcher -> people.deceased_person.place_of_death -> Kansas City -> location.statistical_region.population -> g.11b66slpck\n# Answer:\nKansas City"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> Harrison inauguration\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.notable_for -> g.1256hs_cs\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.article -> m.05bz_04\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> base.inaugurations.inauguration_speaker.inauguration -> William Henry Harrison 1841 presidential inauguration -> common.topic.image -> 222px-Harrison_inauguration1841.jpg\n# Answer:\nWilliam Henry Harrison 1841 presidential inauguration", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-461", "prediction": ["# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0hprg7d -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nClay Matthews III -> people.person.education -> m.0n0g06t -> education.education.institution -> Agoura High School\n# Answer:\nAgoura High School", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> common.topic.image -> Northridge Shopping District\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0hprhl_\n# Answer:\namerican_football.football_player.forty_yard_dash_time", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.forty_yard_dash_time -> m.0wkcwx3\n# Answer:\namerican_football.football_player.forty_yard_dash_time", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.people_born_here -> Adam Fortin\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> location.location.containedby -> California\n# Answer:\nNorthridge", "# Reasoning Path:\nClay Matthews III -> american_football.football_player.games -> m.07sh9q1 -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nClay Matthews III -> people.person.place_of_birth -> Northridge -> base.biblioness.bibs_location.state -> California\n# Answer:\nNorthridge"], "ground_truth": ["Agoura High School", "University of Southern California"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-462", "prediction": ["# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nHouston -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07cvhy0 -> measurement_unit.dated_integer.source -> United States Department of Housing and Urban Development\n# Answer:\nUnited States Department of Housing and Urban Development", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b7tcbqts\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> location.place_with_neighborhoods.neighborhoods -> Manchester -> common.topic.notable_types -> Neighborhood\n# Answer:\nManchester"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-463", "prediction": ["# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.actor -> Amy Smart\n# Answer:\nAmy Smart", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.01xpnt9 -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k3r3v -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.010wvf3v -> film.performance.actor -> Eden Espinosa\n# Answer:\nEden Espinosa", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0k2h24 -> film.performance.actor -> Carrie Fisher\n# Answer:\nCarrie Fisher"], "ground_truth": ["Carrie Fisher"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-464", "prediction": ["# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Laos\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Communist state -> government.form_of_government.countries -> Laos\n# Answer:\nCommunist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> North Korea\n# Answer:\nSocialist state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Single-party state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSingle-party state", "# Reasoning Path:\nChina -> location.country.form_of_government -> Communist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nCommunist state", "# Reasoning Path:\nChina -> location.statistical_region.official_development_assistance -> g.12cp_kgkf\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Balikun horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nBalikun horse", "# Reasoning Path:\nChina -> location.country.form_of_government -> Single-party state -> government.form_of_government.countries -> North Korea\n# Answer:\nSingle-party state"], "ground_truth": ["Socialist state", "Communist state", "Single-party state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-465", "prediction": ["# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Viceroy New York\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> United States of America\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> common.topic.notable_types -> Airport\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> common.topic.notable_types -> Airport\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> New York metropolitan area\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Queens\n# Answer:\nJohn F. Kennedy International Airport"], "ground_truth": ["John F. Kennedy International Airport", "Newark Liberty International Airport", "LaGuardia Airport"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-466", "prediction": ["# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Brazilian Portuguese -> language.language_dialect.language -> Portuguese Language\n# Answer:\nBrazilian Portuguese", "# Reasoning Path:\nBrazil -> location.country.languages_spoken -> Portuguese Language -> language.human_language.dialects -> Brazilian Portuguese\n# Answer:\nPortuguese Language", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.high_tech_as_percent_of_manufactured_exports -> g.12cp_jm9h\n# Answer:\nlocation.statistical_region.high_tech_as_percent_of_manufactured_exports"], "ground_truth": ["Italian Language", "Brazilian Portuguese", "Portuguese Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.2857142857142857, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.2857142857142857, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-467", "prediction": ["# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.actor -> Ewan McGregor\n# Answer:\nEwan McGregor", "# Reasoning Path:\nObi-Wan Kenobi -> common.topic.notable_for -> g.1258f4t33\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qy3 -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nStar Wars Episode I: The Phantom Menace", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3r05 -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.010wv9n4 -> film.performance.actor -> Seth Green\n# Answer:\nSeth Green", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.04m6bsr -> film.performance.actor -> James Arnold Taylor\n# Answer:\nJames Arnold Taylor", "# Reasoning Path:\nObi-Wan Kenobi -> film.film_character.portrayed_in_films -> m.0k3qzt -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones"], "ground_truth": ["Ewan McGregor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-468", "prediction": ["# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> United States of America\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> California\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Carl H. Freeman\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.geolocation -> m.0kjgl_\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> common.topic.image -> Location of City of Lynwood in Los Angeles County, and in California\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.containedby -> Los Angeles County\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.statistical_region.population -> g.11b66hc7d0\n# Answer:\nLynwood", "# Reasoning Path:\nVenus Williams -> people.person.place_of_birth -> Lynwood -> location.location.people_born_here -> Ace Mask\n# Answer:\nLynwood"], "ground_truth": ["Lynwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-469", "prediction": ["# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Ecuador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> El Salvador\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> base.coinsdaily.coin_type.country -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.12cp_k2q2\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Puerto Rico\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> United States Dollar -> common.topic.notable_types -> Currency\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPanama -> location.country.currency_used -> Panamanian balboa -> common.topic.image -> \u00bd balboa(front)\n# Answer:\nPanamanian balboa", "# Reasoning Path:\nPanama -> location.statistical_region.official_development_assistance -> g.1hhc37pjb\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Panamanian balboa", "United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> United States of America\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> California\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.geolocation -> m.0dc449x\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Ryan Braun\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.containedby -> Los Angeles County\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.country_of_origin -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-470", "prediction": ["# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> location.location.containedby -> Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_types -> Location\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.article -> m.07cdmx\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.image -> Location of Mwinilunga in Zambia\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> geography.river.origin -> Mwinilunga -> common.topic.notable_for -> g.1258hl5bb\n# Answer:\nMwinilunga", "# Reasoning Path:\nZambezi -> location.location.partially_contained_by -> m.0wg9155 -> location.partial_containment_relationship.partially_contained_by -> Zambia\n# Answer:\nZambia", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> travel.tourist_attraction.near_travel_destination -> Livingstone\n# Answer:\nVictoria Falls", "# Reasoning Path:\nZambezi -> geography.river.cities -> Victoria Falls -> common.topic.notable_for -> g.1258_gg1d\n# Answer:\nVictoria Falls"], "ground_truth": ["Mwinilunga"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-472", "prediction": ["# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.052b2p8 -> sports.sports_team_roster.team -> Sport Club Corinthians Paulista\n# Answer:\nSport Club Corinthians Paulista", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z4r -> sports.sports_team_roster.team -> Cruzeiro Esporte Clube\n# Answer:\nCruzeiro Esporte Clube", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6z71 -> sports.sports_team_roster.team -> PSV Eindhoven\n# Answer:\nPSV Eindhoven", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6zbn -> sports.sports_team_roster.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nRonaldo -> soccer.football_player.statistics -> m.0w9ntqq -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nRonaldo -> soccer.football_player.statistics -> m.0w8_xw6 -> soccer.football_player_stats.team -> Sport Club Corinthians Paulista\n# Answer:\nSport Club Corinthians Paulista", "# Reasoning Path:\nRonaldo -> people.person.nationality -> Brazil -> sports.sports_team_location.teams -> Brazil national football team\n# Answer:\nBrazil", "# Reasoning Path:\nRonaldo -> sports.pro_athlete.teams -> m.0nh6zfw -> sports.sports_team_roster.team -> Inter Milan\n# Answer:\nInter Milan"], "ground_truth": ["Real Madrid C.F.", "Brazil national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-473", "prediction": ["# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.0bh5kjj -> people.place_lived.location -> San Francisco\n# Answer:\nSan Francisco", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> San Francisco\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> people.person.places_lived -> m.03pqrmt -> people.place_lived.location -> Bloomfield Hills\n# Answer:\nBloomfield Hills", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.featured_film_locations -> Oakland\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> award.award_nominated_work.award_nominations -> m.08__619\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> film.producer.film -> Mrs. Doubtfire -> film.film.edited_by -> Raja Gosnell\n# Answer:\nMrs. Doubtfire", "# Reasoning Path:\nRobin Williams -> people.person.nationality -> United States of America -> common.topic.notable_types -> Country\n# Answer:\nUnited States of America"], "ground_truth": ["San Francisco", "Bloomfield Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.5, "path_recall": 0.8, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-474", "prediction": ["# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.award_winner -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.01086803 -> award.award_honor.award_winner -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.0b68zn5 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.ceremony -> 2009 Teen Choice Awards\n# Answer:\n2009 Teen Choice Awards", "# Reasoning Path:\nJacob Black -> film.film_character.portrayed_in_films -> m.05tflhw -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nJacob Black -> book.book_character.appears_in_book -> Twilight -> award.award_winning_work.awards_won -> m.0nfjtkv\n# Answer:\naward.award_winning_work.awards_won", "# Reasoning Path:\nTwilight -> award.award_winning_work.awards_won -> m.0nfjtkv -> award.award_honor.award -> Teen Choice Award for Choice Movie Breakout Star - Male\n# Answer:\nTeen Choice Award for Choice Movie Breakout Star - Male"], "ground_truth": ["Taylor Lautner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.5555555555555556, "path_precision": 0.625, "path_recall": 0.5, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-477", "prediction": ["# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> aviation.airport.serves -> Ezeiza, Buenos Aires\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.article -> m.01ky3n\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> aviation.airport.serves -> Greater Buenos Aires\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.03l9zc6\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> location.location.events -> Ezeiza massacre\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.image -> Ministro Pistarini International Airport - Terminal A - Buenos Aires\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.statistical_region.population -> g.11bc88m3zq\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.040ql4w\n# Answer:\nMinistro Pistarini International Airport", "# Reasoning Path:\nBuenos Aires -> location.location.nearby_airports -> Ministro Pistarini International Airport -> common.topic.webpage -> m.05nn9j7\n# Answer:\nMinistro Pistarini International Airport"], "ground_truth": ["Don Torcuato Airport", "Ministro Pistarini International Airport", "Aeroparque Jorge Newbery"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48484848484848486, "ans_precission": 0.8888888888888888, "ans_recall": 0.3333333333333333, "path_f1": 0.48484848484848486, "path_precision": 0.8888888888888888, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48484848484848486, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-479", "prediction": ["# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.team -> Minnesota Vikings\n# Answer:\nMinnesota Vikings", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nAdrian Peterson -> sports.pro_athlete.teams -> m.03gkk7c -> sports.sports_team_roster.position -> Running back\n# Answer:\nRunning back", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> award.award_nomination.award -> NFL Play of the Year Award\n# Answer:\nNFL Play of the Year Award", "# Reasoning Path:\nAdrian Peterson -> award.award_winner.awards_won -> m.04kt42p -> award.award_honor.award -> National Football League Rookie of the Year Award\n# Answer:\nNational Football League Rookie of the Year Award", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qwmrv -> freebase.valuenotation.has_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nAdrian Peterson -> award.award_nominee.award_nominations -> m.0_qxjbh -> award.award_nomination.ceremony -> 2nd Annual NFL Honors\n# Answer:\n2nd Annual NFL Honors", "# Reasoning Path:\nAdrian Peterson -> american_football.football_player.games -> m.07nvp4q -> american_football.player_game_statistics.team -> Minnesota Vikings\n# Answer:\nMinnesota Vikings"], "ground_truth": ["University of Oklahoma"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> base.aareas.schema.administrative_area.administrative_parent -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.citytown -> Suwon\n# Answer:\nSuwon", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.employer.number_of_employees -> m.0f7q8f_\n# Answer:\nbusiness.employer.number_of_employees", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.country -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> common.topic.article -> m.07gv7d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.geolocation -> m.0239ks3\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf -> location.mailing_address.postal_code -> 443-742\n# Answer:\n443-742", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-480", "prediction": ["# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.place_of_birth -> Bristol\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Giles Penn\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.nationality -> United States of America\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> West Virginia\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Beryl\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> common.topic.image -> Admiral Sir William Penn, 1621\u20131670 by Sir Peter Lely, painted 1665\u20131666.\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> symbols.namesake.named_after -> William Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nWilliam Penn", "# Reasoning Path:\nPennsylvania -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America"], "ground_truth": ["William Penn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-481", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_containedby -> Czech Republic\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_contained_by -> m.0wg8r4c\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_containedby -> Austria\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-482", "prediction": ["# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> common.topic.article -> m.02rx1t3\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> film.film.starring -> m.02tb1p0 -> film.performance.character -> g.1232h_22\n# Answer:\nfilm.performance.character", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9503y -> film.film_crew_gig.crewmember -> Lynda Armstrong\n# Answer:\nLynda Armstrong", "# Reasoning Path:\nBatman -> film.film.starring -> m.02wk6dq -> film.performance.character -> Bruce Wayne\n# Answer:\nBruce Wayne", "# Reasoning Path:\nBatman -> film.film.other_crew -> m.0v9506s -> film.film_crew_gig.crewmember -> Nick Dudman\n# Answer:\nNick Dudman", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> award.award_nominated_work.award_nominations -> m.0ndvr43\n# Answer:\nBatman Film Series", "# Reasoning Path:\nBatman -> fictional_universe.work_of_fiction.setting -> DC Universe -> comic_books.comic_book_fictional_universe.characters_primarily_appearing_in_this_universe -> Martha Wayne\n# Answer:\nDC Universe", "# Reasoning Path:\nBatman -> film.film.film_series -> Batman Film Series -> common.topic.article -> m.0gyjf78\n# Answer:\nBatman Film Series"], "ground_truth": ["Danny Trejo", "Matthew Wagner", "Carlos Alazraqui", "Tom Hardy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-484", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Kiribati\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "Lojban", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-487", "prediction": ["# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.person.parents -> Andr\u00e9 de Toth\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.deceased_person.place_of_death -> Olympia\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Elaine Detlie -> people.person.parents -> John S. Detlie\n# Answer:\nElaine Detlie", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.03lf38m -> people.marriage.spouse -> John S. Detlie\n# Answer:\nJohn S. Detlie", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Diana De Toth -> people.person.parents -> Andr\u00e9 de Toth\n# Answer:\nDiana De Toth", "# Reasoning Path:\nVeronica Lake -> people.person.spouse_s -> m.02kkn6n -> people.marriage.spouse -> Andr\u00e9 de Toth\n# Answer:\nAndr\u00e9 de Toth", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> common.topic.notable_for -> g.12550nl90\n# Answer:\nAndre Michael De Toth III", "# Reasoning Path:\nVeronica Lake -> people.person.children -> Andre Michael De Toth III -> people.person.sibling_s -> m.0qzbn8m\n# Answer:\nAndre Michael De Toth III"], "ground_truth": ["Robert Carleton-Munro", "Joseph A. McCarthy", "Andr\u00e9 de Toth", "John S. Detlie"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.625, "path_precision": 0.625, "path_recall": 0.625, "path_ans_f1": 0.5555555555555556, "path_ans_precision": 0.625, "path_ans_recall": 0.5}
{"id": "WebQTest-488", "prediction": ["# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.spouse -> Mary Newman\n# Answer:\nMary Newman", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.spouse -> Elizabeth Sydenham\n# Answer:\nElizabeth Sydenham", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmz1 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nFrancis Drake -> people.person.parents -> Edmund Drake -> people.person.spouse_s -> m.0qzzk__\n# Answer:\nEdmund Drake", "# Reasoning Path:\nFrancis Drake -> people.person.spouse_s -> m.0k1nmzh -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nFrancis Drake -> people.person.parents -> Mary Mylwaye Drake -> people.person.spouse_s -> m.0qzzk__\n# Answer:\nMary Mylwaye Drake", "# Reasoning Path:\nFrancis Drake -> common.topic.article -> m.031td\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFrancis Drake -> military.military_commander.military_commands -> m.0j2c6ft -> military.military_command.military_combatant -> Kingdom of England\n# Answer:\nKingdom of England", "# Reasoning Path:\nFrancis Drake -> people.person.parents -> Edmund Drake -> common.topic.notable_for -> g.126sln6fc\n# Answer:\nEdmund Drake"], "ground_truth": ["Elizabeth Sydenham", "Mary Newman"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-489", "prediction": ["# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.actor -> Katherine Heigl\n# Answer:\nKatherine Heigl", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_r2n -> film.performance.actor -> Patrick Fischler\n# Answer:\nPatrick Fischler", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvwwl8 -> film.performance.actor -> Daniel Sunjata\n# Answer:\nDaniel Sunjata", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gv_fdr -> film.performance.character -> Stephanie Plum\n# Answer:\nStephanie Plum", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhj8 -> film.film_crew_gig.crewmember -> Ray Bivins\n# Answer:\nRay Bivins", "# Reasoning Path:\nOne for the Money -> film.film.starring -> m.0gvxn74 -> film.performance.actor -> Sherri Shepherd\n# Answer:\nSherri Shepherd", "# Reasoning Path:\nOne for the Money -> film.film.release_date_s -> m.0jsmys9 -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nOne for the Money -> film.film.other_crew -> m.0gwdhjn -> film.film_crew_gig.crewmember -> David Lingenfelser\n# Answer:\nDavid Lingenfelser"], "ground_truth": ["Katherine Heigl"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.notable_for -> g.1yl5ndjfk\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Allah -> common.topic.article -> m.0jfq\n# Answer:\nAllah", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> people.person.religion -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> God -> common.topic.notable_for -> g.125d2mw4b\n# Answer:\nGod"], "ground_truth": ["Monotheism", "Tawhid", "Prophets in Islam", "God in Islam", "Predestination in Islam", "Mahdi", "Islamic holy books", "Masih ad-Dajjal", "Islamic view of angels", "Qiyamah", "\u1e6c\u016bb\u0101", "Sharia", "Entering Heaven alive"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-490", "prediction": ["# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.writing_system -> Greek alphabet\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> book.written_work.subjects -> Germanic peoples\n# Answer:\nThe Roman Empire and Its Germanic Peoples", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> language.human_language.language_family -> Hellenic languages\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> base.rosetta.languoid.local_name -> Greek\n# Answer:\nGreek Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Latin Language -> language.human_language.language_family -> Italic languages\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> location.country.official_language -> Latin Language -> language.human_language.countries_spoken_in -> Duchy of Burgundy\n# Answer:\nLatin Language", "# Reasoning Path:\nRoman Empire -> fictional_universe.fictional_setting.languages -> Greek Language -> media_common.netflix_genre.titles -> Young Aphrodites\n# Answer:\nGreek Language"], "ground_truth": ["Latin Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-491", "prediction": ["# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer-songwriter -> people.profession.specialization_of -> Singer\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nSinger", "# Reasoning Path:\nRingo Starr -> music.featured_artist.recordings -> Act Naturally -> music.recording.artist -> Buck Owens\n# Answer:\nAct Naturally", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer-songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer-songwriter -> common.topic.subject_of -> Stephen Melton\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nRingo Starr -> music.artist.contribution -> m.0vv1_m9 -> music.recording_contribution.album -> Cloud Nine\n# Answer:\nCloud Nine", "# Reasoning Path:\nRingo Starr -> people.person.parents -> Richard Starkey -> common.topic.notable_for -> g.125h3_b65\n# Answer:\nRichard Starkey", "# Reasoning Path:\nRingo Starr -> people.person.profession -> Singer-songwriter -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger-songwriter"], "ground_truth": ["One", "Scouse the Mouse", "Golden Blunders", "King of Broken Hearts", "Me and You", "As Far as We Can Go (original version)", "Let Love Lead", "Instant Amnesia", "Scouse's Dream", "I Really Love Her", "Goodnight Vienna", "The Christmas Dance", "Caterwaul", "Wake Up", "Six O'Clock (Extended Version)", "In a Heartbeat", "Dear Santa", "All By Myself", "Goodnight Vienna (reprise)", "Sure To Fall", "Lady Gaye", "As Far as We Can Go", "No One to Blame", "Wings", "Living in a Pet Shop", "Lipstick Traces", "Write One for Me", "Brandy", "Wine, Women and Loud Happy Songs", "Now That She's Gone Away", "Beaucoups of Blues", "You're Sixteen (You're Beautiful and You're Mine)", "This Be Called a Song", "Six O'Clock", "You Belong to Me", "I'm a Fool to Care", "Weight of the World", "All the Young Dudes", "Harry's Song", "All by Myself", "OO-WEE", "Walk With You", "Wine, Women, and Loud Happy Songs", "Dream", "Hand Gun Promos", "Don't Be Cruel", "Wrong All the Time", "Everybody's in a Hurry but Me", "Spooky Wierdness", "Love Is a Many Splendoured Thing", "Hard Times", "Oo-Wee", "The Really 'Serious' Introduction", "No No Song", "Running Free", "Oh My Lord", "I'm the Greatest", "Some People", "I'd be Talking all the Time", "Think It Over", "Wonderful", "You Never Know", "Out on the Streets", "I've Got Blisters...", "Lay Down Your Arms (feat. Stevie Nicks)", "Where Did Our Love Go", "A Dose of Rock 'n' Roll", "Husbands And Wives", "Night and Day", "I'm Yours", "English Garden / I Really Love Her", "A Mouse Like Me", "Stop and Take the Time to Smell the Roses", "Vertical Man", "Fading in Fading Out", "Love Don't Last Long", "I'll Still Love You", "It's No Secret", "Mindfield", "Can She Do It Like She Dances", "Don't Go Where the Road Don't Go", "Christmas Time Is Here Again", "Eye to Eye", "Love Bizarre", "The Little Drummer Boy", "The Turnaround", "Your Sixteen", "Runaways", "Fading In Fading Out", "Bye Bye Blackbird", "With a Little Help From My Friends (reprise)", "Act Naturally", "Hard to Be True", "Tango All Night", "For Love", "After All These Years", "Slow Down", "The No No Song", "I Don't Believe You", "Have You Seen My Baby (Hold On)", "Gone Are the Days", "Glamorous Life", "Loser's Lounge", "White Christmas", "Old Time Relovin'", "Silent Homecoming", "Only You (and You Alone)", "Imagine Me There", "Love First, Ask Questions Later", "Woman of the Night", "Devil Woman", "Red and Black Blues", "In My Car", "(It's All Down to) Good Night Vienna (single version)", "Pax Um Biscum (Peace Be With You)", "Love Is", "No No Song/Skokiaan", "Without Her", "Blue Christmas", "Samba", "Satisfied", "All in the Name of Love", "Give a Little Bit", "I Wouldn't Have You Any Other Way", "I Still Love Rock 'n' Roll", "I Was Walkin'", "Never Without You", "Logical Song", "You Always Hurt the One You Love", "Postcards From Paradise", "Simple Love Song", "Give Me Back the Beat", "Snookeroo", "In Liverpool", "Be My Baby", "Sunshine Life for Me (Sail Away Raymond)", "Private Property", "I'd Be Talking All the Time", "The End", "Anthem", "Fill in the Blanks", "Tommy's Holiday Camp", "Peace Dream", "Monkey See, Monkey Do", "You\u2019re Sixteen", "What Love Wants to Be", "Winter Wonderland", "Sweet Little Sixteen", "A Man Like Me", "The No-No Song", "Matchbox", "Fastest Growing Heartache in the West", "Occapella", "Step Lightly", "Iko Iko", "Wrack My Brain", "Alibi", "Rudolph the Red\u2010Nosed Reindeer", "Honey Don't", "$15 Draw", "You Don't Know Me at All", "Don't Hang Up", "Hey Baby", "Trippin' on My Own Tears", "You Bring the Party Down", "Liverpool 8", "With a Little Help From My Friends (live)", "What Goes Around", "The Other Side of Liverpool", "Drift Away", "Lucky Man", "Let the Rest of the World Go By", "Boat Ride", "Down and Out", "Call Me", "La De Da", "She's About a Mover", "Island in the Sun", "Bridges", "Don't Know a Thing About Love", "(It's All Down to) Good Night Vienna", "Octopus's Garden", "You and Me (Babe)", "Have You Seen My Baby", "Out On The Streets", "Bamboula", "Don't Pass Me By", "Bad Boy", "With a Little Help From My Friends", "Fiddle About", "Coochy Coochy", "Early 1970", "Drowning In The Sea Of Love", "I Wanna Be Santa Claus", "Give It a Try", "I Wanna Be Your Man", "Touch and Go", "Picture Show Life", "Right Side of the Road", "Elizabeth Reigns", "Fastest Growing Heartache In The West", "Choose Love", "Gave It All Up", "Not Looking Back", "Yellow Submarine", "Pinocchio Medley (\\\"Do You See the Noses Growing?\\\"): Desolation Theme / When You Wish Upon a Star", "Gypsies in Flight", "Don\u2019t Hang Up", "Nashville Jam", "I'll Be Fine Anywhere", "Think About You", "It Don't Come Easy", "If It's Love That You Want", "Fading in and Fading Out", "R U Ready", "Blue, Turning Grey Over You", "Easy For Me", "Photograph", "Rock Island Line", "Attention", "Cookin' (In the Kitchen of Love)", "Drumming Is My Madness", "Love Me Do", "S.O.S.", "Memphis in Your Mind", "Six O\u2019Clock", "Cryin'", "Whispering Grass (Don't Tell the Trees)", "Who's Your Daddy", "Everyone Wins", "Heart on My Sleeve", "Y Not", "Dead Giveaway", "With a Little Help From My Friends / It Don't Come Easy", "Waiting", "Six O'Clock (extended version)", "Rory and the Hurricanes", "I Keep Forgettin'", "Tuff Love", "I Think Therefore I Rock 'n Roll", "Only You (And You Alone)", "Spooky Weirdiness", "Mystery of the Night", "Tonight", "Oh My My", "Drowning in the Sea of Love", "Only You", "Everlasting Love", "Karn Evil 9", "Hopeless", "Sure to Fall (In Love With You)", "What in the... World", "Spooky Weirdness", "You're Sixteen", "Take the Long Way Home", "Sneaking Sally Through the Alley", "Blindman", "Stardust", "Going Down", "Easy for Me", "You Know It Makes Sense", "Las Brisas", "Come on Christmas, Christmas Come On", "Puppet", "Confirmation", "Christmas Eve", "Missouri Loves Company", "Free Drinks", "Pure Gold", "Can't Do It Wrong", "Time", "I Know a Place", "You Can't Fight Lightning", "Stop and Take the Time to Smell the Roses (Original Vocal Version)", "Gypsies In Flight", "Husbands and Wives", "Pasodobles", "Have I Told You Lately That I Love You?", "Sentimental Journey", "Boys", "A Dose of Rock 'N' Roll", "Without Understanding", "Back Off Boogaloo", "Who Needs a Heart", "Nice Way"], "ans_acc": 0.0035087719298245615, "ans_hit": 1, "ans_f1": 0.0068259385665529, "ans_precission": 0.125, "ans_recall": 0.0035087719298245615, "path_f1": 0.07017543859649124, "path_precision": 0.125, "path_recall": 0.04878048780487805, "path_ans_f1": 0.0068259385665529, "path_ans_precision": 0.125, "path_ans_recall": 0.0035087719298245615}
{"id": "WebQTest-492", "prediction": ["# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.containedby -> Sicily\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Syracuse Cathedral\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.containedby -> Italy\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Ancient theatre of Taormina\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> symbols.name_source.namesakes -> Saint Lucia -> location.location.containedby -> Windward Islands\n# Answer:\nSaint Lucia", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Arethuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> location.location.events -> Siege of Syracuse\n# Answer:\nSyracuse", "# Reasoning Path:\nSaint Lucy -> people.person.place_of_birth -> Syracuse -> travel.travel_destination.tourist_attractions -> Cassibile\n# Answer:\nSyracuse"], "ground_truth": ["Syracuse"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-493", "prediction": ["# Reasoning Path:\nAustralia -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.statistical_region.diesel_price_liter -> g.12cp_h_97\n# Answer:\nlocation.statistical_region.diesel_price_liter", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71l32x_\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nAustralia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Caledonia\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Federal monarchy", "Parliamentary system"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-497", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Edward VII\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.parents -> Alexandra of Denmark\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Princess Mary Adelaide of Cambridge\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.deceased_person.cause_of_death -> Euthanasia\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.parents -> Francis, Duke of Teck\n# Answer:\nMary of Teck", "# Reasoning Path:\nGeorge VI -> people.person.parents -> Mary of Teck -> people.person.children -> Prince John of the United Kingdom\n# Answer:\nMary of Teck"], "ground_truth": ["George V"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-500", "prediction": ["# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> common.topic.notable_types -> Postal Code\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> common.topic.notable_for -> g.1255d7h3l\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08607 -> location.postal_code.country -> United States of America\n# Answer:\n08607", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08605 -> location.postal_code.country -> United States of America\n# Answer:\n08605", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08690 -> common.topic.notable_types -> Postal Code\n# Answer:\n08690", "# Reasoning Path:\nTrenton -> location.statistical_region.population -> g.11b66l5kmz\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08607 -> common.topic.notable_types -> Postal Code\n# Answer:\n08607", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08618 -> location.postal_code.country -> United States of America\n# Answer:\n08618", "# Reasoning Path:\nTrenton -> location.citytown.postal_codes -> 08620 -> common.topic.notable_for -> g.1254yr8qr\n# Answer:\n08620"], "ground_truth": ["08666", "08641", "08695", "08604", "08625", "08607", "08650", "08608", "08610", "08605", "08638", "08601", "08628", "08602", "08620", "08645", "08690", "08646", "08606", "08618", "08609", "08611", "08647", "08629", "08603", "08691", "08619"], "ans_acc": 0.18518518518518517, "ans_hit": 1, "ans_f1": 0.3065134099616858, "ans_precission": 0.8888888888888888, "ans_recall": 0.18518518518518517, "path_f1": 0.3065134099616858, "path_precision": 0.8888888888888888, "path_recall": 0.18518518518518517, "path_ans_f1": 0.3065134099616858, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.18518518518518517}
{"id": "WebQTest-501", "prediction": ["# Reasoning Path:\nAshley Greene -> award.award_winner.awards_won -> m.0z83xpm -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0zn7f58 -> film.performance.film -> Wish I Was Here\n# Answer:\nWish I Was Here", "# Reasoning Path:\nAshley Greene -> award.award_winner.awards_won -> m.0gc_p7d -> award.award_honor.honored_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nAshley Greene -> award.award_nominee.award_nominations -> m.0z83xqy -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nAshley Greene -> award.award_winner.awards_won -> m.0z83xpm -> award.award_honor.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.05kdryk -> film.performance.film -> Radio Free Albemuth\n# Answer:\nRadio Free Albemuth", "# Reasoning Path:\nAshley Greene -> award.award_winner.awards_won -> m.0kbb5yq -> award.award_honor.honored_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nAshley Greene -> film.actor.film -> m.0gbwhrn -> film.performance.film -> Shrink\n# Answer:\nShrink"], "ground_truth": ["Americana", "Otis", "The Apparition", "The Boom Boom Room", "The Twilight Saga: New Moon", "Shrink", "Radio Free Albemuth", "Staten Island Summer", "Twilight", "Skateland", "King of California", "A Warrior's Heart", "Random", "Burying the Ex", "CBGB", "The Twilight Saga: Breaking Dawn - Part 1", "Urge", "Summer's Blood", "LOL", "Wish I Was Here", "Butter", "The Twilight Saga: Breaking Dawn - Part 2", "Eclipse"], "ans_acc": 0.21739130434782608, "ans_hit": 1, "ans_f1": 0.34825870646766166, "ans_precission": 0.875, "ans_recall": 0.21739130434782608, "path_f1": 0.358974358974359, "path_precision": 0.875, "path_recall": 0.22580645161290322, "path_ans_f1": 0.34825870646766166, "path_ans_precision": 0.875, "path_ans_recall": 0.21739130434782608}
{"id": "WebQTest-502", "prediction": ["# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Restrepo -> film.film.subjects -> Taliban insurgency\n# Answer:\nRestrepo", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> time.event.includes_event -> American Airlines Flight 11\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> film.film_subject.films -> Killing in the Name -> film.film.subjects -> Terrorism\n# Answer:\nKilling in the Name", "# Reasoning Path:\nAl-Qaeda -> base.disaster2.attacker.attack_s -> m.07nwm7n -> base.disaster2.attack_process.attack_event -> Taliban insurgency\n# Answer:\nTaliban insurgency", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> base.schemastaging.context_name.pronunciation -> g.125_njzfs\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> military.military_combatant.military_conflicts -> m.04yzk7j -> military.military_combatant_group.conflict -> Taliban insurgency\n# Answer:\nTaliban insurgency", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 11'09\\\"01 September 11\n# Answer:\nSeptember 11 attacks", "# Reasoning Path:\nAl-Qaeda -> base.terrorism.terrorist_organization.involved_in_attacks -> September 11 attacks -> film.film_subject.films -> 9/11: The Falling Man\n# Answer:\nSeptember 11 attacks"], "ground_truth": ["September 11 attacks", "Taliban insurgency"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-503", "prediction": ["# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Mexico\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Italy\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nSwitzerland -> location.country.languages_spoken -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nSwitzerland -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nGerman Language", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["German Language", "Italian Language", "French", "Romansh language"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.6382978723404256, "path_precision": 0.5555555555555556, "path_recall": 0.75, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-504", "prediction": ["# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> location.location.people_born_here -> Epictetus\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> common.topic.article -> m.012tk7\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> location.location.containedby -> Turkey\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Andrew the Apostle\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Jesus Christ\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.place_of_death -> Hierapolis -> common.topic.image -> The main thoroughfare of Hierapolis\n# Answer:\nHierapolis", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> people.cause_of_death.people -> Artayctes\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> people.deceased_person.cause_of_death -> Crucifixion -> base.ontologies.ontology_instance.equivalent_instances -> m.09klpz1\n# Answer:\nCrucifixion", "# Reasoning Path:\nPhilip the Apostle -> symbols.name_source.namesakes -> St. Philip's Episcopal Church -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.06zwg2y\n# Answer:\nSt. Philip's Episcopal Church"], "ground_truth": ["Apostle Philip"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.0, "path_ans_precision": 0.0, "path_ans_recall": 1.0}
{"id": "WebQTest-506", "prediction": ["# Reasoning Path:\nEngland -> location.location.time_zones -> Greenwich Mean Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0njwhw5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0hq0kmy\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> location.statistical_region.population -> m.0mv3kv1\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> biology.animal_breed.place_of_origin -> United Kingdom\n# Answer:\nAiredale Terrier", "# Reasoning Path:\nEngland -> common.topic.webpage -> m.09wgz75 -> common.webpage.resource -> Thanksgiving, movies, and reasons to be grateful\n# Answer:\nThanksgiving, movies, and reasons to be grateful", "# Reasoning Path:\nEngland -> government.governmental_jurisdiction.government_bodies -> Parliament of the United Kingdom -> government.governmental_body.jurisdiction -> Wales\n# Answer:\nParliament of the United Kingdom", "# Reasoning Path:\nEngland -> government.governmental_jurisdiction.government_bodies -> Parliament of the United Kingdom -> government.governmental_body.component_bodies -> House of Commons of the United Kingdom\n# Answer:\nParliament of the United Kingdom", "# Reasoning Path:\nEngland -> common.topic.webpage -> m.09wgz75 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Airedale Terrier -> common.topic.notable_types -> Animal breed\n# Answer:\nAiredale Terrier"], "ground_truth": ["Greenwich Mean Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-508", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxt\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-509", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Kenya\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Liberia\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.deposit_interest_rate -> g.11b71tykw3\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nFilm character", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> book.book.characters -> Anastasia Steele\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-512", "prediction": ["# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> people.profession.specialization_of -> Writer\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> base.lightweight.profession.specialization_of -> Writers and Authors\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Essayist -> common.topic.notable_types -> Profession\n# Answer:\nEssayist", "# Reasoning Path:\nMary Shelley -> influence.influence_node.influenced_by -> Mary Wollstonecraft -> people.person.children -> Fanny Imlay\n# Answer:\nMary Wollstonecraft", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> common.topic.notable_for -> g.1259rk51h\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Writer -> people.profession.specializations -> Biographer\n# Answer:\nWriter", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Biographer -> common.topic.notable_types -> Profession\n# Answer:\nBiographer", "# Reasoning Path:\nMary Shelley -> people.person.profession -> Editor -> people.profession.corresponding_type -> Author\n# Answer:\nEditor"], "ground_truth": ["Essayist", "Author", "Writer", "Novelist", "Playwright", "Travel writer", "Biographer", "Editor"], "ans_acc": 0.625, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.7291666666666666, "path_ans_precision": 0.875, "path_ans_recall": 0.625}
{"id": "WebQTest-513", "prediction": ["# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.11b60qdnlp\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60vfr15\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> Esperanto Language -> language.human_language.countries_spoken_in -> Japan\n# Answer:\nEsperanto Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Kiribati\n# Answer:\nEnglish Language", "# Reasoning Path:\nAustralia -> location.statistical_region.gni_in_ppp_dollars -> g.1245_2hrg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Esperanto Language", "Lojban", "English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-514", "prediction": ["# Reasoning Path:\nDenmark -> location.location.containedby -> Northern Europe -> location.location.containedby -> Europe\n# Answer:\nNorthern Europe", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Scandinavia\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.primarily_containedby -> Northern Europe\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Scandinavia -> location.location.containedby -> Nordic countries\n# Answer:\nScandinavia", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Sweden\n# Answer:\nNordic countries", "# Reasoning Path:\nDenmark -> location.location.containedby -> Scandinavia -> location.location.containedby -> Northern Europe\n# Answer:\nScandinavia", "# Reasoning Path:\nDenmark -> periodicals.newspaper_circulation_area.newspapers -> Dagbladet B\u00f8rsen -> book.newspaper.circulation_areas -> Copenhagen\n# Answer:\nDagbladet B\u00f8rsen", "# Reasoning Path:\nDenmark -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71wx6lz\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nDenmark -> location.location.containedby -> Nordic countries -> location.location.contains -> Faroe Islands\n# Answer:\nNordic countries"], "ground_truth": ["Nordic countries", "Scandinavia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-515", "prediction": ["# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Theodore Roosevelt Inaugural National Historic Site\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> common.topic.notable_types -> Event\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> people.person.places_lived -> m.0108qyxb -> people.place_lived.location -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> common.topic.notable_for -> g.125dcmf8k\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1901 presidential inauguration -> time.event.locations -> Buffalo\n# Answer:\nTheodore Roosevelt 1901 presidential inauguration", "# Reasoning Path:\nTheodore Roosevelt -> base.inaugurations.inauguration_speaker.inauguration -> Theodore Roosevelt 1905 presidential inauguration -> base.inaugurations.inauguration.inaugural_address -> Second Inaugural Address of Theodore Roosevelt\n# Answer:\nTheodore Roosevelt 1905 presidential inauguration"], "ground_truth": ["New York City", "Washington, D.C."], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.15384615384615383, "path_precision": 0.25, "path_recall": 0.1111111111111111, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.25, "path_ans_recall": 0.5}
{"id": "WebQTest-516", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Thailand\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> common.topic.notable_types -> Form of Government\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Constitutional monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Cambodia\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> common.topic.notable_types -> Form of Government\n# Answer:\nElective monarchy", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> India\n# Answer:\nParliamentary system", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.country.form_of_government -> Elective monarchy -> government.form_of_government.countries -> United Arab Emirates\n# Answer:\nElective monarchy"], "ground_truth": ["Elective monarchy", "Constitutional monarchy", "Parliamentary system", "Democracy"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7741935483870969, "ans_precission": 0.8, "ans_recall": 0.75, "path_f1": 0.7741935483870969, "path_precision": 0.8, "path_recall": 0.75, "path_ans_f1": 0.7741935483870969, "path_ans_precision": 0.8, "path_ans_recall": 0.75}
{"id": "WebQTest-517", "prediction": ["# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> common.topic.notable_types -> Human Language\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> Lojban -> language.human_language.region -> Americas\n# Answer:\nLojban", "# Reasoning Path:\nCanada -> location.country.languages_spoken -> English Language -> language.human_language.dialects -> Canadian English\n# Answer:\nEnglish Language", "# Reasoning Path:\nCanada -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nCanada -> location.statistical_region.part_time_employment_percent -> g.12cp_k6ss\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Chinese language", "Abenaki language", "Urdu Language", "French", "Inuktitut", "Gwich'in Language", "German Language", "Upper Midwest American English", "Slavey language", "Spanish Language", "Canadian English", "Cree language", "Russian Language", "Tamil Language", "Inuinnaqtun", "Tagalog language", "Portuguese Language", "English Language", "Chipewyan Language", "Korean Language", "Western Canadian Inuktitut Language", "Dogrib Language", "Gujarati Language", "Lojban", "Punjabi language", "Persian Language", "Arabic Language", "Polish Language", "Romanian Language", "Vietnamese Language", "Greek Language"], "ans_acc": 0.12903225806451613, "ans_hit": 1, "ans_f1": 0.17454545454545453, "ans_precission": 0.8888888888888888, "ans_recall": 0.0967741935483871, "path_f1": 0.1721311475409836, "path_precision": 0.7777777777777778, "path_recall": 0.0967741935483871, "path_ans_f1": 0.22535211267605634, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.12903225806451613}
{"id": "WebQTest-518", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> symbols.namesake.named_after -> Christopher Columbus\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.deposit_interest_rate -> g.11b71kjkjp\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nCosta Rica -> location.statistical_region.co2_emissions_per_capita -> g.1245_0_8r\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-519", "prediction": ["# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_mmrcv\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.notable_for -> g.125dhx05_\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> common.topic.webpage -> m.03l8rmb -> common.webpage.resource -> m.0bkjqzl\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nNashville -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nNashville -> location.capital_of_administrative_division.capital_of -> m.0jvw4rl -> location.administrative_division_capital_relationship.administrative_division -> Tennessee\n# Answer:\nTennessee"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> base.ghtech.gh_focus_areas.treatment -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nAldous Huxley"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-521", "prediction": ["# Reasoning Path:\ng.125_cxx77\n# Answer:\ng.125_cxx77"], "ground_truth": ["Ted Bracewell"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-522", "prediction": ["# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.biblioness.bibs_location.country -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Valkyrie\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> location.administrative_division.first_level_division_of -> Germany\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> film.film_location.featured_in_films -> Inglourious Basterds\n# Answer:\nBerlin", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!"], "ground_truth": ["Berlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-523", "prediction": ["# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Eurasia\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> location.location.partially_contains -> Russia -> location.location.partially_containedby -> Asia\n# Answer:\nRussia", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.location.containedby -> Western Europe\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> location.country.languages_spoken -> English Language\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> United Kingdom -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nEurope -> base.locations.continents.countries_within -> Vatican City -> location.location.containedby -> Eurasia\n# Answer:\nVatican City"], "ground_truth": ["Russia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-524", "prediction": ["# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> royalty.kingdom.rulers -> Abdullah of Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.containedby -> Middle East\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.country.official_language -> Arabic Language\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.containedby -> Arabian Peninsula\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> base.locations.countries.continent -> Asia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> time.event.locations -> Persian Gulf -> location.location.time_zones -> Gulf Time Zone\n# Answer:\nPersian Gulf", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> time.event.locations -> Iraq\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia -> location.location.events -> USS Stark Attack\n# Answer:\nSaudi Arabia"], "ground_truth": ["Israel", "Saudi Arabia", "Arabian Peninsula", "Kuwait", "Iraq", "Persian Gulf"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.48275862068965514, "path_precision": 0.875, "path_recall": 0.3333333333333333, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-525", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nJim Webb", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nMark Warner", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.09s62vv -> government.government_position_held.office_holder -> Carter Glass\n# Answer:\nCarter Glass", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles"], "ground_truth": ["Jim Webb", "Tim Kaine", "Mark Warner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-526", "prediction": ["# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Funkadellic\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Black People -> music.recording.artist -> Funkadellic\n# Answer:\nBlack People", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix radio edit) -> music.recording.artist -> Funkadellic\n# Answer:\nMothership Reconnection (Daft Punk remix radio edit)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.artist -> Scott Grooves\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> common.topic.notable_types -> Musical Recording\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.composer.compositions -> Comin' Round the Mountain -> music.recording.artist -> Parliament\n# Answer:\nComin' Round the Mountain", "# Reasoning Path:\nGeorge Clinton -> music.featured_artist.recordings -> Mothership Reconnection (Daft Punk remix) -> music.recording.canonical_version -> Mothership Connection (Star Child)\n# Answer:\nMothership Reconnection (Daft Punk remix)", "# Reasoning Path:\nGeorge Clinton -> music.producer.releases_produced -> One Nation Under a Groove -> music.recording.artist -> Funkadellic\n# Answer:\nOne Nation Under a Groove"], "ground_truth": ["Parliament-Funkadelic", "Parliament", "Funkadellic", "The Parliaments", "George Clinton & Bootsy Collins", "Dolby's Cube"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.04395604395604395, "path_precision": 0.25, "path_recall": 0.024096385542168676, "path_ans_f1": 0.43478260869565216, "path_ans_precision": 0.625, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-527", "prediction": ["# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Poland\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Bulgaria\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> geography.mountain_range.mountains -> Tarnica\n# Answer:\nBieszczady Mountains", "# Reasoning Path:\nSlovakia -> location.statistical_region.co2_emissions_per_capita -> g.1245_9wdf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Czech Republic\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.competitions -> Biathlon at the 2014 Winter Olympics - Women's 7.5 km Sprint\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSlovakia -> location.location.partially_contains -> Bieszczady Mountains -> location.location.partially_contained_by -> m.0wg8tgd\n# Answer:\nBieszczady Mountains"], "ground_truth": ["Austria", "Poland", "Hungary", "Ukraine", "Czech Republic"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06557377049180327, "path_precision": 0.4444444444444444, "path_recall": 0.035398230088495575, "path_ans_f1": 0.5106382978723405, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.6}
{"id": "WebQTest-528", "prediction": ["# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Ashford\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> Everything TEFL LTD\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Aberdeen\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Teachers\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> common.topic.notable_for -> g.1258lhsw5\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Belfast\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> base.descriptive_names.names.descriptive_name -> m.0102yq_f\n# Answer:\nTeacher", "# Reasoning Path:\nJames A. Garfield -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nJames A. Garfield -> people.person.profession -> Teacher -> common.topic.subject_of -> The TEFL Academy Birmingham\n# Answer:\nTeacher"], "ground_truth": ["US President"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-529", "prediction": ["# Reasoning Path:\nPaula Deen -> people.person.education -> m.0n1jznt -> education.education.institution -> Albany High School\n# Answer:\nAlbany High School", "# Reasoning Path:\nPaula Deen -> book.book_edition.isbn -> 9780786296279\n# Answer:\n9780786296279", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.source -> celebritynetworth.com\n# Answer:\ncelebritynetworth.com", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_types -> Deceased Person\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> common.topic.webpage -> m.0bnt2v3 -> common.webpage.category -> Curated Topic\n# Answer:\nCurated Topic", "# Reasoning Path:\nPaula Deen -> base.schemastaging.person_extra.net_worth -> m.0j8mg8t -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Corrie A. Paul -> common.topic.notable_for -> g.125d2hh2z\n# Answer:\nCorrie A. Paul", "# Reasoning Path:\nPaula Deen -> people.person.parents -> Earl Wayne Hiers, Sr -> people.person.gender -> Male\n# Answer:\nEarl Wayne Hiers, Sr", "# Reasoning Path:\nPaula Deen -> book.book_edition.isbn -> 9780743292856\n# Answer:\n9780743292856", "# Reasoning Path:\nPaula Deen -> common.topic.webpage -> m.0bnt2v3 -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index"], "ground_truth": ["Albany High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-530", "prediction": ["# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> United Kingdom\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> common.topic.article -> m.0272y30\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> Wiltshire\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> The Rifles (Berkshire and Wiltshire) Museum\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.containedby -> England\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> common.topic.image -> Salisbury Cathedral\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> travel.travel_destination.tourist_attractions -> Wilton House\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> people.person.place_of_birth -> Salisbury -> location.location.contains -> Sarum College\n# Answer:\nSalisbury", "# Reasoning Path:\nRobert Irvine -> common.topic.image -> Chef Robert Irvine -> common.image.appears_in_topic_gallery -> Dinner: Impossible\n# Answer:\nChef Robert Irvine"], "ground_truth": ["Salisbury"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-531", "prediction": ["# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> common.topic.notable_types -> Musical genre\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> music.genre.parent_genre -> Rhythm and blues\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.parent_genre -> Soul music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Adult contemporary music -> base.radiostations.radio_station_format.artists_played -> Maroon 5\n# Answer:\nAdult contemporary music", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: 80s (Pop)\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> music.genre.subgenre -> House music\n# Answer:\nDisco", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> 1Club.FM: Jammin' Oldies\n# Answer:\nContemporary R&B", "# Reasoning Path:\nMichael Jackson -> music.artist.genre -> Disco -> broadcast.genre.content -> 1.FM Disco Ball\n# Answer:\nDisco"], "ground_truth": ["Rhythm and blues", "Soul music", "Adult contemporary music", "Disco", "Electronic music", "Dance music", "Pop rock", "Funk", "Rock music", "Post-disco", "Pop music", "Contemporary R&B", "Motown Sound", "Dance-pop", "New jack swing"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.5, "path_ans_precision": 1.0, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-532", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9021c -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hj3 -> soccer.football_player_stats.team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w95hjh -> soccer.football_player_stats.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.borrowing_team -> A.C. Milan\n# Answer:\nA.C. Milan"], "ground_truth": ["Football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-533", "prediction": ["# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Person\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specialization_of -> Musician\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> people.profession.specialization_of -> Singer\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> fictional_universe.character_occupation.characters_with_this_occupation -> Dominique Deveraux\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Film Score Composer -> people.profession.specialization_of -> Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> people.profession.specializations -> Bassist\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Guitarist -> freebase.type_hints.included_types -> Topic\n# Answer:\nGuitarist", "# Reasoning Path:\nJulian Lennon -> people.person.profession -> Singer-songwriter -> fictional_universe.character_occupation.characters_with_this_occupation -> Loke Martinzon\n# Answer:\nSinger-songwriter"], "ground_truth": ["Actor", "Singer", "Writer", "Singer-songwriter", "Film Score Composer", "Photographer", "Guitarist", "Record producer", "Musician"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 1.0, "ans_recall": 0.5555555555555556, "path_f1": 0.6153846153846153, "path_precision": 1.0, "path_recall": 0.4444444444444444, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-534", "prediction": ["# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.notable_types -> Mascot\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie-Brewer.jpg\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.notable_for -> g.125f4t18r\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.article -> m.049km4\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_types -> Mascot\n# Answer:\nHank", "# Reasoning Path:\nMilwaukee Brewers -> base.schemastaging.organization_extra.contact_webpages -> m.010dwkgw -> internet.localized_uri.location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Bernie Brewer -> common.topic.image -> Bernie Brewer (left) in his dugout at Miller Park.\n# Answer:\nBernie Brewer", "# Reasoning Path:\nMilwaukee Brewers -> sports.sports_team.team_mascot -> Hank -> common.topic.notable_for -> g.11b5ltzrvv\n# Answer:\nHank"], "ground_truth": ["Hank", "Bonnie Brewer", "Bernie Brewer"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7567567567567567, "path_ans_precision": 0.875, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-535", "prediction": ["# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> United States, with Territories\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri -> location.location.containedby -> United States, with Territories\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.containedby -> Contiguous United States\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Missouri -> location.location.time_zones -> Central Time Zone\n# Answer:\nMissouri", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> location.location.containedby -> Contiguous United States\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Illinois -> base.locations.states_and_provences.cities_within -> Belleville\n# Answer:\nIllinois", "# Reasoning Path:\nUnited States of America -> location.country.first_level_divisions -> Texas -> location.location.containedby -> Contiguous United States\n# Answer:\nTexas"], "ground_truth": ["Texas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-537", "prediction": ["# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.film -> Crimson Peak\n# Answer:\nCrimson Peak", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.film -> Pacific Rim\n# Answer:\nPacific Rim", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0gxp6xq -> film.performance.film -> The Ledge\n# Answer:\nThe Ledge", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0113ygm8 -> film.performance.character -> Dr. Alan McMichael\n# Answer:\nDr. Alan McMichael", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0gx5yc7 -> film.performance.film -> Deadfall\n# Answer:\nDeadfall", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0jy_sj -> film.performance.film -> Green Street\n# Answer:\nGreen Street", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0h0yd7r -> film.performance.character -> Raleigh Becket\n# Answer:\nRaleigh Becket", "# Reasoning Path:\nCharlie Hunnam -> film.actor.film -> m.0gxp6xq -> film.performance.character -> Gavin Nichols\n# Answer:\nGavin Nichols"], "ground_truth": ["Children of Men", "Frankie Go Boom", "The Ledge", "Crimson Peak", "Pacific Rim", "Deadfall", "Abandon", "Green Street", "Whatever Happened to Harold Smith?", "Cold Mountain", "Knights of the Roundtable: King Arthur", "Nicholas Nickleby"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.625, "ans_recall": 0.4166666666666667, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.5, "path_ans_precision": 0.625, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-538", "prediction": ["# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0v90n_n -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0nfm_g7 -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0yvs61y -> award.award_honor.honored_for -> Love Story\n# Answer:\nLove Story", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0v90n_n -> award.award_honor.award -> Billboard Music Award for Top Digital Songs Artist\n# Answer:\nBillboard Music Award for Top Digital Songs Artist", "# Reasoning Path:\nTaylor Swift -> film.actor.film -> m.0y4q1x7 -> film.performance.film -> The Giver\n# Answer:\nThe Giver", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0v90n_n -> award.award_honor.ceremony -> 2013 Billboard Music Awards\n# Answer:\n2013 Billboard Music Awards", "# Reasoning Path:\nTaylor Swift -> award.award_winner.awards_won -> m.0nfm_g7 -> award.award_honor.award -> American Music Award for Favorite Pop/Rock Female Artist\n# Answer:\nAmerican Music Award for Favorite Pop/Rock Female Artist", "# Reasoning Path:\nTaylor Swift -> music.artist.album -> Red -> music.album.release_type -> Album\n# Answer:\nRed"], "ground_truth": ["The Giver", "Jonas Brothers: The Concert Experience", "The Lorax", "Hannah Montana: The Movie", "Valentine's Day"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.15384615384615385, "ans_precission": 0.125, "ans_recall": 0.2, "path_f1": 0.125, "path_precision": 0.125, "path_recall": 0.125, "path_ans_f1": 0.15384615384615385, "path_ans_precision": 0.125, "path_ans_recall": 0.2}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.basic_title -> Chancellor\n# Answer:\nChancellor", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.appointed_by -> Paul von Hindenburg\n# Answer:\nPaul von Hindenburg", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Germany -> location.country.official_language -> German Language\n# Answer:\nGermany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nF\u00fchrer"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.2222222222222222, "path_recall": 0.18181818181818182, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-540", "prediction": ["# Reasoning Path:\nBrett Favre -> sports.drafted_athlete.drafted -> m.03w8mk1 -> sports.sports_league_draft_pick.school -> University of Southern Mississippi\n# Answer:\nUniversity of Southern Mississippi", "# Reasoning Path:\nBrett Favre -> people.person.education -> m.03gkqk_ -> education.education.institution -> University of Southern Mississippi\n# Answer:\nUniversity of Southern Mississippi", "# Reasoning Path:\nBrett Favre -> sports.drafted_athlete.drafted -> m.03w8mk1 -> sports.sports_league_draft_pick.team -> Atlanta Falcons\n# Answer:\nAtlanta Falcons", "# Reasoning Path:\nBrett Favre -> sports.drafted_athlete.drafted -> m.03w8mk1 -> sports.sports_league_draft_pick.draft -> 1991 NFL draft\n# Answer:\n1991 NFL draft", "# Reasoning Path:\nBrett Favre -> people.person.religion -> Catholicism -> base.popstra.religion.membership -> m.064bnd1\n# Answer:\nCatholicism", "# Reasoning Path:\nBrett Favre -> common.topic.image -> brett-favre.jpg -> common.image.size -> m.063ffjf\n# Answer:\nbrett-favre.jpg", "# Reasoning Path:\nBrett Favre -> american_football.football_player.receiving -> m.07914qq -> american_football.player_receiving_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nBrett Favre -> common.topic.image -> BrettFavre -> common.image.size -> m.029jwyn\n# Answer:\nBrettFavre"], "ground_truth": ["University of Southern Mississippi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-542", "prediction": ["# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> common.topic.subject_of -> Sweet Magnolia Tours\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> base.biblioness.bibs_location.state -> Louisiana\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> location.location.containedby -> United States of America\n# Answer:\nNew Orleans", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> people.person.parents -> Sidney Chilton -> people.person.gender -> Male\n# Answer:\nSidney Chilton", "# Reasoning Path:\nAlex Chilton -> music.group_member.membership -> g.11b6cpx0cn\n# Answer:\nmusic.group_member.membership", "# Reasoning Path:\nAlex Chilton -> music.artist.album -> 19 Years: A Collection of Alex Chilton -> common.topic.notable_for -> g.1256t_zr4\n# Answer:\n19 Years: A Collection of Alex Chilton", "# Reasoning Path:\nAlex Chilton -> people.deceased_person.place_of_death -> New Orleans -> location.location.containedby -> Louisiana\n# Answer:\nNew Orleans"], "ground_truth": ["New Orleans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-543", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.institution -> Morehouse College\n# Answer:\nMorehouse College", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nCrozer Theological Seminary", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.institution -> Washington High School\n# Answer:\nWashington High School", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.major_field_of_study -> Systematic theology\n# Answer:\nSystematic theology", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.major_field_of_study -> Sociology\n# Answer:\nSociology"], "ground_truth": ["Crozer Theological Seminary", "Morehouse College", "Boston University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-545", "prediction": ["# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> common.topic.notable_for -> g.125f8rws1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09w_c2s\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> people.cause_of_death.people -> Gwen Welles\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wx5h4\n# Answer:\nAnal cancer", "# Reasoning Path:\nFarrah Fawcett -> media_common.quotation_subject.quotations_about_this_subject -> Farrah was one of the iconic beauties of our time. Her girl-next-door charm combined with stunning looks made her a star on film, TV and the printed page. -> media_common.quotation.author -> Hugh Hefner\n# Answer:\nFarrah was one of the iconic beauties of our time. Her girl-next-door charm combined with stunning looks made her a star on film, TV and the printed page.", "# Reasoning Path:\nFarrah Fawcett -> people.deceased_person.cause_of_death -> Anal cancer -> common.topic.webpage -> m.09wzc81\n# Answer:\nAnal cancer"], "ground_truth": ["Anal cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-546", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> common.topic.notable_types -> Person\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.sibling_s -> m.0j217k5\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.gender -> Female\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q2m7 -> base.popstra.dated.participant -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-547", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> symbols.namesake.named_after -> Christopher Columbus\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.deposit_interest_rate -> g.11b71kjkjp\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nCosta Rica -> location.statistical_region.co2_emissions_per_capita -> g.1245_0_8r\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-548", "prediction": ["# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> music.instrument.variation -> Kalindula\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> music.instrument.family -> Guitar\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.webpage -> m.09wnh17\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> common.topic.subject_of -> Reverb\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.group_member.instruments_played -> Bass guitar -> education.field_of_study.students_majoring -> m.0cn9pt_\n# Answer:\nBass guitar", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> music.guitar.guitarists -> Adrian Smith\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> common.topic.image -> Fender strat\n# Answer:\nFender Stratocaster", "# Reasoning Path:\nJoe Walsh -> music.guitarist.guitars_played -> Fender Stratocaster -> symbols.namesake.named_after -> Leo Fender\n# Answer:\nFender Stratocaster"], "ground_truth": ["Fender Stratocaster"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-549", "prediction": ["# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0d6v -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> freebase.valuenotation.is_reviewed -> Number\n# Answer:\nNumber", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> base.schemastaging.phone_sandbox.service_language -> Portuguese Language\n# Answer:\nPortuguese Language", "# Reasoning Path:\nEuropean Union -> base.schemastaging.organization_extra.phone_number -> m.010d0fvn -> freebase.valuenotation.has_no_value -> Saturday closing time\n# Answer:\nSaturday closing time", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245yvl64\n# Answer:\nlocation.statistical_region.broadband_penetration_rate"], "ground_truth": ["German Language", "Portuguese Language", "Swedish Language", "French", "English Language", "Spanish Language", "Italian Language", "Polish Language", "Dutch Language", "Greek Language"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.6, "ans_recall": 0.4, "path_f1": 0.5714285714285713, "path_precision": 0.6, "path_recall": 0.5454545454545454, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> http://www.discogs.com/artist/Michael+Bubl%E9 -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/Michael+Bubl%E9", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website"], "ground_truth": ["Actor", "Songwriter", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-550", "prediction": ["# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.series -> Reality Check\n# Answer:\nReality Check", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.series -> Gladiators 2000\n# Answer:\nGladiators 2000", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmqp0y -> tv.regular_tv_appearance.series -> Click\n# Answer:\nClick", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmvvcj -> tv.regular_tv_appearance.series -> American Juniors\n# Answer:\nAmerican Juniors", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0g9k8k5 -> tv.regular_tv_appearance.series -> Reality Check\n# Answer:\nReality Check", "# Reasoning Path:\nRyan Seacrest -> tv.tv_producer.programs_produced -> g.11byrxg698\n# Answer:\ntv.tv_producer.programs_produced", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.04f2t4z -> tv.regular_tv_appearance.character -> Jack Craft\n# Answer:\nJack Craft", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.guest_roles -> m.0bgmqnf -> tv.tv_guest_role.episodes_appeared_in -> 5/21/04\n# Answer:\n5/21/04", "# Reasoning Path:\nRyan Seacrest -> tv.tv_actor.starring_roles -> m.0bmltz0 -> tv.regular_tv_appearance.special_performance_type -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["On Air with Ryan Seacrest", "American Top 40"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-552", "prediction": ["# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.region -> Europe\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.main_country -> Slovenia\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> common.topic.notable_types -> Human Language\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.country.languages_spoken -> Italian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nItalian Language", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.official_language -> Italian Language -> language.human_language.countries_spoken_in -> Brazil\n# Answer:\nItalian Language"], "ground_truth": ["Italian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-554", "prediction": ["# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Portugal\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Andorra\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nSpain -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Germany\n# Answer:\nEuro"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-555", "prediction": ["# Reasoning Path:\nDrew Barrymore -> base.popstra.celebrity.substance_abuse -> m.063gck7 -> base.popstra.substance_abuse.substance -> alcohol and drugs\n# Answer:\nalcohol and drugs", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Comedy\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.language -> English Language\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Drama\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.production_companies -> Flower Films\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> award.award_nominee.award_nominations -> m.010wr2wn -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actress: Comedy\n# Answer:\nTeen Choice Award for Choice Movie Actress: Comedy", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> film.film.genre -> Sports films\n# Answer:\nWhip It", "# Reasoning Path:\nDrew Barrymore -> film.director.film -> Whip It -> media_common.netflix_title.netflix_genres -> Comedies\n# Answer:\nWhip It"], "ground_truth": ["Alcoholic beverage", "Cocaine"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-556", "prediction": ["# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.notable_types -> Day Of Year\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> common.topic.article -> m.0tj3\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> common.image.size -> m.029pkbb\n# Answer:\ncommon.image.size", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> World Book Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.calendar_system -> Gregorian calendar\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> base.saints.feast_day.saint -> Saint George\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> time.holiday.featured_in_religions -> Christianity -> religion.religion.is_part_of -> Abrahamic religions\n# Answer:\nChristianity", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> National Cherry Cheesecake Day\n# Answer:\nApril 23", "# Reasoning Path:\nSt George's Day -> common.topic.article -> m.0glsbr7\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSt George's Day -> time.holiday.day_of_year -> April 23 -> time.day_of_year.holidays -> All Ukrainian Day of psychologist\n# Answer:\nApril 23"], "ground_truth": ["April 23"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-557", "prediction": ["# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_for -> g.125b8tl9d\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Nazi Germany\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.notable_types -> Form of Government\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> book.book_subject.works -> The Roman Empire and Its Germanic Peoples -> book.written_work.subjects -> Germanic peoples\n# Answer:\nThe Roman Empire and Its Germanic Peoples", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> government.form_of_government.countries -> Tsardom of Russia\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> base.schemastaging.context_name.pronunciation -> g.11btvx1x_j\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> location.country.form_of_government -> Autocracy -> common.topic.article -> m.0d8tx\n# Answer:\nAutocracy", "# Reasoning Path:\nRoman Empire -> film.film_subject.films -> The Robe -> film.film.country -> United States of America\n# Answer:\nThe Robe"], "ground_truth": ["Autocracy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-559", "prediction": ["# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Violin -> common.topic.notable_types -> Musical instrument\n# Answer:\nViolin", "# Reasoning Path:\nRobin Gibb -> music.group_member.instruments_played -> Piano -> common.topic.notable_types -> Musical instrument\n# Answer:\nPiano", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.track -> Islands in the Stream\n# Answer:\nIslands in the Stream", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110253_ -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Universal Music Group -> common.topic.notable_types -> Record label\n# Answer:\nUniversal Music Group", "# Reasoning Path:\nRobin Gibb -> music.artist.track_contributions -> m.0110195t -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRobin Gibb -> film.actor.film -> m.0gxgcrl -> film.performance.film -> Robin Gibb with the Frankfurt Neue Philharmonic Orchestra: Live\n# Answer:\nRobin Gibb with the Frankfurt Neue Philharmonic Orchestra: Live", "# Reasoning Path:\nRobin Gibb -> music.artist.label -> Universal Music Group -> business.business_operation.industry -> Music\n# Answer:\nUniversal Music Group"], "ground_truth": ["Drum machine", "Vocals", "Piano", "Harmonica", "Violin"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.5, "ans_recall": 0.6, "path_f1": 0.14545454545454545, "path_precision": 0.5, "path_recall": 0.0851063829787234, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.5, "path_ans_recall": 0.6}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-560", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zsqt -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["Matt Lanter"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-561", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> people.person.nationality -> United States of America -> military.military_combatant.military_commanders -> m.04h_gx9\n# Answer:\nUnited States of America", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0446bdb -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.district_represented -> Illinois's 7th congressional district\n# Answer:\nIllinois's 7th congressional district", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> film.film_subject.films -> Abraham Lincoln's Clemency -> film.film.directed_by -> Theodore Wharton\n# Answer:\nAbraham Lincoln's Clemency"], "ground_truth": ["Abraham Lincoln 1865 presidential inauguration", "Abraham Lincoln 1861 presidential inauguration"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-562", "prediction": ["# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Arlington\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.geolocation -> m.02_t4f8\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> Rangers Ballpark in Arlington\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> location.location.containedby -> Texas\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.arena_stadium -> Rangers Ballpark in Arlington -> business.sponsored_recipient.sponsorship -> m.060crqp\n# Answer:\nRangers Ballpark in Arlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.containedby -> Texas\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.metropolitan_area_anchor.metropolitan_area -> Fort Worth-Arlington, TX Metropolitan Division\n# Answer:\nArlington", "# Reasoning Path:\nTexas Rangers -> sports.sports_team.location -> Arlington -> location.location.contains -> University of Texas at Arlington School of Architecture\n# Answer:\nArlington"], "ground_truth": ["Rangers Ballpark in Arlington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-564", "prediction": ["# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.nearby_airports -> Clinton National Airport\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.gender -> Male\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.containedby -> United States of America\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.profession -> Architect\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> common.topic.image -> The Capitol Building -> common.image.appears_in_topic_gallery -> Arkansas General Assembly\n# Answer:\nThe Capitol Building", "# Reasoning Path:\nArkansas State Capitol -> location.location.containedby -> Little Rock -> location.location.time_zones -> Central Time Zone\n# Answer:\nLittle Rock", "# Reasoning Path:\nArkansas State Capitol -> common.image.appears_in_topic_gallery -> George R. Mann -> people.person.place_of_birth -> Syracuse\n# Answer:\nGeorge R. Mann", "# Reasoning Path:\nArkansas State Capitol -> common.topic.article -> m.05yzfm\n# Answer:\ncommon.topic.article"], "ground_truth": ["Little Rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-567", "prediction": ["# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.containedby -> Santa Cruz County\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.hud_foreclosure_area.ofheo_price_change -> m.07g9y5n\n# Answer:\nlocation.hud_foreclosure_area.ofheo_price_change", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> location.postal_code.country -> United States of America\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.statistical_region.population -> g.11b66h2c6g\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.location.contains -> Lourdes Catholic School\n# Answer:\n85621", "# Reasoning Path:\nNogales -> common.topic.image -> Nogales 1899 -> common.image.appears_in_topic_gallery -> Battle of Ambos Nogales\n# Answer:\nNogales 1899", "# Reasoning Path:\nNogales -> common.topic.article -> m.0qqwk\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85621 -> location.postal_code.country -> United States of America\n# Answer:\n85621", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85628 -> common.topic.notable_for -> g.125614f5r\n# Answer:\n85628", "# Reasoning Path:\nNogales -> location.citytown.postal_codes -> 85662 -> common.topic.notable_for -> g.125d1ds3d\n# Answer:\n85662"], "ground_truth": ["85662", "85628", "85621"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-568", "prediction": ["# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07sh25z -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07sh25z -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07tdcdk -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07vzk25 -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.rushing -> m.07sh2b2 -> american_football.player_rushing_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07yc98n -> american_football.player_game_statistics.team -> Philadelphia Eagles\n# Answer:\nPhiladelphia Eagles", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07z27dl -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nMichael Vick -> american_football.football_player.games -> m.07tdcdk -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["2009"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-570", "prediction": ["# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Judge -> common.topic.notable_types -> Profession\n# Answer:\nJudge", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subjects -> Accident Advice Helpline\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nWilliam Howard Taft -> base.yalebase.secret_society_member.belonged_to -> m.046xl1n -> base.yalebase.secret_society_membership.society -> Skull and Bones\n# Answer:\nSkull and Bones", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> people.profession.specializations -> Jurist\n# Answer:\nLawyer", "# Reasoning Path:\nWilliam Howard Taft -> people.person.profession -> Lawyer -> common.topic.subject_of -> Accident Advice Helpline\n# Answer:\nLawyer"], "ground_truth": ["Jurist", "Lawyer", "Judge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.7058823529411765, "path_precision": 0.75, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-572", "prediction": ["# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> United Kingdom\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> common.topic.notable_for -> g.125bt8v5y\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Inverness-shire\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Urquhart Castle\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.containedby -> Scotland\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.statistical_region.population -> m.0cl9fgl\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> travel.travel_destination.tourist_attractions -> Aigas Field Centre\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> people.deceased_person.place_of_death -> Inverness -> location.location.events -> Siege of Inverness\n# Answer:\nInverness", "# Reasoning Path:\nRobin Cook -> tv.tv_actor.guest_roles -> m.09nbpck -> tv.tv_guest_role.episodes_appeared_in -> 06 January 2002\n# Answer:\n06 January 2002"], "ground_truth": ["Inverness"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-573", "prediction": ["# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> base.biblioness.bibs_location.state -> Arkansas\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.places_lived -> m.03pds23 -> people.place_lived.location -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.containedby -> United States of America\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.containedby -> Area code 501\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.citytown.postal_codes -> 72201\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.containedby -> Arkansas\n# Answer:\nLittle Rock", "# Reasoning Path:\nMelba Pattillo Beals -> people.person.place_of_birth -> Little Rock -> location.location.containedby -> Pulaski County\n# Answer:\nLittle Rock"], "ground_truth": ["Arkansas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-575", "prediction": ["# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.04ktp68 -> government.government_position_held.office_holder -> William C. C. Claiborne\n# Answer:\nWilliam C. C. Claiborne", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.09tmlgw -> government.government_position_held.office_holder -> Huey Long\n# Answer:\nHuey Long", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws06r -> government.government_position_held.office_holder -> Huey Long\n# Answer:\nHuey Long", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.04ktp68 -> government.government_position_held.office_position_or_title -> Governor of Louisiana\n# Answer:\nGovernor of Louisiana", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pjr -> government.government_position_held.office_holder -> Scott Angelle\n# Answer:\nScott Angelle", "# Reasoning Path:\nLouisiana -> government.political_district.representatives -> m.0bws0xr -> government.government_position_held.office_holder -> Edward Livingston\n# Answer:\nEdward Livingston", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.04ktp68 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.governing_officials -> m.010f0pn6 -> government.government_position_held.office_holder -> Jay Dardenne\n# Answer:\nJay Dardenne"], "ground_truth": ["Mary Landrieu", "Eligius Fromentin", "Charles Dominique Joseph Bouligny", "James Brown", "Russell B. Long", "Robert C. Nicholas", "John Slidell", "David Vitter", "George A. Waggaman", "Jeremiah B. Howell", "Pierre Soul\u00e9", "Josiah S. Johnston", "Edward Douglass White", "Alexander Barrow", "Jean Noel Destr\u00e9han", "Huey Long", "William C. C. Claiborne", "John Breaux", "Alexander Porter", "Henry Johnson", "Judah P. Benjamin", "Allan B. Magruder", "Thomas Posey", "Edward Livingston", "Alexandre Mouton", "Charles Magill Conrad"], "ans_acc": 0.11538461538461539, "ans_hit": 1, "ans_f1": 0.1875, "ans_precission": 0.5, "ans_recall": 0.11538461538461539, "path_f1": 0.2, "path_precision": 0.5, "path_recall": 0.125, "path_ans_f1": 0.1875, "path_ans_precision": 0.5, "path_ans_recall": 0.11538461538461539}
{"id": "WebQTest-576", "prediction": ["# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> location.location.contains -> Talamancan montane forests\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Honduras -> location.country.languages_spoken -> English Language\n# Answer:\nHonduras", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.notable_for -> g.12596bzjx\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.country.languages_spoken -> English Language\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.02h92xs\n# Answer:\nLa Amistad International Park", "# Reasoning Path:\nCentral America -> location.location.contains -> Panama -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nPanama", "# Reasoning Path:\nCentral America -> location.location.contains -> Belize -> location.location.contains -> Belize District\n# Answer:\nBelize", "# Reasoning Path:\nCentral America -> location.location.contains -> La Amistad International Park -> common.topic.article -> m.080_k8\n# Answer:\nLa Amistad International Park"], "ground_truth": ["Honduras", "Costa Rica", "El Salvador", "Belize", "Guatemala", "Gran Colombia", "Panama"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.5, "ans_recall": 0.42857142857142855, "path_f1": 0.4615384615384615, "path_precision": 0.5, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.5, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-577", "prediction": ["# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.school -> University of Arkansas at Little Rock\n# Answer:\nUniversity of Arkansas at Little Rock", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.0sw01cy -> education.education.institution -> University of Arkansas\n# Answer:\nUniversity of Arkansas", "# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.draft -> 1996 NBA draft\n# Answer:\n1996 NBA draft", "# Reasoning Path:\nDerek Fisher -> sports.drafted_athlete.drafted -> m.0cr2kgn -> sports.sports_league_draft_pick.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nDerek Fisher -> people.person.education -> m.030qjb6 -> education.education.institution -> University of Arkansas at Little Rock\n# Answer:\nUniversity of Arkansas at Little Rock", "# Reasoning Path:\nDerek Fisher -> tv.tv_actor.guest_roles -> m.09nchzy -> tv.tv_guest_role.episodes_appeared_in -> April 7, 2003\n# Answer:\nApril 7, 2003", "# Reasoning Path:\nDerek Fisher -> sports.pro_athlete.teams -> m.0z58tq8 -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard", "# Reasoning Path:\nDerek Fisher -> sports.pro_athlete.teams -> m.0n4p4t5 -> sports.sports_team_roster.position -> Point guard\n# Answer:\nPoint guard"], "ground_truth": ["University of Arkansas at Little Rock", "University of Arkansas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-578", "prediction": ["# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.team -> Charlotte Hornets\n# Answer:\nCharlotte Hornets", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.draft -> 1996 NBA draft\n# Answer:\n1996 NBA draft", "# Reasoning Path:\nKobe Bryant -> sports.drafted_athlete.drafted -> m.02h7cfz -> sports.sports_league_draft_pick.school -> Lower Merion High School\n# Answer:\nLower Merion High School", "# Reasoning Path:\nKobe Bryant -> sports.pro_athlete.teams -> m.0j2gtp7 -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> base.schemastaging.athlete_extra.salary -> m.0l3xvm2 -> base.schemastaging.athlete_salary.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> base.schemastaging.athlete_extra.salary -> m.0l3xvm2 -> freebase.valuenotation.is_reviewed -> Athlete\n# Answer:\nAthlete", "# Reasoning Path:\nKobe Bryant -> basketball.basketball_player.player_statistics -> m.04qb_g9 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nKobe Bryant -> sports.pro_athlete.teams -> m.0j2gtp7 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Charlotte Hornets"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-579", "prediction": ["# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.03lnb3b\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> location.location.containedby -> Charlotte\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.arena_stadium -> The Time Warner Cable Arena -> sports.sports_facility.teams -> Charlotte Checkers\n# Answer:\nThe Time Warner Cable Arena", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.07ldsgk\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> location.statistical_region.population -> g.11b674q2v0\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.09w7_w9\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> government.governmental_jurisdiction.governing_officials -> m.0103bhl1\n# Answer:\nCharlotte", "# Reasoning Path:\nCharlotte Hornets -> sports.sports_team.location -> Charlotte -> common.topic.webpage -> m.09wmq_5\n# Answer:\nCharlotte"], "ground_truth": ["Charlotte"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Second baseman\n# Answer:\nSecond baseman", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-580", "prediction": ["# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.article -> m.05zyx79\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Numidian language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNumidian language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.notable_types -> Human Language\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Proto-Berber language -> common.topic.notable_for -> g.125cql7q5\n# Answer:\nProto-Berber language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Numidian language -> common.topic.notable_types -> Human Language\n# Answer:\nNumidian language", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> language.human_language.countries_spoken_in -> Turkey\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b71sdykm\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nAlgeria -> location.country.languages_spoken -> Arabic Language -> common.topic.notable_types -> Human Language\n# Answer:\nArabic Language", "# Reasoning Path:\nAlgeria -> location.country.official_language -> Arabic Language -> language.human_language.countries_spoken_in -> Turkey\n# Answer:\nArabic Language"], "ground_truth": ["Western Berber languages", "Numidian language", "Arabic Language", "French", "Proto-Berber language", "Northern Berber languages", "Guanche Language"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5783132530120482, "ans_precission": 0.8888888888888888, "ans_recall": 0.42857142857142855, "path_f1": 0.5526315789473685, "path_precision": 0.7777777777777778, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5783132530120482, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-581", "prediction": ["# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.containedby -> London\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> travel.tourist_attraction.near_travel_destination -> London\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> film.film_subject.films -> The Other Boleyn Girl -> film.film.featured_film_locations -> Elstree Studios\n# Answer:\nThe Other Boleyn Girl", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.containedby -> London Borough of Tower Hamlets\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> base.prison.prison.prisoners -> m.0d7rjq3\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.contains -> White Tower\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_for -> g.125h51fpc\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> location.location.people_born_here -> Katherine Tudor\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> people.deceased_person.place_of_death -> Tower of London -> base.prison.prison.prisoners -> m.04dn8sr\n# Answer:\nTower of London", "# Reasoning Path:\nAnne Boleyn -> common.topic.notable_for -> g.126t9jcg5\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Tower of London"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-582", "prediction": ["# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> people.profession.specialization_of -> Scientist\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Werner Heisenberg\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Daniel Faraday\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.descriptive_names.names.descriptive_name -> m.0101ddrk\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> people.profession.part_of_professional_field -> Physics\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Astronomer -> common.topic.image -> Johannes Hevelius\n# Answer:\nAstronomer", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> base.schemastaging.context_name.pronunciation -> g.125_nmg0m\n# Answer:\nPhysicist", "# Reasoning Path:\nGalileo Galilei -> people.person.profession -> Physicist -> fictional_universe.character_occupation.characters_with_this_occupation -> Emmett Brown\n# Answer:\nPhysicist"], "ground_truth": ["Astrologer", "Astronomer", "Physicist", "Scientist", "Mathematician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 1.0, "ans_recall": 0.4, "path_f1": 0.5714285714285715, "path_precision": 1.0, "path_recall": 0.4, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 1.0, "path_ans_recall": 0.6}
{"id": "WebQTest-583", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.institution -> Boston University\n# Answer:\nBoston University", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.institution -> Crozer Theological Seminary\n# Answer:\nCrozer Theological Seminary", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0n_xlq0 -> education.education.institution -> Washington High School\n# Answer:\nWashington High School", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.04hddst -> education.education.institution -> Morehouse College\n# Answer:\nMorehouse College", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.02wp75f -> education.education.major_field_of_study -> Systematic theology\n# Answer:\nSystematic theology", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> education.education.degree -> Bachelor of Divinity\n# Answer:\nBachelor of Divinity", "# Reasoning Path:\nMartin Luther King, Jr. -> people.person.education -> m.0dh6jzz -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study"], "ground_truth": ["Boston University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-584", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.statistical_region.population -> g.11bc8807jd\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.geolocation -> m.0cm04zy\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> influence.influence_node.influenced -> Vasubandhu -> people.person.place_of_birth -> India\n# Answer:\nVasubandhu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu"], "ground_truth": ["Kapilavastu"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-585", "prediction": ["# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.currency_used -> Pound sterling\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> royalty.kingdom.rulers -> Henry VII of England\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.country -> Principality of Wales\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.members -> Edmund Tudor, Duke of Somerset\n# Answer:\nTudor dynasty", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> royalty.kingdom.rulers -> Alfred the Great\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.kingdom -> England -> location.country.languages_spoken -> Old English\n# Answer:\nEngland", "# Reasoning Path:\nHenry VIII of England -> royalty.monarch.royal_line -> Tudor dynasty -> people.family.founder -> Henry VII of England\n# Answer:\nTudor dynasty"], "ground_truth": ["England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-586", "prediction": ["# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Vincristine\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> film.actor.film -> g.11b7563h44\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.notable_people_with_this_condition -> Clive James\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nCancer", "# Reasoning Path:\nLarry Hagman -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Cytarabine\n# Answer:\nLeukemia"], "ground_truth": ["Head and neck cancer", "Leukemia"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6086956521739131, "ans_precission": 0.7777777777777778, "ans_recall": 0.5, "path_f1": 0.6086956521739131, "path_precision": 0.7777777777777778, "path_recall": 0.5, "path_ans_f1": 0.6086956521739131, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.5}
{"id": "WebQTest-588", "prediction": ["# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04fv8sm -> military.military_command.military_conflict -> Battle of Appomattox Court House\n# Answer:\nBattle of Appomattox Court House", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.0h1l91r -> military.military_command.military_conflict -> Western Virginia Campaign\n# Answer:\nWestern Virginia Campaign", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04yv3f8 -> military.military_command.military_conflict -> Siege of Petersburg\n# Answer:\nSiege of Petersburg", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.04fvgcw -> military.military_command.military_conflict -> Battle of Cold Harbor\n# Answer:\nBattle of Cold Harbor", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_conflict -> Battle of Gettysburg\n# Answer:\nBattle of Gettysburg", "# Reasoning Path:\nRobert E. Lee -> people.person.sibling_s -> m.0w4gc9q -> people.sibling_relationship.sibling -> Sydney Smith Lee\n# Answer:\nSydney Smith Lee", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.book.genre -> Non-fiction\n# Answer:\nRobert E. Lee's Civil War"], "ground_truth": ["Second Battle of Rappahannock Station", "Battle of the Crater", "Mexican\u2013American War", "Battle of Glendale", "Western Virginia Campaign", "Second Battle of Petersburg", "Battle of Antietam", "Battle of Williamsport", "Battle of Cheat Mountain", "Battle of Franklin's Crossing", "Siege of Fort Pulaski", "Battle of White Oak Road", "Maryland Campaign", "Northern Virginia Campaign", "Battle of Gaines's Mill", "Battle of Appomattox Court House", "Battle of South Mountain", "Gettysburg Campaign", "Battle of Cold Harbor", "American Civil War", "Battle of Totopotomoy Creek", "Battle of Chancellorsville", "Battle of Fort Stedman", "John Brown's raid on Harpers Ferry", "Battle of Malvern Hill", "Battle of Oak Grove", "Battle of Cumberland Church", "Battle of Frederick", "Overland Campaign", "Battle of Beaver Dam Creek", "Richmond in the American Civil War", "Battle of Gettysburg", "Battle of Chaffin's Farm", "Second Battle of Bull Run", "Third Battle of Petersburg", "Second Battle of Deep Bottom", "Battle of Mine Run", "Battle of the Wilderness", "Stoneman's 1863 Raid", "Battle of Spotsylvania Court House", "Battle of Fredericksburg", "Seven Days Battles", "Battle of Darbytown and New Market Roads", "Siege of Petersburg"], "ans_acc": 0.13636363636363635, "ans_hit": 1, "ans_f1": 0.1923076923076923, "ans_precission": 0.625, "ans_recall": 0.11363636363636363, "path_f1": 0.1923076923076923, "path_precision": 0.625, "path_recall": 0.11363636363636363, "path_ans_f1": 0.23076923076923075, "path_ans_precision": 0.75, "path_ans_recall": 0.13636363636363635}
{"id": "WebQTest-589", "prediction": ["# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.region -> Eurasia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Belarus\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.writing_system -> Cyrillic script\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Abaza Language -> language.human_language.region -> Europe\n# Answer:\nAbaza Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Georgia\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Russian Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.place_of_birth -> Ossining\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Play -> type.type.properties -> Composer\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> base.militaryinfiction.location_in_fiction.contained_by -> New York\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Soundtracks\n# Answer:\nSoundtracks", "# Reasoning Path:\nAnnie -> film.film.featured_film_locations -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nAnnie -> film.film.story_by -> Thomas Meehan -> people.person.nationality -> United States of America\n# Answer:\nThomas Meehan", "# Reasoning Path:\nAnnie -> film.film.story_by -> Harold Gray -> comic_strips.comic_strip_creator.comic_strips_written -> m.0gwbjrm\n# Answer:\nHarold Gray"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-590", "prediction": ["# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> base.culturalevent.event.entity_involved -> Tripolitania\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> common.topic.notable_for -> g.12577rzkz\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> government.governmental_jurisdiction.governing_officials -> m.0105y7bt -> government.government_position_held.office_holder -> Abdullah al-Thani\n# Answer:\nAbdullah al-Thani", "# Reasoning Path:\nLibya -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hym\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> time.event.locations -> Ottoman Tripolitania\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1920 Jabal al-Gharbi civil war -> common.topic.article -> m.0gw_q9y\n# Answer:\n1920 Jabal al-Gharbi civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1793\u201395 Tripolitanian civil war -> common.topic.notable_types -> Military Conflict\n# Answer:\n1793\u201395 Tripolitanian civil war", "# Reasoning Path:\nLibya -> location.location.events -> 1835\u201358 revolt in Ottoman Tripolitania -> base.culturalevent.event.entity_involved -> Libyans\n# Answer:\n1835\u201358 revolt in Ottoman Tripolitania", "# Reasoning Path:\nLibya -> location.location.events -> 1835\u201358 revolt in Ottoman Tripolitania -> common.topic.notable_types -> Military Conflict\n# Answer:\n1835\u201358 revolt in Ottoman Tripolitania"], "ground_truth": ["Abdullah al-Thani"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.16666666666666666, "path_precision": 0.1111111111111111, "path_recall": 0.3333333333333333, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-592", "prediction": ["# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> 2014 Boston Tornado\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> base.schemastaging.context_name.pronunciation -> g.125_lt26z\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Actions of 5\u20136 May 1945\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent", "# Reasoning Path:\nShays' Rebellion -> common.topic.notable_for -> g.125b3gyz_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> government.governmental_jurisdiction.official_symbols -> m.0hzdk1k\n# Answer:\nMassachusetts", "# Reasoning Path:\nShays' Rebellion -> time.event.locations -> Massachusetts -> location.location.events -> Chappaquiddick Incident\n# Answer:\nMassachusetts"], "ground_truth": ["1786"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-593", "prediction": ["# Reasoning Path:\nMontel Williams -> common.topic.article -> m.018pv0\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Dysphagia\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Vision loss\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Zeki Kuneralp\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Blurred vision\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.notable_people_with_this_condition -> Jack Osbourne\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> medicine.disease.symptoms -> Amnesia\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> people.cause_of_death.people -> Anne Volant Rowling\n# Answer:\nMultiple sclerosis", "# Reasoning Path:\nMontel Williams -> medicine.notable_person_with_medical_condition.condition -> Multiple sclerosis -> base.bioventurist.bv_medical_condition.company_involved -> Receptos, Inc.\n# Answer:\nMultiple sclerosis"], "ground_truth": ["Multiple sclerosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-595", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> time.event.locations -> Mercedes-Benz Superdome\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> 2001 AFC Championship Game -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\n2001 AFC Championship Game", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> common.topic.notable_types -> American football team -> type.type.domain -> American football\n# Answer:\nAmerican football team"], "ground_truth": ["Super Bowl XLVII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-596", "prediction": ["# Reasoning Path:\nKansas City Royals -> sports.sports_team.arena_stadium -> Kauffman Stadium -> location.location.containedby -> Kansas City\n# Answer:\nKauffman Stadium", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> United States of America\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> travel.travel_destination.tourist_attractions -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.arena_stadium -> Kauffman Stadium -> sports.sports_facility.home_venue_for -> m.0wz1z5q\n# Answer:\nKauffman Stadium", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.time_zones -> Central Time Zone\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.location.containedby -> Missouri\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.location -> Kansas City -> location.place_with_neighborhoods.neighborhoods -> Crown Center\n# Answer:\nKansas City", "# Reasoning Path:\nKansas City Royals -> sports.sports_team.arena_stadium -> Kauffman Stadium -> location.location.events -> 1973 Major League Baseball All-Star Game\n# Answer:\nKauffman Stadium"], "ground_truth": ["Kansas City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-597", "prediction": ["# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Country rock -> music.genre.parent_genre -> Country\n# Answer:\nCountry rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk music -> broadcast.genre.content -> Acoustic Caf.\n# Answer:\nFolk music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Blues rock -> music.genre.parent_genre -> Rock music\n# Answer:\nBlues rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Folk rock -> music.genre.parent_genre -> Folk music\n# Answer:\nFolk rock", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> broadcast.genre.content -> SoulfulSmoothJazz.com\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.parent_genre -> Blues\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> music.genre.subgenre -> Blue-eyed soul\n# Answer:\nSoul music", "# Reasoning Path:\nJohn Mayer -> music.artist.genre -> Soul music -> common.topic.notable_types -> Musical genre\n# Answer:\nSoul music"], "ground_truth": ["Indie rock", "Soul music", "Alternative rock", "Country", "Pop rock", "Blue-eyed soul", "Blues", "Rock music", "Folk music", "Folk rock", "Acoustic music", "Country rock", "Soft rock", "Pop music", "Soul rock", "Blues rock"], "ans_acc": 0.5625, "ans_hit": 1, "ans_f1": 0.6086956521739131, "ans_precission": 1.0, "ans_recall": 0.4375, "path_f1": 0.6086956521739131, "path_precision": 1.0, "path_recall": 0.4375, "path_ans_f1": 0.72, "path_ans_precision": 1.0, "path_ans_recall": 0.5625}
{"id": "WebQTest-599", "prediction": ["# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> law.invention.inventor -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> common.image.size -> m.02ctc4t\n# Answer:\ncommon.image.size", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> base.argumentmaps.original_idea.innovator -> Eldridge R. Johnson\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> common.topic.notable_types -> Invention\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Quadruplex telegraph -> common.topic.notable_for -> g.1257t43gt\n# Answer:\nQuadruplex telegraph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Phonograph -> common.topic.article -> m.063mq\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> base.argumentmaps.innovator.original_ideas -> Phonograph -> award.ranked_item.appears_in_ranked_lists -> m.0h717jf\n# Answer:\nPhonograph", "# Reasoning Path:\nThomas Edison -> law.inventor.inventions -> Quadruplex telegraph -> common.topic.article -> m.02r0w8t\n# Answer:\nQuadruplex telegraph"], "ground_truth": ["Carbon microphone", "Incandescent light bulb", "Phonograph cylinder", "Movie camera", "Phonograph", "Mimeograph", "Electric Power Distribution", "Quadruplex telegraph"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3902439024390244, "ans_precission": 0.8888888888888888, "ans_recall": 0.25, "path_f1": 0.28571428571428575, "path_precision": 0.3333333333333333, "path_recall": 0.25, "path_ans_f1": 0.3902439024390244, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.25}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> United States of America\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.total_residential_addresses -> m.07ghzrr\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Mobile County\n# Answer:\nMobile"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.deceased_person.place_of_death -> New York City\n# Answer:\nJohn Aspinwall Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> You learn by living -> book.book_edition.book -> You Learn by Living\n# Answer:\nYou learn by living"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-600", "prediction": ["# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> base.biblioness.bibs_location.country -> Yugoslavia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.containedby -> Serbia\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.contains -> Belgrade Nikola Tesla Airport\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> location.location.containedby -> Europe\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Danas\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> sports.sports_team_location.teams -> KK Crvena zvezda\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.place_of_birth -> Belgrade -> periodicals.newspaper_circulation_area.newspapers -> Politika\n# Answer:\nBelgrade", "# Reasoning Path:\nNovak Djokovic -> people.person.nationality -> Serbia -> location.country.capital -> Belgrade\n# Answer:\nSerbia"], "ground_truth": ["Serbia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-602", "prediction": ["# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Vermont\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> Northeastern United States -> location.location.containedby -> United States of America\n# Answer:\nNortheastern United States", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Vermont\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Vermont\n# Answer:\nUnited States of America", "# Reasoning Path:\nNew England -> location.location.contains -> Hillsborough County -> location.location.containedby -> United States of America\n# Answer:\nHillsborough County", "# Reasoning Path:\nNew England -> location.location.containedby -> Eastern United States -> location.location.containedby -> United States of America\n# Answer:\nEastern United States", "# Reasoning Path:\nNew England -> location.location.contains -> White Mountains -> location.location.containedby -> United States of America\n# Answer:\nWhite Mountains"], "ground_truth": ["Northeastern United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-603", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage", "# Reasoning Path:\nTurkey -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxt\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b7gsv6fr\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-604", "prediction": ["# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Rock music -> common.topic.notable_types -> Musical genre\n# Answer:\nRock music", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues rock -> music.genre.parent_genre -> Rock music\n# Answer:\nBlues rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_for -> g.1257tm1jy\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> music.genre.parent_genre -> Psychedelic rock\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.notable_types -> Musical genre\n# Answer:\nAcid rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues rock -> music.genre.parent_genre -> Jam band\n# Answer:\nBlues rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Blues rock -> common.topic.notable_types -> Musical genre\n# Answer:\nBlues rock", "# Reasoning Path:\nJimi Hendrix -> music.artist.genre -> Acid rock -> common.topic.webpage -> m.09xphgr\n# Answer:\nAcid rock"], "ground_truth": ["Rhythm and blues", "Heavy metal", "Blues", "Rock music", "Psychedelic rock", "Progressive rock", "Hard rock", "Funk rock", "Psychedelia", "Experimental rock", "Acid rock", "Blues rock"], "ans_acc": 0.4166666666666667, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5882352941176471, "path_ans_precision": 1.0, "path_ans_recall": 0.4166666666666667}
{"id": "WebQTest-605", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.degree -> Bachelor of Arts\n# Answer:\nBachelor of Arts", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgn2 -> education.education.degree -> Juris Doctor\n# Answer:\nJuris Doctor", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.major_field_of_study -> Political Science\n# Answer:\nPolitical Science", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.institution -> Columbia University\n# Answer:\nColumbia University", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgmw -> education.education.specialization -> International relations\n# Answer:\nInternational relations", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nBarack Obama -> people.person.education -> m.02nqgpk -> education.education.institution -> Occidental College\n# Answer:\nOccidental College", "# Reasoning Path:\nBarack Obama -> book.book_subject.works -> Yes We Can: A Biography of President Barack Obama -> book.written_work.subjects -> United States of America\n# Answer:\nYes We Can: A Biography of President Barack Obama"], "ground_truth": ["Juris Doctor", "Bachelor of Arts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-606", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0w17rt4 -> people.sibling_relationship.sibling -> Edward Charles Bell\n# Answer:\nEdward Charles Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.parents -> Mabel Gardiner Hubbard\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.sibling_s -> m.0k257qs -> people.sibling_relationship.sibling -> Melville James Bell\n# Answer:\nMelville James Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.parents -> Alexander Melville Bell -> people.person.children -> Edward Charles Bell\n# Answer:\nAlexander Melville Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Edward Bell -> people.person.sibling_s -> m.0w17pb4\n# Answer:\nEdward Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Elsie Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nElsie Bell", "# Reasoning Path:\nAlexander Graham Bell -> people.person.parents -> Eliza Grace Symonds -> people.person.children -> Edward Charles Bell\n# Answer:\nEliza Grace Symonds", "# Reasoning Path:\nAlexander Graham Bell -> people.person.children -> Marian Hubbard Bell -> people.person.sibling_s -> m.0w17p51\n# Answer:\nMarian Hubbard Bell"], "ground_truth": ["Melville James Bell", "Edward Charles Bell"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-608", "prediction": ["# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> sports.sports_championship_event.championship -> Football League Cup\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> time.event.locations -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> freebase.valuenotation.has_no_value -> Comment\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 2012 Football League Cup Final -> soccer.football_match.held_at -> Wembley Stadium\n# Answer:\n2012 Football League Cup Final", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.awards -> m.05l1m64\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> soccer.football_team.league -> m.0j508x9 -> soccer.football_league_participation.league -> Premier League\n# Answer:\nPremier League", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1964\u201365 FA Cup -> sports.sports_league_season.championship -> 1965 FA Cup Final\n# Answer:\n1964\u201365 FA Cup", "# Reasoning Path:\nLiverpool F.C. -> sports.sports_team.championships -> 1965 FA Cup Final -> soccer.football_match.teams -> Leeds United F.C.\n# Answer:\n1965 FA Cup Final"], "ground_truth": ["2012 Football League Cup Final"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-609", "prediction": ["# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.actor -> Bill Daily\n# Answer:\nBill Daily", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8sfq -> tv.regular_tv_appearance.actor -> Larry Hagman\n# Answer:\nLarry Hagman", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t90y8 -> tv.regular_tv_appearance.actor -> Hayden Rorke\n# Answer:\nHayden Rorke", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.theme_song -> Jeannie -> tv.tv_character.appeared_in_tv_program -> m.04lqxmt\n# Answer:\nJeannie", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.seasons -> I Dream of Jeannie, Season 5\n# Answer:\nI Dream of Jeannie, Season 5", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.04lqxmt -> tv.regular_tv_appearance.actor -> Barbara Eden\n# Answer:\nBarbara Eden", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.0krrbb_ -> tv.regular_tv_appearance.actor -> Barton MacLane\n# Answer:\nBarton MacLane", "# Reasoning Path:\nI Dream of Jeannie -> tv.tv_program.regular_cast -> m.02t8rjb -> tv.regular_tv_appearance.character -> Roger Healey\n# Answer:\nRoger Healey"], "ground_truth": ["Barbara Eden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.15384615384615385, "path_precision": 0.125, "path_recall": 0.2, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nHinduism", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v4rs1\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Protestantism", "Catholicism", "Islam", "Hinduism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.5, "path_recall": 0.8, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-611", "prediction": ["# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> St. Peter's Basilica\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> location.location.time_zones -> Central European Time Zone\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> base.biblioness.bibs_location.country -> Italy\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Vatican City\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> base.arthist.helynevek.intezmeny -> Santa Maria sopra Minerva\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> freebase.valuenotation.has_no_value -> Date dissolved\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> people.deceased_person.place_of_death -> Rome -> travel.travel_destination.tourist_attractions -> Santa Maria sopra Minerva\n# Answer:\nRome", "# Reasoning Path:\nSaint Peter -> visual_art.art_subject.artwork_on_the_subject -> Virgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor -> visual_art.artwork.art_subject -> John the Baptist\n# Answer:\nVirgin and Child Enthroned with Saints Peter, Paul, John the Baptist, Dominic and a Donor"], "ground_truth": ["Rome"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-612", "prediction": ["# Reasoning Path:\nJeremy Shockey -> base.schemastaging.athlete_extra.salary -> m.0j7s521 -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09tctgy -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> base.schemastaging.athlete_extra.salary -> m.0j7s521 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09rmcz9 -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> common.image.size -> m.02ggrjg\n# Answer:\ncommon.image.size", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.09tctgy -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.0791gjj -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.games -> m.09rmcbm -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJeremy Shockey -> american_football.football_player.receiving -> m.07mmpkl -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-613", "prediction": ["# Reasoning Path:\nDeion Sanders -> people.person.education -> m.02kygcm -> education.education.institution -> Florida State University\n# Answer:\nFlorida State University", "# Reasoning Path:\nDeion Sanders -> people.person.education -> m.04_7nf0 -> education.education.institution -> North Fort Myers High School\n# Answer:\nNorth Fort Myers High School", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n1k -> sports.sports_team_roster.team -> Cincinnati Reds\n# Answer:\nCincinnati Reds", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n19 -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n21 -> sports.sports_team_roster.team -> San Francisco Giants\n# Answer:\nSan Francisco Giants", "# Reasoning Path:\nDeion Sanders -> sports.pro_athlete.teams -> m.04_7n1k -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nDeion Sanders -> base.popstra.celebrity.operation -> m.064ffy8 -> base.popstra.operation.procedure -> Crescent Court\n# Answer:\nCrescent Court", "# Reasoning Path:\nDeion Sanders -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale"], "ground_truth": ["Florida State University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-614", "prediction": ["# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.location.time_zones -> Central Time Zone\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78201\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> travel.travel_destination.tourist_attractions -> Alamo Mission in San Antonio\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> William B. Travis -> military.military_person.participated_in_conflicts -> Texas Revolution\n# Answer:\nWilliam B. Travis", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.statistical_region.population -> g.11b66cbs7_\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> location.citytown.postal_codes -> 78202\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> time.event.locations -> San Antonio -> travel.travel_destination.tourist_attractions -> San Antonio Zoo and Aquarium\n# Answer:\nSan Antonio", "# Reasoning Path:\nBattle of the Alamo -> military.military_conflict.military_personnel_involved -> William B. Travis -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nWilliam B. Travis"], "ground_truth": ["1836-02-23"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-615", "prediction": ["# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> common.topic.article -> m.02sx6l\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.spouse_s -> m.03mlj1z\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> common.topic.article -> m.0hn9rcf\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.place_of_birth -> Blackmore\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry, Duke of Cornwall -> people.person.nationality -> Kingdom of England\n# Answer:\nHenry, Duke of Cornwall", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Mary I of England -> people.person.religion -> Catholicism\n# Answer:\nMary I of England", "# Reasoning Path:\nHenry VIII of England -> symbols.name_source.namesakes -> King Henry's Drive tram stop -> common.topic.image -> King Henry's Drive tramstop look south\n# Answer:\nKing Henry's Drive tram stop", "# Reasoning Path:\nHenry VIII of England -> people.person.children -> Henry FitzRoy, 1st Duke of Richmond and Somerset -> people.person.parents -> Elizabeth Blount\n# Answer:\nHenry FitzRoy, 1st Duke of Richmond and Somerset"], "ground_truth": ["Edward VI of England", "Henry FitzRoy, 1st Duke of Richmond and Somerset", "Henry, Duke of Cornwall", "Elizabeth I of England", "Mary I of England"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.711864406779661, "ans_precission": 0.875, "ans_recall": 0.6, "path_f1": 0.711864406779661, "path_precision": 0.875, "path_recall": 0.6, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-616", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.children -> Prince Harry\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.parents -> Prince Philip, Duke of Edinburgh\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.ethnicity -> English people -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish people", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.children -> Prince William, Duke of Cambridge\n# Answer:\nCharles, Prince of Wales", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.children -> James, Viscount Severn\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.children -> Prince Edward, Earl of Wessex -> people.person.place_of_birth -> Buckingham Palace\n# Answer:\nPrince Edward, Earl of Wessex", "# Reasoning Path:\nElizabeth II -> people.person.spouse_s -> m.028zms8 -> people.marriage.spouse -> Prince Philip, Duke of Edinburgh\n# Answer:\nPrince Philip, Duke of Edinburgh", "# Reasoning Path:\nElizabeth II -> people.person.children -> Charles, Prince of Wales -> people.person.religion -> Anglicanism\n# Answer:\nCharles, Prince of Wales"], "ground_truth": ["Elizabeth II", "Lilibet"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-617", "prediction": ["# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Philippines\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.containedby -> Metro Manila\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> base.biblioness.bibs_location.country -> Philippines\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> common.topic.notable_for -> g.1255t_dt0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Alphaland Makati Tower\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> location.location.contains -> Makati Coliseum\n# Answer:\nMakati", "# Reasoning Path:\nTim Tebow -> people.person.place_of_birth -> Makati -> common.topic.image -> Makati skyline at night\n# Answer:\nMakati"], "ground_truth": ["Makati"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-618", "prediction": ["# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.location.containedby -> Maryland\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> common.topic.article -> m.0g83b\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.body_of_water_spanned -> Chesapeake Bay -> location.location.containedby -> United States of America\n# Answer:\nChesapeake Bay", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.population -> g.11b66hy55n\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.statistical_region.rent50_3 -> m.05gcgxc\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Queen Anne's County -> location.location.containedby -> Baltimore-Towson, MD Metropolitan Statistical Area\n# Answer:\nQueen Anne's County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.location.containedby -> Maryland\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nChesapeake Bay Bridge -> transportation.bridge.locale -> Anne Arundel County -> location.statistical_region.co2_emissions_residential -> m.045k33p\n# Answer:\nAnne Arundel County"], "ground_truth": ["Anne Arundel County", "Queen Anne's County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-619", "prediction": ["# Reasoning Path:\nThe Big Valley -> award.award_winning_work.awards_won -> m.07tvltr -> award.award_honor.award_winner -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.actor -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.07tws31 -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck", "# Reasoning Path:\nThe Big Valley -> award.award_winning_work.awards_won -> m.0_y8ysx -> award.award_honor.award -> NAACP Image Award for TV Programs that have Improved the Presentation of Negroes\n# Answer:\nNAACP Image Award for TV Programs that have Improved the Presentation of Negroes", "# Reasoning Path:\nThe Big Valley -> award.award_winning_work.awards_won -> m.07tvltr -> award.award_honor.ceremony -> 18th Primetime Emmy Awards\n# Answer:\n18th Primetime Emmy Awards", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576g -> tv.regular_tv_appearance.character -> Victoria Barkley\n# Answer:\nVictoria Barkley", "# Reasoning Path:\nThe Big Valley -> tv.tv_program.regular_cast -> m.04d576m -> tv.regular_tv_appearance.actor -> Linda Evans\n# Answer:\nLinda Evans", "# Reasoning Path:\nThe Big Valley -> award.award_nominated_work.award_nominations -> m.0n1xrhl -> award.award_nomination.award_nominee -> Barbara Stanwyck\n# Answer:\nBarbara Stanwyck"], "ground_truth": ["Barbara Stanwyck"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4210526315789474, "path_precision": 0.5, "path_recall": 0.36363636363636365, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> people.cause_of_death.includes_causes_of_death -> Drive-by shooting\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> Countryway Gunshop\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.image -> M&Prevolver\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> people.cause_of_death.includes_causes_of_death -> Accidental discharge\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> common.topic.subject_of -> TinkerSource\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm -> base.weapons.weapon.subtypes -> Handgun\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.location.contains -> Wholesale Row\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11b66mljjm\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> common.topic.notable_for -> g.125d6503n\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesse James -> common.topic.notable_for -> g.1254z5y4y\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-620", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> travel.travel_destination.tourist_attractions -> John Fitzgerald Kennedy National Historic Site\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> United States Capitol\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> location.location.containedby -> United States of America\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> people.deceased_person.place_of_death -> Dallas -> travel.travel_destination.tourist_attractions -> Dealey Plaza\n# Answer:\nDallas", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> business.business_location.parent_company -> 16 Handles brookline\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> people.person.place_of_birth -> Brookline -> organization.organization_scope.organizations_with_this_scope -> 16 Handles brookline\n# Answer:\nBrookline", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> time.event.locations -> Washington, D.C.\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration", "# Reasoning Path:\nJohn F. Kennedy -> base.inaugurations.inauguration_speaker.inauguration -> John F. Kennedy 1961 presidential inauguration -> base.inaugurations.inauguration.inaugural_address -> Inaugural address of John F. Kennedy\n# Answer:\nJohn F. Kennedy 1961 presidential inauguration"], "ground_truth": ["United States Capitol", "Washington, D.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.26666666666666666, "path_precision": 0.25, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-621", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> location.country.languages_spoken -> French\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria"], "ground_truth": ["Austria", "Switzerland", "Belgium", "Luxembourg", "Liechtenstein", "Germany", "East Germany"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.691358024691358, "path_precision": 0.875, "path_recall": 0.5714285714285714, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-622", "prediction": ["# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.03z97l9 -> military.military_combatant_group.conflict -> War on Terror\n# Answer:\nWar on Terror", "# Reasoning Path:\nTaliban -> common.topic.article -> m.07jqt\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.04ftxl9 -> military.military_combatant_group.conflict -> War in Afghanistan\n# Answer:\nWar in Afghanistan", "# Reasoning Path:\nTaliban -> book.book_subject.works -> Afghanistan's Endless War: State Failure, Regional Politics and the Rise of the Taliban -> book.written_work.subjects -> Afghanistan\n# Answer:\nAfghanistan's Endless War: State Failure, Regional Politics and the Rise of the Taliban", "# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.04y_trm -> military.military_combatant_group.conflict -> Battle of Tora Bora\n# Answer:\nBattle of Tora Bora", "# Reasoning Path:\nTaliban -> military.military_combatant.military_conflicts -> m.04y_7_c -> military.military_combatant_group.conflict -> 2001 uprising in Herat\n# Answer:\n2001 uprising in Herat", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> location.location.events -> Taliban insurgency\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> organization.organization.geographic_scope -> Afghanistan -> base.aareas.schema.administrative_area.pertinent_type -> Afghan province\n# Answer:\nAfghanistan", "# Reasoning Path:\nTaliban -> book.book_subject.works -> Afghanistan's Endless War: State Failure, Regional Politics and the Rise of the Taliban -> book.book.editions -> Afghanistan's endless war : state failure, regional politics, and the rise of the Taliban\n# Answer:\nAfghanistan's Endless War: State Failure, Regional Politics and the Rise of the Taliban"], "ground_truth": ["Afghanistan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-624", "prediction": ["# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.actor -> Nina Dobrev\n# Answer:\nNina Dobrev", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.series -> The Vampire Diaries\n# Answer:\nThe Vampire Diaries", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.romantically_involved_with -> m.0bfmsv6 -> fictional_universe.romantic_involvement.partner -> Stefan Salvatore\n# Answer:\nStefan Salvatore", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nElena Gilbert -> fictional_universe.fictional_character.romantically_involved_with -> m.0790spf -> fictional_universe.romantic_involvement.partner -> g.12176g5r\n# Answer:\nfictional_universe.romantic_involvement.partner", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.seasons -> The Vampire Diaries - Season 1\n# Answer:\nThe Vampire Diaries - Season 1", "# Reasoning Path:\nElena Gilbert -> common.topic.image -> s1e1.jpg -> common.image.size -> m.029rz84\n# Answer:\ns1e1.jpg", "# Reasoning Path:\nElena Gilbert -> tv.tv_character.appeared_in_tv_program -> m.0790rw5 -> tv.regular_tv_appearance.seasons -> The Vampire Diaries - Season 2\n# Answer:\nThe Vampire Diaries - Season 2"], "ground_truth": ["Nina Dobrev"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-626", "prediction": ["# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> White River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.people_born_here -> Adam Emerson\n# Answer:\nSouth Dakota", "# Reasoning Path:\nMissouri River -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> book.book_subject.works -> The 100 Greatest Moments in St. Louis Sports\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> Missouri -> location.location.partially_contains -> Mississippi River\n# Answer:\nMissouri", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.people_born_here -> Adam Johnson\n# Answer:\nSouth Dakota", "# Reasoning Path:\nMissouri River -> location.location.partially_containedby -> South Dakota -> location.location.partiallycontains -> m.0wg90p7\n# Answer:\nSouth Dakota"], "ground_truth": ["Missouri", "Montana", "Kansas", "North Dakota", "Iowa", "South Dakota", "Nebraska"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.4307692307692308, "ans_precission": 0.875, "ans_recall": 0.2857142857142857, "path_f1": 0.4307692307692308, "path_precision": 0.875, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 1.0, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-627", "prediction": ["# Reasoning Path:\nTaylor Lautner -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> United States of America\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.nationality -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTaylor Lautner -> people.person.nationality -> United States of America -> base.locations.countries.continent -> North America\n# Answer:\nUnited States of America", "# Reasoning Path:\nTaylor Lautner -> base.popstra.celebrity.friendship -> m.06524mx -> base.popstra.friendship.participant -> Cayden Boyd\n# Answer:\nCayden Boyd", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> location.location.containedby -> Kent County\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> travel.travel_destination.tourist_attractions -> DeVos Place Convention Center\n# Answer:\nGrand Rapids", "# Reasoning Path:\nTaylor Lautner -> people.person.place_of_birth -> Grand Rapids -> base.biblioness.bibs_location.state -> Michigan\n# Answer:\nGrand Rapids"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-628", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.symbol -> California grizzly bear\n# Answer:\nCalifornia grizzly bear", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.symbol -> I Love You, California\n# Answer:\nI Love You, California", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279y -> location.location_symbol_relationship.symbol -> Square dance\n# Answer:\nSquare dance", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.0462792 -> location.location_symbol_relationship.Kind_of_symbol -> State animal\n# Answer:\nState animal", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.04627b8 -> location.location_symbol_relationship.symbol -> Bodie\n# Answer:\nBodie", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.04627bn -> location.location_symbol_relationship.symbol -> Californian\n# Answer:\nCalifornian", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.official_symbols -> m.046279k -> location.location_symbol_relationship.Kind_of_symbol -> State song\n# Answer:\nState song", "# Reasoning Path:\nCalifornia -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Claire Bennet -> fictional_universe.fictional_character.gender -> Female\n# Answer:\nClaire Bennet"], "ground_truth": ["California grizzly bear"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-629", "prediction": ["# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_location -> Earth\n# Answer:\nEarth", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.service_language -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0pm55rd -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> organization.organization.founders -> Phil Knight -> organization.organization_founder.organizations_founded -> Laika\n# Answer:\nPhil Knight", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m1v -> base.schemastaging.phone_sandbox.contact_category -> Media relations\n# Answer:\nMedia relations", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95m8k -> base.schemastaging.phone_sandbox.service_location -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNike, Inc. -> base.schemastaging.organization_extra.phone_number -> m.0s95mhr -> base.schemastaging.phone_sandbox.service_location -> United Kingdom\n# Answer:\nUnited Kingdom"], "ground_truth": ["Savier Inc", "NIKE UK Holding B.V.", "PMG International Ltd", "NIKE Group Holding B.V.", "NIKE Deutschland GmbH", "Cole Haan Japan", "NIKE (UK) Ltd", "NIKE Denmark ApS", "Manchester United Merchandising Ltd", "NIKE Sports (China) Co Ltd", "NIKE Australia Pty. Ltd", "NIKE (Switzerland) GmbH", "NIKE Sports Korea Co Ltd", "Umbro Asia Sourcing Ltd", "Umbro", "PT NIKE Indonesia", "NIKE Canada Corp", "Umbro International Ltd", "Umbro Licensing Ltd", "NIKE de Mexico S de R.L. de C.V.", "NIKE Sourcing India Private Ltd", "NIKE IHM Inc", "Futbol Club Barcelona SL", "Cole Haan", "NIKE USA Inc", "NIKE Laser Holding B.V.", "Bragano Trading S.r.l.", "NIKE Sweden AB", "BRS NIKE Taiwan Inc", "NIKE Philippines Inc", "Converse Canada Corp", "Converse Canada Holding B.V.", "NIKE Jump Ltd", "NIKE Israel Ltd", "NIKE Waffle", "NIKE BH B.V.", "NIKE Chile B.V.", "NIKE Retail B.V.", "NIKE South Africa Ltd", "NIKE New Zealand Co", "Converse Hong Kong Holding Co Ltd", "Nike Brand Kitchen", "Converse Sporting Goods (China) Co Ltd", "NIKE Logistics Yugen Kaisha", "NIKE de Chile Ltda", "NIKE Global Services PTE. LTD", "NIKE Hellas EPE", "NIKE Finance Ltd", "NIKE Japan Corp", "Converse (Asia Pacific) Ltd", "Umbro Worldwide Ltd", "NIKE International Ltd", "NIKE Argentina Srl", "Hurley 999 SL", "NIKE Lavadome", "NIKE TN Inc", "Cole Haan Hong Kong Ltd", "NIKE (Thailand) Ltd", "Umbro International JV", "Converse Trading Co B.V.", "NIKE Global Trading PTE. LTD", "Exeter Brands Group LLC", "Hurley Australia Pty Ltd", "NIKE Asia Holding B.V.", "NIKE Offshore Holding B.V.", "Converse", "Exeter Hong Kong Ltd", "Hurley International Holding B.V.", "NIKE Retail Poland sp. z o. o.", "NIKE Vapor Ltd", "Converse Netherlands B.V.", "NIKE Suzhou Holding HK Ltd", "Umbro.com", "Juventus Merchandising S.r.l.", "NIKE Italy S.R.L.", "Nike Golf", "NIKE Trading Co B.V.", "Converse Hong Kong Ltd", "Converse Footwear Technical Service (Zhongshan) Co Ltd", "NIKE Galaxy Holding B.V.", "NIKE International LLC", "NIKE NZ Holding B.V.", "NIKE Finland OY", "NIKE Zoom LLC", "USISL Inc", "NIKE SINGAPORE PTE LTD", "Triax Insurance Inc", "Twin Dragons Global Ltd", "NIKE Max LLC", "Hurley999 UK Ltd", "Yugen Kaisha Hurley Japan", "NIKE Africa Ltd", "NIKE Poland Sp.zo.o", "PT Hurley Indonesia", "NIKE Retail LLC", "Twin Dragons Holding B.V.", "NIKE Global Holding B.V.", "NIKE Tailwind", "NIKE European Operations Netherlands B.V.", "NIKE (Suzhou) Sports Co Ltd", "NIKE Mexico Holdings LLC", "Umbro HK Ltd", "NIKE Licenciamentos do Brasil Ltda", "NIKE Servicios de Mexico S. de R.L. de C.V.", "Hurley International", "NIKE Pegasus", "NIKE Russia LLC", "Nike Vision", "NIKE CA LLC", "NIKE Flight", "NIKE India Holding B.V.", "NIKE Hong Kong Ltd", "Umbro International Holdings Ltd", "NIKE South Africa Holdings LLC", "NIKE India Private Ltd", "NIKE Vision Timing & Techlab LP", "NIKE France SAS.", "NIKE 360 Holding B.V.", "NIKE Canada Holding B.V.", "NIKE Vietnam Co", "NIKE Huarache", "Umbro Ltd", "NIKE GmbH", "NIKE Europe Holding B.V.", "NIKE Australia Holding B.V.", "NIKE SALES (MALAYSIA) SDN. BHD.", "NIKE International Holding B.V.", "Umbro Finance Ltd", "NIKE Holding LLC", "NIKE do Brasil Comercio e Participacoes Ltda", "Cole Haan Co Store", "NIKE China Holding HK Ltd", "NIKE International Holding Inc", "American NIKE SL", "Umbro JV Ltd", "NIKE Cortez", "NIKE Dunk Holding B.V.", "NIKE Retail Services Inc", "Umbro Schweiz Ltd", "Umbro Sportwear Ltd"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.children -> Charles Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> common.topic.article -> m.03mpv\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.gender -> Male\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> people.person.profession -> Politician\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> celebrities.celebrity.sexual_relationships -> m.0j7fcl_ -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin -> people.person.spouse_s -> m.0c1h2r7\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAndrew Johnson"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-630", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language"], "ground_truth": ["Wayuu Language", "Macuna Language", "P\u00e1ez language", "Piapoco Language", "Playero language", "Siona Language", "Inga, Jungle Language", "Piaroa Language", "Tunebo, Angosturas Language", "Achawa language", "Tucano Language", "Providencia Sign Language", "Omejes Language", "Waimaj\u00e3 Language", "Koreguaje Language", "Cof\u00e1n Language", "Macagu\u00e1n Language", "Cumeral Language", "Guahibo language", "Guambiano Language", "Cams\u00e1 Language", "Minica Huitoto", "Hupd\u00eb Language", "Carijona Language", "Baudo language", "Arhuaco Language", "Cuiba language", "Nukak language", "Piratapuyo Language", "Cabiyar\u00ed Language", "Puinave Language", "Spanish Language", "Awa-Cuaiquer Language", "Totoro Language", "Cagua Language", "Andoque Language", "Uwa language", "Romani, Vlax Language", "Siriano Language", "Tunebo, Barro Negro Language", "Murui Huitoto language", "Coxima Language", "Bora Language", "Curripaco Language", "Ponares Language", "Tanimuca-Retuar\u00e3 Language", "Ticuna language", "Runa Language", "Islander Creole English", "Kuna, Border Language", "Tunebo, Central Language", "Tuyuca language", "Palenquero Language", "Carabayo Language", "Bar\u00ed Language", "Natagaimas Language", "Ember\u00e1, Northern Language", "Tunebo, Western Language", "Tomedes Language", "Tinigua language", "Yucuna Language", "Macaguaje Language", "Inga Language", "Yukpa Language", "Andaqui Language", "Tama Language", "Barasana Language", "Ocaina Language", "Quechua, Napo Lowland Language", "Kogi Language", "Catio language", "Anserma Language", "Cocama language", "Cubeo Language", "Nonuya language", "Malayo Language", "Guanano Language", "Desano Language", "Nheengatu language", "Coyaima Language", "S\u00e1liba Language", "Chipiajes Language", "Pijao Language", "Muinane Language", "Guayabero Language", "Colombian Sign Language"], "ans_acc": 0.011627906976744186, "ans_hit": 1, "ans_f1": 0.022988505747126436, "ans_precission": 1.0, "ans_recall": 0.011627906976744186, "path_f1": 0.02316602316602317, "path_precision": 0.75, "path_recall": 0.011764705882352941, "path_ans_f1": 0.022988505747126436, "path_ans_precision": 1.0, "path_ans_recall": 0.011627906976744186}
{"id": "WebQTest-631", "prediction": ["# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.season -> 2006\u201307 NBA season\n# Answer:\n2006\u201307 NBA season", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qcnpp -> basketball.basketball_player_stats.season -> 2002\u201303 NBA season\n# Answer:\n2002\u201303 NBA season", "# Reasoning Path:\nRay Allen -> people.person.sibling_s -> m.0j2h3vr -> people.sibling_relationship.sibling -> Kristie Raye\n# Answer:\nKristie Raye", "# Reasoning Path:\nRay Allen -> people.person.children -> Wynstan Ryan Allen -> common.topic.notable_for -> g.1256c2b0z\n# Answer:\nWynstan Ryan Allen", "# Reasoning Path:\nRay Allen -> people.person.education -> m.0bmyybb -> education.education.institution -> University of Connecticut\n# Answer:\nUniversity of Connecticut", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qbrjm -> basketball.basketball_player_stats.team -> Seattle Supersonics\n# Answer:\nSeattle Supersonics", "# Reasoning Path:\nRay Allen -> basketball.basketball_player.player_statistics -> m.04qdv8z -> basketball.basketball_player_stats.season -> 1997\u201398 NBA season\n# Answer:\n1997\u201398 NBA season", "# Reasoning Path:\nRay Allen -> sports.pro_athlete.teams -> m.05cf4bc -> sports.sports_team_roster.team -> Connecticut Huskies men's basketball\n# Answer:\nConnecticut Huskies men's basketball"], "ground_truth": ["1996"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-633", "prediction": ["# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.notable_types -> Profession\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Voice Actor -> people.profession.specialization_of -> Actor\n# Answer:\nVoice Actor", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subject_of -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Michael Palance\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Television producer -> common.topic.subjects -> Billy Sorrentino\n# Answer:\nTelevision producer", "# Reasoning Path:\nClaire Danes -> people.person.profession -> Actor -> common.topic.subjects -> Michael Palance\n# Answer:\nActor"], "ground_truth": ["Voice Actor", "Actor", "Television producer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-634", "prediction": ["# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> United States of America\n# Answer:\nComal County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.location.containedby -> Texas\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.hud_foreclosure_area.ofheo_price_change -> m.07cs268\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Bexar County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nBexar County", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> Area code 210 -> location.location.containedby -> Texas\n# Answer:\nArea code 210", "# Reasoning Path:\nSan Antonio -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Antonio -> location.hud_county_place.county -> Comal County -> location.location.containedby -> San Antonio, TX Metropolitan Statistical Area\n# Answer:\nComal County"], "ground_truth": ["Comal County", "Bexar County", "Medina County"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7058823529411765, "ans_precission": 0.75, "ans_recall": 0.6666666666666666, "path_f1": 0.4, "path_precision": 0.5, "path_recall": 0.3333333333333333, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.75, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-635", "prediction": ["# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.character -> Wizard of Oz\n# Answer:\nWizard of Oz", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.02t9_gm -> film.performance.character -> Dave the Lighting Guy\n# Answer:\nDave the Lighting Guy", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman -> fictional_universe.fictional_character.character_created_by -> Trey Parker\n# Answer:\nEric Cartman", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvx -> film.performance.film -> South Park: Bigger, Longer & Uncut\n# Answer:\nSouth Park: Bigger, Longer & Uncut", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bmzsb6 -> film.performance.character -> Kyle / Kenny / Jesus\n# Answer:\nKyle / Kenny / Jesus", "# Reasoning Path:\nMatt Stone -> fictional_universe.fictional_character_creator.fictional_characters_created -> Eric Cartman -> fictional_universe.fictional_character.parents -> Ms. Cartman\n# Answer:\nEric Cartman", "# Reasoning Path:\nMatt Stone -> film.actor.film -> m.0bthgvn -> film.performance.character -> Kyle Broflovski\n# Answer:\nKyle Broflovski"], "ground_truth": ["Ethan Hawke", "Terrance Henry Stoot", "Hermaphrodite", "Danny Glover", "Jimbo Kearn", "Kenny McCormick", "Chris", "Doug Remer", "Kyle Broflovski", "Bill Gates", "Saddam Hussein", "Dave the Lighting Guy", "Ticket Taker", "Wizard of Oz", "Taper Guy", "Kyle / Kenny / Jesus", "George Clooney", "Former Columbine Student", "Gerald Broflovski", "James Humphries", "Man toasting drinks at MCA outdoor bar"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.27586206896551724, "ans_precission": 0.5, "ans_recall": 0.19047619047619047, "path_f1": 0.1875, "path_precision": 0.375, "path_recall": 0.125, "path_ans_f1": 0.27586206896551724, "path_ans_precision": 0.5, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-636", "prediction": ["# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_begins -> Second Sunday in March\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Eastern Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nMichigan -> location.location.time_zones -> UTC\u221205:00 -> common.topic.notable_types -> Time Zone\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nMichigan -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone"], "ground_truth": ["Eastern Time Zone", "UTC\u221205:00", "Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-637", "prediction": ["# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.article -> m.0glnt1\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> common.topic.notable_for -> g.1259zl217\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nSoviet Union -> location.country.form_of_government -> Parliamentary republic -> common.topic.image -> Form of government parliamentary\n# Answer:\nParliamentary republic"], "ground_truth": ["Parliamentary republic", "Socialist state", "Soviet democracy", "Communist state"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-638", "prediction": ["# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> United Kingdom\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Australia\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.main_country -> United States of America\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Canada\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> Japanese Language -> language.human_language.region -> Asia\n# Answer:\nJapanese Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> China\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.languages_spoken -> English Language -> language.human_language.countries_spoken_in -> Guam\n# Answer:\nEnglish Language", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Esperanto Language", "English Language", "Ainu Language", "Korean Language", "Japanese Language"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5333333333333333, "ans_precission": 0.8, "ans_recall": 0.4, "path_f1": 0.5333333333333333, "path_precision": 0.8, "path_recall": 0.4, "path_ans_f1": 0.5333333333333333, "path_ans_precision": 0.8, "path_ans_recall": 0.4}
{"id": "WebQTest-639", "prediction": ["# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.team -> Detroit Pistons\n# Answer:\nDetroit Pistons", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.team -> Chicago Bulls\n# Answer:\nChicago Bulls", "# Reasoning Path:\nDennis Rodman -> sports.sports_award_winner.awards -> m.02kbc93 -> sports.sports_award.season -> 1990\u201391 NBA season\n# Answer:\n1990\u201391 NBA season", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qx2_s -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDennis Rodman -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwx1g -> sports.sports_team_roster.position -> Power forward\n# Answer:\nPower forward", "# Reasoning Path:\nDennis Rodman -> sports.sports_award_winner.awards -> m.02kbcp3 -> sports.sports_award.season -> 1989\u201390 NBA season\n# Answer:\n1989\u201390 NBA season", "# Reasoning Path:\nDennis Rodman -> sports.pro_athlete.teams -> m.0_qwyt6 -> sports.sports_team_roster.team -> San Antonio Spurs\n# Answer:\nSan Antonio Spurs"], "ground_truth": ["1995"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nFezziwig", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.nominated_for -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bob Cratchit -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nBob Cratchit", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.award_nominee -> Orson Welles\n# Answer:\nOrson Welles", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist -> theater.theater_character.plays_appears_in -> Oliver!\n# Answer:\nOliver Twist"], "ground_truth": ["A Tale of Two Cities (Soundings)", "A Christmas Carol (Soundings)", "Great expectations", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Saddleback Classics)", "Great Expectations.", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "The Old Curiosity Shop", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Nelson Graded Readers)", "Bleak House.", "A Christmas Carol (Classic Fiction)", "Our mutual friend", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "Martin Chuzzlewit", "A Christmas Carol (Children's Theatre Playscript)", "Dombey and Son", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "The Pickwick Papers", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "Bleak House", "A Tale of Two Cities (Penguin Classics)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Saddleback Classics)", "David Copperfield.", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Puffin Classics)", "David Copperfield", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Dodo Press)", "Sketches by Boz", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Gollancz Children's Classics)", "Oliver Twist", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (R)", "A Tale of Two Cities (Longman Classics, Stage 2)", "Hard times", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Tor Classics)", "A Christmas Carol (Puffin Choice)", "Great Expectations", "Dombey and Son.", "The mystery of Edwin Drood", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "The Pickwick papers", "Little Dorrit", "A Tale of Two Cities (Wordsworth Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Classic Literature with Classical Music)", "The old curiosity shop", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Cover to Cover)", "Our mutual friend.", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Large Print Edition)", "The Mystery of Edwin Drood", "The life and adventures of Nicholas Nickleby", "The old curiosity shop.", "The cricket on the hearth", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Everyman's Library (Paper))", "A CHRISTMAS CAROL", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Enriched Classics)", "Great expectations.", "A Christmas Carol (Reissue)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Whole Story)", "A Tale of Two Cities (Silver Classics)", "Bleak house", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Thornes Classic Novels)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Longman Fiction)", "Dombey and son", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Bookcassette(r) Edition)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.03314917127071824, "ans_precission": 0.25, "ans_recall": 0.01775147928994083, "path_f1": 0.4556962025316456, "path_precision": 0.75, "path_recall": 0.32727272727272727, "path_ans_f1": 0.034682080924855495, "path_ans_precision": 0.75, "path_ans_recall": 0.01775147928994083}
{"id": "WebQTest-640", "prediction": ["# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_types -> Profession\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> common.topic.notable_for -> g.1254xhyd0\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Tentmaker -> people.profession.specialization_of -> Blue-collar worker\n# Answer:\nTentmaker", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> media_common.quotation_subject.quotations_about_this_subject -> As each Sister is to become a Co-Worker of Christ in the slums, each ought to understand what God and the Missionaries of Charity expect from her. Let Christ radiate and live his life in her and through her in the slums. Let the poor, seeing her, be drawn to Christ and invite him to enter their homes and their lives. Let the sick and suffering find in her a real angel of comfort and consolation. Let the little ones of the streets cling to her because she reminds them of him, the friend of the little ones.\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngj4q\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Missionary -> base.schemastaging.context_name.pronunciation -> g.125_p_yp_\n# Answer:\nMissionary", "# Reasoning Path:\nPaul the Apostle -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nPaul the Apostle -> people.person.profession -> Prophet -> fictional_universe.character_occupation.characters_with_this_occupation -> Usutu\n# Answer:\nProphet"], "ground_truth": ["Writer", "Prophet", "Missionary", "Tentmaker"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.8076923076923077, "path_precision": 0.875, "path_recall": 0.75, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-642", "prediction": ["# Reasoning Path:\nVin Scully -> people.person.education -> m.02wnq86 -> education.education.institution -> Fordham University\n# Answer:\nFordham University", "# Reasoning Path:\nVin Scully -> people.person.education -> m.0svtrlm -> education.education.institution -> Fordham Preparatory School\n# Answer:\nFordham Preparatory School", "# Reasoning Path:\nVin Scully -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nVin Scully -> common.topic.image -> Scully pub -> common.image.size -> m.029sr_k\n# Answer:\nScully pub", "# Reasoning Path:\nVin Scully -> tv.tv_personality.tv_regular_appearances -> m.0gl26pg -> tv.tv_regular_personal_appearance.program -> It Takes Two\n# Answer:\nIt Takes Two", "# Reasoning Path:\nVin Scully -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nVin Scully -> film.actor.film -> m.0y5d62l -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nVin Scully -> common.topic.image -> VinScully0308 -> common.image.appears_in_topic_gallery -> Los Angeles Dodgers Radio Network\n# Answer:\nVinScully0308"], "ground_truth": ["Fordham University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-643", "prediction": ["# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.place_of_birth -> g.120mc0gk\n# Answer:\nfictional_universe.fictional_character.place_of_birth", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.setting_type -> Planet\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> common.topic.notable_for -> g.1254xtwfh\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Breha Organa\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> common.topic.article -> m.065lj5\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.universe -> Star Wars Expanded Universe\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> fictional_universe.fictional_character.places_lived -> Alderaan -> fictional_universe.fictional_setting.characters_that_have_lived_here -> g.1237vxbv\n# Answer:\nAlderaan", "# Reasoning Path:\nLeia Organa -> tv.tv_character.appeared_in_tv_program -> m.0h1255z -> tv.regular_tv_appearance.actor -> Amy Smart\n# Answer:\nAmy Smart", "# Reasoning Path:\nLeia Organa -> film.film_character.portrayed_in_films -> m.0wc74kc -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens"], "ground_truth": ["Alderaan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-644", "prediction": ["# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Maximiliano Hern\u00e1ndez Mart\u00ednez\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Osm\u00edn Aguirre y Salinas\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.notable_for -> g.125btm68w\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6c_pzzl\n# Answer:\nlocation.statistical_region.poverty_rate_2dollars_per_day", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Colombia\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.statistical_region.external_debt_stock -> g.11b71xxt46\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Andr\u00e9s Eduardo Men\u00e9ndez", "Elena Diaz", "Doroteo Vasconcelos", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Rub\u00e9n Zamora", "Armando Chac\u00f3n", "Jose Orlando Martinez", "William Renderos Iraheta", "Victor Manuel Ochoa", "Joel Aguilar", "Francisca Gonz\u00e1lez", "Fausto Omar V\u00e1squez", "Johnny Lopez", "Saturnino Osorio", "Juan Rafael Bustillo", "Xenia Estrada", "Gerardo Barrios", "Emilio Guardado", "DJ Quest", "Rene Moran", "Enrique \u00c1lvarez C\u00f3rdova", "F\u00e9lix Pineda", "Jos\u00e9 Inocencio Alas", "Salvador Castaneda Castro", "Rafael Menj\u00edvar Ochoa", "V\u00edctor Ram\u00edrez", "Arturo Rivera y Damas", "Sarah Ramos", "Mario Wilfredo Contreras", "Eva Dimas", "Pedro Geoffroy Rivas", "Roberto Rivas", "Francisco Funes", "Jose Solis", "Melvin Barrera", "Richard Oriani", "Damaris Qu\u00e9les", "Nicolas F. Shi", "Roberto Carlos Martinez", "Francisco Due\u00f1as", "Jos\u00e9 Castellanos Contreras", "Carlos Barrios", "Selvin Gonz\u00e1lez", "Eduardo Hern\u00e1ndez", "Jos\u00e9 Francisco Valiente", "Mauricio Alfaro", "\u00c1ngel Orellana", "Am\u00e9rico Gonz\u00e1lez", "Guillermo Garc\u00eda", "Mario Montoya", "Jos\u00e9 Luis Rugamas", "Papa A.P.", "Erwin McManus", "Miguel Angel Deras", "Keoki", "Norman Quijano", "Victor Lopez", "Edwin Ramos", "William Armando", "Bobby Rivas", "Robert Renderos", "Rafael Campo", "Jorge Rivera", "Isa\u00edas Choto", "Miguel Cruz", "\u00d3scar Antonio Ulloa", "Diego Vel\u00e1zquez", "Julio Adalberto Rivera Carballo", "Carlos Linares", "Ana Sol Gutierrez", "Jaime Portillo", "Alfredo Ruano", "Rutilio Grande", "Tom\u00e1s Medina", "Prudencia Ayala", "g.11b8058v7j", "Laura Molina", "Ana Maria de Martinez", "Mauricio Alonso Rodr\u00edguez", "Gualberto Fern\u00e1ndez", "Genaro Serme\u00f1o", "Ricardo L\u00f3pez Tenorio", "Paula Heredia", "Jose B. Gonzalez", "Malin Arvidsson", "Pedro Jos\u00e9 Escal\u00f3n", "Bernard Lewinsky", "Francisco Gavidia", "Steve Montenegro", "Claudia Lars", "Miguel Ca\u00f1izalez", "Mauricio Alvarenga", "Alexander M\u00e9ndoza", "Ernesto Aparicio", "Consuelo de Saint Exup\u00e9ry", "Ra\u00fal Cicero", "Patricia Chica", "Takeshi Fujiwara", "Pedro Chavarria", "Juan Ram\u00f3n S\u00e1nchez", "Manuel Enrique Araujo", "Wilfredo Iraheta", "Camilo Minero", "Alexander Campos", "Ruben Cedillos", "Jorge Mel\u00e9ndez", "Elmer Acevedo", "Francisco Men\u00e9ndez", "Jorge B\u00facaro", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Jos\u00e9 Manfredi Portillo", "Marlon Menj\u00edvar", "William L\u00f3pez", "Milton Palacios", "Arturo Armando Molina", "Santiago \\\"Jimmy\\\" Mellado"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-645", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.corresponding_type -> Visual Artist\n# Answer:\nSculptor", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.specialization_of -> Artist\n# Answer:\nSculptor", "# Reasoning Path:\nLeonardo da Vinci -> book.book_subject.works -> Leonardo Da Vinci -> common.topic.notable_for -> g.1255j1dx2\n# Answer:\nLeonardo Da Vinci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nPainting"], "ground_truth": ["Drawing", "Sculpture", "Painting"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-646", "prediction": ["# Reasoning Path:\nMary Bell -> common.topic.notable_for -> g.125bm4y8j\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> base.aareas.schema.administrative_area.administrative_area_type -> English metropolitan borough city\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> common.topic.article -> m.01j32d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> periodicals.newspaper_circulation_area.newspapers -> The Courier\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> United Kingdom\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.geolocation -> m.0239t8w\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Woolsington\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.contains -> Basil's Kitchen\n# Answer:\nNewcastle upon Tyne", "# Reasoning Path:\nMary Bell -> book.book_subject.works -> Cries Unheard: Why Children Kill - The Story of Mary Bell -> book.book.genre -> Non-fiction\n# Answer:\nCries Unheard: Why Children Kill - The Story of Mary Bell", "# Reasoning Path:\nMary Bell -> people.person.place_of_birth -> Newcastle upon Tyne -> location.location.containedby -> Tyne and Wear\n# Answer:\nNewcastle upon Tyne"], "ground_truth": ["Newcastle upon Tyne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-648", "prediction": ["# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.spouse -> Brad Paisley\n# Answer:\nBrad Paisley", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.location_of_ceremony -> Pepperdine University\n# Answer:\nPepperdine University", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.article -> m.05d5ml\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nKimberly Williams-Paisley -> people.person.spouse_s -> m.02kkn0v -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.062tr2x -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nKimberly Williams-Paisley -> tv.tv_actor.starring_roles -> m.0cg67sd -> tv.regular_tv_appearance.series -> The 10th Kingdom\n# Answer:\nThe 10th Kingdom", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.062tr2x -> common.webpage.resource -> m.0h24qs_\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nKimberly Williams-Paisley -> common.topic.webpage -> m.0948l9p -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Brad Paisley"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1111111111111111, "path_recall": 0.25, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-649", "prediction": ["# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nCrazy, Stupid, Love.", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn605r -> award.award_nomination.nominated_for -> The Ides of March\n# Answer:\nThe Ides of March", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.nominated_for -> Crazy, Stupid, Love.\n# Answer:\nCrazy, Stupid, Love.", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.ceremony -> 69th Golden Globe Awards\n# Answer:\n69th Golden Globe Awards", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn5tnq -> award.award_nomination.nominated_for -> Drive\n# Answer:\nDrive", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hnjpjf -> award.award_nomination.award -> Golden Globe Award for Best Actor \u2013 Motion Picture Musical or Comedy\n# Answer:\nGolden Globe Award for Best Actor \u2013 Motion Picture Musical or Comedy", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0hn605r -> award.award_nomination.award -> Critics' Choice Movie Award for Best Acting Ensemble\n# Answer:\nCritics' Choice Movie Award for Best Acting Ensemble", "# Reasoning Path:\nRyan Gosling -> award.award_nominee.award_nominations -> m.0z83ls_ -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actor: Comedy\n# Answer:\nTeen Choice Award for Choice Movie Actor: Comedy"], "ground_truth": ["The Slaughter Rule", "The Nice Guys", "Drunk History Christmas - Feat. Ryan Gosling, Jim Carrey & Eva Mendes", "The Ides of March", "The Big Short", "Nothing Too Good for a Cowboy", "Crazy, Stupid, Love.", "I'm Still Here: Real Diaries of Young People Who Lived During the Holocaust", "Touch of Evil", "Blue Valentine", "Stay", "The United States of Leland", "Half Nelson", "The Believer", "Murder by Numbers", "Gangster Squad", "Fracture", "All Good Things", "Weightless", "The Place Beyond the Pines", "Lars and the Real Girl", "The Unbelievables", "Remember the Titans", "Drive", "Logan's Run", "The Notebook", "Only God Forgives"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.5, "ans_recall": 0.1111111111111111, "path_f1": 0.0898876404494382, "path_precision": 0.5, "path_recall": 0.04938271604938271, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.5, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_conflict -> Cuban Missile Crisis\n# Answer:\nCuban Missile Crisis", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0dl79fr -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.armed_forces -> Red Army -> business.employer.employees -> m.02wh5kd\n# Answer:\nRed Army", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-650", "prediction": ["# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.046vpjr -> government.political_party_tenure.party -> Australian Labor Party\n# Answer:\nAustralian Labor Party", "# Reasoning Path:\nJulia Gillard -> government.politician.party -> m.0lr1xnw -> government.political_party_tenure.party -> Queensland Labor Party\n# Answer:\nQueensland Labor Party", "# Reasoning Path:\nJulia Gillard -> common.topic.article -> m.02kx9y\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.deceased_person.place_of_death -> Adelaide\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.children -> Alison Gillard\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.children -> Alison Gillard\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> John Gillard -> people.person.profession -> Registered nurse\n# Answer:\nJohn Gillard", "# Reasoning Path:\nJulia Gillard -> people.person.parents -> Moira Gillard -> people.person.nationality -> Australia\n# Answer:\nMoira Gillard", "# Reasoning Path:\nJulia Gillard -> government.politician.government_positions_held -> m.046vpj_ -> government.government_position_held.governmental_body -> Australian House of Representatives\n# Answer:\nAustralian House of Representatives"], "ground_truth": ["Queensland Labor Party", "Australian Labor Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-651", "prediction": ["# Reasoning Path:\nGermany -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v9zy7\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.government_debt_percent_gdp -> g.1245_38m_\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nGermany -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdl8\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_containedby -> Czech Republic\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_contained_by -> m.0wg8r4c\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava", "# Reasoning Path:\nGermany -> location.location.partially_contains -> Euregio Bayerischer Wald-B\u00f6hmerwald-Sumava -> location.location.partially_containedby -> Austria\n# Answer:\nEuregio Bayerischer Wald-B\u00f6hmerwald-Sumava"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-652", "prediction": ["# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> common.topic.notable_types -> Ethnicity\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> music.artist.album -> g.1hc0gf71l\n# Answer:\nmusic.artist.album", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Alex Russo\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> award.award_nominee.award_nominations -> m.010wr51v -> award.award_nomination.award -> Teen Choice Award for Choice Smile\n# Answer:\nTeen Choice Award for Choice Smile", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> people.ethnicity.geographic_distribution -> Mexico\n# Answer:\nMexicans", "# Reasoning Path:\nSelena Gomez -> people.person.ethnicity -> Mexicans -> common.topic.notable_types -> Ethnicity\n# Answer:\nMexicans"], "ground_truth": ["Italian American", "Mexicans"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-653", "prediction": ["# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.spouse -> Lee Mead\n# Answer:\nLee Mead", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nDenise van Outen -> people.person.spouse_s -> m.062thbp -> people.marriage.location_of_ceremony -> Seychelles\n# Answer:\nSeychelles", "# Reasoning Path:\nDenise van Outen -> people.person.children -> Betsy Mead -> people.person.parents -> Lee Mead\n# Answer:\nBetsy Mead", "# Reasoning Path:\nDenise van Outen -> people.person.children -> Betsy Mead -> common.topic.notable_for -> g.1259bfryz\n# Answer:\nBetsy Mead", "# Reasoning Path:\nDenise van Outen -> music.artist.track -> Tyler King -> music.recording.releases -> Tell Me on a Sunday\n# Answer:\nTyler King", "# Reasoning Path:\nDenise van Outen -> people.person.children -> Betsy Mead -> people.person.gender -> Female\n# Answer:\nBetsy Mead", "# Reasoning Path:\nDenise van Outen -> base.popstra.celebrity.dated -> m.065pw0h -> base.popstra.dated.participant -> Nick Moran\n# Answer:\nNick Moran"], "ground_truth": ["Lee Mead"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-654", "prediction": ["# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_children -> Al Sharqia Governorate -> location.location.containedby -> North Africa\n# Answer:\nAl Sharqia Governorate", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_children -> Alexandria Governorate -> location.location.containedby -> North Africa\n# Answer:\nAlexandria Governorate", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> base.aareas.schema.administrative_area.administrative_children -> Suez Governorate -> base.aareas.schema.administrative_area.administrative_area_type -> Egyptian governorate\n# Answer:\nSuez Governorate", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> common.topic.notable_types -> Governmental Body\n# Answer:\nHouse of Representatives"], "ground_truth": ["Egyptian pound"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-655", "prediction": ["# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_types -> Postal Code\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> common.topic.notable_for -> g.1256qhz91\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.geolocation -> m.03ds4y_\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49519 -> location.postal_code.country -> United States of America\n# Answer:\n49519", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11b674hfwb\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> common.topic.notable_for -> g.125dngrry\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> location.location.containedby -> Kent County\n# Answer:\n49509", "# Reasoning Path:\nWyoming -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Michigan\n# Answer:\nUnited States of America", "# Reasoning Path:\nWyoming -> location.statistical_region.population -> g.11bymkw_85\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nWyoming -> location.citytown.postal_codes -> 49509 -> common.topic.notable_types -> Postal Code\n# Answer:\n49509"], "ground_truth": ["49509", "49519"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-656", "prediction": ["# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0j5qnx9 -> government.government_position_held.office_holder -> Laura Chinchilla\n# Answer:\nLaura Chinchilla", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6lmd -> government.government_position_held.office_holder -> \u00d3scar Arias\n# Answer:\n\u00d3scar Arias", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6ln4 -> government.government_position_held.office_holder -> Abel Pacheco\n# Answer:\nAbel Pacheco", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.010g6lnp -> government.government_position_held.office_holder -> Miguel \u00c1ngel Rodr\u00edguez\n# Answer:\nMiguel \u00c1ngel Rodr\u00edguez", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCosta Rica -> government.governmental_jurisdiction.governing_officials -> m.0105mtsf -> government.government_position_held.office_position_or_title -> President of Costa Rica\n# Answer:\nPresident of Costa Rica"], "ground_truth": ["Laura Chinchilla"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-657", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Atlanta\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Thomas Nkobi\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activists -> Robert F. Kennedy\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.nobelprizes.nobel_subject_area.nobel_awards -> m.063k0hx\n# Answer:\nAnti-Apartheid"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-658", "prediction": ["# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> people.person.parents -> Salome\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> common.topic.notable_for -> g.125brlxm_\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Anglican Church of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.notable_types -> Author\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> common.topic.image -> John-the-evangelist-francesco-furini\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> book.written_work.subjects -> Jesus Christ -> people.person.ethnicity -> Jewish people\n# Answer:\nJesus Christ", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Cathedral of St. John the Evangelist\n# Answer:\nJohn the Evangelist", "# Reasoning Path:\nGospel of John -> media_common.adapted_work.adaptations -> The Gospel of John -> film.film.written_by -> John Goldsmith\n# Answer:\nThe Gospel of John", "# Reasoning Path:\nGospel of John -> book.written_work.author -> John the Evangelist -> symbols.name_source.namesakes -> Church of St John the Evangelist, Poulton-le-Fylde\n# Answer:\nJohn the Evangelist"], "ground_truth": ["John the Evangelist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-659", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Wales\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Republic of Ireland\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Anguilla\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Antigua and Barbuda\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Sierra Leone\n# Answer:\nUnitary state", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nGary Glitter"], "ground_truth": ["Constitutional monarchy", "Unitary state", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> United States of America\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> common.topic.article -> m.09nqr\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> British Virgin Islands\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Caribbean special municipalities of the Netherlands\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\nlocation.statistical_region.gdp_deflator_change"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-661", "prediction": ["# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.team -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.04mb0rc -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder", "# Reasoning Path:\nSamir Nasri -> base.schemastaging.athlete_extra.salary -> m.0j6tdy1 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.team -> France national football team\n# Answer:\nFrance national football team", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j6tdwm -> sports.sports_team_roster.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSamir Nasri -> soccer.football_player.statistics -> m.0w8yfb2 -> soccer.football_player_stats.team -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nSamir Nasri -> sports.pro_athlete.teams -> m.0j52lrl -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder"], "ground_truth": ["Olympique de Marseille"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-662", "prediction": ["# Reasoning Path:\nCzech Republic -> location.country.currency_used -> Czech koruna -> common.topic.notable_types -> Currency\n# Answer:\nCzech koruna", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Czechoslovakia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.statistical_region.internet_users_percent_population -> g.11b60nhcd0\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_containedby -> Ukraine\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> common.topic.notable_types -> Human Language\n# Answer:\nCzech Language", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> Beskids -> location.location.partially_contained_by -> m.0wg8sdh\n# Answer:\nBeskids", "# Reasoning Path:\nCzech Republic -> location.location.partially_contains -> B\u00f3br -> geography.river.mouth -> Oder\n# Answer:\nB\u00f3br", "# Reasoning Path:\nCzech Republic -> location.country.official_language -> Czech Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nCzech Language"], "ground_truth": ["Czech koruna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-663", "prediction": ["# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> location.location.containedby -> England\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> location.location.containedby -> United Kingdom\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j4kvc6 -> people.marriage.spouse -> Hannah Callowhill Penn\n# Answer:\nHannah Callowhill Penn", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> symbols.name_source.namesakes -> William Penn High School -> common.topic.notable_types -> School\n# Answer:\nWilliam Penn High School", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Joan Gilbert\n# Answer:\nJoan Gilbert", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> Bristol -> location.administrative_division.country -> United Kingdom\n# Answer:\nBristol", "# Reasoning Path:\nWilliam Penn -> people.person.place_of_birth -> London -> location.location.containedby -> United Kingdom\n# Answer:\nLondon", "# Reasoning Path:\nWilliam Penn -> people.person.spouse_s -> m.0j6bdtn -> people.marriage.spouse -> Gulielma Maria Springett\n# Answer:\nGulielma Maria Springett", "# Reasoning Path:\nWilliam Penn -> people.person.parents -> Giles Penn\n# Answer:\nGiles Penn"], "ground_truth": ["1672-04-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-664", "prediction": ["# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> base.ontologies.ontology_instance.equivalent_instances -> m.07ng00y\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> type.type.expected_by -> Works on web browser\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.languages_used -> Cascading Style Sheets -> common.topic.subject_of -> Edward Beckett\n# Answer:\nCascading Style Sheets", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> computer.software_genre.subgenres -> Text-based web browser\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> Web browser -> cvg.cvg_platform.games -> Happy Wheels\n# Answer:\nWeb browser", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Zotero -> computer.software.license -> Affero General Public License\n# Answer:\nZotero", "# Reasoning Path:\nFirefox -> computer.web_browser.extensions -> Zotero -> common.topic.article -> m.026lvyb\n# Answer:\nZotero", "# Reasoning Path:\nFirefox -> computer.software.software_genre -> News aggregator -> internet.website_category.sites -> World News Network\n# Answer:\nNews aggregator"], "ground_truth": ["XUL", "C++", "JavaScript", "XBL", "Cascading Style Sheets"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.19047619047619047, "ans_precission": 0.125, "ans_recall": 0.4, "path_f1": 0.19047619047619047, "path_precision": 0.125, "path_recall": 0.4, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 1.0, "path_ans_recall": 0.4}
{"id": "WebQTest-665", "prediction": ["# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Greece\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> film.film.country -> United States of America\n# Answer:\nYogi Bear", "# Reasoning Path:\nNew Zealand -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Japan\n# Answer:\nParliamentary system", "# Reasoning Path:\nNew Zealand -> location.statistical_region.gni_in_ppp_dollars -> g.1245_22l4\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nNew Zealand -> film.film_location.featured_in_films -> Yogi Bear -> media_common.netflix_title.netflix_genres -> Family\n# Answer:\nYogi Bear"], "ground_truth": ["Constitutional monarchy", "Unitary state", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-666", "prediction": ["# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Americas\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> Western Hemisphere\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> common.topic.article -> m.04zwg3\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.containedby -> DVD Region 4\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.geolocation -> m.0cl_dz2\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> War of the Pacific\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Maricoxi\n# Answer:\nSouth America", "# Reasoning Path:\nIsthmus of Panama -> base.schemastaging.context_name.pronunciation -> g.125_s69r1\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nIsthmus of Panama -> common.topic.image -> Pm-map -> common.image.size -> m.02bh8qh\n# Answer:\nPm-map", "# Reasoning Path:\nIsthmus of Panama -> location.location.containedby -> South America -> location.location.events -> 2009 flu pandemic\n# Answer:\nSouth America"], "ground_truth": ["South America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-667", "prediction": ["# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.containedby -> Ireland\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> location.location.time_zones -> Irish Time Zone\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> common.topic.notable_types -> City/Town/Village\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdmsw\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> organization.organization_scope.organizations_with_this_scope -> Voltaire Diamonds\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s2z\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm09\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.country.capital -> Dublin -> periodicals.newspaper_circulation_area.newspapers -> The Nation\n# Answer:\nDublin", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37x9c\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nRepublic of Ireland -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgs\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Dublin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-668", "prediction": ["# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.nominated_for -> The Twilight Saga: Breaking Dawn - Part 1\n# Answer:\nThe Twilight Saga: Breaking Dawn - Part 1", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0sgl9q8 -> award.award_nomination.nominated_for -> The Twilight Saga: New Moon\n# Answer:\nThe Twilight Saga: New Moon", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.ceremony -> 2012 Teen Choice Awards\n# Answer:\n2012 Teen Choice Awards", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0b3w_ry -> award.award_nomination.nominated_for -> Twilight\n# Answer:\nTwilight", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z87_4g -> award.award_nomination.nominated_for -> Eclipse\n# Answer:\nEclipse", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83n2p -> award.award_nomination.award -> Teen Choice Award for Choice Movie Actress: Romance\n# Answer:\nTeen Choice Award for Choice Movie Actress: Romance", "# Reasoning Path:\nKristen Stewart -> award.award_nominee.award_nominations -> m.0z83xlj -> award.award_nomination.award_nominee -> Robert Pattinson\n# Answer:\nRobert Pattinson"], "ground_truth": ["Speak", "Adventureland", "Still Alice", "The Big Shoe", "Welcome to the Rileys", "Panic Room", "The Twilight Saga: New Moon", "Catch That Kid", "Zathura", "Snow White and the Huntsman 2", "The Messengers", "The Yellow Handkerchief", "Twilight", "The Cake Eaters", "Undertow", "Cold Creek Manor", "Billy Lynn's Long Halftime Walk", "Cutlass", "The Flintstones in Viva Rock Vegas", "Jumper", "Fierce People", "Snow White and the Huntsman", "The Twilight Saga: Breaking Dawn - Part 1", "On the Road", "Equals", "In the Land of Women", "Into the Wild", "The Runaways", "The Safety of Objects", "Camp X-Ray", "The Twilight Saga: Breaking Dawn - Part 2", "Anesthesia", "The Thirteenth Year", "American Ultra", "Clouds of Sils Maria", "Eclipse", "What Just Happened"], "ans_acc": 0.10810810810810811, "ans_hit": 1, "ans_f1": 0.18433179723502305, "ans_precission": 0.625, "ans_recall": 0.10810810810810811, "path_f1": 0.07575757575757576, "path_precision": 0.625, "path_recall": 0.04032258064516129, "path_ans_f1": 0.18433179723502305, "path_ans_precision": 0.625, "path_ans_recall": 0.10810810810810811}
{"id": "WebQTest-669", "prediction": ["# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> people.profession.specializations -> Psychologist\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> base.activism.activist.area_of_activism -> Nazism -> base.activism.activism_issue.supporting_political_parties -> Nazi Party\n# Answer:\nNazism", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A patient going to a doctor for his first visit was asked, And whom did you consult before coming to me? Only the village druggist, was the answer. And what sort of foolish advice did that numbskull give you? asked the doctor, his tone and manner denoting his contempt for the advice of the layman. Oh, replied his patient, with no malice aforethought, he told me to come and see you.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> film.film_subject.films -> The Boys from Brazil -> film.film.subjects -> Racism\n# Answer:\nThe Boys from Brazil", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A skilful leech is better far, than half a hundred men of war.\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> base.descriptive_names.names.descriptive_name -> m.0105bg71\n# Answer:\nPhysician", "# Reasoning Path:\nJosef Mengele -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A surgeon should be young a physician old.\n# Answer:\nPhysician"], "ground_truth": ["Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Cholecalciferol\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Dementia\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Epilepsy\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.disease_cause.diseases -> Head pressing\n# Answer:\nBrain tumor"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-670", "prediction": ["# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.region -> Africa\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.parent -> Arabic Group\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.writing_system -> Arabic alphabet\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Arabic, Sudanese Spoken Language -> language.human_language.language_family -> Afroasiatic languages\n# Answer:\nArabic, Sudanese Spoken Language", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.languages_spoken -> Nobiin Language -> common.topic.notable_types -> Human Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Modern Standard Arabic"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-671", "prediction": ["# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.0jsjnnj -> government.government_position_held.office_holder -> Neil Abercrombie\n# Answer:\nNeil Abercrombie", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.010dzk7k -> government.government_position_held.office_holder -> Shan Tsutsui\n# Answer:\nShan Tsutsui", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030qq8b -> government.government_position_held.office_holder -> Daniel Kahikina Akaka\n# Answer:\nDaniel Kahikina Akaka", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.0jsjnnj -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.010hbwx9 -> government.government_position_held.office_holder -> Brian Schatz\n# Answer:\nBrian Schatz", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.0jsjnnj -> government.government_position_held.office_position_or_title -> Governor of Hawaii\n# Answer:\nGovernor of Hawaii", "# Reasoning Path:\nHawaii -> government.governmental_jurisdiction.governing_officials -> m.010r0b91 -> government.government_position_held.office_holder -> David M. Louie\n# Answer:\nDavid M. Louie", "# Reasoning Path:\nHawaii -> government.political_district.representatives -> m.030r1fw -> government.government_position_held.office_holder -> Daniel Inouye\n# Answer:\nDaniel Inouye"], "ground_truth": ["Mazie Hirono"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-672", "prediction": ["# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Vietnam\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.notable_types -> Form of Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.image -> Socialist states all\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f719\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Venezuela\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> common.topic.notable_for -> g.1259zl217\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.statistical_region.electricity_consumption_per_capita -> g.1245_0sg2\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Socialist state -> government.form_of_government.countries -> Azerbaijan People's Government\n# Answer:\nSocialist state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Single-party state -> base.schemastaging.context_name.pronunciation -> g.125_klj_x\n# Answer:\nSingle-party state", "# Reasoning Path:\nNorth Korea -> location.country.form_of_government -> Juche -> government.political_ideology.political_parties -> Anti-Imperialist National Democratic Front\n# Answer:\nJuche"], "ground_truth": ["Socialist state", "Juche", "Single-party state"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-673", "prediction": ["# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.04hwvgz -> education.education.institution -> University of San Diego\n# Answer:\nUniversity of San Diego", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0646hdy -> education.education.institution -> University of Southern California\n# Answer:\nUniversity of Southern California", "# Reasoning Path:\nRobert Kardashian -> people.person.education -> m.0glw5zj -> education.education.institution -> University of San Diego School of Law\n# Answer:\nUniversity of San Diego School of Law", "# Reasoning Path:\nRobert Kardashian -> common.topic.notable_types -> Deceased Person -> common.topic.article -> m.022w789\n# Answer:\nDeceased Person", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0100z128\n# Answer:\nBusinessperson", "# Reasoning Path:\nRobert Kardashian -> people.person.children -> Kourtney Kardashian -> people.person.education -> m.04m5b4g\n# Answer:\nKourtney Kardashian", "# Reasoning Path:\nRobert Kardashian -> common.topic.notable_types -> Deceased Person -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nDeceased Person", "# Reasoning Path:\nRobert Kardashian -> people.person.profession -> Businessperson -> base.descriptive_names.names.descriptive_name -> m.0105_bmw\n# Answer:\nBusinessperson"], "ground_truth": ["University of San Diego", "University of Southern California", "University of San Diego School of Law"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-675", "prediction": ["# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Florida -> location.location.containedby -> United States of America\n# Answer:\nFlorida", "# Reasoning Path:\nSt. James City -> location.location.containedby -> Lee County -> location.location.containedby -> Florida\n# Answer:\nLee County", "# Reasoning Path:\nSt. James City -> location.statistical_region.population -> m.04lv8bl\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07jqpx9\n# Answer:\nlocation.hud_foreclosure_area.bls_unemployment_rate", "# Reasoning Path:\nSt. James City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Florida\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. James City -> location.hud_county_place.county -> Lee County -> location.location.containedby -> South Florida water management district\n# Answer:\nLee County"], "ground_truth": ["Lee County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-676", "prediction": ["# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Eurasia\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> location.location.geolocation -> m.0wmpxx7\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.location.containedby -> Western Asia\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Iraq -> location.country.administrative_divisions -> Nineveh Governorate\n# Answer:\nIraq", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> common.topic.notable_types -> City/Town/Village\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.geolocation -> m.02_h7yk\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> common.topic.notable_for -> g.12572ffqb\n# Answer:\nAkkad", "# Reasoning Path:\nAkkadian Empire -> location.location.containedby -> Baghdad Governorate -> location.location.containedby -> Iraq\n# Answer:\nBaghdad Governorate", "# Reasoning Path:\nAkkadian Empire -> location.location.contains -> Akkad -> common.topic.article -> m.0n541jg\n# Answer:\nAkkad"], "ground_truth": ["Akkad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-677", "prediction": ["# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_types -> Unit of frequency\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.notable_for -> g.125d3_wnj\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> freebase.unit_profile.dimension -> Time\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> time.recurring_event.current_frequency -> Quadrennial -> common.topic.article -> m.05yw4x1\n# Answer:\nQuadrennial", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> common.topic.article -> m.02ht13p\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_for -> g.1255hfvqd\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_profile.kind -> Classification\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> type.type.properties -> Websites of this genre\n# Answer:\nWebsite Category", "# Reasoning Path:\nOlympic Games -> common.topic.notable_types -> Website Category -> freebase.type_hints.included_types -> Topic\n# Answer:\nWebsite Category"], "ground_truth": ["Quadrennial"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-678", "prediction": ["# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> common.topic.notable_types -> Form of Government\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Angola\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> common.topic.image -> States with semi-presidential systems are shown in yellow\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.statistical_region.gni_in_ppp_dollars -> g.11b60z05j3\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bahamas\n# Answer:\nUnitary state", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Semi-presidential system -> base.schemastaging.context_name.pronunciation -> g.125_l584v\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Republic -> common.topic.image -> Roman Empire\n# Answer:\nRepublic", "# Reasoning Path:\nGuyana -> location.statistical_region.co2_emissions_per_capita -> g.1245_1j9r\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nGuyana -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Bangladesh\n# Answer:\nUnitary state"], "ground_truth": ["Semi-presidential system", "Unitary state", "Republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d2d -> tv.regular_tv_appearance.actor -> Peter Parros\n# Answer:\nPeter Parros", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 4 -> tv.tv_series_season.regular_cast -> m.03lj4m5\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.character -> KITT\n# Answer:\nKITT"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-680", "prediction": ["# Reasoning Path:\nSergio Ag\u00fcero -> base.schemastaging.athlete_extra.salary -> m.0j4b53g -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9hrct -> soccer.football_player_stats.team -> Club Atl\u00e9tico Independiente\n# Answer:\nClub Atl\u00e9tico Independiente", "# Reasoning Path:\nSergio Ag\u00fcero -> base.schemastaging.athlete_extra.salary -> m.0j4b53g -> base.schemastaging.athlete_salary.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.goals_scored -> m.010h60sm -> soccer.football_goal.match -> 2014 Manchester City F.C. vs West Bromwich Albion F.C.\n# Answer:\n2014 Manchester City F.C. vs West Bromwich Albion F.C.", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.goals_scored -> m.010h60sm -> soccer.football_goal.point_awarded_to -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.statistics -> m.0w9d95t -> soccer.football_player_stats.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.matches_played -> m.0g9n9jg -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nSergio Ag\u00fcero -> soccer.football_player.matches_played -> m.0g5nq_z -> soccer.football_player_match_participation.team -> Argentina national football team\n# Answer:\nArgentina national football team"], "ground_truth": ["Club Atl\u00e9tico Independiente", "Manchester City F.C.", "Atl\u00e9tico Madrid", "Argentina national football team"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.44444444444444436, "path_precision": 0.75, "path_recall": 0.3157894736842105, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-681", "prediction": ["# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nJerzy Buzek", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.office_holder -> Jerzy Buzek\n# Answer:\nJerzy Buzek", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0h2zs1k -> government.government_position_held.office_holder -> Jana Bobo\u0161\u00edkov\u00e1\n# Answer:\nJana Bobo\u0161\u00edkov\u00e1", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.office_position_or_title -> President of the European Parliament\n# Answer:\nPresident of the European Parliament", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0ghfs25 -> government.government_position_held.office_holder -> Emilie Turunen\n# Answer:\nEmilie Turunen", "# Reasoning Path:\nEuropean Union -> location.statistical_region.broadband_penetration_rate -> g.1245_6ndt\n# Answer:\nlocation.statistical_region.broadband_penetration_rate", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82m -> government.government_position_held.governmental_body -> European Parliament\n# Answer:\nEuropean Parliament", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.0g5k82w -> government.government_position_held.district_represented -> Silesian\n# Answer:\nSilesian", "# Reasoning Path:\nEuropean Union -> government.governmental_jurisdiction.governing_officials -> m.010p8k3t -> government.government_position_held.office_holder -> Nick Griffin\n# Answer:\nNick Griffin"], "ground_truth": ["Jerzy Buzek"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-683", "prediction": ["# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> award.competition.competitors -> Tara Wilson\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> common.topic.article -> m.03lp2n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMiss USA -> award.recurring_competition.individual_competitions -> Miss USA 1960 -> award.competition.competitors -> Linda Bement\n# Answer:\nMiss USA 1960", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> time.event.locations -> Long Beach\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> time.event.locations -> Branson\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> award.competition.type_of_competition -> Beauty Pageant\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 1952 -> common.topic.article -> m.04g28r2\n# Answer:\nMiss USA 1952", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2000 -> award.competition.competitors -> Jamie Kern\n# Answer:\nMiss USA 2000", "# Reasoning Path:\nMiss USA -> time.recurring_event.instances -> Miss USA 2004 -> time.event.locations -> Dolby Theatre\n# Answer:\nMiss USA 2004"], "ground_truth": ["Miss USA 1993"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-684", "prediction": ["# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Chad\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Federal republic -> government.form_of_government.countries -> United States of America\n# Answer:\nFederal republic", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Guinea\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sgz\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Kenya\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> Liberia\n# Answer:\nPresidential system", "# Reasoning Path:\nNigeria -> location.statistical_region.external_debt_stock -> g.11b71rqxzt\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nNigeria -> location.statistical_region.co2_emissions_per_capita -> g.1245_22x5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nNigeria -> location.statistical_region.deposit_interest_rate -> g.11b71tykw3\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Federal republic", "Presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-685", "prediction": ["# Reasoning Path:\nIna Garten -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> New York City\n# Answer:\nUnited States of America", "# Reasoning Path:\nIna Garten -> people.person.places_lived -> m.04hv_n0 -> people.place_lived.location -> Brooklyn\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Botanic Garden\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Bridge\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> periodicals.newspaper_circulation_area.newspapers -> The Wall Street Journal\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Historical Society Building\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> location.place_with_neighborhoods.neighborhoods -> Bath Beach\n# Answer:\nBrooklyn", "# Reasoning Path:\nIna Garten -> people.person.place_of_birth -> Brooklyn -> travel.travel_destination.tourist_attractions -> Brooklyn Museum\n# Answer:\nBrooklyn"], "ground_truth": ["Brooklyn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-686", "prediction": ["# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> sports.sports_championship_event.season -> 1995 Major League Baseball Season\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> time.event.locations -> Atlanta\u2013Fulton County Stadium\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> common.topic.article -> m.04j717\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> common.topic.notable_for -> g.1257282kq\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.sports_team.championships -> 1995 World Series -> time.event.locations -> Progressive Field\n# Answer:\n1995 World Series", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.05sr8kb -> sports.sports_league_draft_pick.player -> Delwyn Young\n# Answer:\nDelwyn Young", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.owner_s -> Liberty Media Corporation -> sports.sports_team_owner.teams_owned -> Danville Braves\n# Answer:\nLiberty Media Corporation", "# Reasoning Path:\nAtlanta Braves -> sports.professional_sports_team.draft_picks -> m.05sr8kb -> sports.sports_league_draft_pick.school -> Santa Barbara City College\n# Answer:\nSanta Barbara City College"], "ground_truth": ["1995 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-688", "prediction": ["# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> common.topic.notable_types -> Human Language\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> common.topic.notable_types -> Human Language\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.official_language -> Persian Language -> language.human_language.region -> Asia\n# Answer:\nPersian Language", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> language.human_language.countries_spoken_in -> Iraq\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal_per_capita -> g.11b60rrlgr\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> language.human_language.region -> Asia\n# Answer:\nTurkmen Language", "# Reasoning Path:\nIran -> location.statistical_region.gdp_nominal -> g.11bbwyx0z8\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nIran -> location.country.languages_spoken -> Turkmen Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nTurkmen Language"], "ground_truth": ["Qashqa'i Language", "Pashto language", "Gilaki Language", "Kurdish languages", "Persian Language", "Arabic Language", "Balochi language", "Mazanderani Language", "Kumzari Language", "Khalaj, Turkic Language", "Azerbaijani language", "Afshar language", "Turkmen Language", "Armenian Language", "Assyrian Neo-Aramaic Language", "Talysh language", "Luri language", "Khorasani Turkish Language"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19512195121951217, "ans_precission": 0.8, "ans_recall": 0.1111111111111111, "path_f1": 0.19512195121951217, "path_precision": 0.8, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19512195121951217, "path_ans_precision": 0.8, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-689", "prediction": ["# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Spanish Language -> language.human_language.region -> Europe\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.languages_spoken -> Basque Language -> language.human_language.countries_spoken_in -> France\n# Answer:\nBasque Language", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.country.official_language -> Spanish Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSpanish Language", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Spanish Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.us_county.county_seat -> Franklin\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.co2_emissions_total -> m.045hnmm\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.statistical_region.rent50_2 -> m.05gfsl8\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.us_state.capital -> Nashville\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-690", "prediction": ["# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> music.instrument.family -> Struck string instruments\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> music.instrument.variation -> Grand piano\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> interests.collectable_item.in_collections -> m.0104pjzp\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xb_\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.article -> m.06ch5b\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Piano -> education.field_of_study.students_majoring -> m.02wmsgv\n# Answer:\nPiano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> common.topic.webpage -> m.09xz26q\n# Answer:\nJazz piano", "# Reasoning Path:\nScott Joplin -> music.group_member.instruments_played -> Jazz piano -> music.performance_role.regular_performances -> m.0z84xd1\n# Answer:\nJazz piano"], "ground_truth": ["Piano", "Jazz piano"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-691", "prediction": ["# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> education.education.institution -> Heritage Elementary School\n# Answer:\nHeritage Elementary School", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nMiley Cyrus -> music.artist.album -> Hannah Montana -> tv.tv_program.country_of_origin -> United States of America\n# Answer:\nHannah Montana", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMiley Cyrus -> freebase.valuenotation.is_reviewed -> ID -> rdf-schema#domain -> Object\n# Answer:\nID", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMiley Cyrus -> people.person.education -> m.0h2yk16 -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nMiley Cyrus -> freebase.valuenotation.is_reviewed -> ID -> type.property.authorities -> m.0ldk_jn\n# Answer:\nID"], "ground_truth": ["Heritage Elementary School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-693", "prediction": ["# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> common.topic.image -> Dari lang\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.local_name -> Farsi, Eastern\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Farsi, Eastern Language -> base.rosetta.languoid.parent -> Persian Group\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> common.topic.image -> Dari lang\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.cpi_inflation_rate -> g.11b60q43bt\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> base.rosetta.languoid.local_name -> Farsi, Eastern\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> location.country.official_language -> Pashto language -> language.human_language.countries_spoken_in -> Pakistan\n# Answer:\nPashto language", "# Reasoning Path:\nAfghanistan -> location.statistical_region.time_required_to_start_a_business -> g.11b6p6pyh9\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nAfghanistan -> location.country.languages_spoken -> Farsi, Eastern Language -> base.rosetta.languoid.parent -> Persian Group\n# Answer:\nFarsi, Eastern Language", "# Reasoning Path:\nAfghanistan -> book.book_subject.works -> A Bed of Red Flowers: In Search of My Afghanistan -> book.written_work.subjects -> History\n# Answer:\nA Bed of Red Flowers: In Search of My Afghanistan"], "ground_truth": ["Pashto language", "Farsi, Eastern Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-694", "prediction": ["# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> government.government_position_held.office_holder -> Portia Simpson-Miller\n# Answer:\nPortia Simpson-Miller", "# Reasoning Path:\nPrime Minister of Jamaica -> common.topic.article -> m.01sm6d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> government.government_position_held.jurisdiction_of_office -> Jamaica\n# Answer:\nJamaica", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp6mv -> government.government_position_held.office_holder -> Alexander Bustamante\n# Answer:\nAlexander Bustamante", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp7qb -> government.government_position_held.office_holder -> Donald Sangster\n# Answer:\nDonald Sangster", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp8pl -> government.government_position_held.office_holder -> Hugh Shearer\n# Answer:\nHugh Shearer", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.0hqg83f -> government.government_position_held.basic_title -> Prime minister\n# Answer:\nPrime minister", "# Reasoning Path:\nPrime Minister of Jamaica -> government.government_office_or_title.office_holders -> m.010fp9cc -> government.government_position_held.office_holder -> Michael Manley\n# Answer:\nMichael Manley"], "ground_truth": ["Portia Simpson-Miller"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-695", "prediction": ["# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Bass\n# Answer:\nBass", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Lead Vocals\n# Answer:\nLead Vocals", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.role -> Backing vocalist\n# Answer:\nBacking vocalist", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> music.group_membership.group -> The Beatles\n# Answer:\nThe Beatles", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.01vgmqh -> music.group_membership.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_rbdpp\n# Answer:\nmusic.group_member.membership", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> m.02hrk87 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11b7_s0vtk\n# Answer:\nmusic.group_member.membership", "# Reasoning Path:\nPaul McCartney -> base.activism.activist.area_of_activism -> Animal rights\n# Answer:\nAnimal rights", "# Reasoning Path:\nPaul McCartney -> music.group_member.membership -> g.11bt_50wrv\n# Answer:\nmusic.group_member.membership"], "ground_truth": ["Bass", "Lead Vocals", "Backing vocalist"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-696", "prediction": ["# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.instance_of_recurring_event -> Supercoppa Italiana\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.next_in_series -> g.11btrbw12v\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> time.event.locations -> Italy\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 2014 Supercoppa Italiana -> common.topic.notable_types -> Football Match\n# Answer:\n2014 Supercoppa Italiana", "# Reasoning Path:\nJuventus F.C. -> soccer.football_team.matches -> 2014 A. S. Roma vs. Juventus F.C -> soccer.football_match.teams -> A.S. Roma\n# Answer:\n2014 A. S. Roma vs. Juventus F.C", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1977 UEFA Cup Final -> time.event.locations -> Stadio Olimpico di Torino\n# Answer:\n1977 UEFA Cup Final", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.colors -> White -> common.topic.image -> White peacock\n# Answer:\nWhite", "# Reasoning Path:\nJuventus F.C. -> sports.sports_team.championships -> 1977 UEFA Cup Final -> common.topic.notable_types -> Football Match\n# Answer:\n1977 UEFA Cup Final"], "ground_truth": ["1996 UEFA Champions League Final"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-698", "prediction": ["# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> common.topic.notable_types -> Military Person\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.spouse_s -> m.0kcbtkr\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Lawrence E. Roberts, Jr.\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.children -> Sally-Ann Roberts\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> common.topic.notable_for -> g.125fjdxy2\n# Answer:\nLucimarian Tolliver", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lawrence E. Roberts -> people.person.education -> m.0gb1_q_\n# Answer:\nLawrence E. Roberts", "# Reasoning Path:\nRobin Roberts -> people.person.parents -> Lucimarian Tolliver -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLucimarian Tolliver"], "ground_truth": ["Lawrence E. Roberts"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-699", "prediction": ["# Reasoning Path:\nVictoria -> location.administrative_division.capital -> m.0jvvh95 -> location.administrative_division_capital_relationship.capital -> Melbourne\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.administrative_children -> City of Ballarat -> location.location.containedby -> Australia\n# Answer:\nCity of Ballarat", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Yarra Valley\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> symbols.namesake.named_after -> Queen Victoria -> people.person.gender -> Female\n# Answer:\nQueen Victoria", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> common.topic.image -> Melbourne Infobox Montage\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.administrative_children -> City of Ballarat -> common.topic.notable_for -> g.125fxg37y\n# Answer:\nCity of Ballarat", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> travel.travel_destination.tourist_attractions -> Melbourne City Centre\n# Answer:\nMelbourne", "# Reasoning Path:\nVictoria -> base.aareas.schema.administrative_area.capital -> Melbourne -> location.statistical_region.population -> g.11bcdlkx9x\n# Answer:\nMelbourne"], "ground_truth": ["Melbourne"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.hud_foreclosure_area.total_residential_addresses -> m.07h218k\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-700", "prediction": ["# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Seychelles -> location.location.containedby -> Africa\n# Answer:\nSeychelles", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> location.country.first_level_divisions -> Martinique\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Seychelles -> location.country.official_language -> English Language\n# Answer:\nSeychelles", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Gabon -> location.location.containedby -> Africa\n# Answer:\nGabon", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Seychelles -> base.aareas.schema.administrative_area.administrative_area_type -> Sovereign state\n# Answer:\nSeychelles", "# Reasoning Path:\nFrench -> language.human_language.main_country -> France -> organization.organization_founder.organizations_founded -> World Trade Organization\n# Answer:\nFrance", "# Reasoning Path:\nFrench -> base.rosetta.local_name.locale -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda", "# Reasoning Path:\nFrench -> language.human_language.countries_spoken_in -> Rwanda -> location.country.official_language -> English Language\n# Answer:\nRwanda"], "ground_truth": ["Chad", "Dominica", "Switzerland", "Seychelles", "Cambodia", "Niger", "Luxembourg", "France", "Vatican City", "Rwanda", "Laos", "Tunisia", "Madagascar", "Barbados", "Senegal", "Equatorial Guinea", "Mali", "Qatar", "Benin", "Cyprus", "Monaco", "Haiti", "Comoros", "Lebanon", "Cameroon", "South Vietnam", "Algeria", "Vanuatu", "Burundi", "Ivory Coast", "Guinea", "Belgium", "Gabon", "Burkina Faso", "Morocco", "Holy Roman Empire", "Central African Republic", "Habsburg Netherlands", "Martinique", "Djibouti", "Mauritius", "Congo", "Jersey"], "ans_acc": 0.11627906976744186, "ans_hit": 1, "ans_f1": 0.1702127659574468, "ans_precission": 1.0, "ans_recall": 0.09302325581395349, "path_f1": 0.03921568627450981, "path_precision": 0.125, "path_recall": 0.023255813953488372, "path_ans_f1": 0.20833333333333334, "path_ans_precision": 1.0, "path_ans_recall": 0.11627906976744186}
{"id": "WebQTest-701", "prediction": ["# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09tctbs -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07sh67q -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.09rmczp -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09tctbs -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.09grpy_ -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.rushing -> m.07z2cq4 -> american_football.player_rushing_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.09rmczp -> american_football.player_receiving_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nReggie Bush -> american_football.football_player.receiving -> m.09grq2h -> american_football.player_receiving_statistics.team -> New Orleans Saints\n# Answer:\nNew Orleans Saints"], "ground_truth": ["New Orleans Saints", "Miami Dolphins"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.1846153846153846, "path_precision": 0.75, "path_recall": 0.10526315789473684, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-702", "prediction": ["# Reasoning Path:\nEmily Osment -> people.person.education -> m.0n1fn0k -> education.education.institution -> Flintridge Preparatory School\n# Answer:\nFlintridge Preparatory School", "# Reasoning Path:\nEmily Osment -> people.person.education -> g.11b62v8w8j\n# Answer:\npeople.person.education", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Winifred Burkle\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> base.popstra.celebrity.vacations_in -> m.06526k7 -> base.popstra.vacation_choice.location -> Cleveland\n# Answer:\nCleveland", "# Reasoning Path:\nEmily Osment -> award.award_winner.awards_won -> m.0t5t4lr -> award.award_honor.award -> Canadian Screen Award for Best Performance by an Actress in a Leading Role in a Dramatic Program or Mini-Series\n# Answer:\nCanadian Screen Award for Best Performance by an Actress in a Leading Role in a Dramatic Program or Mini-Series", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> fictional_universe.fictional_setting.characters_that_have_lived_here -> Linwood Murrow\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> people.person.place_of_birth -> Los Angeles -> base.ndbcd.buoy.region -> California\n# Answer:\nLos Angeles", "# Reasoning Path:\nEmily Osment -> film.actor.film -> m.010tw9__ -> film.performance.special_performance_type -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["Flintridge Preparatory School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-703", "prediction": ["# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_0s31\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.11b60v4rn6\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_38p7\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2j1t\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.gender_balance_members_of_parliament -> g.11b71yj1cp\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nMexico -> location.statistical_region.co2_emissions_per_capita -> g.1245_569j\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nMexico -> location.statistical_region.cpi_inflation_rate -> g.12tb6hbx9\n# Answer:\nlocation.statistical_region.cpi_inflation_rate"], "ground_truth": ["Mexican peso"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-704", "prediction": ["# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> people.profession.specialization_of -> Camera Operator\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> common.topic.subject_of -> Book\n# Answer:\nScientist", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> common.topic.subject_of -> Eugenio Polgovsky\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> freebase.valuenotation.is_reviewed -> Profession -> rdf-schema#domain -> Person\n# Answer:\nProfession", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> common.topic.article -> m.0dgf7\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> common.topic.subject_of -> Lew M. Parry\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Cinematographer -> film.film_job.films_with_this_crew_job -> m.012hwfwf\n# Answer:\nCinematographer", "# Reasoning Path:\nThor Heyerdahl -> people.person.profession -> Scientist -> tv.tv_subject.tv_programs -> The Secret Life of Scientists & Engineers\n# Answer:\nScientist"], "ground_truth": ["Film Producer", "Ethnographer", "Cinematographer", "Scientist", "Adventurer", "Film director"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48275862068965514, "ans_precission": 0.875, "ans_recall": 0.3333333333333333, "path_f1": 0.48275862068965514, "path_precision": 0.875, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48275862068965514, "path_ans_precision": 0.875, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-706", "prediction": ["# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.team -> Dallas Cowboys\n# Answer:\nDallas Cowboys", "# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.teams -> m.0hpxsvm -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTroy Aikman -> common.image.size -> m.02gxnjq\n# Answer:\ncommon.image.size", "# Reasoning Path:\nTroy Aikman -> sports.pro_athlete.sports_played_professionally -> m.0d5j7sd -> sports.pro_sports_played.sport -> American football\n# Answer:\nAmerican football", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> award.award_nomination.award -> Sports Emmy Award for Outstanding Sports Personality - Sports Event Analyst\n# Answer:\nSports Emmy Award for Outstanding Sports Personality - Sports Event Analyst", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nAward", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work", "# Reasoning Path:\nTroy Aikman -> award.award_nominee.award_nominations -> m.0zgx48n -> award.award_nomination.ceremony -> 25th Sports Emmy Awards\n# Answer:\n25th Sports Emmy Awards"], "ground_truth": ["Dallas Cowboys"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-707", "prediction": ["# Reasoning Path:\nCletus Hogg -> tv.tv_character.appeared_in_tv_program -> m.03hqvml -> tv.regular_tv_appearance.actor -> Rick Hurst\n# Answer:\nRick Hurst", "# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.actor -> Jack Polick\n# Answer:\nJack Polick", "# Reasoning Path:\nCletus Hogg -> common.topic.article -> m.0bxccc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_for -> g.1257kq07s\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCletus Hogg -> film.film_character.portrayed_in_films -> m.03hqvvw -> film.performance.film -> The Dukes of Hazzard\n# Answer:\nThe Dukes of Hazzard", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episode Segments\n# Answer:\nTV Character", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_types -> TV Character -> type.type.expected_by -> Character\n# Answer:\nTV Character", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_types -> TV Character -> type.type.properties -> Appeared In TV Episodes\n# Answer:\nTV Character", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_types -> TV Character -> type.type.domain -> TV\n# Answer:\nTV Character", "# Reasoning Path:\nCletus Hogg -> common.topic.notable_types -> TV Character -> freebase.type_profile.published -> Published\n# Answer:\nTV Character"], "ground_truth": ["Jack Polick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-708", "prediction": ["# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05l1m7b -> sports.sports_award.season -> 1988\u201389 FA Cup\n# Answer:\n1988\u201389 FA Cup", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.award_winner -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nFA Cup -> common.topic.notable_for -> g.125f13scs\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.0bfpn_5 -> sports.sports_award.season -> 1984\u201385 FA Cup\n# Answer:\n1984\u201385 FA Cup", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.010lkdyj -> sports.sports_award.award_winner -> Arsenal F.C.\n# Answer:\nArsenal F.C.", "# Reasoning Path:\nFA Cup -> common.image.appears_in_topic_gallery -> FA Cup Final referees\n# Answer:\nFA Cup Final referees", "# Reasoning Path:\nFA Cup -> freebase.valuenotation.has_no_value -> Date of final occurrence -> rdf-schema#range -> Date/Time\n# Answer:\nDate of final occurrence", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkcss -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C.", "# Reasoning Path:\nFA Cup -> sports.sports_award_type.winners -> m.05kkh7k -> sports.sports_award.award_winner -> Liverpool F.C.\n# Answer:\nLiverpool F.C."], "ground_truth": ["Southampton F.C."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-709", "prediction": ["# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.includes_causes_of_death -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> people.deceased_person.cause_of_death -> Pneumonia -> medicine.disease.includes_diseases -> Aspiration pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.written_work.subjects -> American Civil War\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> people.person.sibling_s -> m.0w4gc9q -> people.sibling_relationship.sibling -> Sydney Smith Lee\n# Answer:\nSydney Smith Lee", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> book.book.genre -> Non-fiction\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> military.military_commander.military_commands -> m.048z_8v -> military.military_command.military_conflict -> Battle of Gettysburg\n# Answer:\nBattle of Gettysburg", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Robert E. Lee's Civil War -> common.topic.notable_types -> Book\n# Answer:\nRobert E. Lee's Civil War", "# Reasoning Path:\nRobert E. Lee -> book.book_subject.works -> Uncertain Glory: Lee's Generalship Re-Examined -> common.topic.notable_for -> g.125fs0sd6\n# Answer:\nUncertain Glory: Lee's Generalship Re-Examined"], "ground_truth": ["Pneumonia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Schuyler Frances Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSchuyler Frances Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q2m7 -> base.popstra.dated.participant -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.gender -> Male\n# Answer:\nSam Michael Fox"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-710", "prediction": ["# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Th\u00e9o van Rysselberghe\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Albert Dubois-Pillet\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artworks -> Sunday\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> book.book_subject.works -> Matisse -> common.topic.notable_for -> g.125dhx5l1\n# Answer:\nMatisse", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Modernism -> visual_art.art_period_movement.associated_artworks -> Winged Figure\n# Answer:\nModernism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> visual_art.art_period_movement.associated_artists -> Alfred William Finch\n# Answer:\nNeo-impressionism", "# Reasoning Path:\nHenri Matisse -> visual_art.visual_artist.associated_periods_or_movements -> Neo-impressionism -> common.topic.image -> Georges Seurat - Un dimanche apr\u00c3\u00a8s-midi \u00c3\u00a0 l'\u00c3\u008ele de la Grande Jatte v2\n# Answer:\nNeo-impressionism"], "ground_truth": ["Impressionism", "Modern art", "Neo-impressionism", "Modernism", "Fauvism"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.711864406779661, "ans_precission": 0.875, "ans_recall": 0.6, "path_f1": 0.5490196078431373, "path_precision": 0.875, "path_recall": 0.4, "path_ans_f1": 0.711864406779661, "path_ans_precision": 0.875, "path_ans_recall": 0.6}
{"id": "WebQTest-711", "prediction": ["# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.actor -> Kristen Bell\n# Answer:\nKristen Bell", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.actor -> Ed Westwick\n# Answer:\nEd Westwick", "# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 6\n# Answer:\nGossip Girl - Season 6", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.seasons -> Gossip Girl - Season 6\n# Answer:\nGossip Girl - Season 6", "# Reasoning Path:\nGossip Girl -> tv.tv_program.regular_cast -> m.010blsdj -> tv.regular_tv_appearance.actor -> Margaret Colin\n# Answer:\nMargaret Colin", "# Reasoning Path:\nChuck Bass -> tv.tv_character.appeared_in_tv_program -> m.04hby1k -> tv.regular_tv_appearance.series -> Gossip Girl\n# Answer:\ntv.regular_tv_appearance.series", "# Reasoning Path:\nGossip Girl -> tv.tv_character.appeared_in_tv_program -> m.0dk0lbd -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nChuck Bass -> fictional_universe.fictional_character.romantically_involved_with -> m.0771cwt -> fictional_universe.romantic_involvement.partner -> Georgina Sparks\n# Answer:\nGeorgina Sparks"], "ground_truth": ["Ed Westwick"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-712", "prediction": ["# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Phoenix Islands\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.containedby -> Pacific Ocean\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Tarawa\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.languages_spoken -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Australia -> location.country.currency_used -> Australian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nOceania -> location.location.contains -> Tuvalu -> location.location.containedby -> Polynesia\n# Answer:\nTuvalu", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.country.official_language -> English Language\n# Answer:\nKiribati", "# Reasoning Path:\nOceania -> location.location.contains -> Kiribati -> location.location.contains -> Line Islands\n# Answer:\nKiribati"], "ground_truth": ["Kiribati", "Indonesia", "Pitcairn Islands", "Solomon Islands", "Norfolk Island", "New Zealand", "Samoa", "Palau", "Niue", "American Samoa", "Tonga", "Papua New Guinea", "Fiji", "Wallis and Futuna", "Federated States of Micronesia", "Australia", "Nauru", "Tuvalu", "Vanuatu", "New Caledonia", "French Polynesia", "Northern Mariana Islands", "Timor-Leste", "Tokelau", "Marshall Islands", "Guam", "Cook Islands"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 1.0, "ans_recall": 0.1111111111111111, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 1.0, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-713", "prediction": ["# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minnesota\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.hud_foreclosure_area.hhuniv -> m.07h4z_p\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.hud_county_place.county -> Hennepin County -> location.location.containedby -> Minnesota\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.location.containedby -> Minneapolis-St. Paul-Bloomington, MN-WI Metropolitan Statistical Area\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Minneapolis\u2013Saint Paul -> location.location.containedby -> Washington County\n# Answer:\nMinneapolis\u2013Saint Paul", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.hud_foreclosure_area.ofheo_price_change -> m.07h4_0z\n# Answer:\nHennepin County", "# Reasoning Path:\nSt. Louis Park -> location.statistical_region.population -> g.11b66k1kc0\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. Louis Park -> location.location.containedby -> Hennepin County -> location.statistical_region.population -> g.11b674hc3d\n# Answer:\nHennepin County"], "ground_truth": ["Hennepin County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-714", "prediction": ["# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Parliamentary republic -> government.form_of_government.countries -> Botswana\n# Answer:\nParliamentary republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_4m5f\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Mozambique\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f730\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> France\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.electricity_consumption_per_capita -> g.1245_f78y\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nSouth Africa -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71tykzb\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nSouth Africa -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> Greece\n# Answer:\nConstitutional republic", "# Reasoning Path:\nSouth Africa -> location.statistical_region.labor_participation_rate -> g.11b71xnbcs\n# Answer:\nlocation.statistical_region.labor_participation_rate"], "ground_truth": ["Parliamentary republic", "Constitutional republic"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-716", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.language_family -> Germanic languages -> language.language_family.geographic_distribution -> Northern Europe\n# Answer:\nGermanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> language.language_family.member_of_language_families -> Germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> m.029ftls\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.article -> m.0140r1\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic languages\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.notable_for -> g.125b7l2y7\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> base.schemastaging.context_name.pronunciation -> g.125_nqs0z\n# Answer:\nWest Germanic languages", "# Reasoning Path:\nGerman Language -> language.human_language.language_family -> West Germanic languages -> common.topic.image -> Europe germanic-languages 2\n# Answer:\nWest Germanic languages"], "ground_truth": ["Indo-European languages", "Germanic languages", "West Germanic languages"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.8, "path_precision": 1.0, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-717", "prediction": ["# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.notable_for -> g.125brs1ct\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> location.location.time_zones -> Mountain Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nTucson -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> location.location.containedby -> United States of America\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.location.nearby_airports -> Tucson International Airport -> aviation.airport.serves -> Miraval Resort\n# Answer:\nTucson International Airport", "# Reasoning Path:\nTucson -> location.statistical_region.population -> g.11b66f8s_5\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-718", "prediction": ["# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> New Zealand\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.containedby -> Auckland\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.article -> m.04jnl_\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.notable_types -> Location\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> common.topic.image -> Mt Albert Town Centre Needs Love\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> people.person.place_of_birth -> Mount Albert, New Zealand -> location.location.people_born_here -> Matt Coutts\n# Answer:\nMount Albert, New Zealand", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> rdf-schema#domain -> Person\n# Answer:\nSiblings", "# Reasoning Path:\nLucy Lawless -> freebase.valuenotation.has_value -> Siblings -> type.property.schema -> Person\n# Answer:\nSiblings"], "ground_truth": ["1968-03-29"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-719", "prediction": ["# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> common.topic.notable_types -> Event\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.election_campaigns -> Barack Obama presidential campaign, 2012 -> government.election_campaign.party -> Democratic Party\n# Answer:\nBarack Obama presidential campaign, 2012", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.42857142857142855, "path_precision": 0.375, "path_recall": 0.5, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\nmilitary.military_commander.military_commands", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_conflict -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nStonewall Jackson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Winfield Scott\n# Answer:\nAmerican Civil War"], "ground_truth": ["Romney Expedition", "Battle of Port Republic", "Battle of Chantilly", "Manassas Station Operations", "Jackson's Valley Campaign", "Battle of Front Royal", "Battle of McDowell", "American Civil War", "Battle of Chancellorsville", "First Battle of Kernstown", "Battle of Harpers Ferry", "How Few Remain", "Battle of Cedar Mountain", "Battle of White Oak Swamp", "Battle of Hoke's Run", "Battle of Hancock", "First Battle of Rappahannock Station", "Second Battle of Bull Run", "First Battle of Winchester"], "ans_acc": 0.3157894736842105, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.6666666666666666, "ans_recall": 0.3157894736842105, "path_f1": 0.2222222222222222, "path_precision": 0.4444444444444444, "path_recall": 0.14814814814814814, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.3157894736842105}
{"id": "WebQTest-721", "prediction": ["# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Cancer\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> people.cause_of_death.parent_cause_of_death -> Hibakusha\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> medicine.disease.notable_people_with_this_condition -> Clive James\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.treatments -> Vincristine\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Arthritis\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.article -> m.02bg7v\n# Answer:\nB-cell chronic lymphocytic leukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> Leukemia -> medicine.disease.symptoms -> Bleeding\n# Answer:\nLeukemia", "# Reasoning Path:\nJill Clayburgh -> people.deceased_person.cause_of_death -> B-cell chronic lymphocytic leukemia -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nB-cell chronic lymphocytic leukemia"], "ground_truth": ["B-cell chronic lymphocytic leukemia", "Leukemia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-723", "prediction": ["# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.time_zones -> Australian Eastern Time Zone\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.containedby -> Australia\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Bondi Beach -> location.location.events -> 1992 Flickerfest\n# Answer:\nBondi Beach", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Bondi Beach -> location.location.containedby -> Bondi\n# Answer:\nBondi Beach", "# Reasoning Path:\nSydney -> freebase.valuenotation.is_reviewed -> Time zone(s) -> type.property.master_property -> Locations in this time zone\n# Answer:\nTime zone(s)", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Luna Park Sydney -> location.location.events -> 2013 Sydney Darts Masters\n# Answer:\nLuna Park Sydney", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Bondi Beach -> location.location.time_zones -> Australian Eastern Time Zone\n# Answer:\nBondi Beach", "# Reasoning Path:\nSydney -> travel.travel_destination.tourist_attractions -> Wild Life Sydney -> location.location.geolocation -> m.0cldn30\n# Answer:\nWild Life Sydney"], "ground_truth": ["City2Surf", "North Head Quarantine Station", "Powerhouse Museum", "Sydney Harbour Bridge", "The Rocks", "Sydney Film Festival", "Darling Harbour", "Kamay Botany Bay National Park", "St Mary's Cathedral, Sydney", "Rose Seidler House", "Australian National Maritime Museum", "Featherdale Wildlife Park", "Sydney Markets", "Oatley Park, New South Wales", "Luna Park Sydney", "Bondi Beach", "Port Jackson", "Sydney Opera House", "Hillsong Church", "Wild Life Sydney"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.25609756097560976, "ans_precission": 0.875, "ans_recall": 0.15, "path_f1": 0.25609756097560976, "path_precision": 0.875, "path_recall": 0.15, "path_ans_f1": 0.25609756097560976, "path_ans_precision": 0.875, "path_ans_recall": 0.15}
{"id": "WebQTest-724", "prediction": ["# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Los Angeles\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Los Angeles County\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> freebase.valuenotation.is_reviewed -> Image\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.neighborhood.neighborhood_of -> Los Angeles\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> California\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> organization.organization_scope.organizations_with_this_scope -> Brentwood Public School\n# Answer:\nBrentwood", "# Reasoning Path:\nMarilyn Monroe -> people.deceased_person.place_of_death -> Brentwood -> location.location.containedby -> Greater Los Angeles Area\n# Answer:\nBrentwood"], "ground_truth": ["Brentwood"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-725", "prediction": ["# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> United States, with Territories\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> base.aareas.schema.administrative_area.administrative_children -> White Pine County\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.containedby -> United States of America\n# Answer:\nNevada", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> location.location.partially_contains -> Colorado River\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> California -> organization.organization_scope.organizations_with_this_scope -> 5 Star Loans\n# Answer:\nCalifornia", "# Reasoning Path:\nSierra Nevada -> location.location.partially_containedby -> Nevada -> location.location.people_born_here -> A. J. Haglund\n# Answer:\nNevada"], "ground_truth": ["California", "Nevada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-726", "prediction": ["# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Matter of Dignity\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Touch of Spice\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60nhckb\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nGreece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ld\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Albanian language", "Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-728", "prediction": ["# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Nancy Pelosig -> common.image.appears_in_topic_gallery -> Nancy Pelosi\n# Answer:\nNancy Pelosig", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.office_holder -> John Boehner\n# Answer:\nJohn Boehner", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Nancy Pelosig -> common.image.size -> m.03rxy3k\n# Answer:\nNancy Pelosig", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.03hcgxq -> government.government_position_held.office_holder -> Nancy Pelosi\n# Answer:\nNancy Pelosi", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> base.schemastaging.context_name.plural -> m.0w0qtlx -> base.schemastaging.plural_form.language -> Hebrew\n# Answer:\nHebrew", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.image -> Great Seal of the US -> common.image.size -> m.01x9rd5\n# Answer:\nGreat Seal of the US", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> government.government_office_or_title.office_holders -> m.0g4_6k1 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nSpeaker of the United States House of Representatives -> common.topic.notable_types -> Government Office or Title -> type.type.domain -> Government\n# Answer:\nGovernment Office or Title"], "ground_truth": ["Nancy Pelosi", "John Boehner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-729", "prediction": ["# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Attention deficit hyperactivity disorder\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> tv.tv_character.appeared_in_tv_program -> m.05nphmt -> tv.regular_tv_appearance.actor -> Leslie David Baker\n# Answer:\nLeslie David Baker", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> common.topic.article -> m.026nx7n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> common.topic.article -> m.05zpq8\n# Answer:\nMale", "# Reasoning Path:\nStanley Hudson -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Acute myeloid leukemia\n# Answer:\nMale"], "ground_truth": ["Leslie David Baker"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.article -> m.054w4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.document -> Maasai Phonology\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.main_country -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> base.descriptive_names.names.descriptive_name -> m.011vfn5d\n# Answer:\nEthnicity"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-730", "prediction": ["# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> award.award_nominee.award_nominations -> m.0sgj7wg\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> RCA Records -> common.topic.notable_types -> Record label\n# Answer:\nRCA Records", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Music\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> base.saturdaynightlive.snl_musical_guest.musical_performance -> m.04p4_dv -> base.saturdaynightlive.snl_musical_performance.episode -> SNL - 30.12\n# Answer:\nSNL - 30.12", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony Music Entertainment -> business.business_operation.industry -> Arts, Entertainment, and Recreation\n# Answer:\nSony Music Entertainment", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> 19 Recordings -> common.topic.article -> m.0rpfr86\n# Answer:\n19 Recordings", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> business.defunct_company.reason_for_ceasing_operations -> Buyout\n# Answer:\nSony BMG", "# Reasoning Path:\nKelly Clarkson -> music.artist.label -> Sony BMG -> common.topic.article -> m.03mp8v\n# Answer:\nSony BMG"], "ground_truth": ["RCA Records", "Sony Music Entertainment", "Sony BMG", "19 Recordings"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-731", "prediction": ["# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.containedby -> London\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> travel.travel_destination.tourist_attractions -> Dulwich Picture Gallery\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> common.topic.notable_types -> Location\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> common.topic.notable_for -> g.1257b18ps\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> film.actor.film -> g.11b6_0kntn\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> people.person.place_of_birth -> Dulwich -> location.location.people_born_here -> A. E. W. Mason\n# Answer:\nDulwich", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nTim Roth -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Dulwich"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-733", "prediction": ["# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.image -> 1 \u0111\u1ed3ng 1976\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.notable_types -> Currency\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d1ntkd\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> common.topic.webpage -> m.04m1yc5\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.country.currency_used -> Vietnamese dong -> base.monetarydebase.debased_currency.largest_denomination -> m.05brh_g\n# Answer:\nVietnamese dong", "# Reasoning Path:\nVietnam -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71w7lzm\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6p1vnp7\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.energy_use_per_capita -> g.1245_6vh3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.co2_emissions_per_capita -> g.1245_0sv5\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nVietnam -> location.statistical_region.renewable_freshwater_per_capita -> g.1hhc3bvgz\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Vietnamese dong"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-734", "prediction": ["# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.parents -> Mirka Federer\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Myla Rose Federer -> people.person.parents -> Mirka Federer\n# Answer:\nMyla Rose Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.sibling_s -> m.010h346t\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Lenny Federer -> people.person.parents -> Mirka Federer\n# Answer:\nLenny Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Charlene Riva Federer -> people.person.gender -> Female\n# Answer:\nCharlene Riva Federer", "# Reasoning Path:\nRoger Federer -> people.person.children -> Myla Rose Federer -> common.topic.notable_for -> g.125gwrm0m\n# Answer:\nMyla Rose Federer", "# Reasoning Path:\nRoger Federer -> people.person.spouse_s -> m.0j12_p2 -> people.marriage.spouse -> Mirka Federer\n# Answer:\nMirka Federer"], "ground_truth": ["Mirka Federer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.6153846153846154, "path_precision": 0.5, "path_recall": 0.8, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-735", "prediction": ["# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.image -> 10colonesbcrfront\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> common.topic.webpage -> m.04m2m2k\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60wmkvh\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.country.currency_used -> Costa Rican col\u00f3n -> symbols.namesake.named_after -> Christopher Columbus\n# Answer:\nCosta Rican col\u00f3n", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.11b71w3_6h\n# Answer:\nlocation.statistical_region.external_debt_stock", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_5xgy\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.deposit_interest_rate -> g.11b71kjkjp\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nCosta Rica -> location.statistical_region.co2_emissions_per_capita -> g.1245_0_8r\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nCosta Rica -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_6n42\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nCosta Rica -> location.statistical_region.external_debt_stock -> g.1hhc37pjz\n# Answer:\nlocation.statistical_region.external_debt_stock"], "ground_truth": ["Costa Rican col\u00f3n"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-737", "prediction": ["# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.contains -> Laeken\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.country.capital -> Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nBrussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.11b61jk1pw\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> freebase.valuenotation.is_reviewed -> Official website\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245__rg8\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBelgium -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71qnfgh\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBelgium -> location.country.capital -> City of Brussels -> periodicals.newspaper_circulation_area.newspapers -> Metro\n# Answer:\nCity of Brussels", "# Reasoning Path:\nBelgium -> location.statistical_region.co2_emissions_per_capita -> g.1245_4_nd\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nBelgium -> location.statistical_region.gni_in_ppp_dollars -> g.1245_dgwn\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars"], "ground_truth": ["Brussels", "City of Brussels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-738", "prediction": ["# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Australia\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Canada\n# Answer:\nParliamentary system", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_4lv6\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6gg_n\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJapan -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> New Zealand\n# Answer:\nParliamentary system"], "ground_truth": ["Constitutional monarchy", "Unitary state", "Parliamentary system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-739", "prediction": ["# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Le diable dans le beffroi\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> Claude\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> La chute de la maison Usher\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> people.person.profession -> Music critic -> common.topic.image -> Hector Berlioz\n# Answer:\nMusic critic", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.appears_in_topic_gallery -> String Quartet\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Claude Debussy, ca. 1908 (photo by F\u00e9lix Nadar) -> common.image.size -> m.02bcsxr\n# Answer:\nClaude Debussy, ca. 1908 (photo by F\u00e9lix Nadar)", "# Reasoning Path:\nClaude Debussy -> common.topic.image -> Debussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket -> common.image.size -> m.041td0y\n# Answer:\nDebussy at the Villa M\u00e9dici in Rome, 1885, at centre in the white jacket", "# Reasoning Path:\nClaude Debussy -> music.artist.genre -> French opera -> common.topic.notable_types -> Field of study\n# Answer:\nFrench opera", "# Reasoning Path:\nClaude Debussy -> music.artist.track_contributions -> g.11b88dm64n\n# Answer:\nmusic.artist.track_contributions"], "ground_truth": ["Danse profane", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : I. \\\"Aupr\u00e8s de cette grotte sombre\\\"", "Ariettes oubli\u00e9es, L. 60, CD 63a : V. Aquarelles, 1. Green, m\u00e9lodie pour voix et piano \\\"Voici des fruits, des fleurs, des feuilles\\\"", "Six \u00e9pigraphes antiques, L. 131", "Preludes, Book I: 6. Des pas sur la neige", "Proses Lyriques", "Khamma: Troisi\u00e8me Danse. Tr\u00e8s lent \u2013 Plus p\u00e9n\u00e9trant \u2013 Doucement contenu \u2013", "g.1234bndn", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"M\u00e9lisande...M\u00e9lisande ...\\\" - \\\"Est-ce vous, Golaud?\\\" (Golaud, M\u00e9lisande)", "Chansons de Bilitis, L. 90: La Fl\u00fbte de Pan \u00abPour le jour des Hyacinthies\u00bb", "UTSUKUSHII YUUGURE", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b01", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte II, sc\u00e8ne 1. Une fontaine dans le parc", "Suite bergamasque, L. 75, CD 82 : III. Clair de lune, pour violon et piano", "Prelude to the Afternoon of a Faun", "Pour le piano", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ce n'est pas de cette petite blessure qu'elle peut mourir ...\\\" (Le m\u00e9decin, Arkel, Golaud)", "Pi\u00e8ce pour le V\u00eatement du bless\u00e9 (Page d'album), L. 133", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\"", "\u590f\u306e\u98a8\u306e\u795e\uff08\u300c\uff16\u3064\u306e\u53e4\u4ee3\u5893\u7891\u540d\u300d\u3088\u308a\uff09", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : I. Anim\u00e9 et tr\u00e8s d\u00e9cid\u00e9", "Paysage sentimental, L 45, CD 55, m\u00e9lodie pour voix et piano \\\"Le ciel d\u2019hiver si doux, si triste, si dormant\\\"", "Nocturnes, L 91: I. Nuages", "3 Preludes from Book II: V. Bruyer\u00e8s. Calme", "Chansons de Bilitis", "Preludes, Book II: 12. Feux d'artifice", "L'eau pure du bassin", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : II. Cort\u00e8ge", "F\u00eates Galantes I", "Sonate pour violoncelle et piano: III. Finale", "Petite Suite, L. 65, CD 71, pour violon et piano : III. Menuet", "Preludes, Book II: 1. Brouillards", "Sonata for flute, viola and harp, L. 137: II. Interlude", "\u00c9ventail", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b06", "Recueillement", "Iberia No. 2 from \\\"Images\\\" for Orchestra: III. The Morning of a Holiday", "Rondel chinois, L. 17, CD 11, m\u00e9lodie pour voix et piano \\\"Sur le lac bord\u00e9 d\u2019azal\u00e9e\\\"", "Estampes", "Trois Ballades de Fran\u00e7ois Villon, L. 119: III. Ballade des femmes de Paris", "Ariettes Oubli\u00e9es", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: III. \u00c9ventail \\\"\u00d4 r\u00eaveuse pour que je plonge\\\"", "Bilitis: VI. Pour remercier la pluie au matin", "Six \u00e9pigraphes antiques, L. 131: III. Pour que la nuit soit propice", "Bilitis: II. Pour un tombeau sans nom", "Souhait", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : I. La mer est plus belle que les cath\u00e9drales.", "Images, Livre 2, L. 111: No. 3. Poissons d'or", "F\u00eate galante", "Trois Chansons de Charles d'Orl\u00e9ans", "Proses lyriques, L. 84, CD 90 : I. De R\u00eave, m\u00e9lodie \\\"La nuit a des douceurs de femme\\\"", "\u00c9tudes, L. 136: VIII. Pour les agr\u00e9ments", "Petite Suite: Menuet", "Placet futile", "Nuit d'\u00c9toiles", "Le Martyre de saint S\u00e9bastien", "De soir", "Khamma: Pr\u00e9lude. Mod\u00e9r\u00e9ment anim\u00e9 (comme un lointain tumulte) \u2013", "Les ing\u00e9nus", "Carry", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Oh! Cette pierre est lourde ...\\\" (Yniold, Le Berger)", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano", "Trois Po\u00e8mes de St\u00e9phane Mallarm\u00e9", "Engulfed Cathedral (Debussy)", "Les Chansons de Bilitis, L. 96: No. 2. Les Comparaisons", "Preludes, Book II: 6. General Lavine - Eccentric", "Printemps, L. 61, CD 68b : I. Tr\u00e8s mod\u00e9r\u00e9", "\u00c9tudes, L. 136: X. Pour les sonorit\u00e9s oppos\u00e9es", "La Danseuse aux crotales", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"On dirait que ta voix a pass\u00e9 sur la mer au printemps!\\\" (Choeur, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "Les soirs illumin\u00e9s par l'ardeur du charbon", "Les roses", "Chanson", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : IV. Bacchanale (Cort\u00e8ge et Bacchanale)", "Deux Romances", "Le jet d'eau", "Preludes, Book I: 11. La Danse de Puck", "Le Martyre de Saint-S\u00e9bastien, L. 124: Prologue", "Lia's Recitative and Aria", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b02", "Pr\u00e9ludes, Livre II, L. 123: XI. Les tierces altern\u00e9es. Mod\u00e9r\u00e9ment anim\u00e9", "La Bo\u00eete \u00e0 Joujoux : I. Pr\u00e9lude. Le Sommeil de la bo\u00eete", "Khamma: Sc\u00e8ne 3", "\u00c9tudes, L. 136: III. Pour les quartes", "Suite Bergamasque: Passepied: Allegretto ma non troppo", "g.1234nfvz", "Pell\u00e9as et M\u00e9lisande: Act II, Scene III. \\\"Oui, c'est ici, nous y sommes\\\" (Pell\u00e9as, M\u00e9lisande)", "L'ombre des arbres", "La chevelure", "Jeux", "La plus que lente", "Ariettes oubli\u00e9es, L. 60, CD 63a : I. Le vent dans la plaine suspend son haleine, m\u00e9lodie pour voix et piano \\\"C'est l'extase langoureuse\\\"", "Trio in G Major for Violin, Cello and Piano: I. Andantino con moto allegro", "Trio in G Major for Violin, Cello and Piano: IV. Finale. Appassionato", "Preludes, Book I: 1. Danseuses de Delphes", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Tiens, o\u00f9 est l'anneau que je t'avais donn\u00e9?\\\" (Golaud, M\u00e9lisande)", "Ariettes oubli\u00e9es, L. 60, CD 63a : VI. Aquarelles, 2. Spleen, m\u00e9lodie pour voix et piano \\\"Les roses \u00e9taient toutes rouges\\\"", "Images, Livre 1, L. 110: No. 3. Mouvement", "Pour le piano, L. 95: I. Pr\u00e9lude. \u00c0 Mademoiselle M.W. de Romilly. Assez anim\u00e9 et tr\u00e8s rythm\u00e9 - Tempo di cadenza - Tempo I", "Le faune", "Fantaisie pour piano et orchestre, L. 73, CD 72 : II. Lento e molto espressivo", "N\u00b0 5: The Film: Clair de Lune", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83a (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "R\u00eaverie", "Estampes, L. 100: III. Jardins sous la pluie. Net et vif", "Les Chansons de Bilitis, L. 96: No. 1. Chant pastoral", "L'isle joyeuse", "Trio in G Major for Violin, Cello and Piano: III. Andante espressivo", "Les contes", "Iberia No. 2 from \\\"Images\\\" for Orchestra: I. Through the Streets and Roads", "Masques, L. 105", "Voici que le printemps", "Syrinx huilulle, L 129", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : III. Menuet", "Sim\u00e9on's Recitative and Aria", "Chansons de Bilitis, L. 90: \u00abLa Chevelure \u00abIl m'a dit \u00abCette nuit d'ai r\u00eav\u00e9\u00bb\u00bb", "Les Chansons de Bilitis, L. 96: No. 4. Chanson", "Le temps a laiss\u00e9 son manteau", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"C'est le dernier soir ...\\\" (Pell\u00e9as, M\u00e9lisande)", "Ce qu'a vu le vent d'ouest", "Fantaisie for piano and orchestra", "Violin Sonata", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b04", "Bilitis", "Crois mon conseil, ch\u00e8re Clim\u00e8ne", "Estampes II: La soir e dans Grenade", "Premi\u00e8re rhapsodie", "Le son du cor s'afflige", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : II. Assez vif et bien rythm\u00e9", "Khamma: Au Mouvement \u2013", "En blanc et noir, L. 134, CD 142", "Two movements from \\\"L\u2019Enfant prodigue\\\": Pr\u00e9lude", "Rapsodie pour orchestre et saxophone", "Proses lyriques, L. 84, CD 90 : II. De Gr\u00e8ve, m\u00e9lodie \\\"Sur la mer les cr\u00e9puscules tombent\\\"", "Romance, L. 52, CD 56, m\u00e9lodie pour voix et piano \\\"Voici que le printemps, ce fil l\u00e9ger d\u2019avril\\\"", "Regret, L. 55, CD 59, m\u00e9lodie pour voix et piano \\\"Devant le ciel d\u2019\u00e9t\u00e9, ti\u00e8de et calme\\\"", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b02", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: I. Le Balcon", "Petite Suite, L. 65, CD 71b, pour orchestre : IV. Ballet", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b03", "En blanc et noir: I. Avec emportement", "Cello sonata in D minor: I. Prologue", "Cinq Po\u00e8mes de Baudelaire", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"Vous ne savez pas o\u00f9 je vous ai men\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "Sonate pour violon et piano en sol mineur, L. 140", "Pr\u00e9ludes", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: III. La Passion", "Children's Corner", "Khamma: Deuxi\u00e8me Danse. Assez anim\u00e9 \u2013 Plus anim\u00e9 peu \u00e0 peu \u2013 Tr\u00e8s anim\u00e9 \u2013", "Preludes, Book II: 10. Canope", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b01", "Sonata for flute, viola and harp", "Sonate pour violoncelle et piano: II. S\u00e9r\u00e9nade", "Iberia No. 2 from \\\"Images\\\" for Orchestra: II. The Perfumes of the Night", "Images pour orchestre", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Le Sommeil de Lear", "Quatuor \u00e0 cordes en sol mineur, op. 10, L. 85, CD 91 : IV. Tr\u00e8s mod\u00e9r\u00e9", "Pell\u00e9as et M\u00e9lisande: Act III, Scene III. \\\"Ah! Je respire enfin!\\\" (Pell\u00e9as, Golaud)", "Preludes, Book II: 8. Ondine", "Sonata for flute, viola and harp, L. 137: III. Finale", "Il dort encore", "Ariettes oubli\u00e9es, L. 60, CD 63a : III. Le rossignol qui, du haut d\u2019une branche, m\u00e9lodie pour voix et piano \\\"L\u2019ombre des arbres dans la rivi\u00e8re embrum\u00e9e\\\"", "Bilitis: I. Pour invoquer Pan, dieu du vent d'\u00e9t\u00e9", "Green", "Deux arabesques, L. 66, CD 74, pour piano : No. 2 en sol majeur, Allegretto scherzando", "Romance, L. 43, CD 53, m\u00e9lodie pour voix et piano \\\"Silence ineffable de l\u2019heure\\\"", "Duo", "La Damoiselle \u00e9lue, L. 62, CD 69, po\u00e8me lyrique pour voix de femmes, solo, ch\u0153ur et orchestre", "Khamma: Premi\u00e8re Danse. Grave et lent \u2013 Animez \u2013 Revenez au Mouvement \u2013 Plus lent \u2013 Animez peu \u00e0 peu \u2013", "La Romance d\u2019Ariel, L. 54, CD 58, m\u00e9lodie pour voix et piano \\\"Au long de ces montagnes douces\\\"", "Les papillons", "\u00c9tudes, L. 136: II. Pour les tierces", "Preludes, Book II: 7. La Terrasse des audiences du clair de lune", "Reflets dans l'eau", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Attention... attention\\\" (Arkel, Le m\u00e9decin)", "En blanc et noir: II. Lent, sombre", "Fantaisie pour piano et orchestre, L. 73, CD 72 : I. Andante - Allegro", "Children's Corner, L. 113: III. Serenade for the Doll", "From Dawn Till Noon on the Sea", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte V. Une chambre dans le ch\u00e2teau", "Pr\u00e9lude \u00e0 l'apr\u00e8s-midi d'un faune, L. 86, CD 87", "Images II: I. Cloches trevers les feuilles", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 1", "Fantaisie pour piano et orchestre, L. 73, CD 72 : III. Allegro molto", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b07", "Nocturnes, L. 91, CD 98: II. F\u00eates", "Rondel chinois", "Prelude (From Suite Bergamasque)", "Danse boh\u00e9mienne, L. 9, CD 4, pour piano", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: I. Soupir \\\"Mon \u00e2me vers ton front o\u00f9 r\u00eave, \u00f4 calme s\u0153ur\\\"", "Proses lyriques, L. 84, CD 90 : IV. De Soirs, m\u00e9lodie \\\"Dimanche sur les villes\\\"", "Nocturnes, L 91: II. F\u00eates", "Les Chansons de Bilitis, L. 96: No. 11. Le Souvenir de Mnasidika", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b03", "Chanson triste", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene II. \\\"Quel est ce bruit? On ferme les portes\\\" (Pell\u00e9as, M\u00e9lisande)", "Preludes, Book II: 3. La Puerta del Vino", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte III, sc\u00e8ne 2. Les souterrains du ch\u00e2teau / Acte IV, sc\u00e8ne 2. Un appartement dans le ch\u00e2teau", "Trois Ballades de Fran\u00e7ois Villon, L. 119: II. Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: I. La Cour de lys", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Je suis ... je suis malade ici\\\" (M\u00e9lisande, Golaud)", "Morceau de concours, L. 108", "Le lilas", "Bilitis: V. Pour l'\u00e9gyptienne", "La Belle au Bois dormant", "Printemps, L. 61, CD 68b, suite symphonique en mi majeur pour 2 pianos et orchestre", "Ariettes oubli\u00e9es, L. 60, CD 63a : IV. Paysages belges. Chevaux de bois, m\u00e9lodie pour voix et piano \\\"Tournez, tournez, bons chevaux de bois\\\"", "Yver, vous n'estes qu'un vilain", "Suite Bergamasque - Menuet", "Dans le Jardin", "Les Elfes, CD 25, m\u00e9lodie pour voix et piano \\\"Du sentier des bois aux daims familiers\\\"", "Khamma", "Rondeau: 'Fut-il jamais'", "Pagodes", "Triolet \u00e0 Philis \\\"Z\u00e9phir\\\", L. 12, CD 19, m\u00e9lodie pour voix et piano \\\"Si j\u2019\u00e9tais le z\u00e9phir ail\u00e9\\\"", "Images pour orchestre, L 122: I. Gigues", "Fleurs des bl\u00e9s, L. 7, CD 16, m\u00e9lodie pour voix et piano \\\"Le long des bl\u00e9s que la brise fait onduler\\\"", "Sonate f\u00fcr Violine und Klavier in g-Moll: Finale: Tr\u00e8s anim\u00e9", "Images pour orchestre, L 122: III. Rondes de printemps", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Qu'est-ce qui brille ainsi, au fond de l'eau?\\\" (Golaud, M\u00e9lisande)", "Petite Suite", "Pr\u00e9ludes, Livre I, L. 117: VIII. La fille aux cheveux de lin. Tr\u00e8s calme et doucement expressif", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement de piano, L. 116, CD 124a", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Je n'en dis rien\\\" (Arkel, Genevi\u00e8ve)", "Suite bergamasque, L. 75, CD 82b, pour orchestre : III. Clair de lune", "La pluie au matin", "Les Comparaisons", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : I. F\u00eate", "L'enfant prodigue", "Suite bergamasque, L. 75, CD 82, pour orchestre : III. Clair de lune", "Deux danses pour Harpe, L. 103", "Children's Corner, L. 113: V. The Little Shepherd", "Le Tombeau des Na\u00efades", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Ah! Ah! Petite m\u00e8re a allum\u00e9 sa lampe\\\" (Yniold, Golaud)", "\u00c9tudes, L. 136: IX. Pour les notes r\u00e9p\u00e9t\u00e9es", "Valse romantique", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Il fait sombre dans les jardins\\\" (M\u00e9lisande, Genevi\u00e8ve, Pell\u00e9as)", "La Bo\u00eete \u00e0 joujoux : VI. \u00c9pilogue", "Nuit d\u2019\u00e9toiles, L. 4, CD 2, m\u00e9lodie pour voix et piano \\\"Nuit d\u2019\u00e9toiles, sous tes voiles\\\"", "\u00c9l\u00e9gie, L. 138", "Premi\u00e8re Rapsodie pour clarinette en si b\u00e9mol, avec accompagnement d'orchestre, L. 116, CD 124b", "Beau soir", "Nuits blanches", "Pell\u00e9as et M\u00e9lisande: Act II, Scene I. \\\"C'est au bord d'une fontaine aussi qu'il vous a trouv\u00e9e?\\\" (Pell\u00e9as, M\u00e9lisande)", "\u00c9tudes, L. 136: VII. Pour les degr\u00e9s chromatiques", "Jardins sous la pluie", "La Partie d'osselets", "Coquetterie posthume, L. 39, CD 50, m\u00e9lodie pour voix et piano \\\"Quand je mourrai, que l\u2019on me mette\\\"", "Pour ce que Plaisance est morte", "Petite Suite, L. 65: I. En bateau", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b02", "Six \u00e9pigraphes antiques, L. 131: II. Pour un tombeau sans nom", "Proses lyriques, L. 84, CD 90 : III. De Fleurs, m\u00e9lodie \\\"Dans l\u2019ennui si d\u00e9sol\u00e9ment vert\\\"", "Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.", "Le Martyre de Saint-S\u00e9bastien: IV. Le laurier bless\u00e9: N\u00b01", "La Fl\u00fbte de Pan", "Pr\u00e9ludes, Livre II, L. 123: IX. Hommage \u00e0 S. Pickwick Esq. P.P.M.P.C.. Grave", "Prelude No. 8: The Girl with the Flaxen Hair", "Fantoches", "Trois Chansons de France", "En blanc et noir", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: IV. Recueillement", "De gr\u00e8ve", "F\u00eates galantes II, L. 104: I. Les Ing\u00e9nus", "Ariettes oubli\u00e9es, L. 60, CD 63a : II. Il pleut doucement sur la ville, m\u00e9lodie pour voix et piano \\\"Il pleure dans mon c\u0153ur comme il pleure sur la ville\\\"", "Le Martyre de Saint-S\u00e9bastien: III. Le concile des faux dieux: N\u00b05", "Les Chansons de Bilitis, L. 96: No. 8. Les Courtisanes \u00e9gyptiennes", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Pell\u00e9as part ce soir\\\" (Golaud, Arkel, M\u00e9lisande)", "Pierrot", "Lorsqu'elle est entr\u00e9e", "Aimons-nous et dormons, L. 16, CD 7, m\u00e9lodie pour voix et piano \\\"Aimons-nous et dormons, sans songer au reste du monde\\\"", "Pell\u00e9as et M\u00e9lisande - Concert Suite: Acte I. Une for\u00eat", "La damoiselle \u00e9lue", "Bilitis: IV. Pour la danseuse aux crotales", "Pantomime", "Petite Suite, L. 65, CD 71b, pour orchestre : I. En bateau", "Six \u00e9pigraphes antiques, L. 131: IV. Pour la danseuse aux crotales", "Il pleure dans mon c\u0153ur", "Fantaisie pour piano et orchestre, L. 73, CD 72", "Nocturnes", "S\u00e9r\u00e9nade", "Je tremble en voyant ton visage", "Syrinx", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'y-a-t-il? Qu'est-ce que toutes ces femmes viennent faire ici?\\\" (Golaud, Le m\u00e9decin, Arkel)", "Three Preludes: Feux d'artifice", "En sourdine", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : III. Clair de lune, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "F\u00eates Galantes II", "Six \u00e9pigraphes antiques, L. 131: VI. Pour remercier la pluie au matin", "Images oubli\u00e9es, L. 87: Lent (m\u00e9lancolique et doux)", "Images I: II. Hommages Rameau", "Preludes, Book II: 2. Feuilles mortes", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Voici ce qu'il \u00e9crit \u00e0 son fr\u00e8re Pell\u00e9as: 'Un soir, je l'ai trouv\u00e9e'\\\" (Genevi\u00e8ve)", "La mer est plus belle", "Chant pastoral", "Mandoline, L. 29, CD 43, m\u00e9lodie pour voix et piano \\\"Les donneurs de s\u00e9r\u00e9nades\\\"", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : II. Le son du cor s\u2019afflige vers les bois.", "Sonata for flute, viola and harp, L. 137: I. Pastorale", "Pell\u00e9as et M\u00e9lisande: Act III, Scene II. \\\"Prenez garde; par ici, par ici\\\" (Golaud, Pell\u00e9as)", "Printemps, L. 61, CD 68a, suite symphonique en mi majeur pour 2 pianos et ch\u0153ur", "Chanson espagnole, L. 42, CD 49, duo pour 2 voix \u00e9gales \\\"Nous venions de voir le taureau\\\"", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: II. \\\"Quand j'ay ouy le tabourin\\\"", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b02", "Les Chansons de Bilitis, L. 96: No. 9. L'Eau pure du bassin", "Khamma: Sc\u00e8ne 2 \u2013", "Ministrels for Cello and Piano", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\"", "Le balcon", "Le Souvenir de Mnasidika", "Deux danses pour Harpe, L. 103: Danse sacr\u00e9e", "Ballade (slave), L. 70, CD 78, pour piano", "D'un cahier d'esquisses, L. 99", "L\u2019Archet", "Rodrigue et Chim\u00e8ne", "Suite bergamasque", "\u00c9tudes, L. 136: XII. Pour les accords", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : I. En bateau", "No\u00ebl des enfants qui n'ont plus de maisons", "Le Martyre de Saint-S\u00e9bastien: I. La cour des lys: N\u00b01", "Paysage sentimental", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Maintenant que le p\u00e8re de Pell\u00e9as est sauv\u00e9 ...\\\" (Arkel, M\u00e9lisande)", "Suite: Pour le Piano, L.95: III. Toccata (vif)", "La mort des amants", "Les Chansons de Bilitis, L. 96: No. 10. La Danseuse aux crotales", "Ballade que Villon feit \u00e0 la requeste de sa m\u00e8re pour prier Nostre-Dame", "Dans le jardin, L. 78, CD 107, m\u00e9lodie pour voix et piano \\\"Je regardais dans le jardin\\\"", "Regret", "Children's Corner, L. 113 No. 5: The Little Shepherd", "Streichquartett in g-Moll, Op. 10: Andantino, doucement expressif (Endress-Quartett)", "Images pour orchestre, L 122: II. Iberia", "La grotte", "Des pas sur la neige", "Musique pour \u201cLe Roi Lear\u201d, L. 107: Fanfare d\u2019ouverture", "Children's Corner - I. Doctor Gradus ad Parnassum", "Minstrels", "Children's Corner, L. 113: IV. The Snow is Dancing", "Aupr\u00e8s de cette grotte sombre", "Voiles", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Air de danse", "Le Martyre de Saint-S\u00e9bastien, L. 124: Acte III \\\"Le Concile des faux dieux\\\". Fanfare no. 2", "Nocturnes, L 91: III. Sir\u00e8nes", "F\u00eates galantes (Premier recueil), L. 80, CD 86 : II. Fantoches, m\u00e9lodie pour voix et piano \\\"Scaramouche et Pulcinella\\\"", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Qu'avez-vous fait? Vous allez la tuer\\\" (Arkel, Golaud, M\u00e9lisande)", "\u00c9tudes, L. 136: V. Pour les octaves", "Pell\u00e9as et M\u00e9lisande", "F\u00eate galante, L. 23, CD 31, m\u00e9lodie pour voix et piano \\\"Voil\u00e0 Sylandre et Lycas et Myrtil\\\"", "Sonata for Violin and Piano in G Minor, L 140: I. Allegro vivo", "L'\u00e9chelonnement des haies", "Petite Suite, L. 65, CD 71b, pour orchestre : III. Menuet", "F\u00eates galantes II, L. 104: II. Le Faune", "Le petit N\u00e8gre, L. 114", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Non, non, nous n'avons pas \u00e9t\u00e9 coupables\\\" (M\u00e9lisande, Golaud)", "Deux arabesques", "Danse: Tarantelle styrienne", "Apparition, L. 53, CD 57, m\u00e9lodie pour voix et piano \\\"La lune s\u2019attristait. Des s\u00e9raphins en pleurs\\\"", "Khamma, L 125", "Children's Corner, L. 113 No. 6: Golliwog's Cake-walk", "Les cloches", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Je les noue, je les noue aux branches du saule\\\" (Pell\u00e9as, M\u00e9lisande, Golaud)", "De fleurs", "La Belle au bois dormant, L. 74, CD 81, m\u00e9lodie \\\"Des trous \u00e0 son pourpoint vermeil\\\"", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : III. \\\"Je tremble en voyant ton visage\\\"", "Trois chansons de Charles d'Orl\u00e9ans, pour ch\u0153ur \u00e0 quatre voix mixtes, L. 92, CD 99: III. \\\"Yver, vous n'estes qu'un vilain \\\"", "C'est l'extase", "Trois Ballades de Fran\u00e7ois Villon", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: II. Danse extatique et final du premier acte", "Pantomime, L. 31, CD 47, m\u00e9lodie pour voix et piano \\\"Pierrot, qui n\u2019a rien d\u2019un Clitandre\\\"", "g.11b821q1dm", "Bilitis: III. Pour que la nuit soit propice", "La chute de la maison Usher", "Chevaux de bois", "La Mer: II. Jeux de vagues", "Trio in G Major for Violin, Cello and Piano: II. Scherzo - Intermezzo. Moderato con allegro", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"Apporte-la\\\" (Golaud, Arkel, M\u00e9lisande)", "La Bo\u00eete \u00e0 joujoux : III. 2e tableau. Le Champ de bataille", "\u00c9tudes", "Three Preludes: Feuilles mortes", "Pell\u00e9as et M\u00e9lisande: Act III, Scene IV. \\\"Viens, nous allons nous asseoir ici, Yniold\\\" (Golaud, Yniold)", "Images oubli\u00e9es, L. 87: Quelques aspects de 'Nous n'irons plus au bois' parce qu'il fait un temps insupportable. Tr\u00e8s vite - Mod\u00e9r\u00e9 - Premier Mouvement (vit et joyeax)", "La Fille aux cheveux de lin, L. 33, CD 15, m\u00e9lodie pour voix et piano \\\"Sur la luzerne en fleur\\\"", "Pell\u00e9as et M\u00e9lisande: Act II, Scene II. \\\"Ah! Ah! Tout va bien, ce la ne sera rien\\\" (Golaud, M\u00e9lisande)", "Pagodes from Estampes", "Printemps, L. 61, CD 68b : II. Mod\u00e9r\u00e9", "Ballade de Villon a s'amye", "\u00c9tudes, L. 136: VI. Pour les huits doigts", "Pell\u00e9as et M\u00e9lisande, L. 88, CD 93: Acte IV", "Pell\u00e9as et M\u00e9lisande: Act IV, Scene I. \\\"O\u00f9 vas-tu? Il faut que je te parle ce soir\\\" (Pell\u00e9as, M\u00e9lisande)", "Nocturnes, L. 91, CD 98: III. Sir\u00e8nes", "Calme dans le demi-jour", "Petite Pi\u00e8ce", "Les Courtisanes \u00e9gyptiennes", "Pr\u00e9ludes, Livre II, L. 123: IV. \u00abLes f\u00e9es sont d'exquises danseuses\u00bb. Rapide et l\u00e9ger", "Suite bergamasque, L. 75, CD 82a, pour orchestre : III. Clair de lune", "Musique", "Sonata for Violin and Piano in G Minor, L 140: II. Interm\u00e8de: fantasque et l\u00e9ger", "Brouillards", "Le Tombeau sans nom", "Three Preludes: Ce qu'a vu le vent de l'ouest", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Mes longs cheveux descendent\\\"", "F\u00eates galantes II, L. 104: III. Colloque sentimental", "Clair de lune Samba", "My Reverie", "Petite Suite, L. 65, CD 71b, pour orchestre : II. Cort\u00e8ge", "Nocturnes, L. 91, CD 98: I. Nuages", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: V. La Mort des amants", "Clair de lune, L. 32, CD 45, m\u00e9lodie pour voix et piano \\\"Votre \u00e2me est un paysage choisi\\\"", "Apparition", "Rhapsodie pour saxophone et orchestre, L. 98", "Children's Corner, L. 113: VI. Golliwogg's Cake-Walk", "Menuet", "Tarantelle styrienne, L. 69, CD 77a, (Danse) pour piano", "Le Martyre de Saint-S\u00e9bastien, fragments symphoniques: IV. Le Bon Pasteur", "Mandoline", "Trag\u00e9die", "\u00c9tudes, L. 136: XI. Pour les arp\u00e8ges compos\u00e9s", "Clair de lune", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. \\\"Oh! Oh! Mes cheveux descendent de la tour!\\\" (M\u00e9lisande, Pell\u00e9as)", "Cello Sonata", "III. Le Vent dans la plaine", "Trois Chansons de France, L. 102: II. La Grotte: Aupr\u00e8s de cette grotte sombre", "Romance (Deux Romances, No. 2, 1891)", "De r\u00eave", "Pell\u00e9as et M\u00e9lisande: Act V. \\\"Ouvrez la fen\u00eatre, ouvrez la fen\u00eatre ...\\\" (M\u00e9lisande, Arkel, Le m\u00e9decin)", "Quant j'ai ouy le tabourin", "Dieu! qu'il la fait bon regarder!", "Pell\u00e9as et M\u00e9lisande: Act I, Scene I. \\\"Je ne pourrai plus sortir de cette for\u00eat\\\" (Golaud, M\u00e9lisande)", "Piano Trio", "Preludes, Book I: 2. Voiles", "\u00c9tude retrouv\u00e9e", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : III. R\u00eave", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b02", "Soupir", "Le Martyre de Saint-S\u00e9bastien: II. La chambre magique: N\u00b03", "Berceuse h\u00e9ro\u00efque pour piano, L. 132", "Rapsodie pour saxophone et piano, L. 98", "Le diable dans le beffroi", "Lindaraja", "My Reverie (Debussy's \\\"Reverie\\\")", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains", "Images, Livre 2, L. 111: No. 2. Et la lune descend sur le temple qui fut", "Le promenoir des deux amants", "\u00c9tudes, L. 136: I. Pour les \u00ab cinq doigts \u00bb d'apr\u00e8s monsieur Czerny", "Harmonie du soir", "Romance", "Colloque sentimental", "\u00c9tudes, L. 136: IV. Pour les sixtes", "Trois Ballades de Fran\u00e7ois Villon, L. 119: I. Ballade de Villon \u00e0 s'Amye", "The Tears of Billie Blue", "Pour le piano: Sarabande", "Cort\u00e8ge et Air de danse de \\\"L\u2019Enfant prodigue\\\": Cort\u00e8ge", "Hommage \u00e0 Joseph Haydn, L. 115", "La Bo\u00eete \u00e0 joujoux : II. 1er tableau. Le Magasin de jouets", "Le promenoir des deux amants, L. 118, CD 129, m\u00e9lodies pour voix et piano : II. \\\"Crois mon conseil, ch\u00e8re Clim\u00e8ne\\\"", "Preludes, Book I: 7. Ce qu'a vu le vent d'ouest", "Fleur des Bl\u00e9s", "La Bo\u00eete \u00e0 joujoux : V. 4e tableau. Apr\u00e8s fortune faite", "Les Chansons de Bilitis, L. 96: No. 5. La Partie d'osselets", "La mer", "L\u2019Enfant prodigue, L. 57, CD 61, sc\u00e8ne lyrique \\\"L\u2019ann\u00e9e, en vain chasse l\u2019ann\u00e9e\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene II. \\\"Grand-p\u00e8re, j'ai re\u00e7u en m\u00eame temps que la lettre de mon fr\u00e8re...\\\" (Pell\u00e9as, Arkel, Genevi\u00e8ve)", "Chanson espagnole", "Les Chansons de Bilitis, L. 96: No. 6. Bilitis", "Cinq po\u00e8mes de Baudelaire, L. 64, CD 70: II. Harmonie du soir", "Estampes - I. Pagodes", "R\u00eaverie, L. 8, CD 3, m\u00e9lodie pour voix et piano \\\"Le Z\u00e9phir \u00e0 la douce haleine\\\"", "Pr\u00e9ludes, Livre 1, L. 117 No. 9: La s\u00e9r\u00e9nade interrompue", "Musique, L. 44, CD 54, m\u00e9lodie pour voix et piano \\\"La lune se levait, pure, mais plus glac\u00e9e\\\"", "Pell\u00e9as et M\u00e9lisande: Act I, Scene III. \\\"Ho\u00e9! Hisse ho\u00e9! Ho\u00e9! Ho\u00e9!\\\" (Ch\u0153r, M\u00e9lisande, Pell\u00e9as, Genevi\u00e8ve)", "Petite Suite, L. 65, CD 71a, pour piano \u00e0 4 mains : IV. Ballet", "Flots, palmes, sables, L. 25, CD 38, m\u00e9lodie pour voix et piano (ou harpe) \\\"Loin des yeux du monde\\\"", "Ballade des femmes de Paris", "Les Chansons de Bilitis, L. 96: No. 12. La Pluie au matin", "Le Matelot qui tombe \u00e0 l\u2019eau", "Premi\u00e8re Suite d\u2019orchestre, L. 50, CD 46 : II. Ballet", "La Bo\u00eete \u00e0 joujoux : IV. 3e tableau. La Bergerie \u00e0 vendre", "Coquetterie posthume", "Chansons de Bilitis, L. 90: Le Tombeau des na\u00efades: \u00abLe long du bois couvert de givre\u00bb", "Arabesque 1", "No\u00ebl des enfants qui n'ont plus de maison, L. 139, CD 147, m\u00e9lodie pour voix et piano \\\"Nous n\u2019avons plus de maison\\\"", "Trois Chansons de France, L. 102, CD 115: I. Rondel: Le temps a laiss\u00e9 son manteau", "Mes longs cheveux", "Children's Corner, L. 113: II. Jimbo's Lullaby", "Mazurka, L. 67, CD 75, pour piano", "Berceuse h\u00e9ro\u00efque pour orchestre, L. 132", "La Romance d'Ariel", "Les Chansons de Bilitis, L. 96: No. 3. Les Contes", "Trois po\u00e8mes de St\u00e9phane Mallarm\u00e9, L. 127, CD 135: II. Placet futile \\\"Princesse! \u00c0 jalouser le destin d'une H\u00e9b\u00e9\\\"", "Trois Chansons de France, L. 102: III. Rondel: Pour ce que Plaisance est morte", "Images oubli\u00e9es, L. 87: Dans le mouvement d'une 'Sarabande', c'est-\u00e0-dire avec une \u00e9l\u00e9gance gr\u00e2ve et lente, m\u00eame un peu vieux portrait, souvenir du Louvre, etc.", "Pour le piano: II. Sarabande", "Les Ang\u00e9lus", "Spleen", "String Quartet", "Six \u00e9pigraphes antiques, L. 131: V. Pour l'\u00c9gyptienne", "Trois M\u00e9lodies, L. 81, CD 85, pour une voix avec accompagnement de piano : III. L\u2019\u00e9chelonnement des haies moutonne \u00e0 l\u2019infini", "En blanc et noir: III. Scherzando", "Nocturne, L. 82, CD 89, pour piano", "Nuit sans fin", "L\u2019Enfant prodigue : 2b. Pourquoi m'as-tu quitt\u00e9e (Lia)", "Pell\u00e9as et M\u00e9lisande: Act III, Scene I. Une des tours du ch\u00e2teau", "Les Chansons de Bilitis, L. 96: No. 7. Le Tombeau sans nom", "Dialogue of the Wind and the Sea", "Le Martyre de Saint-S\u00e9bastien: V. Le paradis: N\u00b01", "Marche \u00e9cossaise sur un th\u00e8me populaire, L. 77, CD 83b (Marche des anciens comtes de Ross, d\u00e9di\u00e9e \u00e0 leur descendant le G\u00e9n\u00e9ral Meredith Reid, grand-croix de l'ordre royal du R\u00e9dempteur)", "Khamma: Sc\u00e8ne 1. Le Temple int\u00e9rieur du Grand-Dieu Amun-Ra \u2013", "Star Gazers Theme Song"], "ans_acc": 0.005928853754940711, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.04545454545454545, "path_precision": 0.1111111111111111, "path_recall": 0.02857142857142857, "path_ans_f1": 0.011650485436893203, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.005928853754940711}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> symbols.name_source.namesakes -> Franklin Township\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.spouse_s -> m.0j6f4rh\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.deceased_person.place_of_burial -> Christ Church Burial Ground\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666666, "path_precision": 0.125, "path_recall": 0.25, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-740", "prediction": ["# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nGianna", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.character -> Gianna\n# Answer:\nGianna", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> g.11b7vzwxql\n# Answer:\ntv.tv_actor.starring_roles", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.series -> Barney & Friends\n# Answer:\ntv.regular_tv_appearance.series", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0y55_3z -> tv.regular_tv_appearance.character -> Angela\n# Answer:\nAngela", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.03lyrfq -> tv.regular_tv_appearance.character -> Alex Russo\n# Answer:\nAlex Russo", "# Reasoning Path:\nBarney & Friends -> tv.tv_program.regular_cast -> m.0bgyvq1 -> tv.regular_tv_appearance.actor -> Selena Gomez\n# Answer:\ntv.regular_tv_appearance.actor", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.guest_roles -> m.0y5cxrq -> tv.tv_guest_role.character -> Alex Russo\n# Answer:\nAlex Russo", "# Reasoning Path:\nSelena Gomez -> tv.tv_actor.starring_roles -> m.0bgyvq1 -> tv.regular_tv_appearance.seasons -> Barney & Friends - Season 8\n# Answer:\nBarney & Friends - Season 8"], "ground_truth": ["Gianna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.2222222222222222, "path_recall": 0.11764705882352941, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-741", "prediction": ["# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.actor -> Stanley Tucci\n# Answer:\nStanley Tucci", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kw1 -> film.performance.character -> Caesar Flickerman\n# Answer:\nCaesar Flickerman", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.actor -> Jennifer Lawrence\n# Answer:\nJennifer Lawrence", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv7 -> film.performance.actor -> Liam Hemsworth\n# Answer:\nLiam Hemsworth", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kvg -> film.performance.actor -> Woody Harrelson\n# Answer:\nWoody Harrelson", "# Reasoning Path:\nThe Hunger Games -> common.topic.article -> m.0pdcds1\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kvp -> film.performance.actor -> Josh Hutchersonm\n# Answer:\nJosh Hutchersonm", "# Reasoning Path:\nThe Hunger Games -> film.film.starring -> m.0gw7kv0 -> film.performance.character -> Katniss Everdeen\n# Answer:\nKatniss Everdeen", "# Reasoning Path:\nThe Hunger Games -> film.film.songs -> m.0j6q1bx -> film.film_song_relationship.performers -> Sting\n# Answer:\nSting"], "ground_truth": ["Stanley Tucci"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-742", "prediction": ["# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Adams County\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.containedby -> Pennsylvania\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> common.topic.notable_for -> g.1255f923w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGettysburg Address -> common.topic.article -> m.037z4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Gettysburg National Cemetery\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> book.book_subject.works -> Notes on the 150th Anniversary of the Emancipation Proclamation -> book.written_work.subjects -> Tony Kushner\n# Answer:\nNotes on the 150th Anniversary of the Emancipation Proclamation", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.statistical_region.population -> g.11b66jrztz\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.contains -> Gettysburg National Cemetery\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> travel.travel_destination.tourist_attractions -> Shriver House Museum\n# Answer:\nGettysburg", "# Reasoning Path:\nGettysburg Address -> time.event.locations -> Gettysburg -> location.location.people_born_here -> Beale M. Schmucker\n# Answer:\nGettysburg"], "ground_truth": ["Gettysburg"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-743", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.02_wjwf -> people.sibling_relationship.sibling -> Ted Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbyccm\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Joseph P. Kennedy, Jr.\n# Answer:\nKennedy family", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03lfpfx -> people.sibling_relationship.sibling -> Eunice Kennedy Shriver\n# Answer:\nEunice Kennedy Shriver", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.sibling_s -> m.0pbycd1\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Joseph P. Kennedy, Sr.\n# Answer:\nKennedy family"], "ground_truth": ["Ted Kennedy", "Joseph P. Kennedy, Jr.", "Robert F. Kennedy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.2727272727272727, "path_precision": 0.375, "path_recall": 0.21428571428571427, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-744", "prediction": ["# Reasoning Path:\nTupac Shakur -> music.artist.label -> Interscope Records -> common.topic.notable_types -> Record label\n# Answer:\nInterscope Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> common.topic.notable_types -> Record label\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Young Noble\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> organization.organization.founders -> Suge Knight\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Bad Azz\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> common.topic.notable_for -> g.1256c6brp\n# Answer:\nAmaru Entertainment", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Death Row Records -> music.record_label.artist -> Big Syke\n# Answer:\nDeath Row Records", "# Reasoning Path:\nTupac Shakur -> music.artist.label -> Amaru Entertainment -> music.record_label.artist -> Outlawz\n# Answer:\nAmaru Entertainment"], "ground_truth": ["Interscope Records", "Death Row Records", "Amaru Entertainment"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-745", "prediction": ["# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Vitellius\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.article -> m.01g152\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> common.topic.notable_types -> Character Occupation\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Antoninus Pius\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> base.schemastaging.context_name.pronunciation -> g.125_qksdz\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Caligula\n# Answer:\nRoman emperor", "# Reasoning Path:\nAugustus -> people.person.profession -> Politician -> people.profession.specializations -> Tyrant\n# Answer:\nPolitician", "# Reasoning Path:\nAugustus -> people.person.profession -> Roman emperor -> people.profession.people_with_this_profession -> Caracalla\n# Answer:\nRoman emperor"], "ground_truth": ["Politician", "Roman emperor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-747", "prediction": ["# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.official_language -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.writing_system -> Latin alphabet\n# Answer:\nSpanish Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Kogi Language -> language.human_language.language_family -> Chibchan languages\n# Answer:\nKogi Language", "# Reasoning Path:\nColombia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> Belize\n# Answer:\nSpanish Language"], "ground_truth": ["Wayuu Language", "Macuna Language", "P\u00e1ez language", "Piapoco Language", "Playero language", "Siona Language", "Inga, Jungle Language", "Piaroa Language", "Tunebo, Angosturas Language", "Achawa language", "Tucano Language", "Providencia Sign Language", "Omejes Language", "Waimaj\u00e3 Language", "Koreguaje Language", "Cof\u00e1n Language", "Macagu\u00e1n Language", "Cumeral Language", "Guahibo language", "Guambiano Language", "Cams\u00e1 Language", "Minica Huitoto", "Hupd\u00eb Language", "Carijona Language", "Baudo language", "Arhuaco Language", "Cuiba language", "Nukak language", "Piratapuyo Language", "Cabiyar\u00ed Language", "Puinave Language", "Spanish Language", "Awa-Cuaiquer Language", "Totoro Language", "Cagua Language", "Andoque Language", "Uwa language", "Romani, Vlax Language", "Siriano Language", "Tunebo, Barro Negro Language", "Murui Huitoto language", "Coxima Language", "Bora Language", "Curripaco Language", "Ponares Language", "Tanimuca-Retuar\u00e3 Language", "Ticuna language", "Runa Language", "Islander Creole English", "Kuna, Border Language", "Tunebo, Central Language", "Tuyuca language", "Palenquero Language", "Carabayo Language", "Bar\u00ed Language", "Natagaimas Language", "Ember\u00e1, Northern Language", "Tunebo, Western Language", "Tomedes Language", "Tinigua language", "Yucuna Language", "Macaguaje Language", "Inga Language", "Yukpa Language", "Andaqui Language", "Tama Language", "Barasana Language", "Ocaina Language", "Quechua, Napo Lowland Language", "Kogi Language", "Catio language", "Anserma Language", "Cocama language", "Cubeo Language", "Nonuya language", "Malayo Language", "Guanano Language", "Desano Language", "Nheengatu language", "Coyaima Language", "S\u00e1liba Language", "Chipiajes Language", "Pijao Language", "Muinane Language", "Guayabero Language", "Colombian Sign Language"], "ans_acc": 0.023255813953488372, "ans_hit": 1, "ans_f1": 0.04545454545454545, "ans_precission": 1.0, "ans_recall": 0.023255813953488372, "path_f1": 0.04582651391162029, "path_precision": 0.875, "path_recall": 0.023529411764705882, "path_ans_f1": 0.04545454545454545, "path_ans_precision": 1.0, "path_ans_recall": 0.023255813953488372}
{"id": "WebQTest-748", "prediction": ["# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_for -> g.125fmlxn3\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> location.postal_code.country -> United States of America\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.location.geometry -> m.058lwr7\n# Answer:\n12302", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12301 -> common.topic.notable_types -> Postal Code\n# Answer:\n12301", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12303 -> location.location.geometry -> m.058lwmk\n# Answer:\n12303", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.postal_code.country -> United States of America\n# Answer:\n12302", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12304 -> location.location.geometry -> m.058lw2n\n# Answer:\n12304", "# Reasoning Path:\nSchenectady -> common.topic.notable_for -> g.125bwlwcl\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSchenectady -> location.statistical_region.population -> g.11b66fk06n\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSchenectady -> location.citytown.postal_codes -> 12302 -> location.location.contains -> Schenectady County Airport\n# Answer:\n12302"], "ground_truth": ["12302", "12308", "12304", "12307", "12303", "12306", "12305", "12345", "12301", "12309", "12325"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.5000000000000001, "ans_precission": 0.8, "ans_recall": 0.36363636363636365, "path_f1": 0.5000000000000001, "path_precision": 0.8, "path_recall": 0.36363636363636365, "path_ans_f1": 0.5000000000000001, "path_ans_precision": 0.8, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-749", "prediction": ["# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.office_holder -> Richard Nixon\n# Answer:\nRichard Nixon", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.office_holder -> Harry S. Truman\n# Answer:\nHarry S. Truman", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03q2h_9 -> government.government_position_held.office_holder -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx8cf -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.01xrx8q -> government.government_position_held.office_holder -> George H. W. Bush\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.02h6phn -> government.government_position_held.office_holder -> George W. Bush\n# Answer:\nGeorge W. Bush", "# Reasoning Path:\nPresident of the United States -> government.government_office_or_title.office_holders -> m.03fx817 -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Ronald Reagan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.risk_factor.diseases -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Old age\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer -> medicine.risk_factor.diseases -> Jaundice\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.survival_rates -> m.04nvf96\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.risk_factors -> Obesity\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nPancreatic cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-751", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> law.inventor.inventions -> Google Search\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> people.person.ethnicity -> Jewish people\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> law.inventor.inventions -> PageRank\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> influence.influence_node.influenced -> Mukil Elango\n# Answer:\nLarry Page", "# Reasoning Path:\nGoogle -> computer.software_developer.software -> Google Apps for Work -> organization.organization.founders -> Larry Page\n# Answer:\nGoogle Apps for Work", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Sergey Brin -> people.person.profession -> Entrepreneur\n# Answer:\nSergey Brin", "# Reasoning Path:\nGoogle -> organization.organization.founders -> Larry Page -> people.person.nationality -> United States of America\n# Answer:\nLarry Page"], "ground_truth": ["Patrick Pichette", "Larry Page", "Nikesh Arora", "David Drummond", "Vic Gundotra"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.22222222222222224, "ans_precission": 0.25, "ans_recall": 0.2, "path_f1": 0.2, "path_precision": 0.25, "path_recall": 0.16666666666666666, "path_ans_f1": 0.26086956521739135, "path_ans_precision": 0.375, "path_ans_recall": 0.2}
{"id": "WebQTest-752", "prediction": ["# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb760\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> base.popstra.product.sold_to -> m.064hhfy\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76d\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> common.topic.webpage -> m.02sr4y1\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.ingredient.more_general_ingredient -> Soft drink\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb76s\n# Answer:\nCoca-Cola", "# Reasoning Path:\nJohn Pemberton -> symbols.name_source.namesakes -> Pemberton Place -> location.location.geolocation -> m.01227nwv\n# Answer:\nPemberton Place", "# Reasoning Path:\nJohn Pemberton -> law.inventor.inventions -> Coca-Cola -> food.food.nutrients -> m.07xb774\n# Answer:\nCoca-Cola"], "ground_truth": ["Coca-Cola"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-753", "prediction": ["# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> sports.sports_position.sport -> Football\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> sports.pro_athlete.sports_played_professionally -> m.0d5fbrd -> sports.pro_sports_played.sport -> Football\n# Answer:\nFootball", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> sports.sport.pro_athletes -> m.011p3wnv\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> common.topic.article -> m.013s75\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.webpage -> m.09w2grw\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Forward -> common.topic.notable_types -> Football Position\n# Answer:\nForward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Outside forward -> sports.sports_position.players -> m.011l0tb4\n# Answer:\nOutside forward", "# Reasoning Path:\nSir Stanley Matthews -> soccer.football_player.position_s -> Outside forward -> common.topic.image -> m.02gydjp\n# Answer:\nOutside forward", "# Reasoning Path:\nSir Stanley Matthews -> people.person.profession -> Football player -> common.topic.notable_types -> Profession\n# Answer:\nFootball player"], "ground_truth": ["Midfielder", "Outside forward", "Forward"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.6666666666666666, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.6666666666666666, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-755", "prediction": ["# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> Mary of Modena\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.children -> Henry Benedict Stuart\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> Maria Clementina Sobieska -> people.person.children -> Henry Benedict Stuart\n# Answer:\nMaria Clementina Sobieska", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.parents -> James II of England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.ethnicity -> Scottish people -> people.ethnicity.languages_spoken -> Scottish Gaelic\n# Answer:\nScottish people", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.nationality -> England\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.parents -> James Francis Edward Stuart -> people.person.places_lived -> m.04hxf2h\n# Answer:\nJames Francis Edward Stuart", "# Reasoning Path:\nCharles Edward Stuart -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Dupuytren's contracture\n# Answer:\nMale", "# Reasoning Path:\nCharles Edward Stuart -> common.topic.notable_for -> g.125fbmkkn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Edward Stuart -> common.topic.notable_for -> g.126stx883\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Charles Edward Stuart"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-756", "prediction": ["# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.04hbsqv\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> people.person.profession -> Voice Actor\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Meg Griffin\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_actor.starring_roles -> m.02ntr0h\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> tv.tv_producer.programs_produced -> m.010hsx_r\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.siblings -> m.05sr55v -> fictional_universe.sibling_relationship_of_fictional_characters.siblings -> Bertram Griffin\n# Answer:\nBertram Griffin", "# Reasoning Path:\nStewie Griffin -> tv.tv_character.appeared_in_tv_program -> m.02kk65p -> tv.regular_tv_appearance.actor -> Seth MacFarlane\n# Answer:\nSeth MacFarlane", "# Reasoning Path:\nStewie Griffin -> fictional_universe.fictional_character.character_created_by -> Seth MacFarlane -> fictional_universe.fictional_character_creator.fictional_characters_created -> Avery Bullock\n# Answer:\nSeth MacFarlane"], "ground_truth": ["Seth MacFarlane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-757", "prediction": ["# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Rolland F. Bertrand\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.parents -> Lois June Gouwens\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> tv.tv_actor.guest_roles -> m.09p2yk0\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nJon Voight", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.children -> James Haven\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.profession -> Film Producer\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Marcheline Bertrand -> people.person.gender -> Female\n# Answer:\nMarcheline Bertrand", "# Reasoning Path:\nAngelina Jolie -> people.person.parents -> Jon Voight -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nJon Voight"], "ground_truth": ["Marcheline Bertrand", "Jon Voight"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-758", "prediction": ["# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.politician.government_positions_held -> m.04l4slb\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> common.image.appears_in_topic_gallery -> Indiana gubernatorial election, 2008\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> government.political_appointer.appointees -> m.010hbc1b\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> common.image.size -> m.0kl_q0\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> common.topic.notable_for -> g.125cwrx6l\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGovernor of Indiana -> common.topic.image -> Mitch Daniels -> people.person.nationality -> United States of America\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4slb -> government.government_position_held.office_holder -> Mitch Daniels\n# Answer:\nMitch Daniels", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.jurisdiction -> Indiana -> common.topic.image -> Indiana.png\n# Answer:\nIndiana", "# Reasoning Path:\nGovernor of Indiana -> government.government_office_or_title.office_holders -> m.04l4s5v -> government.government_position_held.office_holder -> James B. Ray\n# Answer:\nJames B. Ray"], "ground_truth": ["Mitch Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-759", "prediction": ["# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.0652slx -> base.popstra.canoodled.participant -> Uma Thurman\n# Answer:\nUma Thurman", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.dated -> m.063g4f2 -> base.popstra.dated.participant -> Angelina Jolie\n# Answer:\nAngelina Jolie", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.063h8xj -> base.popstra.canoodled.participant -> Julia Ormond\n# Answer:\nJulia Ormond", "# Reasoning Path:\nBrad Pitt -> film.actor.film -> g.11b6r79157\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.064bhn3 -> base.popstra.canoodled.participant -> Angelina Jolie\n# Answer:\nAngelina Jolie", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.0652sjj -> base.popstra.canoodled.participant -> Lisa Davis\n# Answer:\nLisa Davis", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.canoodled -> m.0652vk5 -> base.popstra.canoodled.participant -> Demi Moore\n# Answer:\nDemi Moore", "# Reasoning Path:\nBrad Pitt -> base.popstra.celebrity.parties -> m.063hd7h -> base.popstra.party_attendance_person.party -> Le Baoli\n# Answer:\nLe Baoli"], "ground_truth": ["Thandie Newton", "Juliette Lewis", "Shalane McCall", "Sinitta", "Robin Givens"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.corresponding_type -> Visual Artist\n# Answer:\nSculptor", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_form -> Mural\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_subject -> Jesus Christ\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> St. John the Baptist -> visual_art.artwork.art_subject -> John the Baptist\n# Answer:\nSt. John the Baptist"], "ground_truth": ["g.1239jd9p", "g.1213jb_b", "Leda and the Swan", "Sala delle Asse", "Madonna of Laroque", "g.12314dm1", "Leonardo's horse", "St. John the Baptist", "Portrait of a Young Fianc\u00e9e", "Benois Madonna", "Madonna of the Yarnwinder", "Drapery for a Seated Figure", "The Baptism of Christ", "Vitruvian Man", "g.1219sb0g", "La belle ferronni\u00e8re", "Adoration of the Magi", "The Holy Infants Embracing", "The Last Supper", "Medusa", "Portrait of Isabella d'Este", "g.121yh91r", "Annunciation", "g.12215rxg", "Portrait of a man in red chalk", "Lady with an Ermine", "Portrait of a Musician", "Madonna of the Carnation", "The Battle of Anghiari", "g.121wt37c", "Mona Lisa", "The Virgin and Child with St Anne and St John the Baptist", "Ginevra de' Benci", "Virgin of the Rocks", "Horse and Rider", "Salvator Mundi", "Bacchus", "g.120vt1gz", "Head of a Woman", "Lucan portrait of Leonardo da Vinci", "The Virgin and Child with St. Anne", "g.1224tf0c", "St. Jerome in the Wilderness", "Madonna and Child with St Joseph", "Madonna Litta"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07947019867549669, "ans_precission": 0.375, "ans_recall": 0.044444444444444446, "path_f1": 0.08108108108108107, "path_precision": 0.375, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07947019867549669, "path_ans_precision": 0.375, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-760", "prediction": ["# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.party -> Democratic Party\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Pepsi Center\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> common.topic.notable_types -> Event\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.speeches -> m.04j00hr\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> government.politician.party -> m.03gjhww -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.venues -> Sports Authority Field at Mile High\n# Answer:\n2008 Democratic National Convention", "# Reasoning Path:\nBarack Obama -> base.politicalconventions.presidential_nominee.nominated_at -> 2008 Democratic National Convention -> base.politicalconventions.political_convention.final_presidential_vote_tally -> m.04j7784\n# Answer:\n2008 Democratic National Convention"], "ground_truth": ["Democratic Party"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-761", "prediction": ["# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> common.topic.notable_types -> Postal Code\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94120 -> common.topic.notable_for -> g.125925x8_\n# Answer:\n94120", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> California\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.postal_code.country -> United States of America\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94118 -> location.location.containedby -> California\n# Answer:\n94118", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.contains -> San Francisco Transbay Terminal\n# Answer:\n94107", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94120 -> location.postal_code.country -> United States of America\n# Answer:\n94120"], "ground_truth": ["94109", "94146", "94134", "94105", "94142", "94151", "94110", "94131", "94199", "94161", "94114", "94101", "94144", "94112", "94156", "94122", "94117-2212", "94102", "94104", "94127", "94118-3208", "94103", "94132", "94163", "94118", "94120", "94159", "94177", "94141", "94121", "94116", "94125", "94172", "94107-7621", "94137", "94143", "94123", "94147", "94164", "94160", "94124", "94162", "94145", "94126", "94153", "94108", "94128", "94140", "94133", "94130", "94111", "94129", "94107", "94171", "94117", "94115", "94102-4733", "94188", "94119", "94139", "94154", "94158"], "ans_acc": 0.04838709677419355, "ans_hit": 1, "ans_f1": 0.09230769230769231, "ans_precission": 1.0, "ans_recall": 0.04838709677419355, "path_f1": 0.09230769230769231, "path_precision": 1.0, "path_recall": 0.04838709677419355, "path_ans_f1": 0.09230769230769231, "path_ans_precision": 1.0, "path_ans_recall": 0.04838709677419355}
{"id": "WebQTest-762", "prediction": ["# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mhjv -> soccer.football_player_stats.team -> Newcastle United F.C.\n# Answer:\nNewcastle United F.C.", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w8wygw -> soccer.football_player_stats.team -> Manchester City F.C.\n# Answer:\nManchester City F.C.", "# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nStephen Ireland -> soccer.football_player.statistics -> m.0w9mf8d -> soccer.football_player_stats.team -> Aston Villa F.C.\n# Answer:\nAston Villa F.C.", "# Reasoning Path:\nStephen Ireland -> common.image.size -> m.02fttzz\n# Answer:\ncommon.image.size", "# Reasoning Path:\nStephen Ireland -> base.schemastaging.athlete_extra.salary -> m.0lh5l0q -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nPound sterling", "# Reasoning Path:\nStephen Ireland -> sports.pro_athlete.teams -> m.0wyjvk6 -> sports.sports_team_roster.team -> Stoke City F.C.\n# Answer:\nStoke City F.C.", "# Reasoning Path:\nStephen Ireland -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nStephen Ireland -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Newcastle United F.C.", "Stoke City F.C.", "Aston Villa F.C.", "Manchester City F.C."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.5882352941176471, "path_precision": 0.5555555555555556, "path_recall": 0.625, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-764", "prediction": ["# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0n0v79l -> education.education.institution -> Fairfield Warde High School\n# Answer:\nFairfield Warde High School", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.02wnjyy -> education.education.institution -> Berklee College of Music\n# Answer:\nBerklee College of Music", "# Reasoning Path:\nJohn Mayer -> people.person.education -> m.0h30083 -> education.education.institution -> Brien McMahon High School\n# Answer:\nBrien McMahon High School", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.category -> Topic Webpage\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> tv.tv_actor.guest_roles -> m.0bv_4q4 -> tv.tv_guest_role.episodes_appeared_in -> Show #2662\n# Answer:\nShow #2662", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.resource -> Celebrity date night at John Mayer's 'revue'\n# Answer:\nCelebrity date night at John Mayer's 'revue'", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> http://www.discogs.com/artist/John+Mayer -> common.webpage.resource -> m.0bm4m94\n# Answer:\nhttp://www.discogs.com/artist/John+Mayer", "# Reasoning Path:\nJohn Mayer -> common.topic.webpage -> m.09w5t6j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Berklee College of Music"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-765", "prediction": ["# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.0652s8g -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.065q5tn -> base.popstra.dated.participant -> Lauren Conrad\n# Answer:\nLauren Conrad", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2n8 -> base.popstra.canoodled.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2nm -> base.popstra.canoodled.participant -> Lauren Conrad\n# Answer:\nLauren Conrad", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.dated -> m.063g5df -> base.popstra.dated.participant -> Paris Hilton\n# Answer:\nParis Hilton", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.063k2rc -> base.popstra.canoodled.participant -> Jessica Simpson\n# Answer:\nJessica Simpson", "# Reasoning Path:\nBrody Jenner -> base.popstra.celebrity.canoodled -> m.0652t3h -> base.popstra.canoodled.participant -> Nicole Richie\n# Answer:\nNicole Richie", "# Reasoning Path:\nBrody Jenner -> celebrities.celebrity.sexual_relationships -> m.04hdsy2 -> celebrities.romantic_relationship.celebrity -> Nicole Richie\n# Answer:\nNicole Richie"], "ground_truth": ["Lauren Conrad", "Kristin Cavallari", "Paris Hilton", "Jayde Nicole", "Nicole Richie", "Haylie Duff", "Ryan J. Parker", "Cora Skinner"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.525, "ans_precission": 0.875, "ans_recall": 0.375, "path_f1": 0.56, "path_precision": 0.875, "path_recall": 0.4117647058823529, "path_ans_f1": 0.525, "path_ans_precision": 0.875, "path_ans_recall": 0.375}
{"id": "WebQTest-766", "prediction": ["# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.03lfx0d\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_types -> Currency\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.notable_for -> g.1258v8707\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.webpage -> m.04m10rm\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.article -> m.01_c9_\n# Answer:\nMalaysian ringgit", "# Reasoning Path:\nMalaysia -> location.location.time_zones -> Malaysia Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nMalaysia Time Zone", "# Reasoning Path:\nMalaysia -> location.statistical_region.size_of_armed_forces -> g.12cp_jm8t\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nMalaysia -> location.statistical_region.gdp_nominal_per_capita -> g.11b60thpy3\n# Answer:\nlocation.statistical_region.gdp_nominal_per_capita", "# Reasoning Path:\nMalaysia -> location.statistical_region.cpi_inflation_rate -> g.11b60wmkxf\n# Answer:\nlocation.statistical_region.cpi_inflation_rate", "# Reasoning Path:\nMalaysia -> location.country.currency_used -> Malaysian ringgit -> common.topic.image -> 50 ringgit\n# Answer:\nMalaysian ringgit"], "ground_truth": ["Malaysian ringgit"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-768", "prediction": ["# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.parent -> m.04kjktn -> organization.organization_relationship.parent -> Volkswagen Group\n# Answer:\nVolkswagen Group", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.person -> Martin Winterkorn\n# Answer:\nMartin Winterkorn", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> organization.organization_founder.organizations_founded -> Volkswagen Group\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> common.topic.notable_types -> Organization founder\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> base.schemastaging.context_name.pronunciation -> g.125_l36r8\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> organization.leadership.role -> Chairman\n# Answer:\nChairman", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.founders -> German Labour Front -> organization.organization.geographic_scope -> Germany\n# Answer:\nGerman Labour Front", "# Reasoning Path:\nVolkswagen Passenger Cars -> organization.organization.leadership -> m.0vbmqq2 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Volkswagen Group"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-769", "prediction": ["# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.serves -> Ciampino\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> aviation.airport.hub_for -> Ryanair\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> common.topic.article -> m.033_5b\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> location.location.geolocation -> m.04fngwt\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Ciampino\u2013G. B. Pastine International Airport -> location.location.containedby -> Italy\n# Answer:\nCiampino\u2013G. B. Pastine International Airport", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Urbe Airport -> aviation.airport.hub_for -> Ala Littoria\n# Answer:\nUrbe Airport", "# Reasoning Path:\nRome -> travel.travel_destination.tourist_attractions -> Roma-Ostia Half Marathon -> common.topic.webpage -> m.0ggd5dd\n# Answer:\nRoma-Ostia Half Marathon", "# Reasoning Path:\nRome -> location.location.nearby_airports -> Leonardo da Vinci\u2013Fiumicino Airport -> aviation.airport.serves -> Fiumicino\n# Answer:\nLeonardo da Vinci\u2013Fiumicino Airport"], "ground_truth": ["Roma Termini railway station"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bludenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Bregenz District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Dornbirn District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_children -> Feldkirch District\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vorarlberg -> location.location.partially_contains -> Dreil\u00e4nderspitze\n# Answer:\nVorarlberg", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.capital -> Vienna -> periodicals.newspaper_circulation_area.newspapers -> Kurier\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-770", "prediction": ["# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.schemastaging.context_name.pronunciation -> g.125_s3hjb\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.disaster2.injury_causing_event.caused_disability -> m.010r9w6b\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nSteve McNair -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nSteve McNair -> people.person.education -> m.02kybw3 -> education.education.institution -> Alcorn State University\n# Answer:\nAlcorn State University"], "ground_truth": ["Ballistic trauma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-771", "prediction": ["# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> organization.organization.founders -> Jerry Brown\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> common.topic.notable_for -> g.1257jlrc9\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> common.topic.article -> m.03zzyq\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> 11-99 Foundation -> common.topic.notable_for -> g.1254xhy5c\n# Answer:\n11-99 Foundation", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> California Conservation Corps -> common.topic.notable_types -> Government Agency\n# Answer:\nCalifornia Conservation Corps", "# Reasoning Path:\nCalifornia -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nCalifornia -> government.governmental_jurisdiction.agencies -> Alameda County Superior Court -> location.location.street_address -> m.08d08r9\n# Answer:\nAlameda County Superior Court", "# Reasoning Path:\nCalifornia -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.locations.countries.states_provinces_within -> Alabama\n# Answer:\nUnited States of America"], "ground_truth": ["Arnold Schwarzenegger"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-772", "prediction": ["# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.actor -> Eva Mendes\n# Answer:\nEva Mendes", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.actor -> Raquel Alessi\n# Answer:\nRaquel Alessi", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0jyrrc -> film.performance.character -> Roxanne Simpson\n# Answer:\nRoxanne Simpson", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jrkkx -> film.performance.actor -> Brett Cullen\n# Answer:\nBrett Cullen", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.03jt22s -> film.performance.actor -> Matt Long\n# Answer:\nMatt Long", "# Reasoning Path:\nGhost Rider -> award.award_nominated_work.award_nominations -> m.059pfmy -> award.award_nomination.award -> Razzie Award for Worst Actor\n# Answer:\nRazzie Award for Worst Actor", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0gdkwxt -> film.performance.actor -> Laurence Breuls\n# Answer:\nLaurence Breuls", "# Reasoning Path:\nGhost Rider -> film.film.starring -> m.0hdy101 -> film.performance.character -> Roxanne Simpson\n# Answer:\nRoxanne Simpson"], "ground_truth": ["Eva Mendes", "Raquel Alessi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-773", "prediction": ["# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> location.mailing_address.citytown -> Minato\n# Answer:\nMinato", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> location.mailing_address.state_province_region -> Kant\u014d region\n# Answer:\nKant\u014d region", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> common.topic.notable_for -> g.1259dkczx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.founders -> Ericsson -> organization.organization.place_founded -> Stockholm\n# Answer:\nEricsson", "# Reasoning Path:\nSony Mobile Communications -> base.schemastaging.organization_extra.phone_number -> m.0109_8hh -> base.schemastaging.phone_sandbox.service_location -> Costa Rica\n# Answer:\nCosta Rica", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.headquarters -> m.040r1dz -> freebase.valuenotation.has_no_value -> Street Address 2\n# Answer:\nStreet Address 2", "# Reasoning Path:\nSony Mobile Communications -> organization.organization.founders -> Ericsson -> organization.organization.headquarters -> m.03lz01v\n# Answer:\nEricsson"], "ground_truth": ["Minato"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-774", "prediction": ["# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Republic\n# Answer:\nRepublic", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.article -> m.04lt88\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.webpage -> m.04m2gwk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_types -> Currency\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.country.currency_used -> Dominican peso -> common.topic.notable_for -> g.12565x5sk\n# Answer:\nDominican peso", "# Reasoning Path:\nDominican Republic -> location.statistical_region.co2_emissions_per_capita -> g.1245_1qfx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nDominican Republic -> location.statistical_region.internet_users_percent_population -> g.11b60vv5zn\n# Answer:\nlocation.statistical_region.internet_users_percent_population", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Spain\n# Answer:\nUnitary state", "# Reasoning Path:\nDominican Republic -> location.statistical_region.electricity_consumption_per_capita -> g.1245_22hm\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nDominican Republic -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Martinique\n# Answer:\nUnitary state"], "ground_truth": ["Dominican peso"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-775", "prediction": ["# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> common.topic.image -> The Manchester Museum\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> location.location.containedby -> England\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Old Trafford -> sports.sports_facility.teams -> Manchester United F.C.\n# Answer:\nOld Trafford", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Chinatown, Manchester -> location.location.geolocation -> m.0d5tg9l\n# Answer:\nChinatown, Manchester", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Old Trafford -> location.location.containedby -> Old Trafford, Greater Manchester\n# Answer:\nOld Trafford", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Bedford Mall\n# Answer:\nBedford Mall", "# Reasoning Path:\nManchester -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07jkbyr\n# Answer:\nlocation.hud_foreclosure_area.total_90_day_vacant_residential_addresses", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Manchester Museum -> architecture.structure.architect -> Alfred Waterhouse\n# Answer:\nManchester Museum", "# Reasoning Path:\nManchester -> location.statistical_region.population -> g.11b66bsm6f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nManchester -> travel.travel_destination.tourist_attractions -> Chinatown, Manchester -> location.location.containedby -> Greater Manchester\n# Answer:\nChinatown, Manchester"], "ground_truth": ["Urbis", "Great Manchester Run", "Albert Hall, Manchester", "Manchester Museum", "Wheel of Manchester", "Manchester Jewish Museum", "Manchester Art Gallery", "Manchester Cathedral", "Old Trafford", "Whitworth Art Gallery", "Chinatown, Manchester", "The Moon Under Water, Manchester", "Imperial War Museum North"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.34710743801652894, "ans_precission": 0.7, "ans_recall": 0.23076923076923078, "path_f1": 0.34710743801652894, "path_precision": 0.7, "path_recall": 0.23076923076923078, "path_ans_f1": 0.34710743801652894, "path_ans_precision": 0.7, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-776", "prediction": ["# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Italy national football team -> sports.sports_team.sport -> Football\n# Answer:\nItaly national football team", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> sports.sports_team.sport -> Basketball\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Italy men's national volleyball team -> common.topic.notable_types -> Sports Team\n# Answer:\nItaly men's national volleyball team", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Italy women's national volleyball team -> common.topic.notable_types -> Sports Team\n# Answer:\nItaly women's national volleyball team", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> sports.sports_team.sport -> Basketball\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> Dinamo Basket Sassari -> common.topic.notable_types -> Basketball Team\n# Answer:\nDinamo Basket Sassari", "# Reasoning Path:\nItaly -> sports.sports_team_location.teams -> AS Vicenza -> common.topic.notable_types -> Basketball Team\n# Answer:\nAS Vicenza", "# Reasoning Path:\nItaly -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Tenerani -> base.thoroughbredracing.thoroughbred_racehorse.foal_if_this_horse_is_sire -> Ribot\n# Answer:\nTenerani"], "ground_truth": ["Italy women's national beach handball team", "Italy national cricket team", "Team Liquigas-Cannondale", "Italy women's national ice hockey team", "Pallacanestro Treviso", "g.11x7vhqcx", "Victoria Libertas Pesaro", "Pallacanestro Virtus Roma", "Juvecaserta Basket", "Pallacanestro Varese", "Italy women's national rugby union team", "Italy national baseball team", "Italy Davis Cup team", "Italy women's national volleyball team", "Partenope Napoli", "Fortitudo Pallacanestro Bologna", "Dinamo Basket Sassari", "Italy women's national football team", "Italy men's national volleyball team", "AS Vicenza", "Italy national speedway team", "Italy national rugby union team", "Italy men's national ice hockey team", "Italy Fed Cup team", "Italy national futsal team", "S.S. Felice Scandone", "Italy national basketball team", "Italy national football team", "Olimpia Milano", "Pallacanestro Cant\u00f9", "Italy women's national water polo team", "Italy national handball team", "Mens Sana 1871 Basket", "Italy men's national pitch and putt team", "Virtus Pallacanestro Bologna", "Italy national rugby league team"], "ans_acc": 0.1388888888888889, "ans_hit": 1, "ans_f1": 0.2317880794701987, "ans_precission": 0.7, "ans_recall": 0.1388888888888889, "path_f1": 0.2317880794701987, "path_precision": 0.7, "path_recall": 0.1388888888888889, "path_ans_f1": 0.2317880794701987, "path_ans_precision": 0.7, "path_ans_recall": 0.1388888888888889}
{"id": "WebQTest-777", "prediction": ["# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.citytown -> Mountain View\n# Answer:\nMountain View", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.state_province_region -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> location.location.containedby -> United States of America\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> location.mailing_address.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGoogle -> organization.organization.headquarters -> m.030qthd -> common.topic.notable_for -> g.1255s417b\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> location.location.containedby -> California\n# Answer:\nMenlo Park", "# Reasoning Path:\nGoogle -> organization.organization.place_founded -> Menlo Park -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMenlo Park"], "ground_truth": ["Mountain View"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-778", "prediction": ["# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> education.education.institution -> Ohio State University\n# Answer:\nOhio State University", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> education.education.institution -> East Technical High School\n# Answer:\nEast Technical High School", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.0n1kct6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nJesse Owens -> people.person.places_lived -> m.03pfl8z -> people.place_lived.location -> Cleveland\n# Answer:\nCleveland", "# Reasoning Path:\nJesse Owens -> people.person.education -> m.02wnc1b -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nJesse Owens -> film.person_or_entity_appearing_in_film.films -> m.0v9dxnk -> film.personal_film_appearance.film -> Salute\n# Answer:\nSalute"], "ground_truth": ["Ohio State University", "East Technical High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-779", "prediction": ["# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwg3 -> people.place_lived.location -> San Francisco\n# Answer:\nSan Francisco", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.03psmj7 -> people.place_lived.location -> Jalisco\n# Answer:\nJalisco", "# Reasoning Path:\nCarlos Santana -> symbols.name_source.namesakes -> Santana -> music.artist.origin -> San Francisco\n# Answer:\nSantana", "# Reasoning Path:\nCarlos Santana -> people.person.places_lived -> m.0h3dwfk -> people.place_lived.location -> Tijuana\n# Answer:\nTijuana", "# Reasoning Path:\nCarlos Santana -> people.person.place_of_birth -> Autl\u00e1n -> location.location.containedby -> Jalisco\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> music.artist.origin -> Autl\u00e1n -> location.location.containedby -> Jalisco\n# Answer:\nAutl\u00e1n", "# Reasoning Path:\nCarlos Santana -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> San Francisco\n# Answer:\nUnited States of America", "# Reasoning Path:\nCarlos Santana -> symbols.name_source.namesakes -> Santana -> award.award_winner.awards_won -> m.05btgv1\n# Answer:\nSantana"], "ground_truth": ["San Francisco", "Tijuana", "Jalisco"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.625, "path_precision": 0.625, "path_recall": 0.625, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.04tnn4f\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> people.cause_of_death.people -> Yvette Wilson\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> common.topic.webpage -> m.09wbnt0\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-780", "prediction": ["# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0gwq3ht -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_distribution_medium -> DVD\n# Answer:\nDVD", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Lindsey Collins -> people.person.profession -> Film Producer\n# Answer:\nLindsey Collins", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0vxb_0b -> film.film_regional_release_date.film_release_distribution_medium -> Theatrical\n# Answer:\nTheatrical", "# Reasoning Path:\nJohn Carter -> film.film.starring -> m.064xypb -> film.performance.actor -> Taylor Kitsch\n# Answer:\nTaylor Kitsch", "# Reasoning Path:\nJohn Carter -> film.film.produced_by -> Lindsey Collins -> common.topic.notable_for -> g.1256468vn\n# Answer:\nLindsey Collins", "# Reasoning Path:\nJohn Carter -> film.film.release_date_s -> m.0k27srr -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Adventure Film", "Action Film", "Science Fiction", "Fantasy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-782", "prediction": ["# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c3 -> organization.organization_membership.member -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60cf -> organization.organization_membership.member -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.membership_organization.members -> m.03w60c8 -> organization.organization_membership.member -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nNorth American Free Trade Agreement -> common.topic.notable_for -> g.125563pkq\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.nationality -> United States of America\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#range -> Location\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> rdf-schema#domain -> Organization\n# Answer:\nPlace founded", "# Reasoning Path:\nNorth American Free Trade Agreement -> organization.organization.founders -> George H. W. Bush -> people.person.profession -> Politician\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nNorth American Free Trade Agreement -> freebase.valuenotation.has_value -> Place founded -> type.property.expected_type -> Location\n# Answer:\nPlace founded"], "ground_truth": ["United States of America", "Canada", "Mexico"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-783", "prediction": ["# Reasoning Path:\nHouston -> location.hud_county_place.county -> Houston County -> location.location.containedby -> Texas\n# Answer:\nHouston County", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 281 -> location.location.containedby -> Texas\n# Answer:\nArea code 281", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 713 -> location.location.containedby -> Texas\n# Answer:\nArea code 713", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> New York\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nHouston -> location.location.containedby -> Area code 832 -> location.location.containedby -> Texas\n# Answer:\nArea code 832"], "ground_truth": ["Montgomery County"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-784", "prediction": ["# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> sports.sports_facility.teams -> Seattle Sounders FC\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Experience Music Project Museum\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> location.location.containedby -> Seattle\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> travel.travel_destination.tourist_attractions -> Pike Place Market\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> soccer.football_pitch.matches -> 2010 Lamar Hunt U.S. Open Cup Final\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> location.hud_county_place.county -> King County\n# Answer:\nSeattle", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.arena_stadium -> CenturyLink Field -> location.location.events -> 2006 NFC Championship Game\n# Answer:\nCenturyLink Field", "# Reasoning Path:\nSeattle Seahawks -> sports.sports_team.location -> Seattle -> location.location.containedby -> United States of America\n# Answer:\nSeattle"], "ground_truth": ["CenturyLink Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-785", "prediction": ["# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nChick Hicks", "# Reasoning Path:\nCars -> film.film.starring -> m.0jw4g9 -> film.performance.character -> Chick Hicks\n# Answer:\nChick Hicks", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6dn6h31\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.film -> Cars\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Chick Hicks\n# Answer:\nWorld of Cars", "# Reasoning Path:\nMichael Keaton -> base.popstra.celebrity.dated -> m.065q84p -> base.popstra.dated.participant -> Rachel Ryan\n# Answer:\nRachel Ryan", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> g.11b6pd5lf0\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.0jw4g9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMichael Keaton -> film.actor.film -> m.010pbd23 -> film.performance.character -> Riggan Thomson\n# Answer:\nRiggan Thomson", "# Reasoning Path:\nCars -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> World of Cars -> fictional_universe.fictional_universe.characters -> Bob Cutlass\n# Answer:\nWorld of Cars"], "ground_truth": ["Chick Hicks"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-787", "prediction": ["# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> United States of America\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Area codes 702 and 725\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Clark County\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.02930wg\n# Answer:\ncommon.image.size", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.containedby -> Nevada\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.nearby_airports -> North Las Vegas Airport\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> location.location.nearby_airports -> Henderson Executive Airport\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> film.film_location.featured_in_films -> Vegas Vacation\n# Answer:\nLas Vegas", "# Reasoning Path:\nRiviera -> common.image.size -> m.03s8648\n# Answer:\ncommon.image.size", "# Reasoning Path:\nRiviera -> location.location.containedby -> Las Vegas -> book.book_subject.works -> In The Desert Of Desire: Las Vegas And The Culture Of Spectacle\n# Answer:\nLas Vegas"], "ground_truth": ["Las Vegas"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-788", "prediction": ["# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> common.topic.article -> m.036z9j\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> common.topic.notable_types -> Religious Text\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> base.skosbase.skos_concept.narrower -> Buddhist civilization\n# Answer:\nBuddhist civilization", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> common.topic.notable_for -> g.1259g2gtl\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Longchen Nyingthig -> common.topic.notable_types -> Religious Text\n# Answer:\nLongchen Nyingthig", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> \u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra -> base.schemastaging.context_name.pronunciation -> m.013151dx\n# Answer:\n\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Longchen Nyingthig -> common.topic.notable_for -> g.125flk0bl\n# Answer:\nLongchen Nyingthig", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Mah\u0101y\u0101na s\u016btras -> common.topic.notable_for -> g.125cxtysq\n# Answer:\nMah\u0101y\u0101na s\u016btras", "# Reasoning Path:\nBuddhism -> religion.religion.texts -> Longchen Nyingthig -> common.topic.article -> m.0c4pcr\n# Answer:\nLongchen Nyingthig"], "ground_truth": ["Mah\u0101vastu", "N\u012blaka\u1e47\u1e6dha Dh\u0101ra\u1e47\u012b", "Amitabha Sutra", "Mah\u0101y\u0101na s\u016btras", "Dhammapada", "U\u1e63\u1e47\u012b\u1e63a Vijaya Dh\u0101ra\u1e47\u012b S\u016btra", "Vimalakirti Sutra", "Chinese Buddhist canon", "\u0100gama", "Tibetan Buddhist canon", "Gang\u014dji Garan Engi", "Longchen Nyingthig", "P\u0101li Canon", "\u015ar\u012bm\u0101l\u0101dev\u012b Si\u1e43han\u0101da S\u016btra"], "ans_acc": 0.21428571428571427, "ans_hit": 1, "ans_f1": 0.34532374100719426, "ans_precission": 0.8888888888888888, "ans_recall": 0.21428571428571427, "path_f1": 0.34532374100719426, "path_precision": 0.8888888888888888, "path_recall": 0.21428571428571427, "path_ans_f1": 0.34532374100719426, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.21428571428571427}
{"id": "WebQTest-789", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> President of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.nationality -> United States of America\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Tazewell Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Senator\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Alice Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size"], "ground_truth": ["John Tyler"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> book.book_subject.works -> Remaking Buddhism for Medieval Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.containedby -> Nepal\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal -> location.location.time_zones -> Nepal Time Zone\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.statistical_region.population -> g.11bc8807jd\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-790", "prediction": ["# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.capital_of_administrative_division.capital_of -> m.0jvvpb_\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> United Kingdom\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> travel.travel_destination.tourist_attractions -> National Football Museum\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> City of Preston, Lancashire\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.containedby -> England\n# Answer:\nPreston, Lancashire", "# Reasoning Path:\nRichard Arkwright -> people.person.place_of_birth -> Preston, Lancashire -> location.location.contains -> St Joseph's Church, Preston\n# Answer:\nPreston, Lancashire"], "ground_truth": ["Preston, Lancashire"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-791", "prediction": ["# Reasoning Path:\nLouis Sachar -> common.topic.article -> m.01bq8n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.03tgtz\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Boy Who Lost His Face -> common.topic.notable_types -> Book\n# Answer:\nThe Boy Who Lost His Face", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> The Cardturner -> award.award_nominated_work.award_nominations -> m.0g2978l\n# Answer:\nThe Cardturner", "# Reasoning Path:\nLouis Sachar -> influence.influence_node.influenced_by -> Richard Price -> influence.influence_node.influenced_by -> Hubert Selby, Jr.\n# Answer:\nRichard Price", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.production_companies -> Walt Disney Pictures\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> book.author.works_written -> Holes -> book.written_work.next_in_series -> Stanley Yelnats' Survival Guide to Camp Green Lake\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> common.topic.article -> m.0dq62d\n# Answer:\nHoles", "# Reasoning Path:\nLouis Sachar -> film.film_story_contributor.film_story_credits -> Holes -> film.film.starring -> m.02tb466\n# Answer:\nHoles"], "ground_truth": ["Sideways Arithmetic From Wayside School", "Sixth grade secrets", "Small Steps (Readers Circle)", "There's a boy in the girls bathroom", "Wayside School is falling down", "A Flying Birthday Cake? (A Stepping Stone Book(TM))", "Someday Angeline", "Louis Sacher Collection", "Sixth Grade Secrets (Apple Paperbacks)", "Someday Angeline (Avon/Camelot Book)", "L\u00f6cher", "Small steps", "Why Pick on Me?", "Wayside School gets a little stranger", "Johnny's in the Basement", "Wayside School Boxed Set", "Monkey soup", "Stanley Yelnats' Survival Guide to Camp Green Lake", "Wayside School is falling down (Celebrate reading, Scott Foresman)", "Kidnapped at Birth?", "Dogs Don't Tell Jokes", "Johnny's in the basement", "Small Steps", "Super Fast, Out of Control!", "More Sideways Arithmetic from Wayside School", "Wayside School Gets A Little Stranger", "Marvin Redpost.", "Pequenos Pasos/ Small Steps", "There's a Boy in the Girls' Bathroom", "Class President (A Stepping Stone Book(TM))", "Class President", "Holes (with \\\"Connections\\\") HRW Library (HRW library)", "Wayside School Collection", "Holes (Cascades)", "Kidnapped at Birth? (A Stepping Stone Book(TM))", "Holes Activity Pack", "The Cardturner", "Boy Who Lost His Face", "More Sideways Arithmetic From Wayside School", "Wayside School is Falling Down", "Wayside School Gets a Little Stranger", "A magic crystal?", "Hoyos/Holes", "Holes (Readers Circle)", "Alone in His Teacher's House", "Holes. (Lernmaterialien)", "The boy who lost his face", "Why Pick on Me? (A Stepping Stone Book(TM))", "Sideways stories from Wayside School", "Hay Un Chico En El Bano De Las Chicas", "Sixth Grade Secrets", "Sideways Arithmetic from Wayside School", "Stanley Yelnats Survival Guide to Camp Green Lake", "Marvin Redpost", "The Boy Who Lost His Face", "Holes (Yearling Books)", "Holes (Listening Library)", "Wayside School Gets a Little Stranger (rack) (Wayside School)", "Il y a un gar\u00e7on dans les toilettes des filles", "A Flying Birthday Cake?", "Der Fluch des David Ballinger. ( Ab 11 J.).", "Super Fast, Out of Control! (A Stepping Stone Book(TM))", "Holes", "Holes (World Book Day 2001)", "Sixth Grade Secrets (An Apple Paperback)", "g.1218f5g0", "Wayside School Is Falling Down", "Sideways Stories from Wayside School"], "ans_acc": 0.10294117647058823, "ans_hit": 1, "ans_f1": 0.1343570057581574, "ans_precission": 0.7777777777777778, "ans_recall": 0.07352941176470588, "path_f1": 0.21666666666666665, "path_precision": 0.3333333333333333, "path_recall": 0.16049382716049382, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.10294117647058823}
{"id": "WebQTest-792", "prediction": ["# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> people.person.place_of_birth -> Great Falls\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> people.person.nationality -> United States of America\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> common.topic.article -> m.023_lv\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> people.deceased_person.place_of_death -> Washington, D.C.\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Complexity of the Individual Income Tax\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Consequences of the Asian Financial Crisis\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Increasing Savings for Retirement\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> International Tax Issues Relating to Globalization\n# Answer:\nWilliam V. Roth, Jr.", "# Reasoning Path:\nRoth IRA -> symbols.namesake.named_after -> William V. Roth, Jr. -> book.author.works_written -> Irs Oversight\n# Answer:\nWilliam V. Roth, Jr."], "ground_truth": ["William V. Roth, Jr."], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-794", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.film -> The Making of Star Wars\n# Answer:\nThe Making of Star Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars"], "ground_truth": ["James Earl Jones", "Dr. Smoov", "Abraham Benrubi", "Matt Lanter", "Zac Efron"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.30303030303030304, "ans_precission": 0.625, "ans_recall": 0.2, "path_f1": 0.5263157894736842, "path_precision": 0.625, "path_recall": 0.45454545454545453, "path_ans_f1": 0.30303030303030304, "path_ans_precision": 0.625, "path_ans_recall": 0.2}
{"id": "WebQTest-796", "prediction": ["# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> You Bastard\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Beremiz Samir\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> common.topic.notable_for -> g.12553cs6s\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> common.topic.notable_types -> Academic -> type.type.expected_by -> Person\n# Answer:\nAcademic", "# Reasoning Path:\nThomas Robert Malthus -> influence.influence_node.influenced_by -> William Godwin -> people.person.profession -> Philosopher\n# Answer:\nWilliam Godwin", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> fictional_universe.character_occupation.characters_with_this_occupation -> Dyhianna Selei\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Mathematician -> base.descriptive_names.names.descriptive_name -> m.01066gkh\n# Answer:\nMathematician", "# Reasoning Path:\nThomas Robert Malthus -> people.person.profession -> Economist -> base.schemastaging.context_name.pronunciation -> g.125_pmh72\n# Answer:\nEconomist"], "ground_truth": ["Economist", "Scientist", "Mathematician", "Demographer"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.75, "ans_recall": 0.5, "path_f1": 0.6, "path_precision": 0.75, "path_recall": 0.5, "path_ans_f1": 0.6, "path_ans_precision": 0.75, "path_ans_recall": 0.5}
{"id": "WebQTest-797", "prediction": ["# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.geolocation -> m.03dnk73\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> United States of America\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.contains -> NewPark Mall\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11b66cggvh\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.geometry -> m.0571wc_\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> common.topic.notable_types -> Postal Code\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.location.containedby -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nNewark -> location.statistical_region.population -> g.11bymn221g\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nNewark -> location.citytown.postal_codes -> 94560 -> location.location.containedby -> Alameda County\n# Answer:\n94560", "# Reasoning Path:\nNewark -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07hb1wj -> measurement_unit.dated_integer.source -> United States Department of Housing and Urban Development\n# Answer:\nUnited States Department of Housing and Urban Development"], "ground_truth": ["94560"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-798", "prediction": ["# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> 1915 New Orleans hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\n1915 New Orleans hurricane", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nHurricane Bob", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> common.topic.notable_for -> g.1255tjcrg\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nTennessee -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nHurricane Bob"], "ground_truth": ["Missouri", "Kentucky", "Virginia", "Arkansas", "Georgia", "Mississippi", "North Carolina", "Alabama"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.06382978723404256, "path_precision": 0.75, "path_recall": 0.03333333333333333, "path_ans_f1": 0.5, "path_ans_precision": 0.75, "path_ans_recall": 0.375}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.article -> m.013cqz\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Bifocals", "Franklin stove", "Lightning rod", "Glass harmonica"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8076923076923077, "ans_precission": 0.875, "ans_recall": 0.75, "path_f1": 0.75, "path_precision": 0.75, "path_recall": 0.75, "path_ans_f1": 0.8076923076923077, "path_ans_precision": 0.875, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.containedby -> Colorado\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> periodicals.newspaper_circulation_area.newspapers -> Greeley Tribune\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> location.location.containedby -> United States of America\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County -> location.location.adjoin_s -> m.03jpst3\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.statistical_region.population -> g.11b66krpqz\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-800", "prediction": ["# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.043wpr_ -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nMilitary - Dead", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nBattle of the Little Bighorn", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.military_conflict -> Battle of the Little Bighorn\n# Answer:\nBattle of the Little Bighorn", "# Reasoning Path:\nUnited States of America -> base.schemastaging.context_name.pronunciation -> g.125_r6z32\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.02zzw3k -> military.casualties.military_conflict -> War of 1812\n# Answer:\nWar of 1812", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rpj -> military.casualties.type_of_casualties -> Military - Dead\n# Answer:\nMilitary - Dead", "# Reasoning Path:\nUnited States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel -> meteorology.tropical_cyclone.affected_areas -> West Virginia\n# Answer:\nHurricane Isabel", "# Reasoning Path:\nUnited States of America -> military.military_combatant.casualties -> m.010h8rq6 -> military.casualties.type_of_casualties -> Military - Wounded\n# Answer:\nMilitary - Wounded", "# Reasoning Path:\nUnited States of America -> meteorology.cyclone_affected_area.cyclones -> Hurricane Isabel -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nHurricane Isabel"], "ground_truth": ["World War II"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-801", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> location.statistical_region.gender_balance_members_of_parliament -> g.1hhc38hnh\n# Answer:\nlocation.statistical_region.gender_balance_members_of_parliament", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-802", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nSomebody to Love", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.music -> Nathan Lanier\n# Answer:\nJustin Bieber's Believe", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> tv.tv_program_guest.appeared_on -> m.0_grm8q -> tv.tv_guest_personal_appearance.episode -> Finalists Chosen #1\n# Answer:\nFinalists Chosen #1", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nMTV Europe Music Voices Award"], "ground_truth": ["Vocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-805", "prediction": ["# Reasoning Path:\nJohn Lennon -> people.deceased_person.place_of_death -> New York City -> music.composition.composer -> Yoko Ono\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Lennon -> people.deceased_person.place_of_death -> New York City -> music.recording.artist -> Yoko Ono\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Up Your Legs Forever -> film.film.directed_by -> Yoko Ono\n# Answer:\nUp Your Legs Forever", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> Big R Radio - Soft Hits -> broadcast.content.genre -> Rock music\n# Answer:\nBig R Radio - Soft Hits", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> Big R Radio - Soft Hits -> common.topic.image -> Big R radio\n# Answer:\nBig R Radio - Soft Hits", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.directed_by -> Yoko Ono\n# Answer:\nApotheosis", "# Reasoning Path:\nJohn Lennon -> people.deceased_person.place_of_death -> New York City -> music.recording.artist -> Elephant's Memory\n# Answer:\nNew York City", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Pop) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 70s (Pop)"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-809", "prediction": ["# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-CIA-TZ\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.notable_for -> g.1257lwlvq\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.image -> IST-Mirzapur\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> DST offset from UTC\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> common.topic.article -> m.02k8gl\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST begins\n# Answer:\nIndia Time Zone", "# Reasoning Path:\nBangalore -> location.location.time_zones -> India Time Zone -> freebase.valuenotation.has_no_value -> Day DST ends\n# Answer:\nIndia Time Zone"], "ground_truth": ["India Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-810", "prediction": ["# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Albanian language -> language.human_language.region -> Southeast Europe\n# Answer:\nAlbanian language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.languages_spoken -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> media_common.netflix_genre.titles -> A Girl in Black\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.country.official_language -> Greek Language -> language.human_language.region -> Europe\n# Answer:\nGreek Language", "# Reasoning Path:\nGreece -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60nhckb\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Albanian language", "Greek Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-811", "prediction": ["# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.architect -> Stanley Cole\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2008 World Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> sports.sports_facility.home_venue_for -> m.0wz1z6g\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.containedby -> Philadelphia\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> location.location.events -> 2009 National League Championship Series\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Citizens Bank Park -> architecture.structure.contractor -> Don Todd Associates, Inc\n# Answer:\nCitizens Bank Park", "# Reasoning Path:\nPhiladelphia Phillies -> sports.sports_team.arena_stadium -> Bright House Field -> base.schemastaging.sports_facility_extra.training_ground_for -> m.0x25ctr\n# Answer:\nBright House Field"], "ground_truth": ["Bright House Field"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-812", "prediction": ["# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.film -> As Good as Dead\n# Answer:\nAs Good as Dead", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kydt -> film.performance.film -> The Ghost of Goodnight Lane\n# Answer:\nThe Ghost of Goodnight Lane", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.09j2r90 -> film.performance.film -> Beauty & the Briefcase\n# Answer:\nBeauty & the Briefcase", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kycv -> film.performance.film -> Wannabe\n# Answer:\nWannabe", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2ky9r -> film.performance.film -> Wyatt Earp's Revenge\n# Answer:\nWyatt Earp's Revenge", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0cg5qxx -> film.performance.character -> Jake\n# Answer:\nJake", "# Reasoning Path:\nMatt Dallas -> film.actor.film -> m.0h2kydt -> film.performance.character -> Ben\n# Answer:\nBen", "# Reasoning Path:\nMatt Dallas -> people.person.spouse_s -> m.0vsbrys -> people.marriage.spouse -> Blue Hamilton\n# Answer:\nBlue Hamilton"], "ground_truth": ["You, Me & The Circus", "Naughty or Nice", "Way of the Vampire", "Hot Dudes with Kittens", "In Between Days", "The Indian", "As Good as Dead", "The Story of Bonnie and Clyde", "The Ghost of Goodnight Lane", "Wannabe", "Wyatt Earp's Revenge", "Camp Slaughter", "Living the Dream", "Beauty & the Briefcase", "Babysitter Wanted"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.43478260869565216, "ans_precission": 0.625, "ans_recall": 0.3333333333333333, "path_f1": 0.4166666666666667, "path_precision": 0.625, "path_recall": 0.3125, "path_ans_f1": 0.43478260869565216, "path_ans_precision": 0.625, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-813", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.parents -> John F. Fitzgerald\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Kathleen Cavendish\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Arabella Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nArabella Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.parents -> Mary Josephine Hannon\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.parents -> Rose Kennedy -> people.person.children -> Robert F. Kennedy\n# Answer:\nRose Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.02_wjwf -> people.sibling_relationship.sibling -> Ted Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.family_member.family -> Kennedy family -> people.family.members -> Ted Kennedy\n# Answer:\nKennedy family"], "ground_truth": ["Eunice Kennedy Shriver", "Robert F. Kennedy", "Patrick Bouvier Kennedy", "Rose Kennedy", "Ted Kennedy", "John F. Kennedy Jr.", "Kathleen Cavendish", "Joseph P. Kennedy, Sr.", "Arabella Kennedy", "Joseph P. Kennedy, Jr.", "Jean Kennedy Smith", "Rosemary Kennedy", "Caroline Kennedy", "Patricia Kennedy Lawford"], "ans_acc": 0.35714285714285715, "ans_hit": 1, "ans_f1": 0.4307692307692308, "ans_precission": 0.875, "ans_recall": 0.2857142857142857, "path_f1": 0.2916666666666667, "path_precision": 1.0, "path_recall": 0.17073170731707318, "path_ans_f1": 0.5263157894736842, "path_ans_precision": 1.0, "path_ans_recall": 0.35714285714285715}
{"id": "WebQTest-814", "prediction": ["# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.citytown -> Milwaukee\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> United States of America\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.state_province_region -> Wisconsin\n# Answer:\nWisconsin", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.mailing_address.postal_code -> 53208\n# Answer:\n53208", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> location.location.geolocation -> m.04np6h4\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> Area code 414\n# Answer:\nMilwaukee", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.headquarters -> m.03v5b_z -> common.topic.notable_for -> g.125f2tw4q\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nHarley-Davidson Motor Company -> organization.organization.place_founded -> Milwaukee -> location.location.containedby -> Milwaukee County\n# Answer:\nMilwaukee"], "ground_truth": ["Milwaukee"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-818", "prediction": ["# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.containedby -> Western Asia\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Sixth Crusade\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wf7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.contains -> Seljuk Empire\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Third Crusade\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wff\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> common.topic.image -> NearEast2\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.statistical_region.population -> m.04k6wfn\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> location.location.events -> Arab\u2013Israeli conflict\n# Answer:\nNear East", "# Reasoning Path:\nOttoman Empire -> location.location.containedby -> Near East -> book.book_subject.works -> Voyage en Orient\n# Answer:\nNear East"], "ground_truth": ["Near East"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-819", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.nearby_airports -> Abbottabad Airport\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> common.topic.notable_types -> Profession\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_types -> Profession\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.761904761904762, "ans_precission": 0.8888888888888888, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.3333333333333333, "path_recall": 0.3333333333333333, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-820", "prediction": ["# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0glbnz6\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Chamber of Secrets -> book.written_work.subjects -> Magic\n# Answer:\nHarry Potter and the Chamber of Secrets", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0j1zm85\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> freebase.valuenotation.is_reviewed -> Directed by\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Deathly Hallows -> fictional_universe.work_of_fiction.setting -> Wizarding world\n# Answer:\nHarry Potter and the Deathly Hallows", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> book.book.genre -> Fantasy\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.release_date_s -> m.0j56gsg\n# Answer:\nHarry Potter and the Prisoner of Azkaban", "# Reasoning Path:\nHarry Potter -> fictional_universe.fictional_universe.works_set_here -> Harry Potter and the Prisoner of Azkaban -> film.film.rating -> PG (USA)\n# Answer:\nHarry Potter and the Prisoner of Azkaban"], "ground_truth": ["Harry Potter and the Deathly Hallows - Part I", "Harry Potter and the Half-Blood Prince", "Harry Potter and the Philosopher's Stone", "Harry Potter and the Deathly Hallows \u2013 Part 2", "Harry Potter and the Prisoner of Azkaban", "Harry Potter and the Chamber of Secrets", "Harry Potter and the Goblet of Fire", "Harry Potter and the Order of the Phoenix"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3888888888888889, "ans_precission": 0.875, "ans_recall": 0.25, "path_f1": 0.3888888888888889, "path_precision": 0.875, "path_recall": 0.25, "path_ans_f1": 0.3888888888888889, "path_ans_precision": 0.875, "path_ans_recall": 0.25}
{"id": "WebQTest-821", "prediction": ["# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Mitch Hedberg\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Drug overdose -> people.cause_of_death.includes_causes_of_death -> Cocaine overdose\n# Answer:\nDrug overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> John Belushi\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Alice Ormsby-Gore\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.parent_cause_of_death -> Drug overdose\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> people.cause_of_death.people -> Andrew Stewart\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> people.deceased_person.cause_of_death -> Heroin overdose -> medicine.disease.parent_disease -> Drug overdose\n# Answer:\nHeroin overdose", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12552mkbz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPamela Courson -> common.topic.notable_for -> g.12599js9g\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Drug overdose", "Heroin overdose"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-822", "prediction": ["# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> people.profession.specialization_of -> Physician\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> common.topic.notable_types -> Profession\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> common.topic.article -> m.01ctkp\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Pathologists\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> freebase.equivalent_topic.equivalent_type -> Musical Artist\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Pathologist -> base.descriptive_names.names.descriptive_name -> m.010fz8xp\n# Answer:\nPathologist", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Painter -> fictional_universe.character_occupation.characters_with_this_occupation -> The Man of the Key\n# Answer:\nPainter", "# Reasoning Path:\nJack Kevorkian -> people.person.profession -> Musician -> common.topic.subjects -> MA$TADON\n# Answer:\nMusician", "# Reasoning Path:\nJack Kevorkian -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character"], "ground_truth": ["Right to die", "Assisted suicide"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-823", "prediction": ["# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> language.human_language.main_country -> Spain\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> language.human_language.countries_spoken_in -> Peru\n# Answer:\nAyacucho Quechua", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> common.topic.notable_types -> Human Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Spanish Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nBolivia -> location.country.languages_spoken -> Ayacucho Quechua -> common.topic.notable_for -> g.125dp74jw\n# Answer:\nAyacucho Quechua", "# Reasoning Path:\nBolivia -> travel.travel_destination.tour_operators -> Bunnik Tours -> travel.tour_operator.travel_destinations -> Peru\n# Answer:\nBunnik Tours", "# Reasoning Path:\nBolivia -> location.location.containedby -> South America -> travel.travel_destination.tour_operators -> Bunnik Tours\n# Answer:\nSouth America"], "ground_truth": ["Peru", "Brazil", "Chile", "Paraguay", "Argentina"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.031007751937984496, "path_precision": 0.25, "path_recall": 0.01652892561983471, "path_ans_f1": 0.22222222222222224, "path_ans_precision": 0.25, "path_ans_recall": 0.2}
{"id": "WebQTest-825", "prediction": ["# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlhm -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl5r -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdl8n -> baseball.batting_statistics.team -> Houston Astros\n# Answer:\nHouston Astros", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlmg -> baseball.batting_statistics.season -> 1988 Major League Baseball Season\n# Answer:\n1988 Major League Baseball Season", "# Reasoning Path:\nNolan Ryan -> baseball.baseball_player.batting_stats -> m.06sdlhm -> baseball.batting_statistics.season -> 1986 Major League Baseball Season\n# Answer:\n1986 Major League Baseball Season", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.0tl4ds3 -> sports.sports_team_roster.team -> New York Mets\n# Answer:\nNew York Mets", "# Reasoning Path:\nNolan Ryan -> sports.pro_athlete.teams -> m.0tl4ds3 -> sports.sports_team_roster.position -> Pitcher\n# Answer:\nPitcher"], "ground_truth": ["Houston Astros", "California Angels"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.2962962962962963, "path_precision": 0.5, "path_recall": 0.21052631578947367, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-827", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.first_level_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> common.topic.notable_types -> City/Town/Village\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford -> location.location.containedby -> Canada\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> Spanish Language\n# Answer:\nCanada"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-828", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hn35mm\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.063dqnv\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Official roberts CJ\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hmsrqy\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> people.appointed_role.appointment -> m.0hmtbtg\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> common.topic.image -> Seal of the Supreme Court\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> law.judicial_title.judges -> m.046x3bs\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> government.governmental_body.offices_positions -> Chief Justice of the United States -> book.book_subject.works -> Leaving the Bench: Supreme Court Justices at the End\n# Answer:\nChief Justice of the United States", "# Reasoning Path:\nSupreme Court of the United States -> common.topic.notable_for -> g.125dtprfx\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Chief Justice of the United States"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-829", "prediction": ["# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Paul C\u00e9zanne\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Georges Braque\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> people.person.religion -> Atheism\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Pablo Picasso\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Special Interest\n# Answer:\nSpecial Interest", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> influence.influence_node.influenced_by -> Saturnino Herr\u00e1n\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> common.topic.image -> Diego-Rivera.jpg\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> film.film.genre -> World cinema\n# Answer:\nWorld cinema", "# Reasoning Path:\nFrida Kahlo -> influence.influence_node.influenced_by -> Diego Rivera -> people.person.children -> Ruth Rivera\n# Answer:\nDiego Rivera", "# Reasoning Path:\nFrida Kahlo -> media_common.netflix_title.netflix_genres -> Art & Design\n# Answer:\nArt & Design"], "ground_truth": ["Diego Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_formerly_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.country.currency_used -> East German mark\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany -> location.country.currency_used -> Deutsche Mark\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany -> location.location.containedby -> Central and Eastern Europe\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> base.aareas.schema.administrative_area.administrative_children -> Vaduz\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Belgium\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.country.first_level_divisions -> Vaduz\n# Answer:\nLiechtenstein"], "ground_truth": ["Austria", "Switzerland", "Belgium", "Luxembourg", "Liechtenstein", "Germany", "East Germany"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.691358024691358, "ans_precission": 0.875, "ans_recall": 0.5714285714285714, "path_f1": 0.5454545454545454, "path_precision": 0.75, "path_recall": 0.42857142857142855, "path_ans_f1": 0.8333333333333333, "path_ans_precision": 1.0, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-830", "prediction": ["# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.state -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Berks County\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.hud_county_place.county -> Berks County\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> people.person.place_of_birth -> Reading -> location.location.containedby -> Pennsylvania\n# Answer:\nReading", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06530f3 -> base.popstra.friendship.participant -> Kellie Pickler\n# Answer:\nKellie Pickler", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461gz -> base.popstra.friendship.participant -> Selena Gomez\n# Answer:\nSelena Gomez", "# Reasoning Path:\nTaylor Swift -> base.popstra.celebrity.friendship -> m.06461h7 -> base.popstra.friendship.participant -> Miley Cyrus\n# Answer:\nMiley Cyrus"], "ground_truth": ["Reading"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-831", "prediction": ["# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp67g -> location.religion_percentage.religion -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp679 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nArgentina -> location.statistical_region.religions -> m.05bp674 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nArgentina -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60prvn4\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.color -> Chestnut\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> biology.organism.sex -> Male\n# Answer:\nAsiatic Boy", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_containedby -> Chile\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> location.location.partially_contains -> Alto San Juan -> location.location.partially_contained_by -> m.0wg8lvc\n# Answer:\nAlto San Juan", "# Reasoning Path:\nArgentina -> base.thoroughbredracing.thoroughbred_racehorse_origin.horses_from_this_location -> Asiatic Boy -> base.thoroughbredracing.thoroughbred_racehorse.sex -> Stallion\n# Answer:\nAsiatic Boy"], "ground_truth": ["Judaism", "Catholicism", "Protestantism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-832", "prediction": ["# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.runner_up -> Kansas City Royals\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> sports.sports_championship_event.championship -> World Series\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2014 World Series -> common.topic.notable_for -> g.11b5pgvyp1\n# Answer:\n2014 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2014 Major League Baseball season -> time.event.participant -> Kansas City Royals\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.championships -> 2012 World Series -> time.event.locations -> Comerica Park\n# Answer:\n2012 World Series", "# Reasoning Path:\nSan Francisco Giants -> time.participant.event -> 2014 Major League Baseball season -> time.event.next_in_series -> 2015 Major League Baseball season\n# Answer:\n2014 Major League Baseball season", "# Reasoning Path:\nSan Francisco Giants -> sports.sports_team.league -> m.0crt4b6 -> sports.sports_league_participation.league -> Major League Baseball\n# Answer:\nMajor League Baseball"], "ground_truth": ["2014 World Series"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-833", "prediction": ["# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> New Zealand\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> common.topic.notable_types -> Human Language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.human_language.countries_spoken_in -> Macau\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.country.official_language -> Standard Chinese -> language.language_dialect.language -> Chinese language\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChina -> location.statistical_region.official_development_assistance -> g.12cp_kgkf\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nChina -> location.location.partially_contains -> Teram Kangri -> base.schemastaging.disputed_location.claimed_by -> Pakistan\n# Answer:\nTeram Kangri", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Yili horse -> common.topic.notable_types -> Organism Classification\n# Answer:\nYili horse", "# Reasoning Path:\nChina -> biology.breed_origin.breeds_originating_here -> Tibetan Terrier -> biology.animal_breed.breed_of -> Dog\n# Answer:\nTibetan Terrier"], "ground_truth": ["Standard Chinese"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-834", "prediction": ["# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Napoleonic era\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.people_born_here -> Vic Lawrence\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> film.film.subjects -> Napoleonic Wars\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> common.topic.image -> Battle of Waterloo 1815 -> common.image.appears_in_topic_gallery -> William Sadler\n# Answer:\nBattle of Waterloo 1815", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> location.location.geolocation -> m.0239s57\n# Answer:\nWaterloo", "# Reasoning Path:\nBattle of Waterloo -> base.ontologies.ontology_instance.equivalent_instances -> m.09bxfm0 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nBattle of Waterloo -> common.topic.notable_types -> Military Conflict -> type.type.domain -> Military\n# Answer:\nMilitary Conflict", "# Reasoning Path:\nBattle of Waterloo -> time.event.locations -> Waterloo -> book.written_work.subjects -> Napoleonic Wars\n# Answer:\nWaterloo"], "ground_truth": ["Waterloo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-836", "prediction": ["# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Alexandre Ilich Ulianov\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Joseph Stalin\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Anna Ilichina Ulianova\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> common.topic.notable_for -> g.125f_5s6_\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Dmitry Ilyich Ulyanov\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.place_of_birth -> Saint Petersburg\n# Answer:\nMaria Alexandrovna Ulyanova", "# Reasoning Path:\nVladimir Lenin -> base.activism.activist.area_of_activism -> Atheism -> base.activism.activism_issue.activists -> Karl Marx\n# Answer:\nAtheism", "# Reasoning Path:\nVladimir Lenin -> people.person.parents -> Maria Alexandrovna Ulyanova -> people.person.children -> Mariya Ilichina Ulianova\n# Answer:\nMaria Alexandrovna Ulyanova"], "ground_truth": ["Nadezhda Krupskaya"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-837", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["Super Bowl XXXV", "Super Bowl XLVII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-838", "prediction": ["# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.point_awarded_to -> Portugal national football team\n# Answer:\nPortugal national football team", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.statistics -> m.0w9gdcb -> soccer.football_player_stats.team -> Sporting-soc Desportiva De\n# Answer:\nSporting-soc Desportiva De", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.0k061tt -> soccer.football_goal.match -> UEFA Euro 2012 Quarter-final I\n# Answer:\nUEFA Euro 2012 Quarter-final I", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzrlj -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010dzr92 -> soccer.football_goal.match -> 2014 Real Madrid CF vs. CA Osasuna football match\n# Answer:\n2014 Real Madrid CF vs. CA Osasuna football match", "# Reasoning Path:\nCristiano Ronaldo -> base.schemastaging.athlete_extra.salary -> m.0w_znzz -> base.schemastaging.athlete_salary.team -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F.", "# Reasoning Path:\nCristiano Ronaldo -> soccer.football_player.goals_scored -> m.010p4jhc -> soccer.football_goal.point_awarded_to -> Real Madrid C.F.\n# Answer:\nReal Madrid C.F."], "ground_truth": ["Real Madrid C.F.", "Portugal national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.35714285714285715, "path_precision": 0.625, "path_recall": 0.25, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> award.hall_of_fame_discipline.halls_of_fame -> Rock and Roll Hall of Fame\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock -> music.genre.parent_genre -> Rock music\n# Answer:\nArt rock"], "ground_truth": ["Pop rock", "Rock music", "Psychedelic rock", "Experimental rock", "Soft rock", "Pop music", "Experimental music", "Art rock", "Blues rock"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 1.0, "ans_recall": 0.4444444444444444, "path_f1": 0.6153846153846153, "path_precision": 1.0, "path_recall": 0.4444444444444444, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 1.0, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-841", "prediction": ["# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Vatican City\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> common.topic.notable_types -> Currency\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71mdn0q\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Austria\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Belgium\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6gbb1\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nItaly -> location.statistical_region.government_debt_percent_gdp -> g.1245_06sj\n# Answer:\nlocation.statistical_region.government_debt_percent_gdp", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> Finland\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.country.currency_used -> Euro -> finance.currency.countries_used -> France\n# Answer:\nEuro", "# Reasoning Path:\nItaly -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc37cxr\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Euro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-843", "prediction": ["# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> education.education.institution -> Humboldt University of Berlin\n# Answer:\nHumboldt University of Berlin", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> education.education.institution -> University of W\u00fcrzburg\n# Answer:\nUniversity of W\u00fcrzburg", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> education.education.institution -> University of Bonn\n# Answer:\nUniversity of Bonn", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0n0tzlj -> freebase.valuenotation.has_value -> Start Date\n# Answer:\nStart Date", "# Reasoning Path:\nTheodor Schwann -> people.deceased_person.place_of_death -> Cologne -> location.location.contains -> University of Bonn\n# Answer:\nCologne", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmxyk -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nTheodor Schwann -> common.topic.image -> Theodore Schwann -> common.image.size -> m.0klz7g\n# Answer:\nTheodore Schwann", "# Reasoning Path:\nTheodor Schwann -> people.person.education -> m.0vzmy28 -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date"], "ground_truth": ["University of W\u00fcrzburg", "Humboldt University of Berlin", "University of Bonn"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-845", "prediction": ["# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_holder -> Tim Kaine\n# Answer:\nTim Kaine", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcw0 -> government.government_position_held.office_holder -> Jim Webb\n# Answer:\nJim Webb", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.05kfcm1 -> government.government_position_held.office_holder -> Mark Warner\n# Answer:\nMark Warner", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.09s62vv -> government.government_position_held.office_holder -> Carter Glass\n# Answer:\nCarter Glass", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.0r99pcd -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nVirginia -> government.political_district.representatives -> m.04g7cb8 -> government.government_position_held.office_holder -> Gerald L. Baliles\n# Answer:\nGerald L. Baliles"], "ground_truth": ["Jim Webb", "Tim Kaine", "Mark Warner"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-846", "prediction": ["# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> common.topic.subject_of -> Michael Godard\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> common.topic.article -> m.02g43\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_1y2\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> common.topic.subject_of -> Dum-Doodles\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Artist -> people.profession.specializations -> Visual Artist\n# Answer:\nArtist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Visual Artist -> base.descriptive_names.names.descriptive_name -> m.0106_2f_\n# Answer:\nVisual Artist", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0111n9h6\n# Answer:\nAnimator", "# Reasoning Path:\nTheodore Lesieg -> people.person.profession -> Animator -> theater.theater_production_staff_role.people_who_have_had_this_role -> m.0_grv_v\n# Answer:\nAnimator"], "ground_truth": ["Theodor Seuss Geisel"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-847", "prediction": ["# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kq7 -> base.unitednations.united_nations_body_membership.member -> Costa Rica\n# Answer:\nCosta Rica", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__knr -> base.unitednations.united_nations_body_membership.member -> Burkina Faso\n# Answer:\nBurkina Faso", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__j0t -> base.unitednations.united_nations_body_membership.member -> France\n# Answer:\nFrance", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kpd -> base.unitednations.united_nations_body_membership.member -> Croatia\n# Answer:\nCroatia", "# Reasoning Path:\nUnited Nations Security Council -> base.unitednations.united_nations_body.members -> m.08__kpx -> base.unitednations.united_nations_body_membership.member -> Vietnam\n# Answer:\nVietnam", "# Reasoning Path:\nUnited Nations Security Council -> base.schemastaging.context_name.pronunciation -> g.125_nxwdp\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> United Nations Security Council Resolution 1318\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.image.size -> m.04xxdf1\n# Answer:\ncommon.image.size", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.size -> m.0291vv2\n# Answer:\nUN security council 2005", "# Reasoning Path:\nUnited Nations Security Council -> common.topic.image -> UN security council 2005 -> common.image.appears_in_topic_gallery -> Arnstein Arneberg\n# Answer:\nUN security council 2005"], "ground_truth": ["China", "Costa Rica", "Burkina Faso", "France", "Croatia", "Russia", "United Kingdom", "Vietnam", "United States of America"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.5263157894736842, "ans_precission": 0.5, "ans_recall": 0.5555555555555556, "path_f1": 0.5263157894736842, "path_precision": 0.5, "path_recall": 0.5555555555555556, "path_ans_f1": 0.5263157894736842, "path_ans_precision": 0.5, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-848", "prediction": ["# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Mariehamn sub-region\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> base.schemastaging.context_name.pronunciation -> g.125_kxbsc\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Archipelago\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.partially_contained_by -> m.0_gjrmv -> location.partial_containment_relationship.partially_contained_by -> Northern Europe\n# Answer:\nNorthern Europe", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Countryside\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.nearby_airports -> Mariehamn Airport\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.contains -> Ecker\u00f6\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.people_born_here -> Paul Gustavson\n# Answer:\n\u00c5land Islands", "# Reasoning Path:\nNordic countries -> location.location.contains -> \u00c5land Islands -> location.location.primarily_containedby -> Finland\n# Answer:\n\u00c5land Islands"], "ground_truth": ["Greenland", "\u00c5land Islands", "Finland", "Faroe Islands", "Sweden", "Norway", "Denmark", "Iceland"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.2153846153846154, "ans_precission": 0.7777777777777778, "ans_recall": 0.125, "path_f1": 0.2153846153846154, "path_precision": 0.7777777777777778, "path_recall": 0.125, "path_ans_f1": 0.3783783783783784, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 0.25}
{"id": "WebQTest-849", "prediction": ["# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Mendes de Melo\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> g.11btv6zfvr\n# Answer:\nprotected_sites.listed_site.designation_as_natural_or_cultural_site", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Afonso Pires Gato\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_9736\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Lu\u00eds de Sousa, 2nd Marquis of Minas\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.country.capital -> Lisbon\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> location.location.people_born_here -> Ant\u00f3nio Maria da Silva\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> government.governmental_jurisdiction.governing_officials -> m.04_975f\n# Answer:\nKingdom of Portugal", "# Reasoning Path:\nFerdinand Magellan -> people.person.nationality -> Kingdom of Portugal -> royalty.kingdom.rulers -> Sebastian of Portugal\n# Answer:\nKingdom of Portugal"], "ground_truth": ["Kingdom of Portugal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wy4l -> government.government_position_held.office_holder -> Ken Salazar\n# Answer:\nKen Salazar", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpb7n -> government.government_position_held.office_holder -> Hank Brown\n# Answer:\nHank Brown", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-850", "prediction": ["# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Erlotinib\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> common.topic.article -> m.022smc\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Peter McWilliams\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> common.topic.image -> Cancer1\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nVera-Ellen -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Deborah King\n# Answer:\nCancer"], "ground_truth": ["Cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-851", "prediction": ["# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Al-Ahram Weekly\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> location.location.containedby -> North Africa\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.capital -> Cairo -> common.topic.subject_of -> As Egypt Howls and History Tweets\n# Answer:\nCairo", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo Governorate -> common.topic.notable_types -> Administrative Division\n# Answer:\nCairo Governorate", "# Reasoning Path:\nEgypt -> location.country.administrative_divisions -> Cairo -> periodicals.newspaper_circulation_area.newspapers -> Almesryoon\n# Answer:\nCairo"], "ground_truth": ["Cairo"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-852", "prediction": ["# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.location.containedby -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Rada of the Belarusian Democratic Republic -> common.topic.article -> m.0tkkqmp\n# Answer:\nRada of the Belarusian Democratic Republic", "# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Ontario -> location.administrative_division.capital -> m.0jvvny7\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> base.aareas.schema.administrative_area.administrative_parent -> Canada\n# Answer:\nOntario", "# Reasoning Path:\nToronto -> location.location.containedby -> Canada -> location.country.official_language -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nToronto -> base.biblioness.bibs_location.state -> Ontario -> location.administrative_division.country -> Canada\n# Answer:\nOntario"], "ground_truth": ["Ontario"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-855", "prediction": ["# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.pertinent_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.12cp_jcjy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.11b71vdgdz\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> South America\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.gni_in_ppp_dollars -> g.11b60s1wwg\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc390hy\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.uncommon.topic.exceptions -> City/Town/Village should pertain to Ghana, since City/Town/Village subdivides Ghanaian Municipal District.\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.12tb6fm01\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp", "# Reasoning Path:\nBrazil -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.aareas.schema.administrative_area.subdividing_type -> Sovereign state\n# Answer:\nEarth", "# Reasoning Path:\nBrazil -> location.statistical_region.official_development_assistance -> g.1hhc39sjp\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Brazilian real"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-856", "prediction": ["# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> visual_art.art_period_movement.associated_artworks -> Mickey\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subjects -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> common.topic.subject_of -> Wil Emling\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Modern art -> interests.collection_category.parent_category -> Art\n# Answer:\nModern art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.art_forms -> Printmaking\n# Answer:\nPrintmaking", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.product_lines -> TAG Registries\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> business.product_theme.products -> Andy Warhol Registry\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artworks -> Woman in Tub\n# Answer:\nPop art", "# Reasoning Path:\nAndy Warhol -> visual_art.visual_artist.associated_periods_or_movements -> Pop art -> visual_art.art_period_movement.associated_artists -> Takashi Murakami\n# Answer:\nPop art"], "ground_truth": ["Modern art", "Pop art"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-857", "prediction": ["# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Alabama\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Choctawhatchee River -> location.location.partially_containedby -> Alabama\n# Answer:\nChoctawhatchee River", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> CRHOME -> organization.organization.geographic_scope -> Alabama\n# Answer:\nCRHOME", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> B'nai B'rith District Grand Lodge 7 -> organization.organization.geographic_scope -> Alabama\n# Answer:\nB'nai B'rith District Grand Lodge 7", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Chattahoochee River -> location.location.partially_containedby -> Georgia\n# Answer:\nChattahoochee River", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> EPIC Systems, Inc -> organization.organization.geographic_scope -> Arkansas\n# Answer:\nEPIC Systems, Inc", "# Reasoning Path:\nFlorida -> location.location.partially_contains -> Blackwater River -> location.location.partially_containedby -> Alabama\n# Answer:\nBlackwater River", "# Reasoning Path:\nFlorida -> organization.organization_scope.organizations_with_this_scope -> David Conner + Associates -> organization.organization.geographic_scope -> Tampa\n# Answer:\nDavid Conner + Associates"], "ground_truth": ["Georgia", "Alabama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.32432432432432434, "path_precision": 0.75, "path_recall": 0.20689655172413793, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-859", "prediction": ["# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyk -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nRussia -> location.statistical_region.religions -> m.06jmmyb -> location.religion_percentage.religion -> Russian Orthodox Church\n# Answer:\nRussian Orthodox Church", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.12tb6gh0b\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6flqv\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.1hhc3f_fx\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.location.partiallycontains -> m.0_gjb9b -> location.partial_containment_relationship.partially_contains -> Karelia\n# Answer:\nKarelia"], "ground_truth": ["Russian Orthodox Church", "Islam"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.first_level_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> \u00c5land Islands\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> biology.breed_origin.breeds_originating_here -> Greenland Dog\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Kingdom of Denmark -> base.aareas.schema.administrative_area.administrative_children -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> North Denmark Region\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Northern Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nGreenland -> location.statistical_region.merchandise_trade_percent_of_gdp -> g.1hhc3bn1j\n# Answer:\nlocation.statistical_region.merchandise_trade_percent_of_gdp"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-861", "prediction": ["# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.0j81th4 -> american_football.football_historical_coach_position.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.coach -> John Fox\n# Answer:\nJohn Fox", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yc -> american_football.football_historical_coach_position.coach -> Dom Capers\n# Answer:\nDom Capers", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.historical_coaching_staff -> m.04xg_yl -> american_football.football_historical_coach_position.coach -> George Seifert\n# Answer:\nGeorge Seifert", "# Reasoning Path:\nCarolina Panthers -> sports.sports_team.coaches -> m.0_3r22s -> sports.sports_team_coach_tenure.position -> Head coach\n# Answer:\nHead coach", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> american_football.football_coach.coaching_history -> m.0hpyjn8\n# Answer:\nRon Rivera", "# Reasoning Path:\nCarolina Panthers -> american_football.football_team.current_head_coach -> Ron Rivera -> award.award_winner.awards_won -> m.0_qw472\n# Answer:\nRon Rivera"], "ground_truth": ["Ron Rivera"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-862", "prediction": ["# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> William Fichtner\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07f2vhv\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> common.topic.article -> m.01bq8n\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Adam Busch\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.hud_foreclosure_area.estimated_number_foreclosures -> m.07f2vh4\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Betsy Aidem\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> common.topic.image -> East-meadow-ny-map\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Billy Staples\n# Answer:\nEast Meadow", "# Reasoning Path:\nLouis Sachar -> people.person.place_of_birth -> East Meadow -> location.location.people_born_here -> Brandon Moore\n# Answer:\nEast Meadow"], "ground_truth": ["East Meadow"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-865", "prediction": ["# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.article -> m.063g20\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> amusement_parks.park.rides -> Tornado\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> common.topic.notable_for -> g.125dd3mwm\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Hanna\u2013Barbera Land -> common.topic.article -> m.0dhl0m\n# Answer:\nHanna\u2013Barbera Land", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Space Center Houston -> common.topic.notable_for -> g.1257yfxcp\n# Answer:\nSpace Center Houston", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Wet'n'Wild SplashTown -> location.location.containedby -> United States of America\n# Answer:\nWet'n'Wild SplashTown", "# Reasoning Path:\nHouston -> location.statistical_region.population -> g.11b66fk05m\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Hanna\u2013Barbera Land -> common.topic.notable_for -> g.1259w1v4t\n# Answer:\nHanna\u2013Barbera Land", "# Reasoning Path:\nHouston -> travel.travel_destination.tourist_attractions -> Kemah Boardwalk -> location.location.containedby -> United States of America\n# Answer:\nKemah Boardwalk"], "ground_truth": ["George R. Brown Convention Center", "Bayou Bend Collection and Gardens", "Sam Houston Race Park", "Space Center Houston", "Hanna\u2013Barbera Land", "Houston Arboretum and Nature Center", "Gerald D. Hines Waterwall Park", "Lyndon B. Johnson Space Center", "The Galleria", "Rothko Chapel", "Museum of Fine Arts, Houston", "USS Texas (BB-35)", "Lakewood Church", "Houston Museum of Natural Science", "Houston Zoo", "Downtown Aquarium, Houston", "Toyota Center", "Houston Marathon", "Kemah Boardwalk", "Wet'n'Wild SplashTown", "Children's Museum of Houston"], "ans_acc": 0.19047619047619047, "ans_hit": 1, "ans_f1": 0.3137254901960784, "ans_precission": 0.8888888888888888, "ans_recall": 0.19047619047619047, "path_f1": 0.3137254901960784, "path_precision": 0.8888888888888888, "path_recall": 0.19047619047619047, "path_ans_f1": 0.3137254901960784, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.19047619047619047}
{"id": "WebQTest-866", "prediction": ["# Reasoning Path:\nOtto Frank -> people.person.education -> m.0gft51q -> education.education.institution -> Heidelberg University\n# Answer:\nHeidelberg University", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_1380.jpg\n# Answer:\n100_1380.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp1lx -> base.popstra.friendship.participant -> Max Goldberg\n# Answer:\nMax Goldberg", "# Reasoning Path:\nOtto Frank -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nOtto Frank -> common.topic.image -> 100_2022.jpg\n# Answer:\n100_2022.jpg", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2cx -> base.popstra.friendship.participant -> Hilde Goldberg\n# Answer:\nHilde Goldberg", "# Reasoning Path:\nOtto Frank -> people.person.nationality -> Germany -> location.country.administrative_divisions -> Frankfurt\n# Answer:\nGermany", "# Reasoning Path:\nOtto Frank -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nOtto Frank -> base.popstra.celebrity.friendship -> m.0gvp2g0 -> base.popstra.friendship.participant -> Cor Suijk\n# Answer:\nCor Suijk", "# Reasoning Path:\nOtto Frank -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character"], "ground_truth": ["Heidelberg University"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-867", "prediction": ["# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0zn92zk -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw5gg -> award.award_nomination.nominated_for -> The Hunger Games: Catching Fire\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0r9hngb -> award.award_nomination.nominated_for -> House at the End of the Street\n# Answer:\nHouse at the End of the Street", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0zn92zk -> award.award_nomination.ceremony -> 19th Critics' Choice Awards\n# Answer:\n19th Critics' Choice Awards", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0z8822f -> award.award_nomination.nominated_for -> X-Men: First Class\n# Answer:\nX-Men: First Class", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0_vw7_t -> award.award_nomination.nominated_for -> American Hustle\n# Answer:\nAmerican Hustle", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0zn92zk -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony", "# Reasoning Path:\nJennifer Lawrence -> award.award_nominee.award_nominations -> m.0zn92zk -> award.award_nomination.award -> BFCA Critics' Choice Award for Best Actress in an Action Movie\n# Answer:\nBFCA Critics' Choice Award for Best Actress in an Action Movie"], "ground_truth": ["The Hunger Games: Mockingjay, Part 2", "Company Town", "Burial Rites", "Like Crazy", "The Burning Plain", "Devil You Know", "Joy", "Serena", "The Poker House", "Silver Linings Playbook", "Garden Party", "The Hunger Games: Catching Fire", "X-Men: Days of Future Past", "The Beaver", "Not Another High School Show", "American Hustle", "X-Men: First Class", "The Hunger Games: Mockingjay, Part 1", "The Glass Castle", "East of Eden", "Winter\u2019s Bone", "X-Men: Apocalypse", "House at the End of the Street", "The Hunger Games"], "ans_acc": 0.20833333333333334, "ans_hit": 1, "ans_f1": 0.3125, "ans_precission": 0.625, "ans_recall": 0.20833333333333334, "path_f1": 0.0641025641025641, "path_precision": 0.625, "path_recall": 0.033783783783783786, "path_ans_f1": 0.3125, "path_ans_precision": 0.625, "path_ans_recall": 0.20833333333333334}
{"id": "WebQTest-868", "prediction": ["# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> government.government_position_held.office_holder -> Jim Cawley\n# Answer:\nJim Cawley", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.011crdxd -> government.government_position_held.office_holder -> Rosemary Brown\n# Answer:\nRosemary Brown", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.048157l -> government.government_position_held.office_holder -> Catherine Baker Knoll\n# Answer:\nCatherine Baker Knoll", "# Reasoning Path:\nPennsylvania -> business.employer.employees -> m.04ds08c -> business.employment_tenure.title -> Director, Office of Health Care Reform\n# Answer:\nDirector, Office of Health Care Reform", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.04kstp3 -> government.government_position_held.office_holder -> Thomas Mifflin\n# Answer:\nThomas Mifflin", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.04kstp7 -> government.government_position_held.office_holder -> Thomas McKean\n# Answer:\nThomas McKean", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nBasic title", "# Reasoning Path:\nPennsylvania -> government.governmental_jurisdiction.governing_officials -> m.010f42tf -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Tom Corbett"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-869", "prediction": ["# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Eurasia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Middle East\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.geolocation -> m.0cmwfl8\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Arab world\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.location.containedby -> Asia\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.11b60vk8mh\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.gdp_real -> g.11b61jk247\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.internet_users_percent_population -> g.1245_2gxy\n# Answer:\nLebanon", "# Reasoning Path:\nPhoenicia -> location.location.containedby -> Lebanon -> location.statistical_region.diesel_price_liter -> g.12cp_jcgf\n# Answer:\nLebanon"], "ground_truth": ["Lebanon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.9411764705882353, "path_precision": 0.8888888888888888, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.location.geolocation -> m.0khstt\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.geolocation -> m.03dyr06\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geometry -> m.055f4wk\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> location.location.geometry -> m.055f4f_\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> location.location.geolocation -> m.03dyr0d\n# Answer:\n98102"], "ground_truth": ["98158", "98164", "98185", "98154", "98134", "98121", "98199", "98160", "98168", "98144", "98198", "98106", "98105", "98139", "98117", "98129", "98107", "98175", "98136", "98188", "98114", "98138", "98116", "98146", "98113", "98126", "98127", "98115", "98148", "98178", "98108", "98184", "98165", "98102", "98122", "98166", "98141", "98118", "98133", "98119", "98170", "98145", "98125", "98181", "98103", "98161", "98109", "98119-4114", "98101", "98104", "98177", "98112", "98174", "98132", "98124", "98111", "98190", "98194", "98131", "98171", "98195", "98191", "98155"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.0903954802259887, "ans_precission": 0.8888888888888888, "ans_recall": 0.047619047619047616, "path_f1": 0.0903954802259887, "path_precision": 0.8888888888888888, "path_recall": 0.047619047619047616, "path_ans_f1": 0.0903954802259887, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-870", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> sports.sports_facility.teams -> Boston Bruins\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> location.location.containedby -> Boston\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> common.topic.notable_types -> Sports Facility\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> travel.tourist_attraction.near_travel_destination -> Boston\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> TD Garden -> location.location.events -> 1999  NCAA Men's Division I Basketball Tournament- East Regional First Round\n# Answer:\nTD Garden", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.events -> WrestleMania XI\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> location.location.geolocation -> m.0v1p8cd\n# Answer:\nXL Center", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.arena_stadium -> XL Center -> sports.sports_facility.teams -> Hartford Wolf Pack\n# Answer:\nXL Center"], "ground_truth": ["XL Center", "TD Garden"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-872", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.nearby_airports -> Abbottabad Airport\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-873", "prediction": ["# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> architecture.structure.architect -> Eberhard Zeidler\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> base.infrastructure.infrastructural_municipality.sewage_treatment_plant -> Ashbridges Bay Wastewater Treatment Plant -> common.topic.article -> m.02vnl80\n# Answer:\nAshbridges Bay Wastewater Treatment Plant", "# Reasoning Path:\nToronto -> base.infrastructure.infrastructural_municipality.sewage_treatment_plant -> Ashbridges Bay Wastewater Treatment Plant -> common.topic.notable_for -> g.1255zq9nb\n# Answer:\nAshbridges Bay Wastewater Treatment Plant", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> travel.tourist_attraction.near_travel_destination -> Newmarket\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> base.infrastructure.infrastructural_municipality.sewage_treatment_plant -> Ashbridges Bay Wastewater Treatment Plant -> common.topic.notable_types -> Building\n# Answer:\nAshbridges Bay Wastewater Treatment Plant", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> common.topic.article -> m.05398q\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Centre for the Arts -> location.location.geolocation -> m.0cqsh3n\n# Answer:\nToronto Centre for the Arts", "# Reasoning Path:\nToronto -> travel.travel_destination.tourist_attractions -> Toronto Islands -> geography.island.body_of_water -> Lake Ontario\n# Answer:\nToronto Islands"], "ground_truth": ["Ashbridges Bay Wastewater Treatment Plant"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-874", "prediction": ["# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.countries_spoken_in -> Croatia\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> common.topic.image -> Serbo croatian language2005\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.official_language -> Croatian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nCroatian language", "# Reasoning Path:\nBosnia and Herzegovina -> location.country.languages_spoken -> Serbo-Croatian Language -> language.human_language.writing_system -> Cyrillic script\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nBosnia and Herzegovina -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_j3km\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp"], "ground_truth": ["Croatian language", "Bosnian language", "Serbo-Croatian Language", "Serbian language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.64, "ans_precission": 0.8888888888888888, "ans_recall": 0.5, "path_f1": 0.64, "path_precision": 0.8888888888888888, "path_recall": 0.5, "path_ans_f1": 0.64, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.5}
{"id": "WebQTest-875", "prediction": ["# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s1m -> education.education.institution -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nSim\u00f3n Bol\u00edvar University", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s0w -> education.education.institution -> Bolivarian Military University of Venezuela\n# Answer:\nBolivarian Military University of Venezuela", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Sim\u00f3n Bol\u00edvar University\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467r_k -> education.education.institution -> Julian Pino School\n# Answer:\nJulian Pino School", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s06 -> education.education.institution -> Daniel Florencio O'Leary School\n# Answer:\nDaniel Florencio O'Leary School", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.education -> m.0467s0w -> education.education.major_field_of_study -> military arts and science\n# Answer:\nmilitary arts and science", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.deceased_person.place_of_death -> Caracas -> location.location.contains -> Anauco Hilton\n# Answer:\nCaracas", "# Reasoning Path:\nHugo Ch\u00e1vez -> people.person.sibling_s -> m.0467rzz -> people.sibling_relationship.sibling -> Ad\u00e1n Ch\u00e1vez\n# Answer:\nAd\u00e1n Ch\u00e1vez"], "ground_truth": ["Daniel Florencio O'Leary School", "Sim\u00f3n Bol\u00edvar University", "Julian Pino School", "Bolivarian Military University of Venezuela"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-876", "prediction": ["# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1952 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1952 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> Australia\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nJapan -> olympics.olympic_participating_country.olympics_participated_in -> 1980 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1980 Winter Olympics", "# Reasoning Path:\nJapan -> location.statistical_region.co2_emissions_per_capita -> g.1245_2h9t\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nJapan -> location.statistical_region.deposit_interest_rate -> g.11b71r82sc\n# Answer:\nlocation.statistical_region.deposit_interest_rate"], "ground_truth": ["Tanzania", "Canada", "Kiribati", "New Zealand", "Sudan", "United States of America"], "ans_acc": 0.16666666666666666, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.25, "path_ans_precision": 0.5, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-877", "prediction": ["# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Raihan Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.children -> Miraya Vadra\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> freebase.valuenotation.has_value -> Height\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Rahul Gandhi -> people.person.parents -> Rajiv Gandhi\n# Answer:\nRahul Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> common.topic.article -> m.02n20d\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> government.politician.party -> m.0p7f152\n# Answer:\nPriyanka Gandhi", "# Reasoning Path:\nSonia Gandhi -> common.topic.notable_types -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Senator\n# Answer:\nPolitician", "# Reasoning Path:\nSonia Gandhi -> people.person.children -> Priyanka Gandhi -> people.person.place_of_birth -> New Delhi\n# Answer:\nPriyanka Gandhi"], "ground_truth": ["Priyanka Gandhi"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-878", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.deities -> Yahweh\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.article -> m.055v4j\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir01\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.notable_for -> g.1yl5ndjfk\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> people.person.religion -> Hinduism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> religion.deity.deity_of -> Jainism\n# Answer:\nRamdev Pir", "# Reasoning Path:\nIslam -> religion.religion.deities -> Ramdev Pir -> common.topic.image -> Ramapir22\n# Answer:\nRamdev Pir"], "ground_truth": ["Allah", "Ramdev Pir", "God"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.48275862068965514, "ans_precission": 0.875, "ans_recall": 0.3333333333333333, "path_f1": 0.48275862068965514, "path_precision": 0.875, "path_recall": 0.3333333333333333, "path_ans_f1": 0.48275862068965514, "path_ans_precision": 0.875, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-879", "prediction": ["# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> film.film_location.featured_in_films -> Secretariat\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> United States of America\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.office_position_or_title -> First Lady of the United States\n# Answer:\nFirst Lady of the United States", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.hud_foreclosure_area.total_90_day_vacant_residential_addresses -> m.07c9slk\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Area code 859\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> travel.travel_destination.tourist_attractions -> Mary Todd Lincoln House\n# Answer:\nLexington", "# Reasoning Path:\nMary Todd Lincoln -> government.politician.government_positions_held -> m.0l0qwng -> government.government_position_held.basic_title -> First Lady\n# Answer:\nFirst Lady", "# Reasoning Path:\nMary Todd Lincoln -> people.person.place_of_birth -> Lexington -> location.location.containedby -> Fayette County\n# Answer:\nLexington"], "ground_truth": ["Lexington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> common.topic.article -> m.03gq437\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> symbols.namesake.named_after -> Jada Rowland\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.spouse_s -> m.02kp6m3\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.album -> 3 -> common.topic.notable_types -> Musical Album\n# Answer:\n3", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-880", "prediction": ["# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> location.location.containedby -> United Kingdom\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.notable_for -> g.12578zv1w\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> common.topic.article -> m.02_67b\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> travel.tourist_attraction.near_travel_destination -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> protected_sites.listed_site.designation_as_natural_or_cultural_site -> m.0djwmxs\n# Answer:\nDown House", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> location.location.containedby -> City of Westminster\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_burial -> Westminster Abbey -> religion.place_of_worship.religion -> Anglicanism\n# Answer:\nWestminster Abbey", "# Reasoning Path:\nCharles Darwin -> people.deceased_person.place_of_death -> Down House -> location.location.people_born_here -> Charles Waring Darwin\n# Answer:\nDown House"], "ground_truth": ["Down House"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-881", "prediction": ["# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.film -> Ball Don't Lie\n# Answer:\nBall Don't Lie", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncf55j -> film.performance.film -> Men in Black II\n# Answer:\nMen in Black II", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.nominated_for -> Love Don't Cost a Thing\n# Answer:\nLove Don't Cost a Thing", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.02tbk6p -> film.performance.film -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.0ncd_c8 -> film.performance.film -> Whatever It Takes\n# Answer:\nWhatever It Takes", "# Reasoning Path:\nNick Cannon -> film.actor.film -> m.062wcnd -> film.performance.character -> Mico\n# Answer:\nMico", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0z9v3c6 -> award.award_nomination.ceremony -> 2004 Teen Choice Awards\n# Answer:\n2004 Teen Choice Awards", "# Reasoning Path:\nNick Cannon -> award.award_nominee.award_nominations -> m.0b4d586 -> award.award_nomination.nominated_for -> Bobby\n# Answer:\nBobby", "# Reasoning Path:\nNick Cannon -> film.actor.film -> g.11bttggxl1\n# Answer:\nfilm.actor.film"], "ground_truth": ["Shall We Dance?", "Whatever It Takes", "Monster House", "American Son", "Drumline", "Even Money", "Chiraq", "Drumline: A New Beat", "Men in Black II", "Underclassman", "Roll Bounce", "The Adventures of Brer Rabbit", "Ball Don't Lie", "A Very School Gyrls Holla-Day", "Garfield: The Movie", "Love Don't Cost a Thing", "Weapons", "Day of the Dead", "Bobby", "The Killing Room", "Goal II: Living the Dream"], "ans_acc": 0.23809523809523808, "ans_hit": 1, "ans_f1": 0.3508771929824562, "ans_precission": 0.6666666666666666, "ans_recall": 0.23809523809523808, "path_f1": 0.30000000000000004, "path_precision": 0.6666666666666666, "path_recall": 0.1935483870967742, "path_ans_f1": 0.3508771929824562, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.23809523809523808}
{"id": "WebQTest-882", "prediction": ["# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Golden Gate Park\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> location.location.containedby -> United States of America\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Yerba Buena Center for the Arts\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> film.film_subject.films -> San Francisco -> travel.travel_destination.tourist_attractions -> Palace of the Legion of Honor\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> base.schemastaging.context_name.pronunciation -> g.125_nh8xf\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> book.book_subject.works -> 1906\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Alcatraz Island\n# Answer:\nSan Francisco", "# Reasoning Path:\n1906 San Francisco earthquake -> time.event.locations -> San Francisco -> travel.travel_destination.tourist_attractions -> Angel Island\n# Answer:\nSan Francisco"], "ground_truth": ["1906-04-18"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-883", "prediction": ["# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Prince Edward, Earl of Wessex\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.family_member.family -> House of Windsor\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.family_member.family -> House of Windsor -> people.family.members -> George V\n# Answer:\nHouse of Windsor", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Anne, Princess Royal\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Princess Margaret, Countess of Snowdon -> people.person.children -> David Armstrong-Jones, Viscount Linley\n# Answer:\nPrincess Margaret, Countess of Snowdon", "# Reasoning Path:\nGeorge VI -> people.person.parents -> George V -> people.person.children -> Edward VIII of the United Kingdom\n# Answer:\nGeorge V", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.spouse_s -> m.028zms8\n# Answer:\nElizabeth II", "# Reasoning Path:\nGeorge VI -> people.person.children -> Elizabeth II -> people.person.children -> Charles, Prince of Wales\n# Answer:\nElizabeth II"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-884", "prediction": ["# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> location.location.containedby -> Middle East\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> common.topic.article -> m.065shd\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Paul the Apostle -> people.person.place_of_birth -> Tarsus\n# Answer:\nPaul the Apostle", "# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> location.location.events -> Siege of Jerusalem\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> people.person.place_of_birth -> Roman Empire\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nJesus Christ -> people.person.place_of_birth -> Judea -> location.location.geolocation -> m.0c1r0zp\n# Answer:\nJudea", "# Reasoning Path:\nJesus Christ -> common.topic.notable_for -> g.12551f1xr\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Augustine of Hippo -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nAugustine of Hippo", "# Reasoning Path:\nJesus Christ -> influence.influence_node.influenced -> Paul the Apostle -> influence.influence_node.influenced -> Thomas Aquinas\n# Answer:\nPaul the Apostle"], "ground_truth": ["Judea"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-885", "prediction": ["# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> people.profession.specialization_of -> Artist\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pmf\n# Answer:\nDesigner", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> common.topic.notable_types -> Profession\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> common.topic.notable_types -> Organization leader -> freebase.type_profile.equivalent_topic -> board member\n# Answer:\nOrganization leader", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Industrial designer -> base.lightweight.profession.professions_similar -> Photographer\n# Answer:\nIndustrial designer", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Engineer -> fictional_universe.character_occupation.characters_with_this_occupation -> Cid Highwind\n# Answer:\nEngineer", "# Reasoning Path:\nJames Dyson -> people.person.nationality -> United Kingdom -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nJames Dyson -> people.person.profession -> Designer -> base.descriptive_names.names.descriptive_name -> m.01053pnt\n# Answer:\nDesigner"], "ground_truth": ["Industrial designer", "Engineer", "Inventor", "Designer"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.75, "ans_precission": 0.75, "ans_recall": 0.75, "path_f1": 0.75, "path_precision": 0.75, "path_recall": 0.75, "path_ans_f1": 0.75, "path_ans_precision": 0.75, "path_ans_recall": 0.75}
{"id": "WebQTest-886", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04htxl0 -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.film -> Star Wars\n# Answer:\nStar Wars", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nv74t -> film.performance.special_performance_type -> Uncredited\n# Answer:\nUncredited", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice"], "ground_truth": ["James Earl Jones"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.625, "path_recall": 0.8333333333333334, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-887", "prediction": ["# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> education.education.institution -> George Washington High School\n# Answer:\nGeorge Washington High School", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> education.education.institution -> California Labor School\n# Answer:\nCalifornia Labor School", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> education.education.degree -> High school\n# Answer:\nHigh school", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Minor\n# Answer:\nMinor", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0vv8k0g -> freebase.valuenotation.has_value -> End Date\n# Answer:\nEnd Date", "# Reasoning Path:\nMaya Angelou -> people.person.education -> m.0n1mdbx -> freebase.valuenotation.has_no_value -> Specialization\n# Answer:\nSpecialization"], "ground_truth": ["George Washington High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-888", "prediction": ["# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Football\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2014 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> United States of America\n# Answer:\n2002 Winter Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Handball\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics -> olympics.olympic_games.sports -> Tennis\n# Answer:\n1896 Summer Olympics", "# Reasoning Path:\nSwitzerland -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics -> olympics.olympic_games.sports -> Water polo\n# Answer:\n1936 Summer Olympics", "# Reasoning Path:\nSwitzerland -> location.statistical_region.renewable_freshwater_per_capita -> g.11b6d0tc3g\n# Answer:\nlocation.statistical_region.renewable_freshwater_per_capita"], "ground_truth": ["Austria", "France", "Liechtenstein", "Germany", "Italy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-889", "prediction": ["# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.jurisdiction_of_office -> Italy\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.office_position_or_title -> Duce\n# Answer:\nDuce", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> government.governmental_jurisdiction.governing_officials -> m.0c9_j0x\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> government.politician.government_positions_held -> m.0c9_j0x -> government.government_position_held.basic_title -> Dictator\n# Answer:\nDictator", "# Reasoning Path:\nBenito Mussolini -> government.politician.party -> m.0pcl_06 -> government.political_party_tenure.party -> Republican Fascist Party\n# Answer:\nRepublican Fascist Party", "# Reasoning Path:\nBenito Mussolini -> people.person.gender -> Male\n# Answer:\nMale", "# Reasoning Path:\nBenito Mussolini -> people.person.nationality -> Italy -> base.locations.countries.continent -> Europe\n# Answer:\nItaly", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> National Fascist Party -> government.political_party.ideology -> Fascism\n# Answer:\nNational Fascist Party", "# Reasoning Path:\nBenito Mussolini -> organization.organization_founder.organizations_founded -> Cinecitt\u00e0 -> base.schemastaging.organization_extra.phone_number -> m.010h48jv\n# Answer:\nCinecitt\u00e0"], "ground_truth": ["Italy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> language.language_writing_system.parent_writing_systems -> Chinese characters\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> base.schemastaging.context_name.pronunciation -> g.125_plpnh\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> language.language_writing_system.languages -> Yue Chinese\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> language.language_writing_system.languages -> Uyghur Language\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> book.book_subject.works -> Lacan's Smile -> book.written_work.subjects -> Jacques Lacan\n# Answer:\nLacan's Smile", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters -> base.schemastaging.context_name.pronunciation -> g.125_q7_ld\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script"], "ground_truth": ["N\u00fcshu script", "Simplified Chinese character", "'Phags-pa script", "Traditional Chinese characters", "Chinese characters"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6666666666666666, "ans_recall": 0.6, "path_f1": 0.5, "path_precision": 0.6666666666666666, "path_recall": 0.4, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.6}
{"id": "WebQTest-891", "prediction": ["# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Li Min\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> common.topic.notable_for -> g.125gbg_q0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Mao Anying\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> influence.influence_node.influenced_by -> Thomas Robert Malthus\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Li Na\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.quotations -> A revolution is not a dinner party, or writing an essay, or painting a picture, or doing embroidery; it cannot be so refined, so leisurely and gentle, so temperate, kind, courteous, restrained and magnanimous. A revolution is an insurrection, an act of violence by which one class overthrows another.\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Zhou Enlai -> government.politician.party -> m.0lr13p4\n# Answer:\nZhou Enlai", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Mao Zedong -> people.person.children -> Mao Anhong\n# Answer:\nMao Zedong", "# Reasoning Path:\nCommunist Party of China -> organization.organization.founders -> Li Dazhao -> people.person.education -> m.04hd15_\n# Answer:\nLi Dazhao"], "ground_truth": ["Li Dazhao", "Mao Zedong", "Chen Duxiu", "Zhou Enlai"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.8135593220338982, "ans_precission": 0.8888888888888888, "ans_recall": 0.75, "path_f1": 0.8135593220338982, "path_precision": 0.8888888888888888, "path_recall": 0.75, "path_ans_f1": 0.8135593220338982, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.75}
{"id": "WebQTest-892", "prediction": ["# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Pneumonia -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nPneumonia", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Suffocated\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> base.schemastaging.context_name.pronunciation -> g.125_ph2ct\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Stroke -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nStroke", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> people.cause_of_death.includes_causes_of_death -> Choking\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.notable_types -> Symptom\n# Answer:\nRespiratory failure", "# Reasoning Path:\nMiles Davis -> people.deceased_person.cause_of_death -> Respiratory failure -> common.topic.article -> m.019dmk\n# Answer:\nRespiratory failure"], "ground_truth": ["Stroke", "Pneumonia", "Respiratory failure"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-893", "prediction": ["# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_for -> g.1258t5sn5\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_for -> g.125ccr2vm\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> location.location.containedby -> Parker County\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> location.postal_code.country -> United States of America\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Texas\n# Answer:\nUnited States of America", "# Reasoning Path:\nAzle -> location.statistical_region.population -> g.11b66gyw0f\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAzle -> common.topic.article -> m.013n5h\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> common.topic.notable_types -> Postal Code\n# Answer:\n76020", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76098 -> common.topic.notable_types -> Postal Code\n# Answer:\n76098", "# Reasoning Path:\nAzle -> location.citytown.postal_codes -> 76020 -> location.location.geolocation -> m.03dxw4c\n# Answer:\n76020"], "ground_truth": ["76098", "76020"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-894", "prediction": ["# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> The London 2012 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.event -> Athletics at the 2008 Summer Olympics - Women's 100 metres hurdles\n# Answer:\nAthletics at the 2008 Summer Olympics - Women's 100 metres hurdles", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> olympics.olympic_participating_country.olympics_participated_in -> 2008 Summer Olympics\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.04hdw0b -> olympics.olympic_medal_honor.olympics -> 2008 Summer Olympics\n# Answer:\n2008 Summer Olympics", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> location.country.administrative_divisions -> New South Wales\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.administrative_children -> New South Wales\n# Answer:\nAustralia", "# Reasoning Path:\nSally Pearson -> olympics.olympic_athlete.medals_won -> m.0kl1f80 -> olympics.olympic_medal_honor.olympics -> The London 2012 Summer Olympics\n# Answer:\nThe London 2012 Summer Olympics"], "ground_truth": ["The London 2012 Summer Olympics"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-895", "prediction": ["# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> common.topic.notable_for -> g.1254zz_2p\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 2004 NBA Finals -> sports.sports_championship_event.season -> 2003\u201304 NBA season\n# Answer:\n2004 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> award.award_nominee.award_nominations -> m.0z6d2v_ -> award.award_nomination.ceremony -> 2004 ESPY Awards\n# Answer:\n2004 ESPY Awards", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.championships -> 1989 NBA Finals -> common.topic.article -> m.08x9ts\n# Answer:\n1989 NBA Finals", "# Reasoning Path:\nDetroit Pistons -> sports.sports_team.roster -> m.010fy68k -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam"], "ground_truth": ["2004 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-896", "prediction": ["# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.article -> m.0hn2_\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Boston Bruins\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> common.topic.notable_for -> g.1256f_q8t\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> sports.sports_championship_event.championship -> Stanley Cup\n# Answer:\n1975 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1974 Stanley Cup Finals -> sports.sports_championship_event.season -> 1973\u201374 NHL season\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.article -> m.03by14z\n# Answer:\n1975 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> common.topic.notable_for -> g.125dc4pr3\n# Answer:\n1975 Stanley Cup Finals", "# Reasoning Path:\nPhiladelphia Flyers -> sports.sports_team.championships -> 1975 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Buffalo Sabres\n# Answer:\n1975 Stanley Cup Finals"], "ground_truth": ["1975 Stanley Cup Finals", "1974 Stanley Cup Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-898", "prediction": ["# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> North Carolina -> location.location.containedby -> United States of America\n# Answer:\nNorth Carolina", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> Eastern North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.hud_county_place.county -> Robeson County -> location.location.containedby -> North Carolina\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> base.wikipedia_infobox.settlement.area_code -> Area code 910 -> location.location.containedby -> North Carolina\n# Answer:\nArea code 910", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> location.statistical_region.population -> g.11b66dwnq2\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.location.containedby -> Robeson County -> common.topic.notable_types -> US County\n# Answer:\nRobeson County", "# Reasoning Path:\nSt. Pauls -> location.statistical_region.population -> g.11b674hfln\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSt. Pauls -> location.statistical_region.population -> g.11bymmdb0d\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Robeson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-899", "prediction": ["# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.location.containedby -> Washington\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> base.usnationalparks.us_national_park.state -> Washington -> location.location.containedby -> West Coast of the United States\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> common.topic.notable_for -> g.1257hvh8r\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> United States, with Territories\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> travel.tourist_attraction.near_travel_destination -> Washington -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Jefferson County -> location.hud_foreclosure_area.estimated_number_of_mortgages -> m.07ftnh2\n# Answer:\nJefferson County", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.location.containedby -> United States of America\n# Answer:\nWashington", "# Reasoning Path:\nOlympic National Park -> location.location.containedby -> Washington -> location.us_state.capital -> Olympia\n# Answer:\nWashington"], "ground_truth": ["Jefferson County", "Washington"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9411764705882353, "ans_precission": 0.8888888888888888, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.9411764705882353, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 1.0}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.children -> Christopher Nixon Cox\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.spouse_s -> m.0j4kt_s\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.spouse_s -> m.0j4k1q4\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> film.person_or_entity_appearing_in_film.films -> m.0vpghfz\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> business.board_member.organization_board_memberships -> m.05nm0rl\n# Answer:\nTricia Nixon Cox"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.125, "path_recall": 0.14285714285714285, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_tlrg2 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.ceremony -> 37th Primetime Emmy Awards\n# Answer:\n37th Primetime Emmy Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Supporting Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Supporting Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff -> award.award_honor.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdt -> tv.regular_tv_appearance.actor -> Paul Benedict\n# Answer:\nPaul Benedict"], "ground_truth": ["Sherman Hemsley", "Franklin Cover", "Paul Benedict", "Damon Evans", "Isabel Sanford", "Jay Hammer", "Marla Gibbs", "Mike Evans", "Berlinda Tolbert", "Zara Cully", "Roxie Roker"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.28169014084507044, "ans_precission": 0.625, "ans_recall": 0.18181818181818182, "path_f1": 0.14925373134328357, "path_precision": 0.625, "path_recall": 0.0847457627118644, "path_ans_f1": 0.28169014084507044, "path_ans_precision": 0.625, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-900", "prediction": ["# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> United States of America\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.us_county.hud_county_place -> Southampton County / County Place 5117599999\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> symbols.namesake.named_after -> Nat Turner -> people.person.place_of_birth -> Southampton County\n# Answer:\nNat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.containedby -> Virginia\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> base.aareas.schema.administrative_area.administrative_parent -> Virginia\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> book.book_subject.works -> The Confessions of Nat Turner -> book.written_work.subjects -> Nat Turner\n# Answer:\nThe Confessions of Nat Turner", "# Reasoning Path:\nNat Turner's slave rebellion -> time.event.locations -> Southampton County -> location.location.contains -> 23827\n# Answer:\nSouthampton County", "# Reasoning Path:\nNat Turner's slave rebellion -> common.topic.notable_types -> Event -> freebase.type_profile.published -> Published\n# Answer:\nEvent"], "ground_truth": ["Southampton County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-901", "prediction": ["# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> tv.tv_character.appeared_in_tv_program -> m.0gy7h8w -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k6jzc -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r1v -> film.performance.actor -> Mark Hamill\n# Answer:\nMark Hamill", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.010fs6tg -> film.performance.film -> Star Wars: The Force Awakens\n# Answer:\nStar Wars: The Force Awakens", "# Reasoning Path:\nLuke Skywalker -> film.film_character.portrayed_in_films -> m.0k3r3j -> film.performance.film -> Return of the Jedi\n# Answer:\nReturn of the Jedi"], "ground_truth": ["Mark Hamill"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.625, "path_recall": 0.7142857142857143, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-903", "prediction": ["# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 1936 Summer Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> United States of America -> sports.sport_country.multi_event_tournaments_participated_in -> 2012 World Mountain Running Championships\n# Answer:\nUnited States of America", "# Reasoning Path:\nTurkish people -> people.ethnicity.population -> m.0k2q2mk\n# Answer:\npeople.ethnicity.population", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Algeria -> location.country.languages_spoken -> Arabic Language\n# Answer:\nAlgeria", "# Reasoning Path:\nTurkish people -> people.ethnicity.geographic_distribution -> Azerbaijan -> location.location.containedby -> Eurasia\n# Answer:\nAzerbaijan", "# Reasoning Path:\nTurkish people -> people.ethnicity.includes_groups -> Iraqi Turkmen -> people.ethnicity.geographic_distribution -> Kirkuk\n# Answer:\nIraqi Turkmen"], "ground_truth": ["France", "Syria", "Republic of Kosovo", "Northern Cyprus", "Denmark", "Canada", "Bulgaria", "United Kingdom", "Netherlands", "United States of America", "Sweden", "Bosnia and Herzegovina", "Azerbaijan", "Algeria", "Iraq", "Austria", "Belgium", "Kyrgyzstan", "Germany", "Turkey", "Norway", "Kazakhstan"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.3018867924528302, "ans_precission": 0.8888888888888888, "ans_recall": 0.18181818181818182, "path_f1": 0.2320441988950276, "path_precision": 0.7777777777777778, "path_recall": 0.13636363636363635, "path_ans_f1": 0.3018867924528302, "path_ans_precision": 0.8888888888888888, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-904", "prediction": ["# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.state -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> fictional_universe.fictional_setting.fictional_characters_born_here -> Barack Obama in comics\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Hawaii\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.place_of_birth -> Honolulu -> location.location.containedby -> Hawaii\n# Answer:\nHonolulu", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> base.locations.countries.states_provinces_within -> Illinois\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarack Obama -> people.person.nationality -> United States of America -> location.country.first_level_divisions -> Hawaii\n# Answer:\nUnited States of America"], "ground_truth": ["Hawaii"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-905", "prediction": ["# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kg9yj -> sports.sports_award.season -> 1990 NFL season\n# Answer:\n1990 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv46 -> sports.sports_award.season -> 1984 NFL season\n# Answer:\n1984 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv8k -> sports.sports_award.season -> 1981 NFL season\n# Answer:\n1981 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcvcr -> sports.sports_award.season -> 1989 NFL season\n# Answer:\n1989 NFL season", "# Reasoning Path:\nJoe Montana -> american_football.football_player.forty_yard_dash_time -> m.0whs0c1\n# Answer:\namerican_football.football_player.forty_yard_dash_time", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kg9yd -> sports.sports_award.season -> 1989 NFL season\n# Answer:\n1989 NFL season", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kg9yj -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nJoe Montana -> sports.sports_award_winner.awards -> m.04kcv46 -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nJoe Montana -> film.actor.film -> m.0h0_0vg -> film.performance.film -> ESPN Honor Roll: The Best of College Football: Vol. 1\n# Answer:\nESPN Honor Roll: The Best of College Football: Vol. 1"], "ground_truth": ["1982-01-24", "1985-01-20", "1989-01-22", "1990-01-28"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-908", "prediction": ["# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nDorothy Canfield Fisher Children's Book Award", "# Reasoning Path:\nGary Paulsen -> award.award_winner.awards_won -> m.09kkr7w -> award.award_honor.award -> Anne V. Zarrow Award for Young Readers' Literature\n# Answer:\nAnne V. Zarrow Award for Young Readers' Literature", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.award -> Dorothy Canfield Fisher Children's Book Award\n# Answer:\nDorothy Canfield Fisher Children's Book Award", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvpyg -> award.award_nomination.award -> John Newbery Medal\n# Answer:\nJohn Newbery Medal", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kbw63 -> award.award_nomination.nominated_for -> How Angel Peterson Got His Name\n# Answer:\nHow Angel Peterson Got His Name", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcvvyw -> award.award_nomination.award -> John Newbery Medal\n# Answer:\nJohn Newbery Medal", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.0zcw56r -> award.award_nomination.award -> John Newbery Medal\n# Answer:\nJohn Newbery Medal", "# Reasoning Path:\nGary Paulsen -> award.award_nominee.award_nominations -> m.09kby6s -> award.award_nomination.nominated_for -> Guts: The True Stories Behind Hatchet and the Brian Books\n# Answer:\nGuts: The True Stories Behind Hatchet and the Brian Books"], "ground_truth": ["Spur Award for Best Juvenile Fiction", "Regina Medal", "Dorothy Canfield Fisher Children's Book Award", "Margaret Edwards Award", "John Newbery Medal", "Spur Award for Best Juvenile Nonfiction", "Anne V. Zarrow Award for Young Readers' Literature"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.75, "ans_recall": 0.42857142857142855, "path_f1": 0.5454545454545454, "path_precision": 0.75, "path_recall": 0.42857142857142855, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.75, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.newspaper.issues -> San Francisco Bay Guardian, 24 Nov 1999\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.periodical.format -> m.02npbt7\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.frequency_or_issues_per_year -> m.09s58hh\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek -> common.topic.notable_types -> Newspaper\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet -> book.newspaper.owner -> Coalition on Homelessness\n# Answer:\nStreet Sheet", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Synapse -> common.topic.notable_types -> Newspaper\n# Answer:\nSynapse"], "ground_truth": ["The San Francisco Examiner", "San Francisco Chronicle", "Street Sheet", "San Francisco Business Times", "San Francisco Call", "San Francisco Bay Guardian", "San Francisco Daily", "San Francisco Bay View", "Synapse", "San Francisco Bay Times", "California Star", "San Francisco Foghorn", "The Golden Era", "Bay Area Reporter", "San Francisco News-Call Bulletin Newspaper", "The Daily Alta California", "AsianWeek", "Dock of the Bay", "Sing Tao Daily", "Free Society"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 1.0, "ans_recall": 0.2, "path_f1": 0.33333333333333337, "path_precision": 1.0, "path_recall": 0.2, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 1.0, "path_ans_recall": 0.2}
{"id": "WebQTest-910", "prediction": ["# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> location.location.containedby -> Nova Scotia\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.article -> m.08npxf\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.notable_for -> g.1258ng70q\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> location.location.geolocation -> m.02_t0ql\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> people.deceased_person.place_of_death -> Beinn Bhreagh, Nova Scotia -> common.topic.notable_types -> Location\n# Answer:\nBeinn Bhreagh, Nova Scotia", "# Reasoning Path:\nAlexander Graham Bell -> law.inventor.us_patents -> Telephone -> law.us_patent.us_classification_category -> 178/48\n# Answer:\nTelephone", "# Reasoning Path:\nAlexander Graham Bell -> common.topic.notable_types -> Book -> freebase.type_profile.published -> Published\n# Answer:\nBook", "# Reasoning Path:\nAlexander Graham Bell -> book.author.works_written -> Memoir upon the formation of a deaf variety of the human race -> common.topic.notable_for -> g.1259rkc_s\n# Answer:\nMemoir upon the formation of a deaf variety of the human race"], "ground_truth": ["Beinn Bhreagh, Nova Scotia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0.7692307692307693, "path_precision": 0.625, "path_recall": 1.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-911", "prediction": ["# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.season -> 1974\u201375 NBA season\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> sports.sports_championship_event.runner_up -> Washington Wizards\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.championships -> 1975 NBA Finals -> common.topic.notable_for -> g.125cx_hnb\n# Answer:\n1975 NBA Finals", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghq5rf -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 1\n# Answer:\n1975 NBA Finals Game 1", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.coaches -> m.0112l01j -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nGolden State Warriors -> sports.sports_team.coaches -> m.0112l01j -> sports.sports_team_coach_tenure.coach -> Jarron Collins\n# Answer:\nJarron Collins", "# Reasoning Path:\nGolden State Warriors -> tv.tv_actor.guest_roles -> m.0ghllvw -> tv.tv_guest_role.episodes_appeared_in -> 1975 NBA Finals Game 3\n# Answer:\n1975 NBA Finals Game 3"], "ground_truth": ["1975 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-912", "prediction": ["# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Revaluation of the Turkish Lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nRevaluation of the Turkish Lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_formerly_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> common.topic.notable_types -> Country -> freebase.type_hints.included_types -> Location\n# Answer:\nCountry", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_formerly_used -> Ottoman Empire\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.country.currency_used -> Turkish lira -> finance.currency.countries_used -> Northern Cyprus\n# Answer:\nTurkish lira", "# Reasoning Path:\nTurkey -> location.statistical_region.minimum_wage -> g.11b60lkkk3\n# Answer:\nlocation.statistical_region.minimum_wage"], "ground_truth": ["Turkish lira"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-914", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> base.aareas.schema.administrative_area.administrative_children -> County Armagh\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.location.containedby -> United Kingdom, with Dependencies and Territories\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> location.country.internet_tld -> eu\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland -> organization.organization_scope.organizations_with_this_scope -> Line Digital Ltd\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography"], "ground_truth": ["England", "Wales", "Scotland", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-915", "prediction": ["# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.article -> m.0642vqz\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XLVII -> sports.sports_championship_event.season -> 2012 NFL season\n# Answer:\nSuper Bowl XLVII", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> sports.sports_championship_event.season -> 2000 NFL season\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> sports.sports_team.championships -> Super Bowl XXXV -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXXV", "# Reasoning Path:\nBaltimore Ravens -> film.film_subject.films -> The Band That Wouldn't Die -> film.film.music -> Marcello Zavros\n# Answer:\nThe Band That Wouldn't Die"], "ground_truth": ["2001 AFC Championship Game", "Super Bowl XXXV", "2013 AFC Championship Game", "Super Bowl XLVII"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6363636363636364, "ans_precission": 0.875, "ans_recall": 0.5, "path_f1": 0.6363636363636364, "path_precision": 0.875, "path_recall": 0.5, "path_ans_f1": 0.6363636363636364, "path_ans_precision": 0.875, "path_ans_recall": 0.5}
{"id": "WebQTest-916", "prediction": ["# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Altaic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> common.topic.notable_types -> Human Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> base.rosetta.languoid.languoid_class -> Language\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> language.human_language.countries_spoken_in -> Sweden\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Yiddish Language -> common.topic.notable_types -> Human Language\n# Answer:\nYiddish Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> language.human_language.countries_spoken_in -> Ukraine\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.country.official_language -> Russian Language -> common.topic.notable_types -> Human Language\n# Answer:\nRussian Language", "# Reasoning Path:\nRussia -> location.statistical_region.gdp_real -> g.11b60vv5th\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nRussia -> location.country.languages_spoken -> Kumyk Language -> language.human_language.language_family -> Turkic languages\n# Answer:\nKumyk Language", "# Reasoning Path:\nRussia -> location.statistical_region.co2_emissions_per_capita -> g.1245_8036\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Tsakhur Language", "Ukrainian Language", "Adyghe Language", "Karachay-Balkar Language", "Khakas Language", "Abaza Language", "Tabassaran Language", "Ingush Language", "Dargwa Language", "Yakut Language", "Buryat language", "Udmurt Language", "Mari language", "Altai language", "Russian Language", "Lezgi Language", "Kabardian Language", "Lak Language", "Tatar Language", "Kalmyk-Oirat Language", "Crimean Turkish Language", "Rutul language", "Chechen Language", "Nogai Language", "Komi language", "Azerbaijani language", "Erzya Language", "Avar Language", "Moksha Language", "Yiddish Language", "Bashkir Language", "Aghul language", "Kumyk Language", "Tuvin Language", "Osetin Language"], "ans_acc": 0.08571428571428572, "ans_hit": 1, "ans_f1": 0.15483870967741936, "ans_precission": 0.8, "ans_recall": 0.08571428571428572, "path_f1": 0.15483870967741936, "path_precision": 0.8, "path_recall": 0.08571428571428572, "path_ans_f1": 0.15483870967741936, "path_ans_precision": 0.8, "path_ans_recall": 0.08571428571428572}
{"id": "WebQTest-918", "prediction": ["# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.office_holder -> Hun Sen\n# Answer:\nHun Sen", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.person.gender -> Male\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> people.deceased_person.place_of_death -> Phnom Penh\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> location.statistical_region.diesel_price_liter -> g.1hhc37pgg\n# Answer:\nlocation.statistical_region.diesel_price_liter", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom of Cambodia -> common.topic.notable_types -> Monarch\n# Answer:\nNorodom of Cambodia", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Norodom Sihanouk -> people.person.children -> Norodom Sihamoni\n# Answer:\nNorodom Sihanouk", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj4c -> government.government_position_held.basic_title -> Second Prime Minister\n# Answer:\nSecond Prime Minister", "# Reasoning Path:\nCambodia -> royalty.kingdom.rulers -> Ang Duong -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nAng Duong", "# Reasoning Path:\nCambodia -> government.governmental_jurisdiction.governing_officials -> m.010ggj20 -> government.government_position_held.office_holder -> Norodom Ranariddh\n# Answer:\nNorodom Ranariddh"], "ground_truth": ["Norodom Sihamoni"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-919", "prediction": ["# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> people.person.parents -> Julia Mae Williams\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> people.person.children -> Venus Williams\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.education -> m.0n1kw8m\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> common.topic.notable_types -> Person or entity appearing in film\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> people.person.children -> Venus Williams\n# Answer:\nOracene Price", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> common.topic.notable_for -> g.1255sg89r\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Richard Williams -> base.schemastaging.sports_coach.athletes_coached -> m.0h_k9kf\n# Answer:\nRichard Williams", "# Reasoning Path:\nSerena Williams -> people.person.parents -> Oracene Price -> common.topic.notable_for -> g.1256r8wrk\n# Answer:\nOracene Price"], "ground_truth": ["Richard Williams", "Oracene Price"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-920", "prediction": ["# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> education.education.institution -> Trinity College, Cambridge\n# Answer:\nTrinity College, Cambridge", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> education.education.institution -> The King's School, Grantham\n# Answer:\nThe King's School, Grantham", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> education.education.degree -> Master of Arts\n# Answer:\nMaster of Arts", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncxj8d -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.02kq1mm -> freebase.valuenotation.has_value -> Major/Field Of Study\n# Answer:\nMajor/Field Of Study", "# Reasoning Path:\nIsaac Newton -> people.person.education -> m.0ncl4g6 -> freebase.valuenotation.has_no_value -> Degree\n# Answer:\nDegree", "# Reasoning Path:\nIsaac Newton -> symbols.name_source.namesakes -> Newton -> astronomy.extraterrestrial_location.geolocation -> m.0w2sw5y\n# Answer:\nNewton"], "ground_truth": ["The King's School, Grantham", "Trinity College, Cambridge"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-921", "prediction": ["# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.04r0btg\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Alan Joyce\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf8n\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Tommy Chong\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> common.topic.webpage -> m.09wv45d\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nProstate cancer", "# Reasoning Path:\nHuell Howser -> people.person.profession -> Voice Actor -> fictional_universe.character_occupation.characters_with_this_occupation -> Jenny Larson\n# Answer:\nVoice Actor", "# Reasoning Path:\nHuell Howser -> people.deceased_person.cause_of_death -> Prostate cancer -> medicine.disease.notable_people_with_this_condition -> Andrew Grove\n# Answer:\nProstate cancer"], "ground_truth": ["Prostate cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-922", "prediction": ["# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.production_companies -> Color Force\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor the Overlander (Underland Chronicles (Turtleback)) -> book.book_edition.book -> Gregor the Overlander\n# Answer:\nGregor the Overlander (Underland Chronicles (Turtleback))", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.written_by -> Michael Arndt\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> film.film.production_companies -> Lions Gate Entertainment\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games: Catching Fire -> award.award_nominated_work.award_nominations -> m.0107lb8y\n# Answer:\nThe Hunger Games: Catching Fire", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> award.award_nominated_work.award_nominations -> m.0_r6fh7\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> film.producer.films_executive_produced -> The Hunger Games -> book.book_edition.place_of_publication -> New York City\n# Answer:\nThe Hunger Games", "# Reasoning Path:\nSuzanne Collins -> book.author.book_editions_published -> Gregor and the Prophecy of Bane (Underland Chronicles (Paperback)) -> book.book_edition.book -> Gregor and the Prophecy of Bane\n# Answer:\nGregor and the Prophecy of Bane (Underland Chronicles (Paperback))"], "ground_truth": ["Gregor and the Prophecy of Bane (Underland Chronicles (Paperback))", "Catching Fire", "Gregor the Overlander (Underland Chronicles)", "The Underland Chronicles Book Three", "Gregor and the Prophecy of Bane (Underland Chronicles (Turtleback))", "Gregor and the Curse of the Warmbloods", "Gregor the Overlander", "FIRE PROOF", "Gregor The Overlander (Underland Chronicles)", "Mockingjay", "Gregor and the Code of Claw (Thorndike Press Large Print Literacy Bridge Series)", "Fire Proof", "Gregor And The Code Of Claw", "Gregor the Overlander (Underland Chronicles (Turtleback))", "Gregor and the Prophecy of Bane (Underland Chronicles (Audio))", "Gregor and the Code of Claw", "Gregor and the marks of secret", "Gregor and the Code of Claw (Underland Chronicles, Book 5)", "Gregor and the curse of the warmbloods", "Gregor and the Marks of Secret", "The Hunger Games Trilogy Boxed Set", "Gregor the Overlander (Underland Chronicles (Sagebrush))", "Gregor and the Prophecy of Bane", "The Hunger Games", "When Charlie McButton lost power"], "ans_acc": 0.32, "ans_hit": 1, "ans_f1": 0.48484848484848486, "ans_precission": 1.0, "ans_recall": 0.32, "path_f1": 0.391304347826087, "path_precision": 1.0, "path_recall": 0.24324324324324326, "path_ans_f1": 0.48484848484848486, "path_ans_precision": 1.0, "path_ans_recall": 0.32}
{"id": "WebQTest-923", "prediction": ["# Reasoning Path:\nEngland -> location.location.partially_contains -> River Wye -> location.location.partially_containedby -> Wales\n# Answer:\nRiver Wye", "# Reasoning Path:\nEngland -> location.location.partially_contains -> River Wye -> geography.river.cities -> Chepstow\n# Answer:\nRiver Wye", "# Reasoning Path:\nEngland -> location.location.partially_contains -> River Wye -> geography.river.origin -> Plynlimon\n# Answer:\nRiver Wye", "# Reasoning Path:\nEngland -> location.location.partially_contains -> River Wye -> common.topic.notable_types -> River\n# Answer:\nRiver Wye", "# Reasoning Path:\nEngland -> location.country.first_level_divisions -> Yorkshire and the Humber -> location.location.partially_contains -> Lincolnshire\n# Answer:\nYorkshire and the Humber", "# Reasoning Path:\nEngland -> biology.breed_origin.breeds_originating_here -> Border Collie -> biology.animal_breed.place_of_origin -> Scotland\n# Answer:\nBorder Collie", "# Reasoning Path:\nEngland -> location.location.partially_contains -> River Wye -> location.location.containedby -> Europe\n# Answer:\nRiver Wye", "# Reasoning Path:\nEngland -> location.location.partially_contains -> River Dee, Wales -> location.location.partially_containedby -> Wales\n# Answer:\nRiver Dee, Wales"], "ground_truth": ["Wales", "Scotland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2, "ans_precission": 0.125, "ans_recall": 0.5, "path_f1": 0.06521739130434782, "path_precision": 0.375, "path_recall": 0.03571428571428571, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-924", "prediction": ["# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.point_awarded_to -> Argentina national football team\n# Answer:\nArgentina national football team", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010r25mp -> soccer.football_goal.match -> 2014 Argentina vs Slovenia friendly match\n# Answer:\n2014 Argentina vs Slovenia friendly match", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010gkw62 -> soccer.football_goal.point_awarded_to -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.statistics -> m.0w9hr2y -> soccer.football_player_stats.team -> FC Barcelona B\n# Answer:\nFC Barcelona B", "# Reasoning Path:\nLionel Messi -> base.schemastaging.athlete_extra.salary -> m.0qtkxlg -> base.schemastaging.athlete_salary.team -> FC Barcelona\n# Answer:\nFC Barcelona", "# Reasoning Path:\nLionel Messi -> soccer.football_player.goals_scored -> m.010h62gm -> soccer.football_goal.match -> 2014 Barcelona vs Athletic Bilbao\n# Answer:\n2014 Barcelona vs Athletic Bilbao", "# Reasoning Path:\nLionel Messi -> soccer.football_player.statistics -> m.0w8wsdx -> soccer.football_player_stats.team -> FC Barcelona\n# Answer:\nFC Barcelona"], "ground_truth": ["FC Barcelona", "Argentina national football team"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.37037037037037035, "path_precision": 0.625, "path_recall": 0.2631578947368421, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-925", "prediction": ["# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.notable_for -> g.125bn2wcf\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.hud_county_place.county -> Decatur County -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nDecatur County", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.image -> UnionSquareAtomicClock\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.webpage -> m.03hc_rb\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rxy8r\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.hud_foreclosure_area.hhuniv -> m.07gpl21 -> measurement_unit.dated_integer.source -> United States Department of Housing and Urban Development\n# Answer:\nUnited States Department of Housing and Urban Development", "# Reasoning Path:\nGreensburg -> location.location.time_zones -> Eastern Time Zone -> common.topic.article -> m.02hcvg\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11b66hhtj5\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nGreensburg -> location.statistical_region.population -> g.11bymk01vg\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Eastern Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-927", "prediction": ["# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0jz6fl -> film.performance.actor -> Ralph Fiennes\n# Answer:\nRalph Fiennes", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.actor -> Christian Coulson\n# Answer:\nChristian Coulson", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0jz6fl -> film.performance.film -> Harry Potter and the Goblet of Fire\n# Answer:\nHarry Potter and the Goblet of Fire", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02t9_zv -> film.performance.actor -> Richard Bremmer\n# Answer:\nRichard Bremmer", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.0pc66_w -> film.performance.film -> Harry Potter and the Deathly Hallows \u2013 Part 2\n# Answer:\nHarry Potter and the Deathly Hallows \u2013 Part 2", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.02vcdgy -> film.performance.actor -> Ralph Fiennes\n# Answer:\nRalph Fiennes", "# Reasoning Path:\nLord Voldemort -> film.film_character.portrayed_in_films -> m.075q0v4 -> film.performance.actor -> Ralph Fiennes\n# Answer:\nRalph Fiennes", "# Reasoning Path:\nLord Voldemort -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale"], "ground_truth": ["Ralph Fiennes", "Richard Bremmer", "Frank Dillane", "Christian Coulson", "Hero Fiennes-Tiffin"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6122448979591836, "ans_precission": 0.625, "ans_recall": 0.6, "path_f1": 0.5, "path_precision": 0.625, "path_recall": 0.4166666666666667, "path_ans_f1": 0.6122448979591836, "path_ans_precision": 0.625, "path_ans_recall": 0.6}
{"id": "WebQTest-928", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmyl -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Huntingdon College", "University of Alabama School of Law", "University of Oxford", "University of Alabama"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-929", "prediction": ["# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> common.topic.notable_types -> Organization founder\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> people.person.gender -> Male\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> common.topic.notable_for -> g.1jlfwbn77\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> Richard R. Reed -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nRichard R. Reed", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> organization.organization_type.organizations_of_this_type -> ANAK Society\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.organization_type -> Secret society -> common.topic.notable_for -> g.12556r00x\n# Answer:\nSecret society", "# Reasoning Path:\nKu Klux Klan -> organization.organization.founders -> J. Calvin Jones -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nJ. Calvin Jones"], "ground_truth": ["J. Calvin Jones", "John C. Lester", "Frank O. McCord", "John B. Kennedy", "Richard R. Reed", "James R. Crowe"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.75, "ans_recall": 0.3333333333333333, "path_f1": 0.46153846153846156, "path_precision": 0.75, "path_recall": 0.3333333333333333, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.75, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> common.topic.subject_of -> Carnegie Moscow Center\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> location.location.partially_contains -> Didi Abuli\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.statistical_region.energy_use_per_capita -> g.1245_391m\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60znv7_\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-931", "prediction": ["# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Ambassador\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> Diplomat\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> type.type.expected_by -> Officeholder\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Politician -> type.type.domain -> Government\n# Answer:\nPolitician", "# Reasoning Path:\nJohn Adams -> government.politician.government_positions_held -> m.09ryhrl -> government.government_position_held.office_position_or_title -> Delegate to the Continental Congress\n# Answer:\nDelegate to the Continental Congress", "# Reasoning Path:\nJohn Adams -> common.topic.notable_for -> g.1256kt07b\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> common.topic.subjects -> Sally & Fitch LLP\n# Answer:\nLawyer", "# Reasoning Path:\nJohn Adams -> people.person.profession -> Lawyer -> business.industry.companies -> Legal Action Workshop\n# Answer:\nLawyer"], "ground_truth": ["Vice President of the United States", "United States Ambassador to the United Kingdom", "United States Ambassador to the Netherlands", "Delegate to the Continental Congress"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1111111111111111, "ans_recall": 0.25, "path_f1": 0.125, "path_precision": 0.1111111111111111, "path_recall": 0.14285714285714285, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.25}
{"id": "WebQTest-932", "prediction": ["# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Shigeru Miyamoto\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> cvg.computer_videogame.sequel -> Super Mario Bros. 3 -> cvg.computer_videogame.sequel -> Super Mario World\n# Answer:\nSuper Mario Bros. 3", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Super Mario Bros.\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.created_by -> Nintendo\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.characters -> Mario\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> common.topic.notable_for -> g.1257jmx_f\n# Answer:\nSuper Mario Universe", "# Reasoning Path:\nSuper Mario Bros. 2 -> cvg.computer_videogame.sequel -> Super Mario Bros. 3 -> fictional_universe.work_of_fiction.setting -> Bowser's Castle\n# Answer:\nSuper Mario Bros. 3", "# Reasoning Path:\nSuper Mario Bros. 2 -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Super Mario Universe -> fictional_universe.fictional_universe.works_set_here -> Dance Dance Revolution Mario Mix\n# Answer:\nSuper Mario Universe"], "ground_truth": ["1988-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-933", "prediction": ["# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.03gjg0m -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Illinois Republican Party\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> United States Secret Service\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyh\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.politician.party -> m.0lr1qnz -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> organization.organization_founder.organizations_founded -> Freedmen's Bureau\n# Answer:\nAbraham Lincoln", "# Reasoning Path:\nAndrew Johnson -> government.us_vice_president.to_president -> Abraham Lincoln -> government.politician.party -> m.03gjfyn\n# Answer:\nAbraham Lincoln"], "ground_truth": ["Republican Party", "National Union Party", "Democratic Party"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-935", "prediction": ["# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> book.written_work.subjects -> Al-Qaeda\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Ali Azmat\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.events -> Death of Osama bin Laden\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Amrit Singh Thapar\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.contains -> Abbottabad Hockey Stadium\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.nearby_airports -> Abbottabad Airport\n# Answer:\nAbbottabad", "# Reasoning Path:\nOsama bin Laden -> book.book_subject.works -> Osama: The Making of a Terrorist -> common.topic.notable_types -> Book\n# Answer:\nOsama: The Making of a Terrorist", "# Reasoning Path:\nOsama bin Laden -> people.deceased_person.place_of_death -> Abbottabad -> location.location.people_born_here -> Faiza Khan\n# Answer:\nAbbottabad"], "ground_truth": ["Abbottabad"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-936", "prediction": ["# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> location.location.containedby -> Aegina\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> location.location.containedby -> Ancient Greece\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> common.topic.notable_for -> g.125529h7s\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Aphaea -> architecture.structure.architectural_style -> Classical architecture\n# Answer:\nTemple of Aphaea", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> symbols.namesake.named_after -> Zeus\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Erechtheion -> location.location.containedby -> Athens\n# Answer:\nErechtheion", "# Reasoning Path:\nAncient Greek temple -> common.topic.notable_for -> g.1255n9_v7\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Temple of Zeus, Olympia -> travel.tourist_attraction.near_travel_destination -> Shahhat\n# Answer:\nTemple of Zeus, Olympia", "# Reasoning Path:\nAncient Greek temple -> architecture.building_function.buildings -> Old Temple of Athena -> location.location.containedby -> Athens\n# Answer:\nOld Temple of Athena"], "ground_truth": ["Shahhat", "Ephesus", "Olympia", "Corfu", "Ku\u015fadas\u0131", "Athens"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.24242424242424243, "ans_precission": 0.4444444444444444, "ans_recall": 0.16666666666666666, "path_f1": 0.3, "path_precision": 0.3333333333333333, "path_recall": 0.2727272727272727, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5}
{"id": "WebQTest-937", "prediction": ["# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Near East\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.geolocation -> m.0zwv97z\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.country.capital -> Ray\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Djibouti -> location.country.languages_spoken -> French\n# Answer:\nDjibouti", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> South Africa -> location.location.containedby -> Africa\n# Answer:\nSouth Africa", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> location.location.containedby -> Central Asia\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Seljuk Empire -> common.topic.notable_types -> Country\n# Answer:\nSeljuk Empire", "# Reasoning Path:\nArabic Language -> language.human_language.countries_spoken_in -> Djibouti -> location.location.containedby -> Arab world\n# Answer:\nDjibouti"], "ground_truth": ["Saudi Arabia", "Yemen", "Bahrain", "Mandatory Palestine", "Libya", "Syria", "Mauritania", "Tunisia", "Canada", "Qatar", "Iran", "Eritrea", "Egypt", "Israel", "South Yemen", "Comoros", "South Africa", "Lebanon", "Oman", "Jordan", "Kuwait", "Algeria", "Iraq", "United Arab Emirates", "Tanzania", "Morocco", "Sudan", "Turkey", "Djibouti", "Seljuk Empire"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 1.0, "ans_recall": 0.1, "path_f1": 0.18181818181818182, "path_precision": 1.0, "path_recall": 0.1, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 1.0, "path_ans_recall": 0.1}
{"id": "WebQTest-938", "prediction": ["# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Cecilia Bowes-Lyon, Countess of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> Mary of Teck\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.parents -> Claude Bowes-Lyon, 14th Earl of Strathmore and Kinghorne\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.family_member.family -> House of Windsor\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> George VI -> people.person.parents -> George V\n# Answer:\nGeorge VI", "# Reasoning Path:\nElizabeth II -> people.person.spouse_s -> m.028zms8 -> people.marriage.spouse -> Prince Philip, Duke of Edinburgh\n# Answer:\nPrince Philip, Duke of Edinburgh", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.children -> Princess Margaret, Countess of Snowdon\n# Answer:\nQueen Elizabeth The Queen Mother", "# Reasoning Path:\nElizabeth II -> people.person.parents -> Queen Elizabeth The Queen Mother -> people.person.sibling_s -> m.0b5lmgx\n# Answer:\nQueen Elizabeth The Queen Mother"], "ground_truth": ["Queen Elizabeth The Queen Mother"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7692307692307693, "ans_precission": 0.625, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7692307692307693, "path_ans_precision": 0.625, "path_ans_recall": 1.0}
{"id": "WebQTest-939", "prediction": ["# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Woman with a Guitar\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> common.topic.image -> Braque\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Analytic cubism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nAnalytic cubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Picasso and the Avant-Garde in Paris\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Surrealism -> influence.influence_node.influenced -> J. G. Ballard\n# Answer:\nSurrealism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> visual_art.art_period_movement.associated_artworks -> Chicago Picasso\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Cubism -> common.topic.notable_types -> Art Period/Movement\n# Answer:\nCubism", "# Reasoning Path:\nPablo Picasso -> visual_art.visual_artist.associated_periods_or_movements -> Analytic cubism -> visual_art.art_period_movement.associated_artworks -> Woman with a Guitar\n# Answer:\nAnalytic cubism"], "ground_truth": ["Synthetic cubism", "Picasso's Blue Period", "Surrealism", "Analytic cubism", "Picasso's Rose Period", "Modern art", "Picasso's African Period", "Cubism"], "ans_acc": 0.375, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 1.0, "ans_recall": 0.375, "path_f1": 0.5454545454545454, "path_precision": 1.0, "path_recall": 0.375, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 1.0, "path_ans_recall": 0.375}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Cocaine\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Coronary artery disease\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Myocardial Ischemia\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aortic dissection\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-941", "prediction": ["# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.person.children -> Frances Da Silva\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.person.spouse_s -> m.0ncm17l\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> freebase.valuenotation.has_value -> Parents\n# Answer:\nWilliam Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> Temperance Fitzgerald -> people.person.gender -> Female\n# Answer:\nTemperance Fitzgerald", "# Reasoning Path:\nElla Fitzgerald -> people.person.parents -> William Fitzgerald -> common.topic.notable_types -> Deceased Person\n# Answer:\nWilliam Fitzgerald"], "ground_truth": ["William Fitzgerald", "Temperance Fitzgerald"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-942", "prediction": ["# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.parents -> Joe McCauley\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.spouse_s -> m.0j4k1mh\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> freebase.valuenotation.has_value -> Parents\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> common.topic.notable_for -> g.1258564_c\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> common.topic.notable_types -> Person\n# Answer:\nGerald Hathaway", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Kate McCauley Hathaway -> people.person.children -> Thomas Hathaway\n# Answer:\nKate McCauley Hathaway", "# Reasoning Path:\nAnne Hathaway -> base.popstra.celebrity.canoodled -> m.0652shw -> base.popstra.canoodled.participant -> Francesco Coco\n# Answer:\nFrancesco Coco", "# Reasoning Path:\nAnne Hathaway -> people.person.parents -> Gerald Hathaway -> common.topic.notable_for -> g.125c_3jpc\n# Answer:\nGerald Hathaway"], "ground_truth": ["Gerald Hathaway", "Kate McCauley Hathaway"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8750000000000001, "ans_precission": 0.7777777777777778, "ans_recall": 1.0, "path_f1": 0.8750000000000001, "path_precision": 0.7777777777777778, "path_recall": 1.0, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-943", "prediction": ["# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Roger Sterling\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Pete Campbell\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Betty Draper\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Peggy Olson\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> film.writer.film -> Are You Here\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> common.topic.notable_for -> g.1259lvfmp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> fictional_universe.fictional_character_creator.fictional_characters_created -> Freddy Rumsen\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> common.topic.notable_types -> TV Writer\n# Answer:\nMatthew Weiner", "# Reasoning Path:\nDon Draper -> fictional_universe.fictional_character.character_created_by -> Matthew Weiner -> award.award_nominee.award_nominations -> m.010bdsp2\n# Answer:\nMatthew Weiner"], "ground_truth": ["Betty Draper"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.1818181818181818, "path_precision": 0.1111111111111111, "path_recall": 0.5, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-944", "prediction": ["# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_for -> g.12h2y4yvf\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> common.topic.notable_types -> Musical Album\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> music.album.release_type -> Single\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> Ain't Talkin' 'Bout Love -> music.album.compositions -> Ain't Talkin' 'bout Love\n# Answer:\nAin't Talkin' 'Bout Love", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.compositions -> Oh, Pretty Woman\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> common.topic.webpage -> m.0d65mgx -> common.webpage.in_index -> Blissful Master Index\n# Answer:\nBlissful Master Index", "# Reasoning Path:\nVan Halen -> music.artist.album -> (Oh) Pretty Woman -> music.album.release_type -> Single\n# Answer:\n(Oh) Pretty Woman", "# Reasoning Path:\nVan Halen -> music.artist.album -> 1984 -> common.topic.notable_types -> Musical Album\n# Answer:\n1984"], "ground_truth": ["Ain't Talkin' 'Bout Love"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-945", "prediction": ["# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> United Kingdom\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> United Kingdom\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nConstitutional monarchy", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Monarchy\n# Answer:\nMonarchy", "# Reasoning Path:\nSpain -> location.statistical_region.deposit_interest_rate -> g.1hhc37x4q\n# Answer:\nlocation.statistical_region.deposit_interest_rate", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> Andorra\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Andorra\n# Answer:\nParliamentary system", "# Reasoning Path:\nSpain -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60xs9d2\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Unitary state -> government.form_of_government.countries -> France\n# Answer:\nUnitary state", "# Reasoning Path:\nSpain -> location.country.form_of_government -> Parliamentary system -> government.form_of_government.countries -> Antigua and Barbuda\n# Answer:\nParliamentary system"], "ground_truth": ["Parliamentary system", "Unitary state", "Constitutional monarchy", "Monarchy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-948", "prediction": ["# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> L. A. Scot Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Capital punishment\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Making Our Democracy Work -> book.written_work.subjects -> United States Constitution\n# Answer:\nMaking Our Democracy Work", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.author -> Lucas A. Powe\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.written_work.subjects -> Politics of the United States\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.previous_in_series -> Savannah Talks Troy Anthony Davis No. 11: Judge Moore says \\\"Not innocent\\\"\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal", "# Reasoning Path:\nSupreme Court of the United States -> common.topic.notable_for -> g.125dtprfx\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> The Warren Court and American Politics -> book.book.genre -> Non-fiction\n# Answer:\nThe Warren Court and American Politics", "# Reasoning Path:\nSupreme Court of the United States -> book.book_subject.works -> Savannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal -> book.written_work.subjects -> Troy Anthony Davis\n# Answer:\nSavannah Talks Troy Anthony Davis No. 12: U.S. Supreme Court Rejects Appeal"], "ground_truth": ["United States v. Playboy Entertainment Group", "Maryland v. Craig", "Mullane v. Central Hanover Bank & Trust Co.", "United States v. Harriss", "Maynard v. Cartwright", "American Insurance Co. v. 356 Bales of Cotton", "Oregon v. Mitchell", "Clay v. United States", "Youngstown Sheet & Tube Co. v. Sawyer", "Riley v. California", "Saenz v. Roe", "Celotex Corp. v. Catrett", "Wabash, St. Louis & Pacific Railway Co. v. Illinois", "Cruzan v. Director, Missouri Department of Health", "Texas v. White", "Interstate Commerce Commission v. Cincinnati, New Orleans & Texas Pacific Railway Co.", "Pierce v. Society of Sisters", "Wheeling Steel Corp. v. Glander", "Baze v. Rees", "Cherokee Nation v. Georgia", "Morse v. Frederick", "General Motors streetcar conspiracy", "Dennis v. United States", "Crosby v. National Foreign Trade Council", "Egbert v. Lippmann", "Sherman v. United States", "United States v. United States District Court", "Miller v. California", "Godfrey v. Georgia", "Pryor v. United States", "Ex parte Bollman", "Hurley v. Irish-American Gay, Lesbian, & Bisexual Group of Boston", "DeFunis v. Odegaard", "Hayburn's Case", "Eldred v. Ashcroft", "Kelo v. City of New London", "Day v. McDonough", "Citizens to Preserve Overton Park v. Volpe", "Perez v. Brownell", "Braunfeld v. Brown", "City of Cleburne v. Cleburne Living Center, Inc.", "Allen v. Wright", "Chung Fook v. White", "Engel v. Vitale", "Griswold v. Connecticut", "Hill v. Wallace", "Reno v. American Civil Liberties Union", "Dowling v. United States", "Dames & Moore v. Regan", "Swidler & Berlin v. United States", "Avegno v. Schmidt", "Poe v. Ullman", "Arbaugh v. Y & H Corp.", "Eisenstadt v. Baird", "Cooper v. Aaron", "Largent v. Texas", "Oregon Waste Systems, Inc. v. Department of Environmental Quality of Oregon", "Saint Francis College v. al-Khazraji", "Gibbons v. Ogden", "Erie Railroad Co. v. Tompkins", "Smith v. Maryland", "Bunting v. Oregon", "Oliphant v. Suquamish Indian Tribe", "Solem v. Helm", "Ex parte Quirin", "United States v. International Boxing Club of New York", "United States v. Paramount Pictures, Inc.", "Gratz v. Bollinger", "Correctional Services Corp. v. Malesko", "New York Times Co. v. Tasini", "Roberts v. United States Jaycees", "Westside School District v. Mergens", "Church of the Holy Trinity v. United States", "Schuette v. Coalition to Defend Affirmative Action", "Follett v. Town of McCormick", "Prigg v. Pennsylvania", "Stanley v. Georgia", "Merck KGaA v. Integra Lifesciences I, Ltd.", "Pfaff v. Wells Electronics, Inc.", "McLaughlin v. Florida", "Gonzales v. Raich", "Burford v. Sun Oil Co.", "Tucker v. Texas", "Hills v. Gautreaux", "Thomas v. Review Board of the Indiana Employment Security Division", "United States v. Verdugo-Urquidez", "Crawford v. Marion County Election Board", "Passenger Cases", "Rapanos v. United States", "Slaughter-House Cases", "Boynton v. Virginia", "United States v. Kagama", "Panetti v. Quarterman", "Campbell v. Acuff-Rose Music, Inc.", "Ingraham v. Wright", "Hepburn v. Griswold", "Boy Scouts of America v. Dale", "Mapp v. Ohio", "2003 term per curiam opinions of the Supreme Court of the United States", "Skinner v. Oklahoma", "Rose v. Locke", "Moyer v. Peabody", "Zorach v. Clauson", "West Virginia State Board of Education v. Barnette", "Bob Jones University v. United States", "Stromberg v. California", "Washington v. Davis", "Tennessee v. Lane", "Watchtower Bible & Tract Society of New York, Inc. v. Village of Stratton", "Cort v. Ash", "Baird v. State Bar of Arizona", "Pickering v. Board of Education", "Feres v. United States", "United States v. Ballard", "American Well Works Co. v. Layne & Bowler Co.", "Edwards v. California", "United States v. Robel", "Panama Refining Co. v. Ryan", "McGowan v. Maryland", "Will v. Michigan Department of State Police", "Walton v. Arizona", "California v. Greenwood", "Bates v. City of Little Rock", "Warner-Jenkinson Co. v. Hilton Davis Chemical Co.", "Yick Wo v. Hopkins", "United States v. Gouveia", "Parisi v. Davidson", "Kansas v. Hendricks", "Roe v. Wade", "Taylor v. United States", "Kumho Tire Co. v. Carmichael", "Dent v. West Virginia", "United States v. Wong Kim Ark", "Marbury v. Madison", "Hirabayashi v. United States", "Easley v. Cromartie", "Estelle v. Smith", "Dred Scott v. Sandford", "United States v. Students Challenging Regulatory Agency Procedures", "Hamdan v. Rumsfeld", "Savana Redding v. Safford Unified School District #1", "Spector v. Norwegian Cruise Line Ltd.", "Florida v. Riley", "Church of Lukumi Babalu Aye v. City of Hialeah", "Tenet v. Doe", "Adair v. United States", "Berea College v. Kentucky", "U.S. Term Limits, Inc. v. Thornton", "Penn Central Transportation Co. v. New York City", "Ozawa v. United States", "Red Lion Broadcasting Co. v. FCC", "Flint v. Stone Tracy Co.", "Rice v. Cayetano", "Kidd v. Pearson", "FCC v. Pacifica Foundation", "Stuart v. Laird", "Bates v. State Bar of Arizona", "Old Chief v. United States", "Garcia v. San Antonio Metropolitan Transit Authority", "Scheidler v. National Organization for Women", "Intel Corp. v. Advanced Micro Devices, Inc.", "Rooker v. Fidelity Trust Co.", "Rogers v. Tennessee", "United States v. Williams", "United States v. O'Brien", "Gonzales v. Carhart", "United Steelworkers v. Weber", "Adkins v. Children's Hospital", "Reynolds v. United States", "Exxon Mobil Corp. v. Saudi Basic Industries Corp.", "California Democratic Party v. Jones", "Universal Camera Corp. v. NLRB", "Schenck v. United States", "United States v. Oakland Cannabis Buyers' Cooperative", "Frontiero v. Richardson", "Osborn v. Bank of the United States", "Sibbach v. Wilson & Co.", "Stanford v. Texas", "Lucas v. South Carolina Coastal Council", "Ohio v. Roberts", "Tennessee Valley Authority v. Hill", "Martin v. Wilks", "Katz v. United States", "Humphrey's Executor v. United States", "Fitzpatrick v. Bitzer", "Schenck v. Pro-Choice Network of Western New York", "Vernonia School District 47J v. Acton", "Schriro v. Summerlin", "Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.", "Dastar Corp. v. Twentieth Century Fox Film Corp.", "Garcetti v. Ceballos", "Penry v. Lynaugh", "Debs v. United States", "Haynes v. United States", "Blanton v. City of North Las Vegas", "Burger King Corp. v. Rudzewicz", "Minnesota v. Mille Lacs Band of Chippewa Indians", "Farrington v. Tokushige", "Frank Lyon Co. v. United States", "Jones v. United States", "Duke Power Co. v. Carolina Environmental Study Group", "United States v. Seeger", "Shapiro v. Thompson", "Employment Division v. Smith", "Pace v. Alabama", "Bivens v. Six Unknown Named Agents", "Swann v. Charlotte-Mecklenburg Board of Education", "Immigration and Naturalization Service v. Cardoza-Fonseca", "Planned Parenthood v. Casey", "City of Indianapolis v. Edmond", "NLRB v. J. Weingarten, Inc.", "Tennessee v. Garner", "United States v. Booker", "FEC v. Akins", "Alexander v. Sandoval", "Clinton v. City of New York", "New York Times Co. v. United States", "Michigan v. Long", "Taylor v. Mississippi", "Totten v. United States", "Brandenburg v. Ohio", "Harris v. Balk", "Epperson v. Arkansas", "Ray v. Blair", "Lochner v. New York", "Dartmouth College v. Woodward", "Sparf v. United States", "Community for Creative Non-Violence v. Reid", "Gonzales v. Oregon", "Nixon v. Shrink Missouri Government PAC", "Troxel v. Granville", "Prince v. Massachusetts", "Rankin v. McPherson", "Rochin v. California", "Hinderlider v. La Plata River & Cherry Creek Ditch Co.", "Turner v. Safley", "Gertz v. Robert Welch, Inc.", "Florida Bar v. Went For It, Inc.", "Cox v. New Hampshire", "Irwin v. Gavit", "United States v. Trans-Missouri Freight Ass'n", "Gitlow v. New York", "City of Richmond v. J.A. Croson Co.", "Daubert v. Merrell Dow Pharmaceuticals, Inc.", "O'Connor v. Donaldson", "Fletcher v. Peck", "Knowles v. Iowa", "Crawford v. Washington", "Northern Insurance Co. of New York v. Chatham County", "Jacobellis v. Ohio", "Gideon v. Wainwright", "National Cable & Telecommunications Ass'n v. Brand X Internet Services", "United States v. Bhagat Singh Thind", "Utah v. Evans", "Chambers v. Florida", "Milkovich v. Lorain Journal Co.", "Gregory v. Helvering", "Toolson v. New York Yankees", "Katzenbach v. Morgan", "Lehnert v. Ferris Faculty Ass'n", "International Shoe Co. v. Washington", "Taft v. Bowers", "Minersville School District v. Gobitis", "Seminole Tribe of Florida v. Florida", "Fuentes v. Shevin", "Florida Lime & Avocado Growers, Inc. v. Paul", "Schweiker v. Chilicky", "City of Boerne v. Flores", "Peretz v. United States", "Webster v. Doe", "Oyama v. California", "Edwards v. South Carolina", "Street v. New York", "Terminiello v. City of Chicago", "Skinner v. Railway Labor Executives Ass'n", "Brady v. Maryland", "Cottage Savings Ass'n v. Commissioner", "TSC Industries, Inc. v. Northway, Inc.", "Board of Education of Kiryas Joel Village School District v. Grumet", "Kyllo v. United States", "Board of Education v. Earls", "Massachusetts v. Environmental Protection Agency", "Forsyth County v. Nationalist Movement", "Cooper Industries, Inc. v. Leatherman Tool Group, Inc.", "Rumsfeld v. Padilla", "Virginia v. Black", "Motor Vehicles Manufacturers Ass'n v. State Farm Mutual Automobile Insurance Co.", "Buckley v. Valeo", "Duncan v. Louisiana", "United States v. Fordice", "Coleman v. Miller", "Brown v. Mississippi", "Palazzolo v. Rhode Island", "Breard v. Greene", "New England Mutual Life Insurance Co. v. Woodworth", "Morissette v. United States", "Hudson v. McMillian", "Bouie v. City of Columbia", "United States v. Harris", "KSR International Co. v. Teleflex Inc.", "Bowsher v. Synar", "Immigration and Naturalization Service v. Doherty", "Qualitex Co. v. Jacobson Products Co.", "Gilbert v. California", "Feist Publications, Inc., v. Rural Telephone Service Co.", "James v. United States", "McCreary County v. American Civil Liberties Union", "Beacon Theatres, Inc. v. Westover", "Munn v. Illinois", "Albertson v. Subversive Activities Control Board", "Gomillion v. Lightfoot", "National Federation of Independent Business v. Sebelius", "Carey v. Musladin", "Almendarez-Torres v. United States", "Bailey v. United States", "Mississippi v. Johnson", "Hudson v. Michigan", "Calder v. Jones", "Board of Airport Commissioners of Los Angeles v. Jews for Jesus, Inc.", "Hodgson v. Minnesota", "United States v. Miller", "Balzac v. Porto Rico", "United States v. Flores-Montano", "Parratt v. Taylor", "Doe v. Chao", "Moore v. Dempsey", "Federal Power Commission v. Tuscarora Indian Nation", "Landmark Communications, Inc. v. Virginia", "Coppage v. Kansas", "Village of Belle Terre v. Boraas", "United States v. Eichman", "Virginia State Pharmacy Board v. Virginia Citizens Consumer Council", "South Dakota v. Dole", "Department of Transportation v. Public Citizen", "Hamdi v. Rumsfeld", "Myers v. United States", "Leary v. United States", "Speiser v. Randall", "The Paquete Habana", "Tahoe-Sierra Preservation Council, Inc. v. Tahoe Regional Planning Agency", "Simmons v. United States", "Florida v. Bostick", "New York v. United States", "Stenberg v. Carhart", "Burlington Northern & Santa Fe Railway Co. v. White", "J.E.B. v. Alabama ex rel. T.B.", "United States v. Dominguez Benitez", "Whitney v. California", "Mathews v. Eldridge", "Fogerty v. Fantasy, Inc.", "Brigham City v. Stuart", "Olmstead v. United States", "Steward Machine Co. v. Davis", "Diamond v. Diehr", "Bolling v. Sharpe", "Merrell Dow Pharmaceuticals Inc. v. Thompson", "Addington v. Texas", "League of United Latin American Citizens v. Perry", "Town of Castle Rock v. Gonzales", "Chaplinsky v. New Hampshire", "Northwestern National Life Insurance Co. v. Riggs", "Washington v. Glucksberg", "Hanna v. Plumer", "Chisholm v. Georgia", "Tee-Hit-Ton Indians v. United States", "Sorrells v. United States", "Henderson v. United States", "Ring v. Arizona", "Ex parte McCardle", "Gibson v. United States", "Osborne v. Ohio", "Trop v. Dulles", "Huddleston v. United States", "Witherspoon v. Illinois", "Thompson v. Oklahoma", "Permanent Mission of India v. City of New York", "Estelle v. Gamble", "Radovich v. National Football League", "Commissioner v. Glenshaw Glass Co.", "PGA Tour, Inc. v. Martin", "Leocal v. Ashcroft", "Immigration and Naturalization Service v. Aguirre-Aguirre", "Planned Parenthood of Central Missouri v. Danforth", "Duro v. Reina", "Muller v. Oregon", "United States v. Leon", "Payton v. New York", "New York Times Co. v. Sullivan", "Barker v. Wingo", "Virginia v. Moore", "United States v. Grubbs", "Ashcroft v. Free Speech Coalition", "Maine v. Taylor", "Ake v. Oklahoma", "Wilkinson v. Austin", "Elk Grove Unified School District v. Newdow", "Loretto v. Teleprompter Manhattan CATV Corp.", "Wyoming v. Colorado", "Morrison v. Olson", "Rumsfeld v. Forum for Academic & Institutional Rights, Inc.", "United States v. Virginia", "Shaffer v. Heitner", "Atkins v. Virginia", "Lovell v. City of Griffin", "Ferguson v. City of Charleston", "Schlesinger v. Councilman", "Indiana v. Edwards", "Continental Paper Bag Co. v. Eastern Paper Bag Co.", "Griggs v. Duke Power Co.", "Railroad Commission v. Pullman Co.", "Vermont Yankee Nuclear Power Corp. v. Natural Resources Defense Council, Inc.", "Hawker v. New York", "Small v. United States", "Reynolds v. Sims", "Palko v. Connecticut", "Kawakita v. United States", "Bowers v. Kerbaugh-Empire Co.", "Hazelwood v. Kuhlmeier", "Flagg Bros., Inc. v. Brooks", "Wooley v. Maynard", "International News Service v. Associated Press", "Standard Oil Co. of New Jersey v. United States", "City of Akron v. Akron Center for Reproductive Health", "Luther v. Borden", "Reed v. Reed", "Philip Morris USA Inc. v. Williams", "United States v. Thompson-Center Arms Co.", "Mississippi University for Women v. Hogan", "United Building & Construction Trades Council v. Mayor and Council of Camden", "Immigration and Naturalization Service v. Abudu", "Owen Equipment & Erection Co. v. Kroger", "Regents of the University of California v. Bakke", "Eli Lilly & Co. v. Medtronic, Inc.", "Marshall v. Marshall", "Johnson v. Eisentrager", "Grutter v. Bollinger", "Cohen v. Cowles Media Co.", "United Public Workers v. Mitchell", "Reid v. Covert", "Dean Milk Co. v. City of Madison", "Whitman v. American Trucking Ass'ns, Inc.", "Ex parte Milligan", "Baker v. Carr", "Coker v. Georgia", "Pennsylvania Coal Co. v. Mahon", "Wisconsin v. Illinois", "Korematsu v. United States", "Hein v. Freedom From Religion Foundation", "Bowen v. Roy", "Wallace v. Cutten", "United States v. E. C. Knight Co.", "England v. Louisiana State Board of Medical Examiners", "United States v. Reynolds", "Romer v. Evans", "Hans v. Louisiana", "SEC v. W. J. Howey Co.", "Hansberry v. Lee", "Warth v. Seldin", "Law v. Siegel", "Strickland v. Washington", "Jones v. Flowers", "Lemon v. Kurtzman", "S. D. Warren Co. v. Maine Board of Environmental Protection", "United States v. Place", "Memoirs v. Massachusetts", "Black and White Taxicab and Transfer Co. v. Brown and Yellow Taxicab and Transfer Co.", "District of Columbia v. Heller", "Dolan v. United States Postal Service", "Perry v. Sindermann", "Bobbs-Merrill Co. v. Straus", "Lisenba v. California", "Burdick v. United States", "Old Colony Trust Co. v. Commissioner", "Chimel v. California", "Brown v. Louisiana", "Nixon v. Fitzgerald", "Pollock v. Farmers' Loan & Trust Co.", "Bernal v. Fainter", "Bailey v. Drexel Furniture Co.", "Texas v. Johnson", "Bauer & Cie. v. O'Donnell", "Egelhoff v. Egelhoff", "Collins v. Yosemite Park & Curry Co.", "Fox Film Corp. v. Muller", "Pocket Veto Case", "NAACP v. Alabama", "Time, Inc. v. Firestone", "Immigration and Naturalization Service v. Stevic", "United States v. Russell", "Edelman v. Jordan", "Adamson v. California", "Bi-Metallic Investment Co. v. State Board of Equalization", "California v. Acevedo", "Hart v. United States", "Barron v. Baltimore", "United States v. Montoya De Hernandez", "Leser v. Garnett", "Nixon v. Condon", "Cumming v. Richmond County Board of Education", "Rust v. Sullivan", "Storer v. Brown", "Florida Prepaid Postsecondary Education Expense Board v. College Savings Bank", "Afroyim v. Rusk", "Oregon v. Guzek", "Mertens v. Hewitt Associates", "Lockyer v. Andrade", "Eisner v. Macomber", "McConnell v. FEC", "Jones v. Alfred H. Mayer Co.", "Holden v. Hardy", "Giles v. Harris", "Sicurella v. United States", "Jacobson v. United States", "Goldwater v. Carter", "Holmes v. South Carolina", "McCleskey v. Kemp", "United States v. Kirby Lumber Co.", "Harper & Row v. Nation Enterprises", "Crandall v. Nevada", "Lynch v. Donnelly", "Nix v. Whiteside", "Commissioner v. Wilcox", "United States v. Raines", "Calder v. Bull", "Enmund v. Florida", "Village of Euclid v. Ambler Realty Co.", "Santa Fe Independent School District v. Doe", "Ogden v. Saunders", "Meyer v. Nebraska", "Exxon Corp. v. Governor of Maryland", "Van Orden v. Perry", "Good News Club v. Milford Central School", "Wickard v. Filburn", "Dun & Bradstreet, Inc. v. Greenmoss Builders, Inc.", "Verizon Communications Inc. v. Law Offices of Curtis V. Trinko, LLP", "Busey v. District of Columbia", "Beauharnais v. Illinois", "Guaranty Trust Co. v. York", "Cramer v. United States", "Strauder v. West Virginia", "United States v. Constantine", "DaimlerChrysler Corp. v. Cuno", "Missouri v. Holland", "NLRB v. Jones & Laughlin Steel Corp.", "Abrams v. United States", "United States v. Salerno", "United States v. Stanley", "Georgia v. Stanton", "Falbo v. United States", "United States v. Schwimmer", "Sipuel v. Board of Regents of the University of Oklahoma", "United States v. Sprague", "Hartford Fire Insurance Co. v. California", "Cox v. Louisiana", "Pennoyer v. Neff", "Twining v. New Jersey", "Wallace v. Jaffree", "Doe v. Bolton", "Asahi Metal Industry Co. v. Superior Court", "Missouri v. Jenkins", "Louisville & Nashville Railroad Co. v. Mottley", "United States v. Bajakajian", "R.A.V. v. City of St. Paul", "Johnson v. M'Intosh", "Brown v. Hotel and Restaurant Employees", "Goss v. Lopez", "United States v. The Amistad", "Stanford v. Kentucky", "Oregon v. Bradshaw", "Illinois v. Lidster", "United States v. Mead Corp.", "Caterpillar, Inc. v. Lewis", "Colorado River Water Conservation District v. United States", "Byrd v. Blue Ridge Rural Electric Cooperative, Inc.", "McNeil v. Wisconsin", "Kennedy v. Louisiana", "Chicago, Milwaukee & St. Paul Railway Co. v. Minnesota", "Texaco Inc. v. Dagher", "Brushaber v. Union Pacific Railroad Co.", "MedImmune, Inc. v. Genentech, Inc.", "Blakely v. Washington", "Illinois v. Rodriguez", "Scott v. Illinois", "Jones v. City of Opelika", "Bethel School District v. Fraser", "Brendlin v. California", "Graver Tank & Manufacturing Co. v. Linde Air Products Co.", "DeLima v. Bidwell", "Cleveland Board of Education v. LaFleur", "Berman v. Parker", "Younger v. Harris", "Illinois Central Railroad Co. v. Illinois", "United States v. Matlock", "Cantwell v. State of Connecticut", "Loving v. Virginia", "Lee v. Weisman", "Agostini v. Felton", "Cantwell v. Connecticut", "United States v. Nixon", "Miller v. Johnson", "Board of Trade of City of Chicago v. Olsen", "Watson v. Jones", "Marquez v. Screen Actors Guild Inc.", "Arizona v. Hicks", "County of Allegheny v. American Civil Liberties Union", "Ewing v. California", "Bush v. Gore", "Microsoft Corp. v. AT&T Corp.", "Banco Nacional de Cuba v. Sabbatino", "Sheldon v. Sill", "District of Columbia Court of Appeals v. Feldman", "Massiah v. United States", "Nebbia v. New York", "Gall v. United States", "Dickinson v. United States", "Domino's Pizza, Inc. v. McDonald", "McDonnell Douglas Corp. v. Green", "Coffin v. United States", "Harmelin v. Michigan", "Carnival Cruise Lines, Inc. v. Shute", "Wilkerson v. Utah", "Ball v. United States", "Schechter Poultry Corp. v. United States", "Ohio v. Robinette", "Board of Regents of State Colleges v. Roth", "Civil Rights Cases", "Jamison v. Texas", "Schneider v. New Jersey", "In re Gault", "Plyler v. Doe", "Abington School District v. Schempp", "Greenholtz v. Inmates of the Nebraska Penal & Correctional Complex", "Roth v. United States", "Feiner v. New York", "Sherbert v. Verner", "Republican Party of Minnesota v. White", "Zablocki v. Redhail", "Tison v. Arizona", "Terry v. Ohio", "National Socialist Party of America v. Village of Skokie", "Scott v. Harris", "Harper v. Virginia State Board of Elections", "Diamond v. Chakrabarty", "Florida v. Royer", "Lum v. Rice", "Immigration and Naturalization Service v. Elias-Zacarias", "Houston East & West Texas Railway Co. v. United States", "South Carolina v. Katzenbach", "Bradwell v. Illinois", "McLaurin v. Oklahoma State Regents", "New York v. Connecticut", "New York City Transit Authority v. Beazer", "Tinker v. Des Moines Independent Community School District", "Fowler v. Rhode Island", "National Gay Task Force v. Board of Education", "Strader v. Graham", "Newberry v. United States", "Saia v. New York", "First National Bank of Boston v. Bellotti", "Barnes v. Glen Theatre, Inc.", "Kansas v. Marsh", "Wolf v. Colorado", "Northern Securities Co. v. United States", "McCollum v. Board of Education", "Linder v. United States", "City of Elizabeth v. American Nicholson Pavement Co.", "Swift v. Tyson", "Heart of Atlanta Motel, Inc. v. United States", "United States v. Cruikshank", "Lawrence v. Texas", "San Antonio Independent School District v. Rodriguez", "Williams v. Mississippi", "Missouri ex rel. Gaines v. Canada", "BP America Production Co. v. Burton", "Ledbetter v. Goodyear Tire & Rubber Co.", "Keystone Bituminous Coal Ass'n v. DeBenedictis", "Hiibel v. Sixth Judicial District Court of Nevada", "Leegin Creative Leather Products, Inc. v. PSKS, Inc.", "Bowers v. Hardwick", "Hammer v. Dagenhart", "Ford v. Wainwright", "McCullen v. Coakley", "Randall v. Sorrell", "United States v. Carolene Products Co.", "Bellotti v. Baird", "Monell v. Department of Social Services of the City of New York", "Locke v. Davey", "Meredith v. Jefferson County Board of Education", "Harris v. Quinn", "Champion v. Ames", "Clearfield Trust Co. v. United States", "Granholm v. Heald", "City of Mobile v. Bolden", "Fullilove v. Klutznick", "Powell v. Alabama", "TrafFix Devices, Inc. v. Marketing Displays, Inc.", "Federal Baseball Club v. National League", "Rosenberger v. University of Virginia", "Ex parte Madrazzo", "Sale v. Haitian Centers Council, Inc.", "West Coast Hotel Co. v. Parrish", "Benton v. Maryland", "Bose Corp. v. Consumers Union of United States, Inc.", "Powell v. McCormack", "Central Hudson Gas & Electric Corp. v. Public Service Commission", "Edmonson v. Leesville Concrete Co.", "Worcester v. Georgia", "Brown v. Board of Education", "Sony Corp. of America v. Universal City Studios, Inc.", "Alaska v. Native Village of Venetie Tribal Government", "Craig v. Boren", "Festo Corp. v. Shoketsu Kinzoku Kogyo Kabushiki Co.", "Heckler v. Chaney", "Little v. Barreme", "Cheek v. United States", "Marsh v. Chambers", "Downes v. Bidwell", "Escobedo v. Illinois", "Chauffeurs, Teamsters, & Helpers Local No. 391 v. Terry", "Island Trees School District v. Pico", "United States v. Curtiss-Wright Export Corp.", "Webster v. Reproductive Health Services", "Oncale v. Sundowner Offshore Services, Inc.", "Quality King Distributors Inc., v. L'anza Research International Inc.", "Meritor Savings Bank v. Vinson", "Board of Trustees of the University of Alabama v. Garrett", "Hernandez v. Texas", "United States v. Forty Barrels & Twenty Kegs of Coca-Cola", "Shaw v. Reno", "Smith v. Allwright", "United Mine Workers of America v. Bagwell", "Federal Election Commission v. Wisconsin Right to Life, Inc.", "Immigration and Naturalization Service v. Chadha", "Runyon v. McCrary", "Batson v. Kentucky", "Cutter v. Wilkinson", "Gonzales v. O Centro Espirita Beneficente Uniao do Vegetal", "New Jersey v. T. L. O.", "Miami Herald Publishing Co. v. Tornillo", "Joseph Burstyn, Inc. v. Wilson", "Adams v. Texas", "Wisconsin v. Mitchell", "National Treasury Employees Union v. Von Raab", "In re Debs", "NBC, Inc. v. United States", "Niemotko v. Maryland", "Graham v. John Deere Co.", "Santa Clara County v. Southern Pacific Railroad Co.", "Late Corp. of the Church of Jesus Christ of Latter-Day Saints v. United States", "Witmer v. United States", "Colorado v. Connelly", "Insular Cases", "Anderson v. Mt. Clemens Pottery Co.", "Burwell v. Hobby Lobby", "Wisconsin v. Yoder", "Hunt v. Washington State Apple Advertising Commission", "Alden v. Maine", "Upjohn Co. v. United States", "Baker v. Selden", "United States v. Continental Can Co.", "eBay Inc. v. MercExchange, L.L.C.", "Central Virginia Community College v. Katz", "Cox v. United States", "Davis v. Washington", "California v. Byers", "Schneider v. State (of New Jersey)", "Furman v. Georgia", "Cannon v. University of Chicago", "Lujan v. Defenders of Wildlife", "Connecticut General Life Insurance Co. v. Johnson", "American Broadcasting Cos. v. Aereo, Inc.", "Wesberry v. Sanders", "Pinkerton v. United States", "Sweatt v. Painter", "Poulos v. New Hampshire", "United States v. Butler", "Buchanan v. Warley", "Ayotte v. Planned Parenthood of Northern New England", "United States v. Sioux Nation of Indians", "Cooley v. Board of Wardens", "Berger v. New York", "United States v. Moreland", "Boumediene v. Bush", "Flast v. Cohen", "Goldberg v. Kelly", "Goesaert v. Cleary", "United States v. Kirby", "Sereboff v. Mid Atlantic Medical Services, Inc.", "Silver v. New York Stock Exchange", "United States v. Shipp", "Arthur Andersen LLP v. United States", "Hurtado v. California", "Minor v. Happersett", "Lechmere, Inc. v. NLRB", "Hunt v. Cromartie", "Commodity Futures Trading Commission v. Schor", "Stump v. Sparkman", "United States v. Shabani", "New State Ice Co. v. Liebmann", "Sanchez-Llamas v. Oregon", "Milliken v. Bradley", "Arkansas Department of Human Services v. Ahlborn", "Hague v. Committee for Industrial Organization", "United States v. Southwestern Cable Co.", "Markman v. Westview Instruments, Inc.", "City of Philadelphia v. New Jersey", "BMW of North America, Inc. v. Gore", "Eastern Associated Coal Corp. v. United Mine Workers of America", "Ex parte Garland", "Lauro Lines v. Chasser", "Murdock v. Pennsylvania", "Pacific States Box & Basket Co. v. White", "Central Laborers' Pension Fund v. Heinz", "North Carolina v. Alford", "United States v. Ninety-Five Barrels Alleged Apple Cider Vinegar", "De Jonge v. Oregon", "Illinois v. Caballes", "Lockett v. Ohio", "Manual Enterprises, Inc. v. Day", "United States v. Klein", "Dickerson v. United States", "Gray v. Sanders", "World-Wide Volkswagen Corp. v. Woodson", "Roper v. Simmons", "Burrow-Giles Lithographic Co. v. Sarony", "Paul v. Virginia", "Branzburg v. Hayes", "Torcaso v. Watkins", "United States v. X-Citement Video, Inc.", "Strawbridge v. Curtiss", "United States v. Hudson", "Northern Pipeline Construction Co. v. Marathon Pipe Line Co.", "Guinn v. United States", "Rasul v. Bush", "Edwards v. Aguillard", "Piper Aircraft Co. v. Reyno", "McCulloch v. Maryland", "Stewart v. Abend", "Martin v. City of Struthers", "Plessy v. Ferguson", "Douglas v. City of Jeannette", "Lau v. Nichols", "County of Sacramento v. Lewis", "Arizona v. Evans", "Marsh v. Alabama", "Curtis Publishing Co. v. Butts", "Hope v. Pelzer", "Samson v. California", "Nebraska Press Ass'n v. Stuart", "Ex parte Young", "Muskrat v. United States", "Harris v. McRae", "Taylor v. Taintor", "United States v. Shoshone Tribe of Indians", "Ex parte Endo", "Sturges v. Crowninshield", "Grosjean v. American Press Co.", "MGM Studios, Inc. v. Grokster, Ltd.", "Illinois Tool Works Inc. v. Independent Ink, Inc.", "Mistretta v. United States", "Nix v. Hedden", "Parker v. Flook", "Bailey v. Alabama", "International Salt Co. v. United States", "Bartnicki v. Vopper", "Bronston v. United States", "Miranda v. Arizona", "C&A Carbone, Inc. v. Town of Clarkstown", "College Savings Bank v. Florida Prepaid Postsecondary Education Expense Board", "Gregg v. Georgia", "Hustler Magazine v. Falwell", "Hickman v. Taylor", "Kohl v. United States", "Herring v. United States", "Dolan v. City of Tigard", "Brown v. Entertainment Merchants Ass'n", "Village of Arlington Heights v. Metropolitan Housing Development Corp.", "Illinois v. Gates", "Credit Suisse Securities (USA) LLC v. Billing", "Williamson v. Lee Optical Co.", "Gade v. National Solid Wastes Management Ass'n", "Atwater v. City of Lago Vista", "United States v. Hubbell", "Charles River Bridge v. Warren Bridge", "Flood v. Kuhn", "Cohen v. California", "Jaffee v. Redmond", "Londoner v. City and County of Denver", "Mitchell v. Forsyth", "Bragdon v. Abbott", "Republic of Austria v. Altmann", "Rummel v. Estelle", "United States v. Morrison", "Hylton v. United States", "Talbot v. Janson", "Prize Cases", "Apprendi v. New Jersey", "Estep v. United States", "Beck v. Alabama", "United States v. Ross", "Bell v. Wolfish", "Kimbrough v. United States", "Octane Fitness, LLC v. ICON Health & Fitness, Inc.", "Yasui v. United States", "National League of Cities v. Usery", "Cohens v. Virginia", "Nixon v. United States", "Pruneyard Shopping Center v. Robins", "Zelman v. Simmons-Harris", "Cunningham v. California", "Clinton v. Jones", "Wheaton v. Peters", "Hawaii Housing Authority v. Midkiff", "Georgia v. Randolph", "Betts v. Brady", "Consolidated Edison Co. v. Public Service Commission", "Nixon v. Herndon", "Near v. Minnesota", "Parents Involved in Community Schools v. Seattle School District No. 1", "Kassel v. Consolidated Freightways Corp.", "Buck v. Bell", "Smith v. Doe", "Rita v. United States", "United States v. Stewart", "Dillon v. Gloss", "United States v. Lopez", "Yates v. United States", "Martin v. Hunter's Lessee", "United States v. Darby Lumber Co.", "Helicopteros Nacionales de Colombia, S. A. v. Hall", "Everson v. Board of Education", "New Mexico v. Texas", "Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc.", "South Dakota v. Opperman", "Head Money Cases", "Rostker v. Goldberg", "Califano v. Yamasaki", "DeShaney v. Winnebago County", "Joint Anti-Fascist Refugee Committee v. McGrath", "Shelley v. Kraemer", "New Negro Alliance v. Sanitary Grocery Co.", "Doyle v. Ohio", "Hollingsworth v. Virginia", "Tellabs, Inc. v. Makor Issues & Rights, Ltd.", "Grove City College v. Bell", "New York v. Ferber", "Lopez v. Gonzales", "Griffith v. Kentucky", "Puerto Rico v. Branstad", "Willson v. Black-Bird Creek Marsh Co.", "Tory v. Cochran", "Baker v. Morton"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-949", "prediction": ["# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.containedby -> United States of America\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Pennsylvania\n# Answer:\nUnited States of America", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> University Park -> location.location.containedby -> Pennsylvania\n# Answer:\nUniversity Park", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.subsidiary_or_constituent_schools -> Smeal College of Business -> location.location.containedby -> University Park\n# Answer:\nSmeal College of Business", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> location.location.nearby_airports -> University Park Airport\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> education.educational_institution.campuses -> Penn State University Creamery -> location.location.geolocation -> m.0z8b_gr\n# Answer:\nPenn State University Creamery", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> Pennsylvania -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nPennsylvania", "# Reasoning Path:\nPennsylvania State University -> location.location.containedby -> University Park -> location.location.containedby -> United States of America\n# Answer:\nUniversity Park"], "ground_truth": ["University Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1851-1855\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_types -> Literary Series\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 11: 1863\n# Answer:\nThe Correspondence of Charles Darwin"], "ground_truth": ["Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Evolutionary Writings: Including the Autobiographies", "Reise eines Naturforschers um die Welt", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Voyage d'un naturaliste autour du monde", "On Natural Selection", "On the tendency of species to form varieties", "Wu zhong qi yuan", "Del Plata a Tierra del Fuego", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Darwin Darwin", "Darwin's notebooks on transmutation of species", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Fertilisation of Orchids", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Charles Darwin", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "La facult\u00e9 motrice dans les plantes", "Evolution by natural selection", "The voyage of Charles Darwin", "Insectivorous Plants", "The foundations of the Origin of species", "ontstaan der soorten door natuurlijke teeltkeus", "Evolution", "Leben und Briefe von Charles Darwin", "Darwin from Insectivorous Plants to Worms", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The action of carbonate of ammonia on the roots of certain plants", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Notebooks on transmutation of species", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Works", "Geological Observations on the Volcanic Islands", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 11: 1863", "Reise um die Welt 1831 - 36", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Life of Erasmus Darwin", "Darwin's journal", "Les mouvements et les habitudes des plantes grimpantes", "La vie et la correspondance de Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "The education of Darwin", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Formation of Vegetable Mould through the Action of Worms", "El Origin De Las Especies", "Geological Observations on South America", "Darwin en Patagonia", "Human nature, Darwin's view", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "Opsht\u0323amung fun menshen", "More Letters of Charles Darwin", "The Correspondence of Charles Darwin, Volume 18: 1870", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The principal works", "The Darwin Reader Second Edition", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Variation of Animals and Plants under Domestication", "The Life and Letters of Charles Darwin Volume 2", "Darwin-Wallace", "Tesakneri tsagume\u030c", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Cartas de Darwin 18251859", "Charles Darwin's marginalia", "Proiskhozhdenie vidov", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin and Henslow", "The Essential Darwin", "On evolution", "Darwin for Today", "Diario del Viaje de Un Naturalista Alrededor", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 16: 1868", "From so simple a beginning", "Kleinere geologische Abhandlungen", "Diary of the voyage of H.M.S. Beagle", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Orgin of Species", "Rejse om jorden", "To the members of the Down Friendly Club", "Motsa ha-minim", "The Darwin Reader First Edition", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Part I: Contributions to the Theory of Natural Selection / Part II", "Darwin's Ornithological notes", "Origins", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Volcanic Islands", "From Darwin's unpublished notebooks", "Charles Darwin's letters", "On a remarkable bar of sandstone off Pernambuco", "The Structure and Distribution of Coral Reefs", "Resa kring jorden", "Die geschlechtliche Zuchtwahl", "The Expression of the Emotions in Man and Animals", "H.M.S. Beagle in South America", "Beagle letters", "Metaphysics, Materialism, & the evolution of mind", "red notebook of Charles Darwin", "The\u0301orie de l'e\u0301volution", "Charles Darwin's natural selection", "Evolution and natural selection", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Voyage of the Beagle", "On the Movements and Habits of Climbing Plants", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Power of Movement in Plants", "vari\u00eberen der huisdieren en cultuurplanten", "Die fundamente zur entstehung der arten", "Darwin Compendium", "Notes on the fertilization of orchids", "The Life and Letters of Charles Darwin Volume 1", "Charles Darwin on the routes of male humble bees", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "A student's introduction to Charles Darwin", "South American Geology", "The Different Forms of Flowers on Plants of the Same Species", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 10: 1862", "Memorias y epistolario i\u0301ntimo", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "monograph on the sub-class Cirripedia", "Darwin", "Darwinism stated by Darwin himself", "The collected papers of Charles Darwin", "A Darwin Selection", "The living thoughts of Darwin", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Correspondence of Charles Darwin, Volume 15: 1867", "On the origin of species by means of natural selection", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Autobiography of Charles Darwin", "Gesammelte kleinere Schriften", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 17: 1869", "Het uitdrukken van emoties bij mens en dier", "Darwin on humus and the earthworm"], "ans_acc": 0.05228758169934641, "ans_hit": 1, "ans_f1": 0.025758969641214356, "ans_precission": 0.875, "ans_recall": 0.013071895424836602, "path_f1": 0.3636363636363636, "path_precision": 1.0, "path_recall": 0.2222222222222222, "path_ans_f1": 0.09937888198757763, "path_ans_precision": 1.0, "path_ans_recall": 0.05228758169934641}
{"id": "WebQTest-952", "prediction": ["# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.country -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.administrative_division.first_level_division_of -> United States of America -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nAlaska -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-953", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County -> location.location.containedby -> Missouri\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> base.biblioness.bibs_location.state -> Missouri -> location.location.containedby -> United States of America\n# Answer:\nMissouri", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone"], "ground_truth": ["Platte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-954", "prediction": ["# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> music.group_membership.member -> Syd Barrett\n# Answer:\nSyd Barrett", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.0z0nj30 -> music.group_membership.member -> Richard Wright\n# Answer:\nRichard Wright", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.02hz8bl -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6bd_cl2\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nPink Floyd -> music.artist.origin -> London -> base.biblioness.bibs_location.country -> United Kingdom\n# Answer:\nLondon", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6bf5_mc\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> m.0z0nj30 -> freebase.valuenotation.is_reviewed -> Member\n# Answer:\nMember", "# Reasoning Path:\nPink Floyd -> music.artist.label -> Harvest Records -> music.record_label.artist -> Syd Barrett\n# Answer:\nHarvest Records", "# Reasoning Path:\nPink Floyd -> music.musical_group.member -> g.11b6c9_l6y\n# Answer:\nmusic.musical_group.member", "# Reasoning Path:\nPink Floyd -> music.artist.origin -> London -> location.administrative_division.country -> United Kingdom\n# Answer:\nLondon"], "ground_truth": ["Psychedelic Session-compilation album"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-955", "prediction": ["# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.spouse -> Loretta Woods\n# Answer:\nLoretta Woods", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> From\n# Answer:\nFrom", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> base.aareas.schema.administrative_area.administrative_children -> Manhattan\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.deceased_person.cause_of_death -> Cerebral hemorrhage -> medicine.disease_cause.diseases -> Stroke\n# Answer:\nCerebral hemorrhage", "# Reasoning Path:\nGranville Woods -> people.deceased_person.place_of_death -> New York City -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nNew York City", "# Reasoning Path:\nGranville Woods -> people.person.spouse_s -> m.0j6f6k_ -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nGranville Woods -> people.deceased_person.cause_of_death -> Cerebral hemorrhage -> medicine.disease.notable_people_with_this_condition -> Terry Scott\n# Answer:\nCerebral hemorrhage"], "ground_truth": ["Loretta Woods"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-957", "prediction": ["# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.main_country -> United States of America\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.language_family -> Siouan languages\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> common.topic.article -> m.0565zd\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> common.topic.notable_for -> g.1258179zc\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> English Language -> fictional_universe.fictional_language.where_spoken -> Heroes in Hell fictional universe\n# Answer:\nEnglish Language", "# Reasoning Path:\nSioux -> people.ethnicity.includes_groups -> Brul\u00e9 -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nBrul\u00e9", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> common.topic.notable_types -> Human Language\n# Answer:\nSioux language", "# Reasoning Path:\nSioux -> people.ethnicity.languages_spoken -> Sioux language -> language.human_language.language_family -> Western Siouan languages\n# Answer:\nSioux language"], "ground_truth": ["Sioux language", "French", "English Language"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7567567567567567, "ans_precission": 0.875, "ans_recall": 0.6666666666666666, "path_f1": 0.7567567567567567, "path_precision": 0.875, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-958", "prediction": ["# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Maximiliano Hern\u00e1ndez Mart\u00ednez\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Constitutional republic -> government.form_of_government.countries -> United States of America\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Osm\u00edn Aguirre y Salinas\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.country.form_of_government -> Presidential system -> government.form_of_government.countries -> United States of America\n# Answer:\nPresidential system", "# Reasoning Path:\nEl Salvador -> location.country.languages_spoken -> Spanish Language -> language.human_language.countries_spoken_in -> United States of America\n# Answer:\nSpanish Language", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.notable_for -> g.125btm68w\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> base.culturalevent.event.entity_involved -> Armed Forces of El Salvador\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.location.events -> 1932 Salvadoran peasant massacre -> common.topic.article -> m.0bjpf4\n# Answer:\n1932 Salvadoran peasant massacre", "# Reasoning Path:\nEl Salvador -> location.statistical_region.poverty_rate_2dollars_per_day -> g.11b6c_pzzl\n# Answer:\nlocation.statistical_region.poverty_rate_2dollars_per_day"], "ground_truth": ["Andr\u00e9s Eduardo Men\u00e9ndez", "Elena Diaz", "Doroteo Vasconcelos", "Eduardo \\\"Volkswagen\\\" Hern\u00e1ndez", "Rub\u00e9n Zamora", "Armando Chac\u00f3n", "Jose Orlando Martinez", "William Renderos Iraheta", "Victor Manuel Ochoa", "Joel Aguilar", "Francisca Gonz\u00e1lez", "Fausto Omar V\u00e1squez", "Johnny Lopez", "Saturnino Osorio", "Juan Rafael Bustillo", "Xenia Estrada", "Gerardo Barrios", "Emilio Guardado", "DJ Quest", "Rene Moran", "Enrique \u00c1lvarez C\u00f3rdova", "F\u00e9lix Pineda", "Jos\u00e9 Inocencio Alas", "Salvador Castaneda Castro", "Rafael Menj\u00edvar Ochoa", "V\u00edctor Ram\u00edrez", "Arturo Rivera y Damas", "Sarah Ramos", "Mario Wilfredo Contreras", "Eva Dimas", "Pedro Geoffroy Rivas", "Roberto Rivas", "Francisco Funes", "Jose Solis", "Melvin Barrera", "Richard Oriani", "Damaris Qu\u00e9les", "Nicolas F. Shi", "Roberto Carlos Martinez", "Francisco Due\u00f1as", "Jos\u00e9 Castellanos Contreras", "Carlos Barrios", "Selvin Gonz\u00e1lez", "Eduardo Hern\u00e1ndez", "Jos\u00e9 Francisco Valiente", "Mauricio Alfaro", "\u00c1ngel Orellana", "Am\u00e9rico Gonz\u00e1lez", "Guillermo Garc\u00eda", "Mario Montoya", "Jos\u00e9 Luis Rugamas", "Papa A.P.", "Erwin McManus", "Miguel Angel Deras", "Keoki", "Norman Quijano", "Victor Lopez", "Edwin Ramos", "William Armando", "Bobby Rivas", "Robert Renderos", "Rafael Campo", "Jorge Rivera", "Isa\u00edas Choto", "Miguel Cruz", "\u00d3scar Antonio Ulloa", "Diego Vel\u00e1zquez", "Julio Adalberto Rivera Carballo", "Carlos Linares", "Ana Sol Gutierrez", "Jaime Portillo", "Alfredo Ruano", "Rutilio Grande", "Tom\u00e1s Medina", "Prudencia Ayala", "g.11b8058v7j", "Laura Molina", "Ana Maria de Martinez", "Mauricio Alonso Rodr\u00edguez", "Gualberto Fern\u00e1ndez", "Genaro Serme\u00f1o", "Ricardo L\u00f3pez Tenorio", "Paula Heredia", "Jose B. Gonzalez", "Malin Arvidsson", "Pedro Jos\u00e9 Escal\u00f3n", "Bernard Lewinsky", "Francisco Gavidia", "Steve Montenegro", "Claudia Lars", "Miguel Ca\u00f1izalez", "Mauricio Alvarenga", "Alexander M\u00e9ndoza", "Ernesto Aparicio", "Consuelo de Saint Exup\u00e9ry", "Ra\u00fal Cicero", "Patricia Chica", "Takeshi Fujiwara", "Pedro Chavarria", "Juan Ram\u00f3n S\u00e1nchez", "Manuel Enrique Araujo", "Wilfredo Iraheta", "Camilo Minero", "Alexander Campos", "Ruben Cedillos", "Jorge Mel\u00e9ndez", "Elmer Acevedo", "Francisco Men\u00e9ndez", "Jorge B\u00facaro", "Jos\u00e9 Mar\u00eda Ca\u00f1as", "Jos\u00e9 Manfredi Portillo", "Marlon Menj\u00edvar", "William L\u00f3pez", "Milton Palacios", "Arturo Armando Molina", "Santiago \\\"Jimmy\\\" Mellado"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-959", "prediction": ["# Reasoning Path:\nBrett Gardner -> base.schemastaging.athlete_extra.salary -> m.0j2r0bt -> base.schemastaging.athlete_salary.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nBrett Gardner -> base.schemastaging.athlete_extra.salary -> m.0j2r0bt -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> sports.sports_team_roster.team -> New York Yankees\n# Answer:\nNew York Yankees", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.batting_stats -> m.06qf764 -> baseball.batting_statistics.season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season", "# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> sports.sports_team_roster.position -> Outfielder\n# Answer:\nOutfielder", "# Reasoning Path:\nBrett Gardner -> sports.pro_athlete.teams -> m.03z8p87 -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nBrett Gardner -> baseball.baseball_player.lifetime_batting_statistics -> m.09kt6v9 -> baseball.lifetime_batting_statistics.starting_season -> 2008 Major League Baseball season\n# Answer:\n2008 Major League Baseball season"], "ground_truth": ["New York Yankees"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> common.topic.notable_types -> City/Town/Village\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.0gmjl4l -> tv.tv_guest_role.episodes_appeared_in -> Day of the Moon (2)\n# Answer:\nDay of the Moon (2)", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-960", "prediction": ["# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.actor -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award_winner -> Steve Carell\n# Answer:\nSteve Carell", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0nbwfjm -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nRainn Wilson", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.02l00g_ -> tv.regular_tv_appearance.character -> Dwight Schrute\n# Answer:\nDwight Schrute", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0z86k10 -> award.award_honor.award -> TV Land Future Classic Award\n# Answer:\nTV Land Future Classic Award", "# Reasoning Path:\nThe Office -> tv.tv_series_season.regular_cast -> m.021y6jp -> tv.regular_tv_appearance.actor -> Steve Carell\n# Answer:\nSteve Carell", "# Reasoning Path:\nThe Office -> award.award_winning_work.awards_won -> m.0ncnl9t -> award.award_honor.award_winner -> Rainn Wilson\n# Answer:\nRainn Wilson"], "ground_truth": ["Rainn Wilson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-962", "prediction": ["# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.containedby -> California\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.hud_county_place.county -> San Diego County -> location.location.containedby -> United States of America\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nSan Diego County", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego\u2013Tijuana -> location.location.containedby -> North America\n# Answer:\nSan Diego\u2013Tijuana", "# Reasoning Path:\nSan Diego -> location.location.time_zones -> Pacific Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nSan Diego -> location.location.containedby -> San Diego County -> location.us_county.hud_county_place -> San Diego County / County Place 0607399999\n# Answer:\nSan Diego County"], "ground_truth": ["San Diego County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-963", "prediction": ["# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.player -> Jarvis Landry\n# Answer:\nJarvis Landry", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.player -> Dan Marino\n# Answer:\nDan Marino", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.05bj8vr -> sports.sports_league_draft_pick.player -> Jake Long\n# Answer:\nJake Long", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.school -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.05bv583 -> sports.sports_league_draft_pick.player -> Ted Ginn, Jr.\n# Answer:\nTed Ginn, Jr.", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.05bv5s_ -> sports.sports_league_draft_pick.player -> Jason Allen\n# Answer:\nJason Allen", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010hdmz0 -> sports.sports_league_draft_pick.draft -> 2014 NFL draft\n# Answer:\n2014 NFL draft", "# Reasoning Path:\nMiami Dolphins -> sports.professional_sports_team.draft_picks -> m.010q80zg -> sports.sports_league_draft_pick.school -> University of Pittsburgh\n# Answer:\nUniversity of Pittsburgh"], "ground_truth": ["Matt Moore", "Ryan Tannehill"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-965", "prediction": ["# Reasoning Path:\nAnne Frank -> base.popstra.celebrity.arrest -> m.0dj_t7p -> base.popstra.arrest.location -> Amsterdam\n# Answer:\nAmsterdam", "# Reasoning Path:\nAnne Frank -> film.film_story_contributor.film_story_credits -> The Diary of Anne Frank -> film.film.featured_film_locations -> Amsterdam\n# Answer:\nThe Diary of Anne Frank", "# Reasoning Path:\nAnne Frank -> base.popstra.celebrity.arrest -> m.0dj_t7p -> base.popstra.arrest.offense -> Karl Silberbauer\n# Answer:\nKarl Silberbauer", "# Reasoning Path:\nAnne Frank -> film.film_story_contributor.film_story_credits -> The Diary of Anne Frank -> film.film.story_by -> Albert Hackett\n# Answer:\nThe Diary of Anne Frank", "# Reasoning Path:\nAnne Frank -> film.film_story_contributor.film_story_credits -> The Diary of Anne Frank -> media_common.netflix_title.netflix_genres -> Biographical Dramas\n# Answer:\nThe Diary of Anne Frank", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> Anne Frank House -> architecture.museum.address -> m.0w4zznj\n# Answer:\nAnne Frank House", "# Reasoning Path:\nAnne Frank -> symbols.name_source.namesakes -> Anne Frank House -> travel.tourist_attraction.near_travel_destination -> Amsterdam\n# Answer:\nAnne Frank House", "# Reasoning Path:\nAnne Frank -> film.film_story_contributor.film_story_credits -> The Diary of Anne Frank -> media_common.netflix_title.netflix_genres -> Classic Dramas\n# Answer:\nThe Diary of Anne Frank"], "ground_truth": ["Montessori Lyceum Amsterdam", "Jewish Lyceum"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-966", "prediction": ["# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.relationship_type -> Engaged to\n# Answer:\nEngaged to", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.spouse -> Shannon Brown\n# Answer:\nShannon Brown", "# Reasoning Path:\nMonica -> celebrities.celebrity.sexual_relationships -> m.05nlg42 -> celebrities.romantic_relationship.celebrity -> Rocko\n# Answer:\nRocko", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.location_of_ceremony -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nMonica -> music.artist.origin -> Atlanta -> base.wikipedia_infobox.settlement.area_code -> Area code 678\n# Answer:\nAtlanta", "# Reasoning Path:\nMonica -> people.person.spouse_s -> m.0h3gsqb -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMonica -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality"], "ground_truth": ["Shannon Brown"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-967", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Suicide -> common.topic.notable_types -> Cause Of Death\n# Answer:\nSuicide", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> base.pethealth.cause.pet_diseases_or_conditions_caused -> Head Trauma in Animals\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.article -> m.02qnd1f\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.cause_of_death -> Ballistic trauma -> common.topic.notable_for -> g.12558k_cd\n# Answer:\nBallistic trauma", "# Reasoning Path:\nAdolf Hitler -> people.person.profession -> Visual Artist -> fictional_universe.character_occupation.characters_with_this_occupation -> Eug\u00e8ne Delacroix\n# Answer:\nVisual Artist", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nAnother Megan Fox letter -- and this one sings her praises!"], "ground_truth": ["Suicide", "Ballistic trauma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7142857142857143, "ans_precission": 0.5555555555555556, "ans_recall": 1.0, "path_f1": 0.7142857142857143, "path_precision": 0.5555555555555556, "path_recall": 1.0, "path_ans_f1": 0.7142857142857143, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 1.0}
{"id": "WebQTest-968", "prediction": ["# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.language_dialect.language -> German Language\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.main_country -> Austria\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Saxon, Upper Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.languages_spoken -> Bavarian Language -> language.human_language.region -> Europe\n# Answer:\nBavarian Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.dialects -> Bavarian Language\n# Answer:\nGerman Language", "# Reasoning Path:\nGermany -> location.country.official_language -> German Language -> language.human_language.region -> Europe\n# Answer:\nGerman Language"], "ground_truth": ["German Language", "Bavarian Language", "Frisian, Northern Language", "Saxon, Upper Language", "Low German", "Turoyo Language"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 1.0, "ans_recall": 0.3333333333333333, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-969", "prediction": ["# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> base.schemastaging.context_name.pronunciation -> g.125_r6xhn\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> sports.sports_championship_event.season -> 1998 NFL season\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> common.topic.notable_for -> g.1255cyd95\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> Super Bowl XXXIII -> common.topic.article -> m.076y5\n# Answer:\nSuper Bowl XXXIII", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> sports.sports_championship_event.season -> 1997 NFL season\n# Answer:\n1998 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1990 AFC Championship Game -> sports.sports_championship_event.season -> 1989 NFL season\n# Answer:\n1990 AFC Championship Game", "# Reasoning Path:\nDenver Broncos -> sports.sports_team.championships -> 1998 AFC Championship Game -> sports.sports_championship_event.championship -> AFC Championship Game\n# Answer:\n1998 AFC Championship Game"], "ground_truth": ["Super Bowl XXXIII"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Don't Look Back -> music.composition.lyricist -> Ronald White\n# Answer:\nDon't Look Back", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> The Tracks of My Tears: The Best Of -> common.topic.notable_types -> Musical Album\n# Answer:\nThe Tracks of My Tears: The Best Of"], "ground_truth": ["Just Like You", "Why Are You Running From My Love", "Little Girl, Little Girl", "Noel", "There Will Come A Day ( I'm Gonna Happen To You )", "If You Can Want", "Food For Thought", "My World", "Pops, We Love You (disco)", "You Really Got a Hold on Me", "Train of Thought", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Baby Come Close", "Love So Fine", "Away in the Manger / Coventry Carol", "The Way You Do (The Things You Do)", "Coincidentally", "Driving Thru Life in the Fast Lane", "Just a Touch Away", "A Tattoo", "A Silent Partner in a Three-Way Love Affair", "Just to See Her", "Cruisin", "We\u2019ve Come Too Far to End It Now", "Nearness of You", "Can't Fight Love", "Photograph in My Mind", "Mother's Son", "Holly", "Fly Me to the Moon (In Other Words)", "I Can't Get Enough", "The Track of My Tears", "Love Don't Give No Reason", "God Rest Ye Merry Gentlemen", "I Have Prayed On It", "Quiet Storm (single version)", "More Than You Know", "My Guy", "I Am I Am", "We Are The Warriors", "As You Do", "And I Don't Love You (Larry Levan instrumental dub)", "Just My Soul Responding", "Please Come Home for Christmas", "Season's Greetings from Smokey Robinson", "The Agony and the Ecstasy", "Standing On Jesus", "Come by Here (Kum Ba Ya)", "Yester Love", "Sleepless Nights", "(It's The) Same Old Love", "Wanna Know My Mind", "Yes It's You Lady", "I Hear The Children Singing", "I'm Glad There Is You", "Theme From the Big Time", "Hold on to Your Love", "Gone Forever", "Don't Wanna Be Just Physical", "Rack Me Back", "Don't Know Why", "The Love Between Me and My Kids", "Night and Day", "I Want You Back", "Quiet Storm (Groove Boutique remix)", "I\u2019ve Got You Under My Skin", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Same Old Love", "Ever Had A Dream", "When Smokey Sings Tears Of A Clown", "There Will Come a Day (I'm Gonna Happen to You)", "Love Is The Light", "You Are So Beautiful (feat. Dave Koz)", "I've Made Love to You a Thousand Times", "The Hurt's On You", "Whatcha Gonna Do", "Come to Me Soon", "Tracks of my Tears", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "I Praise & Worship You Father", "Open", "I Love Your Face", "Tears Of A Clown", "Shoe Soul", "Love Brought Us Here", "More Love", "Bad Girl", "If You Want My Love", "Walk on By", "That Place", "The Tracks of My Heart", "You're the One for Me (feat. Joss Stone)", "Who's Sad", "Everything for Christmas", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "My Girl", "You Are Forever", "Some People Will Do Anything for Love", "Satisfy You", "The Tracks of My Tears", "Why", "Fulfill Your Need", "Did You Know (Berry's Theme)", "Crusin", "Be Careful What You Wish For (instrumental)", "Why Do Happy Memories Hurt So Bad", "You've Really Go a Hold on Me", "Heavy On Pride (Light On Love)", "Tears of a Clown", "If You Wanna Make Love", "Blame It on Love", "Close Encounters of the First Kind", "It's Her Turn to Live", "Will You Love Me Tomorrow?", "Santa Claus is Coming to Town", "Aqui Con Tigo (Being With You)", "Speak Low", "Love Letters", "Save Me", "The Family Song", "Blame It On Love (Duet with Barbara Mitchell)", "Let Your Light Shine On Me", "It's Fantastic", "And I Love Her", "I Can't Find", "I Can't Give You Anything but Love", "Let Me Be the Clock", "Quiet Storm (Groove Boutique Chill Jazz mix)", "You Cannot Laugh Alone", "Everything You Touch", "The Tracks of My Tears (live)", "The Tears Of A Clown", "Pops, We Love You", "A Child Is Waiting", "Baby That's Backatcha", "Winter Wonderland", "Wedding Song", "You've Really Got a Hold on Me", "Ooo Baby Baby (live)", "Be Kind to the Growing Mind", "Because of You It's the Best It's Ever Been", "Tell Me Tomorrow, Part 1", "Crusin'", "He Can Fix Anything", "Unless You Do It Again", "Daylight & Darkness", "Please Don't Take Your Love (feat. Carlos Santana)", "Don't Play Another Love Song", "Just Another Kiss", "Medley: Never My Love / Never Can Say Goodbye", "You're Just My Life (feat. India.Arie)", "Going to a Go-Go", "The Tracks Of My Tears", "Love Don' Give No Reason (12 Inch Club Mix)", "Being With You", "Shop Around", "If You Wanna Make Love (Come 'round Here)", "The Christmas Song", "Jingle Bells", "I'm in the Mood for Love", "Mickey's Monkey", "You Made Me Feel Love", "The Tears of a Clown", "Easy", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Sweet Harmony", "Fallin'", "Deck the Halls", "Christmas Greeting", "You Go to My Head", "Jesus Told Me To Love You", "I Am, I Am", "One Time", "Te Quiero Como Si No Hubiera Un Manana", "Asleep on My Love", "So Bad", "Gang Bangin'", "I Love The Nearness Of You", "I'll Keep My Light In My Window", "Love' n Life", "Time Flies", "I Can\u2019t Stand to See You Cry (Commercial version)", "Ebony Eyes (Duet with Rick James)", "Really Gonna Miss You", "Tea for Two", "Just Passing Through", "Let Me Be The Clock", "Take Me Through The Night", "And I Don't Love You", "Time After Time", "Ebony Eyes", "Will You Still Love Me Tomorrow", "Hanging on by a Thread", "Girlfriend", "I've Got You Under My Skin", "She's Only a Baby Herself", "Ooo Baby Baby", "Double Good Everything", "Tears of a Sweet Free Clown", "Rewind", "It's A Good Night", "Going to a Gogo", "Our Love Is Here to Stay", "Jasmin", "Keep Me", "Christmas Everyday", "We've Saved the Best for Last", "Melody Man", "It's a Good Feeling", "Be Careful What You Wish For", "You Take Me Away", "I Know You by Heart", "Little Girl Little Girl", "Happy (Love Theme From Lady Sings the Blues)", "When A Woman Cries", "Tell Me Tomorrow (12\\\" extended mix)", "The Road to Damascus", "Virgin Man", "You Don't Know What It's Like", "Share It", "Going to a Go Go", "Quiet Storm", "I Second That Emotion", "In My Corner", "Christmas Every Day", "Ooh Baby Baby", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "It's Time to Stop Shoppin' Around", "Will You Love Me Tomorrow", "One Heartbeat", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "I Like Your Face", "Girl I'm Standing There", "Vitamin U", "Just To See Her Again", "Skid Row", "Cruisin'", "Get Ready", "Ain't That Peculiar", "Be Kind To The Growing Mind (with The Temptations)", "Love Bath", "What's Too Much", "Tracks Of My Tears (Live)", "It's Christmas Time", "Tell Me Tomorrow", "I Second That Emotions", "I Care About Detroit", "Tracks of My Tears", "Be Who You Are", "I've Made Love To You A Thousand Times", "Wishful Thinking", "With Your Love Came", "The Agony And The Ecstasy", "No Time to Stop Believing", "Never My Love / Never Can Say Goodbye", "No\u00ebl"], "ans_acc": 0.015625, "ans_hit": 1, "ans_f1": 0.027777777777777776, "ans_precission": 0.125, "ans_recall": 0.015625, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.027777777777777776, "path_ans_precision": 0.125, "path_ans_recall": 0.015625}
{"id": "WebQTest-970", "prediction": ["# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.notable_for -> g.1256qj4g5\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> location.location.containedby -> Campus Martius\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.image -> Artist rendition of the front exterior of the Theatre of Pompey\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.notable_types -> Building\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> people.deceased_person.place_of_death -> Theatre of Pompey -> common.topic.article -> m.05l4pc\n# Answer:\nTheatre of Pompey", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.0lr8678\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulius Caesar -> film.film.starring -> m.010gs_d2 -> film.performance.character -> Cleopatra\n# Answer:\nCleopatra", "# Reasoning Path:\nJulius Caesar -> common.topic.article -> m.02667zw\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nJulius Caesar -> media_common.netflix_title.netflix_genres -> William Shakespeare -> theater.theatrical_lyricist.play_lyrics_written -> Antony and Cleopatra\n# Answer:\nWilliam Shakespeare", "# Reasoning Path:\nJulius Caesar -> media_common.netflix_title.netflix_genres -> William Shakespeare -> common.topic.notable_types -> Author\n# Answer:\nWilliam Shakespeare"], "ground_truth": ["Theatre of Pompey"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-971", "prediction": ["# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Zydeco\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> Jazz\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> book.book_subject.works -> The jazz life\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> common.topic.notable_types -> Interest\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Americana\n# Answer:\nBlues", "# Reasoning Path:\nBessie Smith -> book.written_work.subjects -> Jazz -> book.book_subject.works -> The jazz life\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Jazz -> book.book_subject.works -> The jazz life\n# Answer:\nJazz", "# Reasoning Path:\nBessie Smith -> music.artist.genre -> Blues -> music.genre.subgenre -> Bluegrass\n# Answer:\nBlues"], "ground_truth": ["Blues", "Jazz"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-972", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.season -> 1953 Major League Baseball season\n# Answer:\n1953 Major League Baseball season", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpwy -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.season -> 1956 Major League Baseball Season\n# Answer:\n1956 Major League Baseball Season"], "ground_truth": ["Brooklyn Dodgers", "Montreal Royals", "Los Angeles Bulldogs", "UCLA Bruins football", "Kansas City Monarchs"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5217391304347827, "ans_precission": 0.75, "ans_recall": 0.4, "path_f1": 0.5217391304347827, "path_precision": 0.75, "path_recall": 0.4, "path_ans_f1": 0.5217391304347827, "path_ans_precision": 0.75, "path_ans_recall": 0.4}
{"id": "WebQTest-973", "prediction": ["# Reasoning Path:\nChicago -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b66kxd4l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11b7vbyfz4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> location.statistical_region.population -> g.11btt77639\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nChicago -> base.popstra.location.vacationers -> m.065pcb4 -> base.popstra.vacation_choice.vacationer -> Ashlee Simpson\n# Answer:\nAshlee Simpson", "# Reasoning Path:\nChicago -> location.location.adjoin_s -> m.02wm3lx -> location.adjoining_relationship.adjoins -> Evanston\n# Answer:\nEvanston", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Windy City Times -> common.topic.notable_for -> g.125fmxhqr\n# Answer:\nWindy City Times", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Windy City Times -> book.periodical.first_issue_date -> m.0h_jkgx\n# Answer:\nWindy City Times", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> American Medical News -> common.topic.article -> m.03mf90t\n# Answer:\nAmerican Medical News", "# Reasoning Path:\nChicago -> periodicals.newspaper_circulation_area.newspapers -> Windy City Times -> common.topic.article -> m.05j4f3\n# Answer:\nWindy City Times"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-975", "prediction": ["# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> base.activism.activist.area_of_activism -> Anti-communism\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> people.person.nationality -> United States of America\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> people.person.profession -> Businessperson\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> architecture.building_occupant.buildings_occupied -> m.0dfzl8d -> architecture.occupancy.building -> White House\n# Answer:\nWhite House", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Gender\n# Answer:\nGeorge H. W. Bush", "# Reasoning Path:\nRonald Reagan -> government.us_president.vice_president -> George H. W. Bush -> freebase.valuenotation.is_reviewed -> Place of birth\n# Answer:\nGeorge H. W. Bush"], "ground_truth": ["George H. W. Bush"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9333333333333333, "ans_precission": 0.875, "ans_recall": 1.0, "path_f1": 0.9333333333333333, "path_precision": 0.875, "path_recall": 1.0, "path_ans_f1": 0.9333333333333333, "path_ans_precision": 0.875, "path_ans_recall": 1.0}
{"id": "WebQTest-976", "prediction": ["# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.containedby -> Chicago\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> business.sponsored_recipient.sponsorship -> m.05_g4w7\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> common.topic.notable_types -> Sports Facility\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2003 Major League Baseball All-Star Game\n# Answer:\nU.S. Cellular Field", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.geolocation -> m.02_tzz7\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> location.location.events -> Soccer Bowl '84\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> Comiskey Park -> sports.sports_facility.teams -> Chicago Mustangs\n# Answer:\nComiskey Park", "# Reasoning Path:\nChicago White Sox -> sports.sports_team.arena_stadium -> U.S. Cellular Field -> location.location.events -> 2005 World Series\n# Answer:\nU.S. Cellular Field"], "ground_truth": ["U.S. Cellular Field", "Comiskey Park"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-977", "prediction": ["# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.0jy_bs -> film.performance.character -> Kenickie Murdoch\n# Answer:\nKenickie Murdoch", "# Reasoning Path:\nGrease -> film.film.starring -> m.0y5cjv1 -> film.performance.character -> Girl Fixing Hair in Mirror at Drive-in\n# Answer:\nGirl Fixing Hair in Mirror at Drive-in", "# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.0jy_bs -> film.performance.film -> Grease\n# Answer:\nfilm.performance.film", "# Reasoning Path:\nGrease -> film.film.starring -> m.0jy_bs -> film.performance.character -> Kenickie Murdoch\n# Answer:\nKenickie Murdoch", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nJeff Conaway -> film.actor.film -> m.03lhv6t -> film.performance.film -> Elvira: Mistress of the Dark\n# Answer:\nElvira: Mistress of the Dark", "# Reasoning Path:\nJeff Conaway -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nGrease -> film.film.featured_song -> You're the One That I Want -> music.composition.recordings -> Grease: The Grease Mega-Mix\n# Answer:\nYou're the One That I Want"], "ground_truth": ["Kenickie Murdoch"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-978", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> book.literary_series.works_in_this_series -> The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> book.literary_series.works_in_this_series -> The Papers of Benjamin Franklin, Volume 1: January 1, 1706 through December 31, 1734\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> common.topic.image -> papers of ben franklin vol 1 cover\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> common.topic.notable_types -> Literary Series\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> Letters of Silence Dogood -> book.literary_series.works_in_this_series -> Silence Dogood, No. 1\n# Answer:\nLetters of Silence Dogood", "# Reasoning Path:\nBenjamin Franklin -> book.author.book_editions_published -> A Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors) -> book.book_edition.book -> A Dissertation on Liberty and Necessity, Pleasure and Pain\n# Answer:\nA Dissertation On Liberty, Necessity, Pleasure, And Pain (Notable American Authors)", "# Reasoning Path:\nBenjamin Franklin -> book.author.series_written_or_contributed_to -> The Papers of Benjamin Franklin -> common.topic.notable_for -> g.125chwygj\n# Answer:\nThe Papers of Benjamin Franklin", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["The autobiography of Benjamin Franklin; a restoration of a \\\"fair copy\\\"", "Works of the late Doctor Benjamin Franklin", "The life of Benjamin Franklin, written chiefly by himself", "Faceti\u00e6 Frankliana.   [sic]", "Bite-size Ben Franklin", "Address", "New experiments and observations on electricity", "The life and letters", "Founding Fathers Benjamin Franklin Volume 2", "The letters of Benjamin Franklin & Jane Mecom", "Franklin's wit & folly", "The life of Dr. Benjamin Franklin", "Early to bed, and early to rise, makes a man healthy, wealthy, and wise, or, Early rising, a natural, social, and religious duty", "Letters to the press, 1758-1775", "The Papers of Benjamin Franklin, Volume 1: January 1, 1706 through December 31, 1734", "Autobiography", "A parable", "Letters and papers of Benjamin Franklin and Richard Jackson, 1753-1785", "The works of the late Dr. Benjamin Franklin", "On war and peace", "The wisdom of Benjamin Franklin", "A Dissertation on Liberty and Necessity, Pleasure and Pain", "The writings of Benjamin Franklin", "The Drinker's Dictionary", "Conseils pour s'enrichir", "Authobiography Of Benjamin Franklin", "Satires & Bagatelles", "Sheep will never make insurrections", "What good is a newborn baby?", "Memoirs of the life and writings of Benjamin Franklin", "A letter from B. Franklin to a young man", "Silence Dogood, No. 6", "Representative selections", "Reflection On Courtship And Marriage", "The Writings Of Benjamin Franklin, Vol. 1", "Free silver, and some other things", "The works of Benjamin Franklin", "Poor Richard's Almanack", "The autobiography of Benjamin Franklin and selections from his other writings", "Silence Dogood, No. 1", "The Immortal Mentor", "The Life of Benjamin Franklin", "Poor Richard, or, The way to wealth", "Benjamin Franklin and Jonathan Edwards", "The life and essays, of Dr. Franklin", "Articles of belief", "The way to wealth, or, Poor Richard improved", "Political, Miscellaneous And Philosophical Pieces", "The select works of Benjamin Franklin", "Benjamin Franklin's Proposals for the education of youth in Pennsylvania, 1749", "How to Attain Moral Perfection", "Silence Dogood, No. 13", "Silence Dogood, No. 11", "Select works, including his autobiography", "Franklin's Way to wealth, or, \\\"Poor Richard improved\\\"", "The bagatelles from Passy", "The autobiography, with an introd", "The ingenious Dr. Franklin", "The works of Dr. Benjn. Franklin;", "B. Franklin, innovator", "Silence Dogood, No. 8", "Silence Dogood, No. 9", "Benjamin Franklin's autobiographical writings", "Poor Richard's Horse Keeper", "Observations on the causes and cure of smoky chimneys", "The works of Dr. Benjamin Franklin, in philosophy, politics, and morals", "Father Abraham's speech to a great number of people, at a vendue of merchant-goods", "Not Your Usual Founding Father", "The Autobiography of Benjamin Franklin", "The sayings of Benjamin Franklin", "Some Fruits of Solitude in Reflections and Maxims", "A letter from Mr. Franklin to Mr. Peter Collinson, F.R.S. concerning the effect of lightning ; A letter of Benjamin Franklin, Esq. to Mr. Peter Collinson, F.R.S. concerning an electrical kite", "Silence Dogood, No. 12", "The Papers of Benjamin Franklin, Vol. 28: Volume 28: November 1, 1778 through February 28, 1779", "Private correspondence of Benjamin Franklin", "The Works Of Benjamin Franklin V1", "The works of Dr. Benjamin Franklin", "The Morals of Chess", "Experiments and observations on electricity, made at Philadelphia in America", "Apology for printers", "Observations Concerning the Increase of Mankind, Peopling of Countries, etc.", "My Dear Girl Ii", "Silence Dogood, No. 3", "Memoirs of Benjamin Franklin", "The life of the late Doctor Benjamin Franklin", "America's Big Ben", "Benjamin Franklin's own story", "Franklin's boyhood in Boston", "Silence Dogood, No. 7", "Franklin was there", "The art of making money plenty", "Benjamin Franklin's Experiments", "Benjamin Franklin's the art of virtue", "The political thought of Benjamin Franklin", "The glory of eternity", "The life of Doctor Benjamin Franklin", "Autobiography, Poor Richard, and later writings", "Silence Dogood, No. 5", "Some account of the Pennsylvania Hospital", "Collected Works Of Benjamin Franklin", "A letter of advice to a young man concerning marriage", "Poor Richard day by day", "Selected Works of Benjamin Franklin", "Compleated Autobiography by Benjamin Franklin", "The autobiography of Benjamin Franklin, and a sketch of Franklin's life from the point where the autobiography ends, drawn chiefly from his letters", "Essays And Letters V1", "Silence Dogood, No. 4", "Benjamin Franklin on balloons", "Articles Of Belief And Acts Of Religion Vol.2", "Avis n\u00e9cessaire \u00e0 ceux qui veulent devenir riche", "The Way to Wealth", "The complete works in philosophy, politics, and morals, of the late Dr. Benjamin Franklin, now first collected and arranged: with memoirs of his early life, written by himself", "The life of the late Dr. Benjamin Franklin", "The autobiography and other writings", "The life and essays of the late Doctor Benjamin Franklin", "The Means and Manner of Obtaining Virtue", "Silence Dogood, No. 14", "Silence Dogood, No. 10", "\\\"The sayings of Poor Richard\\\"", "The autobiography, Poor Richard's almanac and other papers", "Silence Dogood, No. 2", "The essays, humorous, moral and literary of the late Dr. Benjamin Franklin", "Poor Richard, 1733"], "ans_acc": 0.032520325203252036, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.13559322033898305, "path_precision": 0.5, "path_recall": 0.0784313725490196, "path_ans_f1": 0.06106870229007635, "path_ans_precision": 0.5, "path_ans_recall": 0.032520325203252036}
{"id": "WebQTest-979", "prediction": ["# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.notable_for -> g.125b468qw\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.article -> m.05hy7y\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> United States of America\n# Answer:\nEarth", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.webpage -> m.04m2ytq\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.country.currency_used -> Barbadian dollar -> common.topic.image -> Current coins in use.\n# Answer:\nBarbadian dollar", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc37pj3\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> North America\n# Answer:\nEarth", "# Reasoning Path:\nBarbados -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37ppn\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nBarbados -> location.statistical_region.official_development_assistance -> g.1hhc382cm\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nBarbados -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.countries_within -> France\n# Answer:\nEarth"], "ground_truth": ["Barbadian dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> common.topic.notable_types -> US President\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.americancomedy.impersonated_celebrity.impersonated_by -> Rich Little\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> base.activism.activist.area_of_activism -> Anti-communism\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.children -> Patrick Bouvier Kennedy -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nPatrick Bouvier Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-980", "prediction": ["# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1974 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Boston Bruins\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1974 Stanley Cup Finals -> sports.sports_championship_event.champion -> Philadelphia Flyers\n# Answer:\n1974 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1967 Stanley Cup Finals -> sports.sports_championship_event.champion -> Toronto Maple Leafs\n# Answer:\n1967 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1964 Stanley Cup Finals -> sports.sports_championship_event.champion -> Toronto Maple Leafs\n# Answer:\n1964 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1954 Stanley Cup Finals -> sports.sports_championship_event.champion -> Detroit Red Wings\n# Answer:\n1954 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1933 Stanley Cup Finals -> sports.sports_championship_event.champion -> New York Rangers\n# Answer:\n1933 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1964 Stanley Cup Finals -> sports.sports_championship_event.runner_up -> Detroit Red Wings\n# Answer:\n1964 Stanley Cup Finals", "# Reasoning Path:\nStanley Cup -> sports.sports_championship.events -> 1974 Stanley Cup Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1974 Stanley Cup Finals"], "ground_truth": ["Frederick Stanley, 16th Earl of Derby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-982", "prediction": ["# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> common.topic.notable_for -> g.125c79vqr\n# Answer:\n13290", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> location.postal_code.country -> United States of America\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> periodicals.newspaper_circulation_area.newspapers -> The Post-Standard -> book.newspaper.circulation_areas -> Onondaga County\n# Answer:\nThe Post-Standard", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> location.location.containedby -> Onondaga County\n# Answer:\n13290", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13202 -> common.topic.notable_types -> Postal Code\n# Answer:\n13202", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13201 -> common.topic.notable_types -> Postal Code\n# Answer:\n13201", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13203 -> location.postal_code.country -> United States of America\n# Answer:\n13203", "# Reasoning Path:\nSyracuse -> location.citytown.postal_codes -> 13290 -> location.location.contains -> Destiny USA\n# Answer:\n13290"], "ground_truth": ["13252", "13207", "13225", "13290", "13204", "13209", "13218", "13214", "13210", "13202", "13205", "13244", "13211", "13219", "13217", "13220", "13224", "13244-1020", "13212", "13261", "13206", "13250", "13203", "13208", "13251", "13201", "13215", "13235", "13221"], "ans_acc": 0.13793103448275862, "ans_hit": 1, "ans_f1": 0.23829787234042552, "ans_precission": 0.875, "ans_recall": 0.13793103448275862, "path_f1": 0.23829787234042552, "path_precision": 0.875, "path_recall": 0.13793103448275862, "path_ans_f1": 0.23829787234042552, "path_ans_precision": 0.875, "path_ans_recall": 0.13793103448275862}
{"id": "WebQTest-983", "prediction": ["# Reasoning Path:\nTennessee Williams -> people.person.education -> m.02wpn3_ -> education.education.institution -> University of Missouri\n# Answer:\nUniversity of Missouri", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n0mldl -> education.education.institution -> The New School\n# Answer:\nThe New School", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n1m266 -> education.education.institution -> Washington University in St. Louis\n# Answer:\nWashington University in St. Louis", "# Reasoning Path:\nTennessee Williams -> people.person.education -> m.0n1mw32 -> education.education.institution -> University of Iowa\n# Answer:\nUniversity of Iowa", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> common.topic.notable_for -> g.1258jbcmd\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> people.person.places_lived -> m.04hdd2t -> people.place_lived.location -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nTennessee Williams -> people.person.quotations -> A high station in life is earned by the gallantry with which appalling experiences are survived with grace. -> media_common.quotation.subjects -> Elegance\n# Answer:\nA high station in life is earned by the gallantry with which appalling experiences are survived with grace.", "# Reasoning Path:\nTennessee Williams -> book.author.works_written -> 27 Wagons Full of Cotton -> common.topic.notable_types -> Play\n# Answer:\n27 Wagons Full of Cotton"], "ground_truth": ["University of Missouri", "The New School", "University of Iowa", "Washington University in St. Louis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-984", "prediction": ["# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Viceroy New York\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> location.location.containedby -> United States of America\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> common.topic.notable_types -> Airport\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> travel.transport_terminus.travel_destinations_served -> m.0519ccq\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> MBSF - Private Jets\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> common.topic.notable_types -> Airport\n# Answer:\nJohn F. Kennedy International Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> LaGuardia Airport -> aviation.airport.serves -> New York metropolitan area\n# Answer:\nLaGuardia Airport", "# Reasoning Path:\nNew York City -> location.location.nearby_airports -> John F. Kennedy International Airport -> aviation.airport.serves -> Queens\n# Answer:\nJohn F. Kennedy International Airport"], "ground_truth": ["John F. Kennedy International Airport"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.8571428571428571, "path_precision": 0.75, "path_recall": 1.0, "path_ans_f1": 0.8571428571428571, "path_ans_precision": 0.75, "path_ans_recall": 1.0}
{"id": "WebQTest-985", "prediction": ["# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.spouse -> Albert, Prince Consort\n# Answer:\nAlbert, Prince Consort", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Victoria, Princess Royal -> people.person.children -> Sophia of Prussia\n# Answer:\nVictoria, Princess Royal", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Alfred, Duke of Saxe-Coburg and Gotha -> people.person.children -> Alfred, Hereditary Prince of Saxe-Coburg and Gotha\n# Answer:\nAlfred, Duke of Saxe-Coburg and Gotha", "# Reasoning Path:\nQueen Victoria -> people.person.spouse_s -> m.02kp9lp -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Prince Arthur, Duke of Connaught and Strathearn -> people.person.parents -> Albert, Prince Consort\n# Answer:\nPrince Arthur, Duke of Connaught and Strathearn", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Victoria, Princess Royal -> people.person.spouse_s -> m.0j4k6l9\n# Answer:\nVictoria, Princess Royal", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.children -> Princess Victoria of the United Kingdom\n# Answer:\nEdward VII", "# Reasoning Path:\nQueen Victoria -> people.person.children -> Edward VII -> people.person.spouse_s -> m.02kp9gy\n# Answer:\nEdward VII"], "ground_truth": ["Albert, Prince Consort"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2105263157894737, "path_precision": 0.25, "path_recall": 0.18181818181818182, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-986", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.places_lived -> m.04lpf2x\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Josiah Franklin Jr.\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.deceased_person.place_of_death -> London\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> people.person.parents -> Deborah Read\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.parents -> Jane White Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nJosiah Franklin"], "ground_truth": ["United States Postal Service", "Josiah Franklin", "James Franklin Printing Shop"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.35294117647058826, "ans_precission": 0.375, "ans_recall": 0.3333333333333333, "path_f1": 0.35294117647058826, "path_precision": 0.375, "path_recall": 0.3333333333333333, "path_ans_f1": 0.35294117647058826, "path_ans_precision": 0.375, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-987", "prediction": ["# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> book.magazine_genre.magazines_in_this_genre -> Better Homes and Gardens\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> base.ontologies.ontology_instance.equivalent_instances -> m.07ngjqb\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Betty Draper\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Samantha\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> common.topic.notable_types -> Media genre\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> First Lady of the United States\n# Answer:\nUnited States of America", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Carmela Soprano\n# Answer:\nHomemaking", "# Reasoning Path:\nBarbara Bush -> people.person.profession -> Homemaking -> fictional_universe.character_occupation.characters_with_this_occupation -> Celia Hodes\n# Answer:\nHomemaking"], "ground_truth": ["First Lady"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-988", "prediction": ["# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> common.topic.notable_for -> g.125h67_9m\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxg3\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> common.topic.image -> Dancers\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> film.actor.film -> g.11bc7_yq5y\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxkn\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Actor -> people.profession.specializations -> Voice Actor\n# Answer:\nActor", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> people.profession.specializations -> Stage Dancer\n# Answer:\nDancer", "# Reasoning Path:\nJacob Latimore -> people.person.profession -> Dancer -> base.descriptive_names.names.descriptive_name -> m.0102lxys\n# Answer:\nDancer"], "ground_truth": ["Dancer", "Actor", "Singer"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 1.0, "ans_recall": 0.6666666666666666, "path_f1": 0.761904761904762, "path_precision": 0.8888888888888888, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8, "path_ans_precision": 1.0, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-989", "prediction": ["# Reasoning Path:\nDunkirk -> common.topic.article -> m.0md6r\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDunkirk -> location.location.containedby -> Nord-Pas-de-Calais -> location.administrative_division.country -> France\n# Answer:\nNord-Pas-de-Calais", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> military.military_conflict.military_personnel_involved -> Bernard de Gomme\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> common.topic.notable_types -> Military Conflict\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> common.topic.image -> LariviereBatailleDunes\n# Answer:\nBattle of the Dunes", "# Reasoning Path:\nDunkirk -> location.location.containedby -> Nord-Pas-de-Calais -> location.location.events -> Battle of the Lys\n# Answer:\nNord-Pas-de-Calais", "# Reasoning Path:\nDunkirk -> location.location.events -> Action of 18 February 1639 -> base.culturalevent.event.entity_involved -> Spain\n# Answer:\nAction of 18 February 1639", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of Dunkirk -> military.military_conflict.casualties -> m.04lkt8p\n# Answer:\nBattle of Dunkirk", "# Reasoning Path:\nDunkirk -> location.location.events -> Battle of the Dunes -> base.culturalevent.event.entity_involved -> Louis, Grand Cond\u00e9\n# Answer:\nBattle of the Dunes"], "ground_truth": ["Battle of Dunkirk", "Siege of Dunkirk"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2222222222222222, "ans_recall": 0.5, "path_f1": 0.30769230769230765, "path_precision": 0.2222222222222222, "path_recall": 0.5, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 0.5}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> South Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Serbia and Montenegro\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-990", "prediction": ["# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> sports.sports_position.sport -> Basketball\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> John Salmons\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> sports.pro_athlete.sports_played_professionally -> m.0d5jh3n -> sports.pro_sports_played.sport -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.notable_types -> Basketball Position\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> people.person.profession -> Basketball player -> common.topic.notable_types -> Profession\n# Answer:\nBasketball player", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.image -> Basketball half-court\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.webpage -> m.09y16j4\n# Answer:\nSmall forward", "# Reasoning Path:\nScottie Pippen -> basketball.basketball_player.position_s -> Small forward -> common.topic.notable_for -> g.1259jp575\n# Answer:\nSmall forward"], "ground_truth": ["Basketball"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-991", "prediction": ["# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.actor -> Janel Moloney\n# Answer:\nJanel Moloney", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 6\n# Answer:\nThe West Wing Season 6", "# Reasoning Path:\nDonnatella Moss -> common.topic.notable_for -> g.125c79t5m\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.series -> The West Wing\n# Answer:\nThe West Wing", "# Reasoning Path:\nDonnatella Moss -> common.topic.article -> m.03vzls\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.characters -> Abbey Bartlet\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.works_set_here -> The West Wing\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> tv.tv_character.appeared_in_tv_program -> m.02kvknt -> tv.regular_tv_appearance.seasons -> The West Wing Season 7\n# Answer:\nThe West Wing Season 7", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> The West Wing Universe -> fictional_universe.fictional_universe.languages -> American English\n# Answer:\nThe West Wing Universe", "# Reasoning Path:\nDonnatella Moss -> fictional_universe.fictional_character.ethnicity -> Irish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Archbishop Gilday\n# Answer:\nIrish American"], "ground_truth": ["Janel Moloney"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-993", "prediction": ["# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.04d4q86 -> tv.regular_tv_appearance.actor -> Lacey Chabert\n# Answer:\nLacey Chabert", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.series -> Family Guy\n# Answer:\nFamily Guy", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0w0v0l2 -> tv.regular_tv_appearance.series -> The Cleveland Show\n# Answer:\nThe Cleveland Show", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0k6pxpv -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.02ntq_4 -> tv.regular_tv_appearance.actor -> Mila Kunis\n# Answer:\nMila Kunis", "# Reasoning Path:\nMeg Griffin -> tv.tv_character.appeared_in_tv_program -> m.0w0v0l2 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nMeg Griffin -> cvg.game_character.games -> m.0ypsv3n -> cvg.game_performance.voice_actor -> Mila Kunis\n# Answer:\nMila Kunis"], "ground_truth": ["Lacey Chabert"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-994", "prediction": ["# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> common.topic.notable_types -> Super bowl\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXX -> common.topic.article -> m.076x4\n# Answer:\nSuper Bowl XXX", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XII", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl VI -> time.event.instance_of_recurring_event -> Super Bowl\n# Answer:\nSuper Bowl VI", "# Reasoning Path:\nDallas Cowboys -> sports.sports_team.championships -> Super Bowl XXVIII -> sports.sports_championship_event.championship -> Super Bowl\n# Answer:\nSuper Bowl XXVIII"], "ground_truth": ["Super Bowl XXX"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-996", "prediction": ["# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> Shamrock Rovers vs Liverpool F.C.\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.events -> 2011\u201312 Pro12\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.teams -> Republic of Ireland national football team\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2010 FAI Cup Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> location.location.containedby -> Lansdowne Road\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2011 FAI Cup Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> soccer.football_pitch.matches -> 2011 UEFA Europa League Final\n# Answer:\nAviva Stadium", "# Reasoning Path:\nIreland national rugby union team -> sports.sports_team.arena_stadium -> Aviva Stadium -> sports.sports_facility.home_venue_for -> m.0wz2kky\n# Answer:\nAviva Stadium"], "ground_truth": ["Aviva Stadium"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-997", "prediction": ["# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.subject -> John F. Kennedy\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> fidel castro self-proclaimed follower was lee harvey oswald -> base.kwebbase.kwconnection.subject -> Fidel Castro\n# Answer:\nfidel castro self-proclaimed follower was lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.relation -> allegedly assassinated by\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> karl marx influenced lee harvey oswald -> base.kwebbase.kwconnection.subject -> Karl Marx\n# Answer:\nkarl marx influenced lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.sentence -> Lee Harvey Oswald, a young ex-Marine with links to the Soviet Union, was arrested.\n# Answer:\njohn fitzgerald kennedy allegedly assassinated by lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_to -> lyndon baines johnson presidency due to lee harvey oswald -> base.kwebbase.kwconnection.subject -> Lyndon B. Johnson\n# Answer:\nlyndon baines johnson presidency due to lee harvey oswald", "# Reasoning Path:\nLee Harvey Oswald -> base.kwebbase.kwtopic.connections_from -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.other -> John F. Kennedy\n# Answer:\nlee harvey oswald allegedly assassinated john fitzgerald kennedy", "# Reasoning Path:\nLee Harvey Oswald -> people.deceased_person.cause_of_death -> Bleeding -> people.cause_of_death.people -> Anthony Minghella\n# Answer:\nBleeding"], "ground_truth": ["Jack Ruby"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-998", "prediction": ["# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.article -> m.0cs1x9\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1970 NBA Finals -> common.topic.notable_for -> g.1255ddd2t\n# Answer:\n1970 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> common.topic.notable_types -> Sports League Championship Event\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> sports.sports_championship_event.runner_up -> Los Angeles Lakers\n# Answer:\n1973 NBA Finals", "# Reasoning Path:\nNew York Knicks -> sports.sports_team.championships -> 1973 NBA Finals -> common.topic.article -> m.0cs1km\n# Answer:\n1973 NBA Finals"], "ground_truth": ["1973 NBA Finals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-999", "prediction": ["# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.runner_up -> Houston Rockets\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 2008 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n2008 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1974 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1974 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1986 NBA Finals -> sports.sports_championship_event.season -> 1985\u201386 NBA season\n# Answer:\n1986 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1959 NBA Finals -> common.topic.article -> m.0cs2j1\n# Answer:\n1959 NBA Finals", "# Reasoning Path:\nBoston Celtics -> sports.sports_team.championships -> 1981 NBA Finals -> sports.sports_championship_event.championship -> The NBA Finals\n# Answer:\n1981 NBA Finals"], "ground_truth": ["1976 NBA Finals", "1981 NBA Finals", "1959 NBA Finals", "1968 NBA Finals", "1969 NBA Finals", "1961 NBA Finals", "1960 NBA Finals", "1962 NBA Finals", "1966 NBA Finals", "1974 NBA Finals", "1984 NBA Finals", "1965 NBA Finals", "1986 NBA Finals", "1957 NBA Finals", "1964 NBA Finals", "1963 NBA Finals", "2008 NBA Finals"], "ans_acc": 0.29411764705882354, "ans_hit": 1, "ans_f1": 0.45454545454545453, "ans_precission": 1.0, "ans_recall": 0.29411764705882354, "path_f1": 0.45454545454545453, "path_precision": 1.0, "path_recall": 0.29411764705882354, "path_ans_f1": 0.45454545454545453, "path_ans_precision": 1.0, "path_ans_recall": 0.29411764705882354}
