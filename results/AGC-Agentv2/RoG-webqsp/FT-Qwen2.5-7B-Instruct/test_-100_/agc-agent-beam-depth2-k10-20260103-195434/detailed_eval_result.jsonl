{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Fay -> meteorology.tropical_cyclone.affected_areas -> Puerto Rico\n# Answer:\nPuerto Rico", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Fay -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Cuba\n# Answer:\nCuba", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Fay -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Cyclone", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> meteorology.tropical_cyclone.affected_areas -> Belize\n# Answer:\nBelize", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Keith -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Cyclone"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specialization_of -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nTitle", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician"], "ground_truth": ["United States Representative", "Speaker of the United States House of Representatives", "Governor of Tennessee"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.125, "ans_recall": 0.3333333333333333, "path_f1": 0.14285714285714288, "path_precision": 0.125, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.125, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.actor -> Ilyssa Fradin\n# Answer:\nIlyssa Fradin", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.actor -> Melinda McGraw\n# Answer:\nMelinda McGraw", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Solicitor\n# Answer:\nSolicitor", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Advocate\n# Answer:\nAdvocate", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> base.descriptive_names.names.descriptive_name -> m.0108dn6c\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk4l\n# Answer:\nbase.descriptive_names.names.descriptive_name"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.36363636363636365, "ans_precission": 0.25, "ans_recall": 0.6666666666666666, "path_f1": 0.36363636363636365, "path_precision": 0.25, "path_recall": 0.6666666666666666, "path_ans_f1": 0.36363636363636365, "path_ans_precision": 0.25, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.season -> 1995\u201396 NBA season\n# Answer:\n1995\u201396 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter"], "ground_truth": ["Boston Celtics", "Phoenix Suns", "Cleveland Cavaliers", "Orlando Magic", "LSU Tigers men's basketball", "Miami Heat", "Los Angeles Lakers"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.5714285714285714, "ans_recall": 0.2857142857142857, "path_f1": 0.21621621621621623, "path_precision": 0.5714285714285714, "path_recall": 0.13333333333333333, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> European American\n# Answer:\nEuropean American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> Scottish people\n# Answer:\nScottish people", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> William paca\n# Answer:\nWilliam paca"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nHuman Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Saek language", "Vietnamese Language", "Akha Language", "Khmer language", "Hmong language", "Mon Language", "Phu Thai language", "Thai Language", "Mlabri Language", "Cham language", "Nyaw Language", "Lao Language", "Malay, Pattani Language"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.11764705882352941, "ans_precission": 0.25, "ans_recall": 0.07692307692307693, "path_f1": 0.23529411764705882, "path_precision": 0.5, "path_recall": 0.15384615384615385, "path_ans_f1": 0.23529411764705882, "path_ans_precision": 0.5, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist"], "ground_truth": ["Author", "Archaeologist", "Writer", "Inventor", "Farmer", "Architect", "Philosopher", "Lawyer", "Teacher", "Statesman"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.3333333333333333, "ans_recall": 0.2, "path_f1": 0.28571428571428575, "path_precision": 0.5, "path_recall": 0.2, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.5, "path_ans_recall": 0.2}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nUnited Kingdom"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Expression of the Emotions in Man and Animals", "The Voyage of the Beagle (Unabridged Classics)", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Correspondence of Charles Darwin, Volume 14", "The expression of the emotions in man and animals.", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Origin Of Species", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "vari\u00eberen der huisdieren en cultuurplanten", "El Origin De Las Especies", "The education of Darwin", "The Structure and Distribution of Coral Reefs", "On the origin of species by means of natural selection", "The Autobiography Of Charles Darwin", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Voyage of the Beagle", "Darwin on humus and the earthworm", "To the members of the Down Friendly Club", "The Orgin of Species", "The Correspondence of Charles Darwin, Volume 10", "Works", "The action of carbonate of ammonia on the roots of certain plants", "The origin of species : complete and fully illustrated", "The Expression of the Emotions in Man And Animals", "Voyage of the Beagle (Harvard Classics, Part 29)", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Notes on the fertilization of orchids", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "On a remarkable bar of sandstone off Pernambuco", "Origin of Species (Harvard Classics, Part 11)", "The structure and distribution of coral reefs", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution and natural selection", "Resa kring jorden", "A student's introduction to Charles Darwin", "The Correspondence of Charles Darwin, Volume 5", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 6", "The living thoughts of Darwin", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Charles Darwin's letters", "On the Movements and Habits of Climbing Plants", "Diario del Viaje de Un Naturalista Alrededor", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Darwin's notebooks on transmutation of species", "Darwin's insects", "Motsa ha-minim", "Voyage of the Beagle", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Structure And Distribution of Coral Reefs", "The Origin of Species (Great Books : Learning Channel)", "genese\u014ds t\u014dn eid\u014dn", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 14: 1866", "The autobiography of Charles Darwin", "Voyage Of The Beagle", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 15: 1867", "La facult\u00e9 motrice dans les plantes", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Origin of Species (Oxford World's Classics)", "The Essential Darwin", "Het uitdrukken van emoties bij mens en dier", "The Voyage of the Beagle (Great Minds Series)", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The principal works", "Beagle letters", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 11", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Rejse om jorden", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Charles Darwin on the routes of male humble bees", "Origins", "ontstaan der soorten door natuurlijke teeltkeus", "The Correspondence of Charles Darwin, Volume 12", "The autobiography of Charles Darwin, 1809-1882", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin (Great Minds Series)", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Origin of Species", "On Natural Selection", "Evolution by natural selection", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 13", "Human nature, Darwin's view", "The voyage of the Beagle.", "Insectivorous Plants", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Different Forms of Flowers on Plants of the Same Species", "The Darwin Reader First Edition", "The Descent of Man and Selection in Relation to Sex", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Voyage of the Beagle (Everyman Paperbacks)", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 9", "The Autobiography of Charles Darwin (Dodo Press)", "The Correspondence of Charles Darwin, Volume 3", "Voyage of the Beagle (NG Adventure Classics)", "The Autobiography of Charles Darwin, and selected letters", "La vie et la correspondance de Charles Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The\u0301orie de l'e\u0301volution", "The Voyage of the Beagle (Mentor)", "The origin of species", "Les mouvements et les habitudes des plantes grimpantes", "Charles Darwin's marginalia", "Notebooks on transmutation of species", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The expression of the emotions in man and animals", "The foundations of the Origin of species", "Origin of Species", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwin", "Darwin en Patagonia", "Darwin and Henslow", "Metaphysics, Materialism, & the evolution of mind", "Kleinere geologische Abhandlungen", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Origin of Species (World's Classics)", "Leben und Briefe von Charles Darwin", "The voyage of Charles Darwin", "Questions about the breeding of animals", "Die geschlechtliche Zuchtwahl", "Diary of the voyage of H.M.S. Beagle", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 1", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Formation of Vegetable Mould through the Action of Worms", "Reise eines Naturforschers um die Welt", "Fertilisation of Orchids", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 8: 1860", "More Letters of Charles Darwin", "Darwin's journal", "From so simple a beginning", "The Expression Of The Emotions In Man And Animals", "The Origin of Species (Enriched Classics)", "On evolution", "A Darwin Selection", "The Origin of Species (Collector's Library)", "The descent of man, and selection in relation to sex.", "The Autobiography of Charles Darwin", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "red notebook of Charles Darwin", "Geological Observations on South America", "Origin of Species (Everyman's University Paperbacks)", "The Correspondence of Charles Darwin, Volume 2", "Darwin's Ornithological notes", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Cartas de Darwin 18251859", "Darwin Compendium", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Origin of Species (Great Minds Series)", "Evolution", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The geology of the voyage of H.M.S. Beagle", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Correspondence of Charles Darwin, Volume 7", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 8", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Origin of Species (Variorum Reprint)", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Life of Erasmus Darwin", "Memorias y epistolario i\u0301ntimo", "The Darwin Reader Second Edition", "Charles Darwin's natural selection", "Voyage of the Beagle (Dover Value Editions)", "From So Simple a Beginning", "Les moyens d'expression chez les animaux", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 15", "Autobiography of Charles Darwin", "The portable Darwin", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 4", "The Autobiography of Charles Darwin (Large Print)", "The Descent of Man, and Selection in Relation to Sex", "The Voyage of the Beagle (Adventure Classics)", "Charles Darwin", "Darwin Darwin", "Proiskhozhdenie vidov", "Gesammelte kleinere Schriften", "The Origin of Species (Mentor)", "Darwinism stated by Darwin himself", "The descent of man and selection in relation to sex.", "The Variation of Animals and Plants under Domestication", "The descent of man, and selection in relation to sex", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N"], "ans_acc": 0.037383177570093455, "ans_hit": 1, "ans_f1": 0.06047516198704103, "ans_precission": 0.4, "ans_recall": 0.03271028037383177, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.07207207207207207, "path_ans_precision": 1.0, "path_ans_recall": 0.037383177570093455}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nNathan Whitaker", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nPerson", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.14285714285714285, "path_recall": 0.3333333333333333, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8t4c -> location.partial_containment_relationship.partially_contained_by -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8s1x -> location.partial_containment_relationship.partially_contained_by -> Poland\n# Answer:\nPoland"], "ground_truth": ["Europe"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> base.kwebbase.kwtopic.has_sentences -> (1821)\n# Answer:\n(1821)", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> base.kwebbase.kwtopic.has_sentences -> A meeting with the poet Southey, influenced Shelley greatly.\n# Answer:\nPercy Bysshe Shelley", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.ethnicity -> Irish people in Great Britain\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Poet", "Author", "Writer", "Bard"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.30769230769230765, "path_precision": 0.2222222222222222, "path_recall": 0.5, "path_ans_f1": 0.8750000000000001, "path_ans_precision": 0.7777777777777778, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nJudy Blundell", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.characters -> Obi-Wan Kenobi\n# Answer:\nObi-Wan Kenobi", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Stars War: Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.genre -> Science Fiction\n# Answer:\nThe Changing of the Guard"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nOntario", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nBillboard Music Award for Top Social Artist", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Year\n# Answer:\nYear", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Ceremony\n# Answer:\nCeremony"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> military.military_conflict.combatants -> m.059q_5c\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> military.military_conflict.combatants -> m.05t6dmh\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.0bhgc70\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> base.culturalevent.event.entity_involved -> H. R. McMaster\n# Answer:\nH. R. McMaster"], "ground_truth": ["United States of America", "Iraq", "Argentina", "United Kingdom", "Australia", "Saudi Arabia", "France"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5084745762711864, "ans_precission": 0.625, "ans_recall": 0.42857142857142855, "path_f1": 0.0923076923076923, "path_precision": 0.375, "path_recall": 0.05263157894736842, "path_ans_f1": 0.5084745762711864, "path_ans_precision": 0.625, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl67t -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nKids' Choice Award for Favorite TV Show", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\n2011 Kids' Choice Awards"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.2, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\n106th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["John Kasich", "Ted Strickland", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.borrowing_team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvddfb -> soccer.football_player_loan.borrowing_team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Tim Thomerson\n# Answer:\nTim Thomerson", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Armando Favela\n# Answer:\nArmando Favela", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0hk9n_4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> g.11b674hjl7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site -> common.topic.article -> m.04668xn\n# Answer:\ncommon.topic.article"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJacqueline Kennedy Onassis", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nbase.popstra.celebrity.friendship", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\n11/22/63", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 16 Questions on the Assassination\n# Answer:\nAssassination of John F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland"], "ground_truth": ["Wales", "England", "Northern Ireland", "Scotland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Red River of the South -> location.location.partially_containedby -> Oklahoma\n# Answer:\nOklahoma", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Red River of the South -> location.location.partially_containedby -> Texas\n# Answer:\nTexas", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Red River of the South -> common.topic.image -> HPIM2184\n# Answer:\nRed River of the South", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Red River of the South -> common.topic.image -> Red watershed\n# Answer:\nRed River of the South", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.partially_containedby -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.partially_containedby -> Illinois\n# Answer:\nIllinois"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> Jacques Lacan\n# Answer:\nJacques Lacan", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced -> John Gray\n# Answer:\nJohn Gray", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> Christiaan Huygens\n# Answer:\nChristiaan Huygens", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced -> Alister McGrath\n# Answer:\nAlister McGrath"], "ground_truth": ["Physician", "Philosopher", "Writer"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.606060606060606, "ans_precission": 0.5555555555555556, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.2222222222222222, "path_recall": 0.6666666666666666, "path_ans_f1": 0.606060606060606, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nSean Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\nJ. Holiday", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> Mama -> music.recording.artist -> Ghostface Killah\n# Answer:\nGhostface Killah", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan"], "ground_truth": ["Francine Lons", "Sal Gibson", "Leon Cole"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.16666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Atlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> base.usnationalparks.us_national_park.state -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nMontgomery", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nAlabama"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> 30135\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.042znwp\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x3mkg -> common.webpage.resource -> 'Twilight' cast to tour Nordstrom stores\n# Answer:\n'Twilight' cast to tour Nordstrom stores", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x3hp5 -> common.webpage.resource -> Rob Hardy to direct 'Stomp the Yard 2: Homecoming'\n# Answer:\nRob Hardy to direct 'Stomp the Yard 2: Homecoming'", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> g.11b7v_3l1h\n# Answer:\nArbor Place Mall"], "ground_truth": ["Omni Coliseum", "Cobb Energy Performing Arts Centre", "Georgia Aquarium", "Hyatt Regency Atlanta", "Georgia State Capitol", "Fernbank Museum of Natural History", "Six Flags White Water", "Arbor Place Mall", "Philips Arena", "Atlanta Cyclorama & Civil War Museum", "Atlanta Ballet", "Georgia Dome", "Martin Luther King, Jr. National Historic Site", "Atlanta History Center", "Fernbank Science Center", "Atlanta Symphony Orchestra", "Peachtree Road Race", "Zoo Atlanta", "World of Coca-Cola", "Variety Playhouse", "Masquerade", "Margaret Mitchell House & Museum", "CNN Center", "Georgia World Congress Center", "Atlanta Marriott Marquis", "Center for Puppetry Arts", "Centennial Olympic Park", "Turner Field", "The Tabernacle", "Fox Theatre", "Underground Atlanta", "Four Seasons Hotel Atlanta", "Woodruff Arts Center", "Jimmy Carter Library and Museum", "Atlanta Jewish Film Festival", "Six Flags Over Georgia"], "ans_acc": 0.05555555555555555, "ans_hit": 1, "ans_f1": 0.10126582278481013, "ans_precission": 0.5714285714285714, "ans_recall": 0.05555555555555555, "path_f1": 0.10309278350515463, "path_precision": 0.7142857142857143, "path_recall": 0.05555555555555555, "path_ans_f1": 0.10309278350515463, "path_ans_precision": 0.7142857142857143, "path_ans_recall": 0.05555555555555555}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2015\n# Answer:\nQueensland state election, 2015", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nQueensland state election, 2009", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\ncommon.image.size"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nGavin & Stacey", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Gavin & Stacey\n# Answer:\nGavin & Stacey", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 1]\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> media_common.literary_genre.books_in_this_genre -> Ivanhoe\n# Answer:\nIvanhoe", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> 100% Senorita\n# Answer:\n100% Senorita", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 2]\n# Answer:\nThur 10 June, 2010 [Episode 2]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nMultipart TV episode"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.location.contains -> Vaitupu\n# Answer:\nVaitupu", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.country.currency_used -> Tuvaluan dollar\n# Answer:\nTuvaluan dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.location.contains -> Filamona Moonlight Lodge\n# Answer:\nFilamona Moonlight Lodge", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_5 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.source -> GNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nGNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_5 -> measurement_unit.dated_money_value.source -> GNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nGNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nMaryland"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Place of birth\n# Answer:\nPlace of birth"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nHistory", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History of the Americas\n# Answer:\nHistory of the Americas", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola"], "ground_truth": ["Michael Peroutka", "Gene Amondson", "John Kerry", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f -> film.personal_film_appearance.film -> The War of the World: A New History of the 20th Century\n# Answer:\nThe War of the World: A New History of the 20th Century", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nThe Ascent of Money"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> San Crist\u00f3bal Canton\n# Answer:\nSan Crist\u00f3bal Canton", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.contains -> Puerto Baquerizo Moreno\n# Answer:\nPuerto Baquerizo Moreno"], "ground_truth": ["Gal\u00e1pagos Province", "Pacific Ocean", "Ecuador"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Wait for a Minute\n# Answer:\nWait for a Minute", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Carnatic Singer\n# Answer:\nCarnatic Singer", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> #thatPower\n# Answer:\n#thatPower", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Actor -> base.lightweight.profession.professions_similar -> Model\n# Answer:\nModel", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Actor -> music.special_music_video_performance_type.special_music_video_performances -> m.0z3vx9q\n# Answer:\nmusic.special_music_video_performance_type.special_music_video_performances"], "ground_truth": ["PYD", "All That Matters", "Bad Day", "All Bad", "Eenie Meenie", "Boyfriend", "Bigger", "Wait for a Minute", "Home to Mama", "All Around The World", "Thought Of You", "Change Me", "Heartbreaker", "Live My Life", "Die in Your Arms", "Lolly", "Never Let You Go", "Right Here", "Hold Tight", "#thatPower", "Somebody to Love", "Confident", "Turn to You (Mother's Day Dedication)", "Never Say Never", "First Dance", "Pray", "Beauty And A Beat", "Baby", "Roller Coaster", "Recovery", "As Long as You Love Me"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.10526315789473685, "ans_precission": 0.2857142857142857, "ans_recall": 0.06451612903225806, "path_f1": 0.052631578947368425, "path_precision": 0.14285714285714285, "path_recall": 0.03225806451612903, "path_ans_f1": 0.10526315789473685, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_for -> g.125cswvwv\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Physician", "Writer", "Journalist", "Publisher", "Statesman"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6666666666666666, "ans_recall": 0.4, "path_f1": 0.5, "path_precision": 0.6666666666666666, "path_recall": 0.4, "path_ans_f1": 0.5, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xw9mx -> common.webpage.resource -> 'Idol': Jordin or Blake? And why?\n# Answer:\n'Idol': Jordin or Blake? And why?", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09y4z3n -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05ckmy9\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Toyota Group\n# Answer:\nToyota Group", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05t5syf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nMahaka Media", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_lnf\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nbase.descriptive_names.names.descriptive_name"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_for -> g.125621qyv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nEnd time", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Christianity\n# Answer:\nChristianity"], "ground_truth": ["Monotheism", "Mahdi", "Qiyamah", "Prophets in Islam", "Tawhid", "Masih ad-Dajjal", "Islamic holy books", "Islamic view of angels", "Sharia", "Entering Heaven alive", "\u1e6c\u016bb\u0101", "God in Islam", "Predestination in Islam"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 0.2, "ans_recall": 0.07692307692307693, "path_f1": 0.2222222222222222, "path_precision": 0.4, "path_recall": 0.15384615384615385, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.4, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nJokes and Jokers", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A liberal is a power worshipper without the power. -> media_common.quotation.subjects -> Liberals\n# Answer:\nLiberals"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0w5qpxw -> military.military_command.military_conflict -> The Blitz\n# Answer:\nThe Blitz", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.04ls_tb -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.04ls_tb -> military.military_command.military_conflict -> Battle of the Bulge\n# Answer:\nBattle of the Bulge", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.33333333333333326, "path_precision": 0.42857142857142855, "path_recall": 0.2727272727272727, "path_ans_f1": 0.6, "path_ans_precision": 0.42857142857142855, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t -> common.webpage.resource -> Grammys expected to go on with Amy Winehouse, Beyonce\n# Answer:\nGrammys expected to go on with Amy Winehouse, Beyonce", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.resource -> Indicates the official home page for an artist.\n# Answer:\nIndicates the official home page for an artist."], "ground_truth": ["Singer", "Songwriter", "Actor"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.4, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5, "path_ans_precision": 0.4, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cv_4t\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2002 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cqh_0\n# Answer:\n2002 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cv_4y\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2002 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cqh_4\n# Answer:\n2002 Protection One 400"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nKansas City Monarchs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.season -> 1947 Major League Baseball season\n# Answer:\n1947 Major League Baseball season"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> people.profession.specializations -> Film Score Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nBook Edition", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Catholicism", "Protestantism", "Islam", "Hinduism"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.5, "path_recall": 0.4, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nMissouri", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of James A. Garfield\n# Answer:\nAssassination of James A. Garfield", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of William McKinley\n# Answer:\nAssassination of William McKinley", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> You never know what's hit you. A gunshot is the perfect way.\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> A shocking crime was committed on the unscrupulous initiative of few individuals, with the blessing of more, and amid the passive acquiescence of all.\n# Answer:\nAssassination"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nFilm character", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.02vdcn_\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nH. G. Wells", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.02tbg1c\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Ray Bradbury\n# Answer:\nRay Bradbury", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> Gustave Flaubert\n# Answer:\nGustave Flaubert"], "ground_truth": ["The Old Curiosity Shop", "Bleak House.", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Whole Story)", "Great Expectations", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Oxford Bookworms Library)", "The old curiosity shop", "A Christmas Carol (Pacemaker Classics)", "Dombey and Son.", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Bookcassette(r) Edition)", "Our mutual friend.", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Tor Classics)", "The Pickwick Papers", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Young Reading Series 2)", "A Christmas Carol (Watermill Classic)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Penguin Classics)", "A CHRISTMAS CAROL", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "Great expectations.", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Collector's Library)", "Little Dorrit", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Dover Thrift Editions)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (Soundings)", "David Copperfield", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Penguin Readers, Level 5)", "Great Expectations.", "The Pickwick papers", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "Great expectations", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol", "Bleak house", "A TALE OF TWO CITIES", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Dramascripts)", "David Copperfield.", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol. (Lernmaterialien)", "Our mutual friend", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (Classic Collection)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities", "The cricket on the hearth", "A Christmas Carol (Reissue)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Penguin Popular Classics)", "Hard times", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Cover to Cover)", "Oliver Twist", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (R)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Oxford Playscripts)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Value Books)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Cyber Classics)", "Martin Chuzzlewit", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Dramascripts Classic Texts)", "The mystery of Edwin Drood", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Silver Classics)", "Dombey and son", "The old curiosity shop.", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "Bleak House", "Dombey and Son", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Penguin Readers, Level 2)", "Sketches by Boz", "A Christmas Carol (Take Part)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Compact English Classics)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (Children's Classics)", "A Tale Of Two Cities (Adult Classics)"], "ans_acc": 0.01775147928994083, "ans_hit": 1, "ans_f1": 0.02162162162162162, "ans_precission": 0.125, "ans_recall": 0.011834319526627219, "path_f1": 0.0759493670886076, "path_precision": 0.125, "path_recall": 0.05454545454545454, "path_ans_f1": 0.02162162162162162, "path_ans_precision": 0.125, "path_ans_recall": 0.011834319526627219}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n58th Army", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> Chechens\n# Answer:\nChechens", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> military.military_conflict.combatants -> m.064ykvf\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> base.culturalevent.event.entity_involved -> Moscow Metro\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> common.topic.article -> m.05b0fkx\n# Answer:\n1977 Moscow bombings"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nReal Estate Investment Trust", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Retail\n# Answer:\nRetail", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Board members\n# Answer:\nBoard members", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.legal_structure -> Limited liability company\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Industry\n# Answer:\nIndustry"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.actor -> Edward Mulhare\n# Answer:\nEdward Mulhare", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.character -> Devon Miles\n# Answer:\nDevon Miles", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> tv.tv_series_season.episodes -> A Nice, Indecent Little Town\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Knight Sounds Feature\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> tv.tv_series_season.episodes -> A Plush Ride\n# Answer:\nKnight Rider - Season 1"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nJasper County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.ceremony -> 51st Primetime Emmy Awards\n# Answer:\n51st Primetime Emmy Awards"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.16666666666666666, "path_recall": 0.125, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_conflict -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\nmilitary.military_commander.military_commands", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> time.event.locations -> United States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> time.event.locations -> Western United States\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> time.event.includes_event -> Battle of White Oak Swamp\n# Answer:\nBattle of White Oak Swamp", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> time.event.includes_event -> Battle of Savage's Station\n# Answer:\nBattle of Savage's Station", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> military.military_conflict.military_personnel_involved -> George B. McClellan\n# Answer:\nSeven Days Battles", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War"], "ground_truth": ["Battle of Chancellorsville", "Battle of White Oak Swamp", "Battle of Cedar Mountain", "American Civil War", "How Few Remain", "First Battle of Winchester", "Battle of Front Royal", "Battle of Chantilly", "Manassas Station Operations", "Second Battle of Bull Run", "Battle of Harpers Ferry", "Romney Expedition", "Battle of Port Republic", "First Battle of Kernstown", "Jackson's Valley Campaign", "Battle of McDowell", "Battle of Hancock", "Battle of Hoke's Run", "First Battle of Rappahannock Station"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.25531914893617025, "ans_precission": 0.6666666666666666, "ans_recall": 0.15789473684210525, "path_f1": 0.13071895424836602, "path_precision": 0.5555555555555556, "path_recall": 0.07407407407407407, "path_ans_f1": 0.25531914893617025, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Sarah Davenport\n# Answer:\nSarah Davenport", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nAnne Harris", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nEcton", "# Reasoning Path:\nBenjamin Franklin\n# Answer:\nSarah Davenport", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\ncommon.image.size", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\ncommon.image.size"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Mona Lisa\n# Answer:\nMona Lisa", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Raphael Cartoons\n# Answer:\nRaphael Cartoons", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Stanze di Raffaello Frescoes\n# Answer:\nStanze di Raffaello Frescoes", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced -> Parmigianino\n# Answer:\nParmigianino", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> base.kwebbase.kwtopic.has_sentences -> Although neither fresco was ever finished, Leonardo's \\\"Battle of Anghiari\\\" and Michelangelo's \\\"Battle of Cascina\\\" had a great influence on the many students, including Raphael, who went to view them and make copies.\n# Answer:\nRaphael", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.artworks -> Noli Me Tangere\n# Answer:\nNoli Me Tangere", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> base.kwebbase.kwtopic.has_sentences -> At this time, Raphael went in search of new influences.\n# Answer:\nAt this time, Raphael went in search of new influences.", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced -> Annibale Carracci\n# Answer:\nAnnibale Carracci", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.artworks -> Adoration of the Child\n# Answer:\nAdoration of the Child"], "ground_truth": ["Sala delle Asse", "Bacchus", "Adoration of the Magi", "Madonna of Laroque", "g.1213jb_b", "St. John the Baptist", "The Virgin and Child with St. Anne", "La belle ferronni\u00e8re", "Portrait of a man in red chalk", "Lucan portrait of Leonardo da Vinci", "Virgin of the Rocks", "g.121wt37c", "g.121yh91r", "The Battle of Anghiari", "The Last Supper", "Portrait of a Young Fianc\u00e9e", "g.1224tf0c", "Portrait of a Musician", "Head of a Woman", "Drapery for a Seated Figure", "Madonna and Child with St Joseph", "g.1239jd9p", "Lady with an Ermine", "Mona Lisa", "Leonardo's horse", "Leda and the Swan", "Benois Madonna", "Medusa", "Madonna of the Carnation", "g.12314dm1", "Annunciation", "The Virgin and Child with St Anne and St John the Baptist", "g.1219sb0g", "g.12215rxg", "Vitruvian Man", "Ginevra de' Benci", "The Holy Infants Embracing", "Horse and Rider", "Madonna Litta", "Salvator Mundi", "g.120vt1gz", "The Baptism of Christ", "Portrait of Isabella d'Este", "Madonna of the Yarnwinder", "St. Jerome in the Wilderness"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.07272727272727272, "ans_precission": 0.2, "ans_recall": 0.044444444444444446, "path_f1": 0.07407407407407407, "path_precision": 0.2, "path_recall": 0.045454545454545456, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxk\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.parent_disease -> Genetic disorder\n# Answer:\nGenetic disorder", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Complications of pregnancy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Eva Per\u00f3n habla a las mujeres -> common.topic.notable_for -> g.1jmcbz5gb\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5454545454545454, "ans_precission": 0.375, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Buddhist meditation\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Vipassan\u0101\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Lightning rod", "Franklin stove", "Bifocals", "Glass harmonica"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.6666666666666666, "ans_recall": 0.5, "path_f1": 0.5714285714285715, "path_precision": 0.6666666666666666, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Detski\u012d al'bom -> common.topic.notable_for -> g.12578_08h\n# Answer:\nDetski\u012d al'bom", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11"], "ground_truth": ["Musician", "Composer", "Librettist"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0129m4b3 -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0129m4b3 -> education.education.student -> Daniel I Block\n# Answer:\nDaniel I Block", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["Switzerland", "Belgium", "Austria", "Luxembourg", "Liechtenstein", "East Germany", "Germany"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.3333333333333333, "ans_recall": 0.2857142857142857, "path_f1": 0.15384615384615383, "path_precision": 0.16666666666666666, "path_recall": 0.14285714285714285, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\nBroadcast Content", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Classic hits\n# Answer:\nClassic hits", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\nBroadcast Content"], "ground_truth": ["Rock music", "Experimental rock", "Pop music", "Art rock", "Pop rock", "Psychedelic rock", "Blues rock", "Experimental music", "Soft rock"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.27906976744186046, "ans_precission": 0.375, "ans_recall": 0.2222222222222222, "path_f1": 0.23529411764705882, "path_precision": 0.25, "path_recall": 0.2222222222222222, "path_ans_f1": 0.27906976744186046, "path_ans_precision": 0.375, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpg7j -> government.government_position_held.office_holder -> Thomas M. Bowen\n# Answer:\nThomas M. Bowen", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\nMilitary unit", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\nMilitary unit"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> King County\n# Answer:\nKing County", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.geolocation -> m.0wmhrx7\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["98103", "98184", "98175", "98119-4114", "98174", "98161", "98102", "98117", "98181", "98148", "98109", "98121", "98146", "98138", "98132", "98107", "98124", "98198", "98158", "98139", "98126", "98171", "98119", "98106", "98127", "98134", "98125", "98165", "98155", "98115", "98122", "98191", "98141", "98114", "98199", "98188", "98116", "98166", "98108", "98118", "98160", "98101", "98177", "98105", "98133", "98136", "98113", "98168", "98194", "98195", "98129", "98164", "98131", "98190", "98185", "98154", "98145", "98144", "98170", "98111", "98112", "98178", "98104"], "ans_acc": 0.031746031746031744, "ans_hit": 1, "ans_f1": 0.057142857142857134, "ans_precission": 0.2857142857142857, "ans_recall": 0.031746031746031744, "path_f1": 0.057142857142857134, "path_precision": 0.2857142857142857, "path_recall": 0.031746031746031744, "path_ans_f1": 0.057142857142857134, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.031746031746031744}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nRobsol Pinkett, Jr.", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nAdrienne Banfield-Jones", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Jaden Smith\n# Answer:\nJaden Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Willard Carroll Trey Smith III\n# Answer:\nWillard Carroll Trey Smith III", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.0wpt66n\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music -> broadcast.genre.content -> #Musik.Club on RauteMusik.FM\n# Answer:\n#Musik.Club on RauteMusik.FM"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.5454545454545454, "path_precision": 0.375, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> The China Story\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Empire of Japan\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia"], "ground_truth": ["Simplified Chinese character", "'Phags-pa script", "Chinese characters", "N\u00fcshu script", "Traditional Chinese characters"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.3333333333333333, "ans_recall": 0.4, "path_f1": 0.3636363636363636, "path_precision": 0.3333333333333333, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.4}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Sarah Ann Nixon\n# Answer:\nSarah Ann Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.125, "path_recall": 0.14285714285714285, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0v4v237 -> award.award_honor.award_winner -> Larry Harris\n# Answer:\nLarry Harris", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shttt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.ceremony -> 16th NAACP Image Awards\n# Answer:\n16th NAACP Image Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtx3 -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtx3 -> award.award_nomination.ceremony -> 32nd Primetime Emmy Awards\n# Answer:\n32nd Primetime Emmy Awards"], "ground_truth": ["Zara Cully", "Berlinda Tolbert", "Isabel Sanford", "Paul Benedict", "Marla Gibbs", "Damon Evans", "Mike Evans", "Sherman Hemsley", "Franklin Cover", "Roxie Roker", "Jay Hammer"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.23529411764705885, "ans_precission": 0.3333333333333333, "ans_recall": 0.18181818181818182, "path_f1": 0.061538461538461535, "path_precision": 0.3333333333333333, "path_recall": 0.03389830508474576, "path_ans_f1": 0.23529411764705885, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Call\n# Answer:\nSan Francisco Call", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Don't Call it Frisco -> common.topic.notable_for -> g.125gwqscg\n# Answer:\nDon't Call it Frisco", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> Suspense\n# Answer:\nSuspense", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_for -> g.1258179ql\n# Answer:\n1906"], "ground_truth": ["The Daily Alta California", "The San Francisco Examiner", "The Golden Era", "California Star", "Street Sheet", "San Francisco Bay Guardian", "San Francisco Bay Times", "San Francisco Bay View", "Synapse", "AsianWeek", "San Francisco Call", "San Francisco Business Times", "San Francisco Chronicle", "Free Society", "Bay Area Reporter", "San Francisco Foghorn", "Dock of the Bay", "San Francisco Daily", "San Francisco News-Call Bulletin Newspaper", "Sing Tao Daily"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.3333333333333333, "ans_recall": 0.1, "path_f1": 0.15384615384615383, "path_precision": 0.3333333333333333, "path_recall": 0.1, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.1}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> geography.river.basin_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg9k2c\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg97p1\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_containedby -> Turkey\n# Answer:\nTurkey"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> Transmural Myocardial Infarction\n# Answer:\nTransmural Myocardial Infarction", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination in ways which appear natural", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> ST segment elevation myocardial infarction\n# Answer:\nST segment elevation myocardial infarction", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nAortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nAbdominal aortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.04j64q8\n# Answer:\nbase.gender.gender_identity.people", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nbase.gender.gender_identity.people"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1837-1843", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nEvolution", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Paleontology\n# Answer:\nPaleontology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nCambridgeshire", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nUnited Kingdom"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 11: 1863", "The education of Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Structure and Distribution of Coral Reefs", "On the origin of species by means of natural selection", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Voyage of the Beagle", "Darwin on humus and the earthworm", "To the members of the Down Friendly Club", "The Orgin of Species", "Works", "The action of carbonate of ammonia on the roots of certain plants", "Geological Observations on the Volcanic Islands", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Notes on the fertilization of orchids", "On a remarkable bar of sandstone off Pernambuco", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Tesakneri tsagume\u030c", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Evolution and natural selection", "Resa kring jorden", "A student's introduction to Charles Darwin", "Reise um die Welt 1831 - 36", "The living thoughts of Darwin", "Charles Darwin's letters", "On the Movements and Habits of Climbing Plants", "Diario del Viaje de Un Naturalista Alrededor", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Darwin's notebooks on transmutation of species", "Darwin's insects", "Motsa ha-minim", "Part I: Contributions to the Theory of Natural Selection / Part II", "genese\u014ds t\u014dn eid\u014dn", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Darwin for Today", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Life and Letters of Charles Darwin Volume 2", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 14: 1866", "The collected papers of Charles Darwin", "Evolutionary Writings: Including the Autobiographies", "The Correspondence of Charles Darwin, Volume 15: 1867", "South American Geology", "La facult\u00e9 motrice dans les plantes", "The Essential Darwin", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Het uitdrukken van emoties bij mens en dier", "The principal works", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Beagle letters", "monograph on the sub-class Cirripedia", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Rejse om jorden", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Charles Darwin on the routes of male humble bees", "Origins", "ontstaan der soorten door natuurlijke teeltkeus", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 9: 1861", "On Natural Selection", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Evolution by natural selection", "Volcanic Islands", "Human nature, Darwin's view", "Insectivorous Plants", "The Different Forms of Flowers on Plants of the Same Species", "The Darwin Reader First Edition", "On the tendency of species to form varieties", "La vie et la correspondance de Charles Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The\u0301orie de l'e\u0301volution", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Les mouvements et les habitudes des plantes grimpantes", "Charles Darwin's marginalia", "Notebooks on transmutation of species", "Darwin from Insectivorous Plants to Worms", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The foundations of the Origin of species", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwin", "Darwin en Patagonia", "Darwin and Henslow", "Metaphysics, Materialism, & the evolution of mind", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Leben und Briefe von Charles Darwin", "The voyage of Charles Darwin", "Questions about the breeding of animals", "Die geschlechtliche Zuchtwahl", "Diary of the voyage of H.M.S. Beagle", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Formation of Vegetable Mould through the Action of Worms", "Reise eines Naturforschers um die Welt", "Fertilisation of Orchids", "H.M.S. Beagle in South America", "The Correspondence of Charles Darwin, Volume 8: 1860", "More Letters of Charles Darwin", "The Life and Letters of Charles Darwin Volume 1", "Darwin's journal", "From so simple a beginning", "On evolution", "A Darwin Selection", "The Autobiography of Charles Darwin", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "red notebook of Charles Darwin", "Geological Observations on South America", "Darwin's Ornithological notes", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Cartas de Darwin 18251859", "Darwin Compendium", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Evolution", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The geology of the voyage of H.M.S. Beagle", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 16: 1868", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Life of Erasmus Darwin", "Memorias y epistolario i\u0301ntimo", "The Darwin Reader Second Edition", "Charles Darwin's natural selection", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Les moyens d'expression chez les animaux", "The Power of Movement in Plants", "The portable Darwin", "From Darwin's unpublished notebooks", "The Descent of Man, and Selection in Relation to Sex", "Charles Darwin", "Darwin Darwin", "Proiskhozhdenie vidov", "Gesammelte kleinere Schriften", "Darwinism stated by Darwin himself", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Variation of Animals and Plants under Domestication", "El Origin De Las Especies", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.060120240480961935, "ans_precission": 0.375, "ans_recall": 0.032679738562091505, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> You've Really Got a Hold on Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nGrammy Legend Award", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone"], "ground_truth": ["You Are So Beautiful (feat. Dave Koz)", "The Tears of a Clown", "The Tracks of My Heart", "Let Me Be the Clock", "It's A Good Night", "Close Encounters of the First Kind", "Ooo Baby Baby", "Don't Know Why", "Jesus Told Me To Love You", "No\u00ebl", "Satisfy You", "In My Corner", "Why Are You Running From My Love", "Tracks Of My Tears (Live)", "Ebony Eyes (Duet with Rick James)", "I Care About Detroit", "And I Don't Love You (Larry Levan instrumental dub)", "Going to a Gogo", "Did You Know (Berry's Theme)", "Cruisin'", "Jasmin", "Don't Wanna Be Just Physical", "Blame It On Love (Duet with Barbara Mitchell)", "Little Girl, Little Girl", "Share It", "Asleep on My Love", "Be Careful What You Wish For", "Yes It's You Lady", "Double Good Everything", "Jingle Bells", "Some People Will Do Anything for Love", "You Cannot Laugh Alone", "Never My Love / Never Can Say Goodbye", "No Time to Stop Believing", "Season's Greetings from Smokey Robinson", "Who's Sad", "There Will Come a Day (I'm Gonna Happen to You)", "The Family Song", "More Love", "Unless You Do It Again", "Going to a Go-Go", "Fulfill Your Need", "Just Passing Through", "Tracks of my Tears", "If You Want My Love", "One Heartbeat", "I've Got You Under My Skin", "It's Her Turn to Live", "Open", "Deck the Halls", "You've Really Go a Hold on Me", "What's Too Much", "And I Don't Love You", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Just Another Kiss", "Holly", "The Agony And The Ecstasy", "I Love Your Face", "With Your Love Came", "Aqui Con Tigo (Being With You)", "You Are Forever", "My Guy", "Heavy On Pride (Light On Love)", "Please Don't Take Your Love (feat. Carlos Santana)", "You Really Got a Hold on Me", "The Hurt's On You", "The Agony and the Ecstasy", "Standing On Jesus", "Get Ready", "I Have Prayed On It", "Yester Love", "My Girl", "Crusin'", "Hanging on by a Thread", "Fly Me to the Moon (In Other Words)", "I Like Your Face", "You've Really Got a Hold on Me", "Just My Soul Responding", "You Don't Know What It's Like", "Baby Come Close", "I Want You Back", "A Silent Partner in a Three-Way Love Affair", "It's Time to Stop Shoppin' Around", "Going to a Go Go", "Te Quiero Como Si No Hubiera Un Manana", "Please Come Home for Christmas", "I Can't Get Enough", "Santa Claus is Coming to Town", "I Know You by Heart", "Will You Love Me Tomorrow?", "Sweet Harmony", "If You Wanna Make Love", "You Made Me Feel Love", "Little Girl Little Girl", "Tears Of A Clown", "Wedding Song", "Ooh Baby Baby", "I've Made Love to You a Thousand Times", "I Can't Give You Anything but Love", "Pops, We Love You", "Blame It on Love", "I'm in the Mood for Love", "Be Who You Are", "Tears of a Sweet Free Clown", "And I Love Her", "Gone Forever", "The Christmas Song", "Ooo Baby Baby (live)", "Tea for Two", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Rack Me Back", "So Bad", "We Are The Warriors", "Why Do Happy Memories Hurt So Bad", "Can't Fight Love", "Time Flies", "Speak Low", "Whatcha Gonna Do", "We\u2019ve Come Too Far to End It Now", "It's Christmas Time", "She's Only a Baby Herself", "Away in the Manger / Coventry Carol", "Mickey's Monkey", "Really Gonna Miss You", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Our Love Is Here to Stay", "Be Kind to the Growing Mind", "Quiet Storm (single version)", "Skid Row", "Rewind", "Girl I'm Standing There", "Why", "The Tracks Of My Tears", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Save Me", "Train of Thought", "The Track of My Tears", "Same Old Love", "Love Don' Give No Reason (12 Inch Club Mix)", "There Will Come A Day ( I'm Gonna Happen To You )", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Keep Me", "Medley: Never My Love / Never Can Say Goodbye", "Shoe Soul", "Girlfriend", "Be Careful What You Wish For (instrumental)", "The Tracks of My Tears", "The Tracks of My Tears (live)", "Night and Day", "Photograph in My Mind", "Walk on By", "Pops, We Love You (disco)", "Tell Me Tomorrow", "Christmas Everyday", "Hold on to Your Love", "The Way You Do (The Things You Do)", "Because of You It's the Best It's Ever Been", "Don't Play Another Love Song", "One Time", "Melody Man", "Just Like You", "It's Fantastic", "Will You Love Me Tomorrow", "Time After Time", "Ain't That Peculiar", "If You Wanna Make Love (Come 'round Here)", "Fallin'", "Quiet Storm (Groove Boutique Chill Jazz mix)", "Gang Bangin'", "Everything for Christmas", "Food For Thought", "When A Woman Cries", "Come by Here (Kum Ba Ya)", "Winter Wonderland", "I Am I Am", "Just To See Her Again", "You're the One for Me (feat. Joss Stone)", "Quiet Storm (Groove Boutique remix)", "Happy (Love Theme From Lady Sings the Blues)", "I've Made Love To You A Thousand Times", "I Can't Find", "I Praise & Worship You Father", "Come to Me Soon", "Crusin", "Cruisin", "That Place", "Ebony Eyes", "Virgin Man", "I Love The Nearness Of You", "Let Your Light Shine On Me", "Sleepless Nights", "Tears of a Clown", "Noel", "God Rest Ye Merry Gentlemen", "Quiet Storm", "Everything You Touch", "Bad Girl", "I Can\u2019t Stand to See You Cry (Commercial version)", "More Than You Know", "You're Just My Life (feat. India.Arie)", "Love Don't Give No Reason", "Being With You", "Tell Me Tomorrow, Part 1", "I\u2019ve Got You Under My Skin", "The Road to Damascus", "Just a Touch Away", "Christmas Every Day", "You Take Me Away", "Theme From the Big Time", "Ever Had A Dream", "Easy", "I'll Keep My Light In My Window", "Daylight & Darkness", "Coincidentally", "Let Me Be The Clock", "Vitamin U", "Mother's Son", "Love So Fine", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "My World", "Baby That's Backatcha", "Tell Me Tomorrow (12\\\" extended mix)", "The Love Between Me and My Kids", "A Tattoo", "Wishful Thinking", "Just to See Her", "Wanna Know My Mind", "You Go to My Head", "Will You Still Love Me Tomorrow", "I Hear The Children Singing", "Shop Around", "A Child Is Waiting", "When Smokey Sings Tears Of A Clown", "He Can Fix Anything", "Love Brought Us Here", "I Second That Emotions", "I Am, I Am", "Nearness of You", "Take Me Through The Night", "Be Kind To The Growing Mind (with The Temptations)", "I Second That Emotion", "It's a Good Feeling", "Love Is The Light", "Christmas Greeting", "Love Letters", "Driving Thru Life in the Fast Lane", "(It's The) Same Old Love", "Love' n Life", "If You Can Want", "The Tears Of A Clown", "We've Saved the Best for Last", "Tracks of My Tears", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Love Bath", "I'm Glad There Is You", "As You Do"], "ans_acc": 0.00390625, "ans_hit": 1, "ans_f1": 0.007633587786259542, "ans_precission": 0.16666666666666666, "ans_recall": 0.00390625, "path_f1": 0.023668639053254437, "path_precision": 0.16666666666666666, "path_recall": 0.012738853503184714, "path_ans_f1": 0.007633587786259542, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.00390625}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> The Death of a President\n# Answer:\nThe Death of a President", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Oswald's Tale\n# Answer:\nOswald's Tale", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbania"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
