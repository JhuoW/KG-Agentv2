{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\nLouisiana", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Puerto Rico\n# Answer:\nPuerto Rico", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Cyclone", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Haiti\n# Answer:\nHaiti", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.category -> Category 4 Hurricane (SSHS)\n# Answer:\n1900 Galveston hurricane"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specialization_of -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nTitle", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician"], "ground_truth": ["Speaker of the United States House of Representatives", "United States Representative", "Governor of Tennessee"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.125, "ans_recall": 0.3333333333333333, "path_f1": 0.14285714285714288, "path_precision": 0.125, "path_recall": 0.16666666666666666, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.125, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\nlocation.statistical_region.energy_use_per_capita"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.actor -> Ilyssa Fradin\n# Answer:\nIlyssa Fradin", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.actor -> Melinda McGraw\n# Answer:\nMelinda McGraw", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Advocate\n# Answer:\nAdvocate", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nA Cold Christmas", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Attorney at law\n# Answer:\nAttorney at law", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Darkness More Than Night\n# Answer:\nA Darkness More Than Night", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pk4l\n# Answer:\nbase.descriptive_names.names.descriptive_name"], "ground_truth": ["Ilyssa Fradin", "Hannah Gunn", "Melinda McGraw"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.season -> 1995\u201396 NBA season\n# Answer:\n1995\u201396 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.season -> 2003\u201304 NBA season\n# Answer:\n2003\u201304 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter"], "ground_truth": ["Orlando Magic", "Miami Heat", "Cleveland Cavaliers", "Phoenix Suns", "Los Angeles Lakers", "LSU Tigers men's basketball", "Boston Celtics"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.38095238095238093, "ans_precission": 0.5714285714285714, "ans_recall": 0.2857142857142857, "path_f1": 0.21621621621621623, "path_precision": 0.5714285714285714, "path_recall": 0.13333333333333333, "path_ans_f1": 0.38095238095238093, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> Mulberry Street NYC c1900 LOC 3g04637u edit\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> European American\n# Answer:\nEuropean American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.04lybjz\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> Scottish people\n# Answer:\nScottish people", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.image -> William paca\n# Answer:\nWilliam paca"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Mon Language", "Thai Language", "Akha Language", "Lao Language", "Vietnamese Language", "Mlabri Language", "Phu Thai language", "Saek language", "Khmer language", "Malay, Pattani Language", "Hmong language", "Nyaw Language", "Cham language"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.23529411764705882, "ans_precission": 0.5, "ans_recall": 0.15384615384615385, "path_f1": 0.23529411764705882, "path_precision": 0.5, "path_recall": 0.15384615384615385, "path_ans_f1": 0.23529411764705882, "path_ans_precision": 0.5, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist"], "ground_truth": ["Lawyer", "Author", "Archaeologist", "Writer", "Farmer", "Teacher", "Statesman", "Inventor", "Architect", "Philosopher"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.3333333333333333, "ans_recall": 0.2, "path_f1": 0.28571428571428575, "path_precision": 0.5, "path_recall": 0.2, "path_ans_f1": 0.28571428571428575, "path_ans_precision": 0.5, "path_ans_recall": 0.2}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nEmma Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin -> people.person.sibling_s -> m.0w3qj26\n# Answer:\npeople.person.sibling_s", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.sibling_s -> m.0w3qcv1\n# Answer:\npeople.person.sibling_s", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.sibling_s -> m.0w3qcvb\n# Answer:\npeople.person.sibling_s", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin -> people.person.sibling_s -> m.0w3lyw0\n# Answer:\nCharles Waring Darwin", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Charles Waring Darwin -> people.family_member.family -> Darwin\u2013Wedgwood family\n# Answer:\nDarwin\u2013Wedgwood family"], "ground_truth": ["The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Evolution by natural selection", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 8", "Voyage of the Beagle (NG Adventure Classics)", "On a remarkable bar of sandstone off Pernambuco", "The Autobiography of Charles Darwin", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Correspondence of Charles Darwin, Volume 2", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin for Today", "A student's introduction to Charles Darwin", "Kleinere geologische Abhandlungen", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Descent of Man and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Origins", "Reise eines Naturforschers um die Welt", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 12: 1864", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Tesakneri tsagume\u030c", "A Darwin Selection", "genese\u014ds t\u014dn eid\u014dn", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Charles Darwin's marginalia", "The Structure and Distribution of Coral Reefs", "La vie et la correspondance de Charles Darwin", "The Origin Of Species", "Proiskhozhdenie vidov", "Voyage Of The Beagle", "Darwin's insects", "The Origin of Species (Oxford World's Classics)", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 6", "Human nature, Darwin's view", "The geology of the voyage of H.M.S. Beagle", "The action of carbonate of ammonia on the roots of certain plants", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "The Voyage of the Beagle (Everyman Paperbacks)", "Resa kring jorden", "The voyage of Charles Darwin", "The Origin of Species (Great Books : Learning Channel)", "From Darwin's unpublished notebooks", "Origin of Species", "The Origin of Species (Variorum Reprint)", "The Expression Of The Emotions In Man And Animals", "Darwinism stated by Darwin himself", "Darwin's notebooks on transmutation of species", "The Origin of Species (Great Minds Series)", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Geological Observations on South America", "The Origin of Species (World's Classics)", "Voyage of the Beagle (Dover Value Editions)", "The Expression of the Emotions in Man And Animals", "The Correspondence of Charles Darwin, Volume 5", "Part I: Contributions to the Theory of Natural Selection / Part II", "Origin of Species (Everyman's University Paperbacks)", "The Autobiography of Charles Darwin (Large Print)", "The descent of man, and selection in relation to sex.", "From so simple a beginning", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "The Descent of Man, and Selection in Relation to Sex", "The origin of species", "Darwin en Patagonia", "The Life of Erasmus Darwin", "Notebooks on transmutation of species", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Darwin Reader First Edition", "Questions about the breeding of animals", "The Voyage of the Beagle", "Les moyens d'expression chez les animaux", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Opsht\u0323amung fun menshen", "The Origin of Species", "Cartas de Darwin 18251859", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Darwin's journal", "red notebook of Charles Darwin", "Diary of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Works", "The descent of man, and selection in relation to sex", "The foundations of the Origin of species", "Darwin and Henslow", "The Structure And Distribution of Coral Reefs", "The autobiography of Charles Darwin", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Darwin's Ornithological notes", "The principal works", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 13", "The Variation of Animals and Plants under Domestication", "H.M.S. Beagle in South America", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Voyage of the Beagle (Adventure Classics)", "The Power of Movement in Plants", "Beagle letters", "Rejse om jorden", "The Formation of Vegetable Mould through the Action of Worms", "The\u0301orie de l'e\u0301volution", "Diario del Viaje de Un Naturalista Alrededor", "On Natural Selection", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 14: 1866", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 9", "The Autobiography of Charles Darwin, and selected letters", "Evolution and natural selection", "The Correspondence of Charles Darwin, Volume 8: 1860", "On evolution", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Essential Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Darwin Darwin", "Voyage of the Beagle", "The Voyage of the Beagle (Unabridged Classics)", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The living thoughts of Darwin", "Charles Darwin on the routes of male humble bees", "Die geschlechtliche Zuchtwahl", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The Autobiography Of Charles Darwin", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Correspondence of Charles Darwin, Volume 1", "The Correspondence of Charles Darwin, Volume 10", "The Autobiography of Charles Darwin (Dodo Press)", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "More Letters of Charles Darwin", "On the origin of species by means of natural selection", "The Origin of Species (Collector's Library)", "Les mouvements et les habitudes des plantes grimpantes", "Fertilisation of Orchids", "Charles Darwin's letters", "The autobiography of Charles Darwin, 1809-1882", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 11", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Autobiography of Charles Darwin (Great Minds Series)", "Memorias y epistolario i\u0301ntimo", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 7", "The Autobiography of Charles Darwin [EasyRead Edition]", "Darwin Compendium", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 12", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Origin of Species (Harvard Classics, Part 11)", "Darwin-Wallace", "The descent of man and selection in relation to sex.", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "monograph on the sub-class Cirripedia", "The origin of species : complete and fully illustrated", "Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Leben und Briefe von Charles Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Volcanic Islands", "Voyage d'un naturaliste autour du monde", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Correspondence of Charles Darwin, Volume 18: 1870", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 15: 1867", "The collected papers of Charles Darwin", "The Correspondence of Charles Darwin, Volume 15", "The Voyage of the Beagle (Mentor)", "The Correspondence of Charles Darwin, Volume 14", "Del Plata a Tierra del Fuego", "The Different Forms of Flowers on Plants of the Same Species", "Metaphysics, Materialism, & the evolution of mind", "The Correspondence of Charles Darwin, Volume 11: 1863", "The expression of the emotions in man and animals", "The Expression of the Emotions in Man and Animals", "El Origin De Las Especies", "The Origin of Species (Mentor)", "The Correspondence of Charles Darwin, Volume 13: 1865", "The education of Darwin", "The portable Darwin", "Die fundamente zur entstehung der arten", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 10: 1862", "Voyage of the Beagle (Harvard Classics, Part 29)", "Wu zhong qi yuan", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The structure and distribution of coral reefs.", "Darwin on humus and the earthworm", "On the Movements and Habits of Climbing Plants", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Reise um die Welt 1831 - 36", "The Darwin Reader Second Edition", "The Orgin of Species", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Evolution", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The structure and distribution of coral reefs"], "ans_acc": 0.009345794392523364, "ans_hit": 1, "ans_f1": 0.027366020524515394, "ans_precission": 0.5714285714285714, "ans_recall": 0.014018691588785047, "path_f1": 0.06666666666666667, "path_precision": 1.0, "path_recall": 0.034482758620689655, "path_ans_f1": 0.018518518518518517, "path_ans_precision": 1.0, "path_ans_recall": 0.009345794392523364}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nNathan Whitaker", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nPerson", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.14285714285714285, "path_recall": 0.3333333333333333, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8t4c -> location.partial_containment_relationship.partially_contained_by -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Ukraine\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8s1x -> location.partial_containment_relationship.partially_contained_by -> Poland\n# Answer:\nPoland"], "ground_truth": ["Europe"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Novelist\n# Answer:\nNovelist", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> base.kwebbase.kwtopic.has_sentences -> (1821)\n# Answer:\n(1821)", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.ethnicity -> Irish people in Great Britain\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> base.kwebbase.kwtopic.has_sentences -> A meeting with the poet Southey, influenced Shelley greatly.\n# Answer:\nPercy Bysshe Shelley"], "ground_truth": ["Writer", "Author", "Poet", "Bard"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.6382978723404256, "ans_precission": 0.5555555555555556, "ans_recall": 0.75, "path_f1": 0.30769230769230765, "path_precision": 0.2222222222222222, "path_recall": 0.5, "path_ans_f1": 0.7058823529411765, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nJudy Blundell", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.characters -> Obi-Wan Kenobi\n# Answer:\nObi-Wan Kenobi", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.part_of_series -> Stars War: Jedi Quest\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.genre -> Science Fiction\n# Answer:\nThe Changing of the Guard"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2, "path_precision": 0.125, "path_recall": 0.5, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nOntario", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nBillboard Music Award for Top Social Artist", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award category\n# Answer:\nAward category", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> freebase.valuenotation.is_reviewed -> Award winner\n# Answer:\nAward winner"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.0bhgc70\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.includes_event -> Operation Southern Watch\n# Answer:\nIraqi no-fly zones", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> military.military_conflict.combatants -> m.04y_t_s\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> base.culturalevent.event.entity_involved -> Saddam Hussein\n# Answer:\nSaddam Hussein", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.includes_event -> Operation Airone\n# Answer:\nOperation Airone", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> base.culturalevent.event.entity_involved -> Iraqi Army\n# Answer:\nIraqi Army", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> common.topic.notable_for -> g.125cxty6s\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["United States of America", "Argentina", "United Kingdom", "Saudi Arabia", "Australia", "Iraq", "France"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.05970149253731343, "path_precision": 0.2, "path_recall": 0.03508771929824561, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl67t -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl70y -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nKids' Choice Award for Favorite TV Show"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.25, "path_recall": 0.5, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\n106th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.borrowing_team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvddfb -> soccer.football_player_loan.borrowing_team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.3333333333333333, "path_precision": 0.4, "path_recall": 0.2857142857142857, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Armando Favela\n# Answer:\nArmando Favela", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Bill Harrison\n# Answer:\nBill Harrison", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0k6n97c\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0k6n97l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site -> common.topic.article -> m.04668xn\n# Answer:\ncommon.topic.article"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJacqueline Kennedy Onassis", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nbase.popstra.celebrity.friendship", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> The Death of a President\n# Answer:\nThe Death of a President", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> 11/22/63\n# Answer:\n11/22/63"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.8, "path_precision": 0.6666666666666666, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland"], "ground_truth": ["Wales", "Northern Ireland", "Scotland", "England"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.5, "path_precision": 0.5, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_containedby -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8_r5\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmv2\n# Answer:\nlocation.location.partially_contained_by", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmvd\n# Answer:\nBayou Bartholomew"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> Jacques Lacan\n# Answer:\nJacques Lacan", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced -> John Gray\n# Answer:\nJohn Gray", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> influence.influence_node.influenced -> Christiaan Huygens\n# Answer:\nChristiaan Huygens", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> influence.influence_node.influenced -> Alister McGrath\n# Answer:\nAlister McGrath"], "ground_truth": ["Writer", "Physician", "Philosopher"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.606060606060606, "ans_precission": 0.5555555555555556, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.2222222222222222, "path_recall": 0.6666666666666666, "path_ans_f1": 0.606060606060606, "path_ans_precision": 0.5555555555555556, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nSean Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\nJ. Holiday", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> Mama -> music.recording.artist -> Ghostface Killah\n# Answer:\nGhostface Killah", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.16666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0.2222222222222222, "path_precision": 0.16666666666666666, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.25, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Atlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nMontgomery", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nAlabama"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.source -> The Cask of Amontillado\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nZoo", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x1g73 -> common.webpage.resource -> Blink-182 postpones more concerts following death of friend DJ AM\n# Answer:\nBlink-182 postpones more concerts following death of friend DJ AM", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x3mkg -> common.webpage.resource -> 'Twilight' cast to tour Nordstrom stores\n# Answer:\n'Twilight' cast to tour Nordstrom stores"], "ground_truth": ["Jimmy Carter Library and Museum", "Center for Puppetry Arts", "Atlanta Symphony Orchestra", "Atlanta History Center", "Fernbank Museum of Natural History", "Peachtree Road Race", "Fernbank Science Center", "Georgia Aquarium", "Four Seasons Hotel Atlanta", "Variety Playhouse", "Cobb Energy Performing Arts Centre", "Atlanta Jewish Film Festival", "Hyatt Regency Atlanta", "Georgia State Capitol", "Six Flags White Water", "CNN Center", "Georgia World Congress Center", "Omni Coliseum", "Masquerade", "Zoo Atlanta", "World of Coca-Cola", "The Tabernacle", "Atlanta Marriott Marquis", "Philips Arena", "Martin Luther King, Jr. National Historic Site", "Margaret Mitchell House & Museum", "Underground Atlanta", "Atlanta Ballet", "Turner Field", "Fox Theatre", "Arbor Place Mall", "Six Flags Over Georgia", "Atlanta Cyclorama & Civil War Museum", "Georgia Dome", "Woodruff Arts Center", "Centennial Olympic Park"], "ans_acc": 0.05555555555555555, "ans_hit": 1, "ans_f1": 0.049999999999999996, "ans_precission": 0.25, "ans_recall": 0.027777777777777776, "path_f1": 0.09999999999999999, "path_precision": 0.5, "path_recall": 0.05555555555555555, "path_ans_f1": 0.14285714285714285, "path_ans_precision": 0.5, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2015\n# Answer:\nQueensland state election, 2015", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nQueensland state election, 2009", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\ncommon.image.size"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 1]\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 2]\n# Answer:\nThur 10 June, 2010 [Episode 2]"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.country.currency_used -> Tuvaluan dollar\n# Answer:\nTuvaluan dollar", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.statistical_region.gdp_nominal -> m.0j6rj_4\n# Answer:\nlocation.statistical_region.gdp_nominal", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Tuvalu -> location.statistical_region.gdp_nominal -> g.11bbwz4dfw\n# Answer:\nlocation.statistical_region.gdp_nominal"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_5 -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.source -> GNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nGNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_5 -> measurement_unit.dated_money_value.source -> GNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nGNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nMaryland"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nMargaret Blair Young"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointee -> Clifford Sobel\n# Answer:\nClifford Sobel", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q61y -> people.appointment.appointee -> William E. Todd\n# Answer:\nWilliam E. Todd", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil"], "ground_truth": ["Michael Peroutka", "Ralph Nader", "John Kerry", "Gene Amondson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f -> film.personal_film_appearance.film -> The War of the World: A New History of the 20th Century\n# Answer:\nThe War of the World: A New History of the 20th Century", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nThe Ascent of Money"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> San Crist\u00f3bal Canton\n# Answer:\nSan Crist\u00f3bal Canton", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.contains -> Puerto Baquerizo Moreno\n# Answer:\nPuerto Baquerizo Moreno"], "ground_truth": ["Gal\u00e1pagos Province", "Pacific Ocean", "Ecuador"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Wait for a Minute\n# Answer:\nWait for a Minute", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> #thatPower\n# Answer:\n#thatPower", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0n1ykxp -> award.award_honor.honored_for -> Baby\n# Answer:\nBaby", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0n1ykxp -> award.award_honor.award -> MTV Video Music Award for Artist to Watch\n# Answer:\nMTV Video Music Award for Artist to Watch", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work"], "ground_truth": ["Right Here", "All Around The World", "As Long as You Love Me", "Thought Of You", "Roller Coaster", "Hold Tight", "First Dance", "Heartbreaker", "Die in Your Arms", "Change Me", "Live My Life", "Home to Mama", "All Bad", "Never Say Never", "Never Let You Go", "Somebody to Love", "Wait for a Minute", "All That Matters", "Bad Day", "Lolly", "Recovery", "Confident", "Boyfriend", "Bigger", "Eenie Meenie", "Baby", "Turn to You (Mother's Day Dedication)", "Pray", "PYD", "#thatPower", "Beauty And A Beat"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.16216216216216214, "ans_precission": 0.5, "ans_recall": 0.0967741935483871, "path_f1": 0.05405405405405405, "path_precision": 0.16666666666666666, "path_recall": 0.03225806451612903, "path_ans_f1": 0.16216216216216214, "path_ans_precision": 0.5, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_for -> g.125cswvwv\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Physician", "Writer", "Statesman", "Publisher", "Journalist"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6666666666666666, "ans_recall": 0.4, "path_f1": 0.5, "path_precision": 0.6666666666666666, "path_recall": 0.4, "path_ans_f1": 0.5, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xw9mx -> common.webpage.resource -> 'Idol': Jordin or Blake? And why?\n# Answer:\n'Idol': Jordin or Blake? And why?", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w1gvc -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05ckmy9\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Toyota Group\n# Answer:\nToyota Group", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05t5syf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nMahaka Media", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.010b9m12\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_lnf\n# Answer:\nbase.descriptive_names.names.descriptive_name"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_for -> g.125621qyv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nEnd time", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Christianity\n# Answer:\nChristianity"], "ground_truth": ["Qiyamah", "Islamic holy books", "Mahdi", "Islamic view of angels", "God in Islam", "Tawhid", "Prophets in Islam", "Masih ad-Dajjal", "Entering Heaven alive", "\u1e6c\u016bb\u0101", "Predestination in Islam", "Monotheism", "Sharia"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 0.2, "ans_recall": 0.07692307692307693, "path_f1": 0.2222222222222222, "path_precision": 0.4, "path_recall": 0.15384615384615385, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.4, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nJokes and Jokers", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A liberal is a power worshipper without the power. -> media_common.quotation.subjects -> Liberals\n# Answer:\nLiberals"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0w5qpxw -> military.military_command.military_conflict -> The Blitz\n# Answer:\nThe Blitz", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.012stfsg -> military.military_command.military_conflict -> Battle of Moscow\n# Answer:\nBattle of Moscow", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.23529411764705885, "path_precision": 0.3333333333333333, "path_recall": 0.18181818181818182, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t -> common.webpage.resource -> Grammys expected to go on with Amy Winehouse, Beyonce\n# Answer:\nGrammys expected to go on with Amy Winehouse, Beyonce", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.resource -> Indicates the official home page for an artist.\n# Answer:\nIndicates the official home page for an artist."], "ground_truth": ["Actor", "Singer", "Songwriter"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.16666666666666666, "ans_recall": 0.3333333333333333, "path_f1": 0.5714285714285715, "path_precision": 0.5, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cv_4t\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2002 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cqh_0\n# Answer:\n2002 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2001 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cv_4y\n# Answer:\n2001 Protection One 400", "# Reasoning Path:\nKansas Speedway -> location.location.events -> 2002 Protection One 400 -> base.nascar.nascar_race_instance.results -> m.05cqh_4\n# Answer:\n2002 Protection One 400"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.3636363636363636, "path_precision": 0.2222222222222222, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nKansas City Monarchs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.season -> 1947 Major League Baseball season\n# Answer:\n1947 Major League Baseball season"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> people.profession.specializations -> Film Score Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nBook Edition", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> book.book_edition.isbn -> 9780231111812\n# Answer:\nCourage in a dangerous world"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> m.0nfb3__ -> measurement_unit.dated_percentage.source -> Military expenditure as percentage of GDP, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nMilitary expenditure as percentage of GDP, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> m.0nfb40x -> measurement_unit.dated_percentage.source -> Military expenditure as percentage of GDP, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nMilitary expenditure as percentage of GDP, World Development Indicators and Global Development Finance, World Bank"], "ground_truth": ["Catholicism", "Islam", "Protestantism", "Hinduism"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.4444444444444445, "path_precision": 0.5, "path_recall": 0.4, "path_ans_f1": 0.5, "path_ans_precision": 0.5, "path_ans_recall": 0.5}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of James A. Garfield\n# Answer:\nAssassination of James A. Garfield", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of William McKinley\n# Answer:\nAssassination of William McKinley", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> You never know what's hit you. A gunshot is the perfect way.\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> A shocking crime was committed on the unscrupulous initiative of few individuals, with the blessing of more, and amid the passive acquiescence of all.\n# Answer:\nAssassination"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Nicholas Nickleby\n# Answer:\nNicholas Nickleby", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nGreat Expectations", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nH. G. Wells", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> tv.tv_character.appeared_in_tv_program -> m.0j7c9jh\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Ray Bradbury\n# Answer:\nRay Bradbury", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> tv.tv_character.appeared_in_tv_program -> m.0gbd83m\n# Answer:\nAbel Magwitch", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nAnthony Burgess", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> Gustave Flaubert\n# Answer:\nGustave Flaubert", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> freebase.valuenotation.is_reviewed -> Place of birth\n# Answer:\nPlace of birth"], "ground_truth": ["A CHRISTMAS CAROL", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "Dombey and son", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Courage Literary Classics)", "Great expectations.", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Progressive English)", "The cricket on the hearth", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Take Part)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Dramatized)", "A Tale of Two Cities (40th Anniversary Edition)", "Little Dorrit", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Chrysalis Children's Classics Series)", "Sketches by Boz", "A Christmas Carol (Cover to Cover)", "Great Expectations.", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Classics Illustrated)", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Puffin Classics)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Whole Story)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Enriched Classics)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Classic Fiction)", "Our mutual friend.", "The mystery of Edwin Drood", "A Tale of Two Cities (Prentice Hall Science)", "The old curiosity shop", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Dover Thrift Editions)", "Oliver Twist", "Bleak House", "The Pickwick papers", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "The Old Curiosity Shop", "Great expectations", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (R)", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Silver Classics)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Ladybird Classics)", "Dombey and Son.", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Cyber Classics)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (10 Cassettes)", "Our mutual friend", "Martin Chuzzlewit", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Unabridged Classics)", "The Pickwick Papers", "A Tale of Two Cities (Cover to Cover Classics)", "The Mystery of Edwin Drood", "The old curiosity shop.", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Masterworks)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Value Books)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Classic Collection)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Large Print)", "A Christmas Carol (Reissue)", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Watermill Classics)", "Hard times", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Scholastic Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Adopted Classic)", "David Copperfield.", "Bleak House.", "David Copperfield", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Wordsworth Classics)", "Great Expectations", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Cassette (1 Hr).)", "Bleak house", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Dodo Press)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A TALE OF TWO CITIES", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Puffin Choice)", "Dombey and Son"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0.03827751196172249, "ans_precission": 0.1, "ans_recall": 0.023668639053254437, "path_f1": 0.00588235294117647, "path_precision": 0.1, "path_recall": 0.0030303030303030303, "path_ans_f1": 0.03827751196172249, "path_ans_precision": 0.1, "path_ans_recall": 0.023668639053254437}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n58th Army", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> Chechens\n# Answer:\nChechens", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> military.military_conflict.combatants -> m.064ykvf\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> common.topic.article -> m.05b0fkx\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> common.topic.notable_types -> Event\n# Answer:\nEvent"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nReal Estate Investment Trust", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Retail\n# Answer:\nRetail", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Board members\n# Answer:\nBoard members", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.legal_structure -> Limited liability company\n# Answer:\nFrontpoint Security Solutions", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Industry\n# Answer:\nIndustry"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.actor -> Edward Mulhare\n# Answer:\nEdward Mulhare", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.character -> Devon Miles\n# Answer:\nDevon Miles", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> tv.tv_series_season.episodes -> A Nice, Indecent Little Town\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 1 -> tv.tv_series_season.episodes -> A Plush Ride\n# Answer:\nKnight Rider - Season 1", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Knight Sounds Feature\n# Answer:\nKnight Rider - Season 0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> tv.tv_series_season.episodes -> Knight Moves Feature\n# Answer:\nKnight Rider - Season 0"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nJasper County", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver Academy -> location.location.geolocation -> m.0127s0kf\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.090y51j -> award.award_nomination.nominated_for -> Family Ties\n# Answer:\nFamily Ties", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.15384615384615385, "path_precision": 0.2, "path_recall": 0.125, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_conflict -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\nmilitary.military_commander.military_commands", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> time.event.locations -> United States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> time.event.locations -> Western United States\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> time.event.includes_event -> Battle of White Oak Swamp\n# Answer:\nBattle of White Oak Swamp", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> military.military_conflict.military_personnel_involved -> George B. McClellan\n# Answer:\nSeven Days Battles", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Abraham Lincoln\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> time.event.includes_event -> Battle of Garnett's & Golding's Farm\n# Answer:\nBattle of Garnett's & Golding's Farm", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> military.military_conflict.military_personnel_involved -> Albion P. Howe\n# Answer:\nSeven Days Battles"], "ground_truth": ["Battle of Chancellorsville", "Battle of Front Royal", "How Few Remain", "Battle of McDowell", "Battle of Port Republic", "Battle of Cedar Mountain", "Battle of Hancock", "American Civil War", "Battle of Hoke's Run", "Battle of Chantilly", "First Battle of Rappahannock Station", "First Battle of Kernstown", "Manassas Station Operations", "Romney Expedition", "Battle of White Oak Swamp", "Second Battle of Bull Run", "Battle of Harpers Ferry", "Jackson's Valley Campaign", "First Battle of Winchester"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.6, "ans_recall": 0.15789473684210525, "path_f1": 0.12903225806451613, "path_precision": 0.5, "path_recall": 0.07407407407407407, "path_ans_f1": 0.25, "path_ans_precision": 0.6, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Sarah Davenport\n# Answer:\nSarah Davenport", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nAnne Harris", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nCommon-law marriage"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.14285714285714285, "path_recall": 0.25, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\ncommon.image.size", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\ncommon.image.size"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Ginevra de' Benci\n# Answer:\nGinevra de' Benci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Mona Lisa\n# Answer:\nMona Lisa", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Raphael Cartoons\n# Answer:\nRaphael Cartoons", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Stanze di Raffaello Frescoes\n# Answer:\nStanze di Raffaello Frescoes", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced -> Parmigianino\n# Answer:\nParmigianino", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> base.kwebbase.kwtopic.has_sentences -> Although neither fresco was ever finished, Leonardo's \\\"Battle of Anghiari\\\" and Michelangelo's \\\"Battle of Cascina\\\" had a great influence on the many students, including Raphael, who went to view them and make copies.\n# Answer:\nRaphael", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.artworks -> Noli Me Tangere\n# Answer:\nNoli Me Tangere", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> base.kwebbase.kwtopic.has_sentences -> At this time, Raphael went in search of new influences.\n# Answer:\nAt this time, Raphael went in search of new influences.", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced -> Annibale Carracci\n# Answer:\nAnnibale Carracci", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.artworks -> Adoration of the Child\n# Answer:\nAdoration of the Child"], "ground_truth": ["Madonna and Child with St Joseph", "Portrait of a Young Fianc\u00e9e", "Portrait of a Musician", "St. Jerome in the Wilderness", "The Holy Infants Embracing", "g.1213jb_b", "Medusa", "Vitruvian Man", "La belle ferronni\u00e8re", "Head of a Woman", "Drapery for a Seated Figure", "Leonardo's horse", "The Last Supper", "g.12314dm1", "Madonna of the Carnation", "Madonna of Laroque", "g.1239jd9p", "Mona Lisa", "Salvator Mundi", "The Virgin and Child with St. Anne", "The Battle of Anghiari", "St. John the Baptist", "Portrait of Isabella d'Este", "Virgin of the Rocks", "g.121wt37c", "g.121yh91r", "g.120vt1gz", "Ginevra de' Benci", "Sala delle Asse", "Horse and Rider", "g.1224tf0c", "The Virgin and Child with St Anne and St John the Baptist", "Bacchus", "Madonna Litta", "Annunciation", "g.12215rxg", "Lucan portrait of Leonardo da Vinci", "The Baptism of Christ", "Lady with an Ermine", "Portrait of a man in red chalk", "Adoration of the Magi", "Leda and the Swan", "Madonna of the Yarnwinder", "g.1219sb0g", "Benois Madonna"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.07272727272727272, "ans_precission": 0.2, "ans_recall": 0.044444444444444446, "path_f1": 0.07407407407407407, "path_precision": 0.2, "path_recall": 0.045454545454545456, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.parent_disease -> Genetic disorder\n# Answer:\nGenetic disorder", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.treatments -> Chemotherapy\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Surgery\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Complications of pregnancy\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.treatments -> Cervical conization\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Discursos completos -> common.topic.notable_for -> g.1jmcc8fyn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> Essays -> common.topic.notable_types -> Book\n# Answer:\nBook"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Buddhist meditation\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Samadhi\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddha Memorial Center -> location.location.geolocation -> m.0wmmlzp\n# Answer:\nBuddha Memorial Center"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent"], "ground_truth": ["Bifocals", "Franklin stove", "Lightning rod", "Glass harmonica"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.6666666666666666, "ans_recall": 0.5, "path_f1": 0.5714285714285715, "path_precision": 0.6666666666666666, "path_recall": 0.5, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.5}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.8, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Charode\u01d0ka -> common.topic.notable_types -> Book\n# Answer:\nBook"], "ground_truth": ["Librettist", "Composer", "Musician"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.5, "ans_recall": 0.6666666666666666, "path_f1": 0.28571428571428575, "path_precision": 0.25, "path_recall": 0.3333333333333333, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.5, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0129m4b3 -> education.education.major_field_of_study -> History\n# Answer:\nHistory", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0129m4b3 -> education.education.student -> Daniel I Block\n# Answer:\nDaniel I Block", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["Liechtenstein", "Germany", "Switzerland", "Belgium", "East Germany", "Luxembourg", "Austria"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.3333333333333333, "ans_recall": 0.2857142857142857, "path_f1": 0.15384615384615383, "path_precision": 0.16666666666666666, "path_recall": 0.14285714285714285, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\nBroadcast Content", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Classic hits\n# Answer:\nClassic hits", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> common.topic.notable_types -> Broadcast Content\n# Answer:\nBroadcast Content"], "ground_truth": ["Pop music", "Psychedelic rock", "Experimental rock", "Soft rock", "Experimental music", "Pop rock", "Blues rock", "Art rock", "Rock music"], "ans_acc": 0.2222222222222222, "ans_hit": 1, "ans_f1": 0.27906976744186046, "ans_precission": 0.375, "ans_recall": 0.2222222222222222, "path_f1": 0.23529411764705882, "path_precision": 0.25, "path_recall": 0.2222222222222222, "path_ans_f1": 0.27906976744186046, "path_ans_precision": 0.375, "path_ans_recall": 0.2222222222222222}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpc0p -> government.government_position_held.office_holder -> Floyd K. Haskell\n# Answer:\nFloyd K. Haskell", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qppf9 -> government.government_position_held.office_holder -> Charles J. Hughes, Jr.\n# Answer:\nCharles J. Hughes, Jr.", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\nMilitary unit", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Infantry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\nMilitary unit"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> m.0hny1kt -> measurement_unit.adjusted_money_value.source -> World Bank, World Development Indicators\n# Answer:\nWorld Bank, World Development Indicators", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> m.0hny1kt -> measurement_unit.adjusted_money_value.adjustment_currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> Washington\n# Answer:\nWashington", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> King County\n# Answer:\nKing County", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.geolocation -> m.0wmhrx7\n# Answer:\nlocation.location.geolocation"], "ground_truth": ["98181", "98138", "98124", "98164", "98194", "98171", "98102", "98109", "98105", "98146", "98119", "98127", "98101", "98103", "98113", "98177", "98195", "98175", "98111", "98104", "98168", "98188", "98114", "98133", "98126", "98184", "98132", "98119-4114", "98165", "98174", "98118", "98144", "98129", "98134", "98112", "98121", "98191", "98122", "98154", "98148", "98106", "98117", "98199", "98166", "98160", "98190", "98125", "98115", "98155", "98170", "98139", "98116", "98108", "98158", "98185", "98136", "98131", "98178", "98141", "98198", "98107", "98145", "98161"], "ans_acc": 0.031746031746031744, "ans_hit": 1, "ans_f1": 0.057142857142857134, "ans_precission": 0.2857142857142857, "ans_recall": 0.031746031746031744, "path_f1": 0.057142857142857134, "path_precision": 0.2857142857142857, "path_recall": 0.031746031746031744, "path_ans_f1": 0.057142857142857134, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 0.031746031746031744}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nRobsol Pinkett, Jr.", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nAdrienne Banfield-Jones", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nPinkett-Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Jaden Smith\n# Answer:\nJaden Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Willard Carroll Trey Smith III\n# Answer:\nWillard Carroll Trey Smith III", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq82\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music -> broadcast.genre.content -> #Musik.Club on RauteMusik.FM\n# Answer:\n#Musik.Club on RauteMusik.FM", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> #Musik.Main on RauteMusik.FM\n# Answer:\n#Musik.Main on RauteMusik.FM", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> tv.tv_actor.guest_roles -> m.09nbq8g\n# Answer:\nWill Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> The China Story\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Empire of Japan\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> Flags of Our Fathers\n# Answer:\nEast Asia"], "ground_truth": ["N\u00fcshu script", "'Phags-pa script", "Simplified Chinese character", "Chinese characters", "Traditional Chinese characters"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.3333333333333333, "ans_recall": 0.4, "path_f1": 0.3636363636363636, "path_precision": 0.3333333333333333, "path_recall": 0.4, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.4}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Harold Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Sarah Ann Nixon\n# Answer:\nSarah Ann Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.parents -> Samuel Brady Nixon\n# Answer:\nFrancis A. Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> common.topic.notable_for -> g.125b3gc_h\n# Answer:\ncommon.topic.notable_for"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.13333333333333333, "path_precision": 0.125, "path_recall": 0.14285714285714285, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0v4v237 -> award.award_honor.award_winner -> Larry Harris\n# Answer:\nLarry Harris", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.award_nominee -> Sherman Hemsley\n# Answer:\nSherman Hemsley", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtpx -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.07shryn -> award.award_honor.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtpx -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actress in a Comedy Series"], "ground_truth": ["Roxie Roker", "Paul Benedict", "Isabel Sanford", "Sherman Hemsley", "Mike Evans", "Damon Evans", "Berlinda Tolbert", "Franklin Cover", "Jay Hammer", "Zara Cully", "Marla Gibbs"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.25000000000000006, "ans_precission": 0.4, "ans_recall": 0.18181818181818182, "path_f1": 0.0625, "path_precision": 0.4, "path_recall": 0.03389830508474576, "path_ans_f1": 0.25000000000000006, "path_ans_precision": 0.4, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Call\n# Answer:\nSan Francisco Call", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Don't Call it Frisco -> common.topic.notable_for -> g.125gwqscg\n# Answer:\nDon't Call it Frisco", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> Suspense\n# Answer:\nSuspense", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.part_of_series -> Women's Murder Club\n# Answer:\n1st to Die"], "ground_truth": ["The Golden Era", "The San Francisco Examiner", "San Francisco Daily", "The Daily Alta California", "AsianWeek", "San Francisco Foghorn", "San Francisco News-Call Bulletin Newspaper", "San Francisco Bay View", "San Francisco Chronicle", "San Francisco Business Times", "Bay Area Reporter", "California Star", "San Francisco Bay Times", "San Francisco Bay Guardian", "Synapse", "Street Sheet", "Free Society", "Dock of the Bay", "San Francisco Call", "Sing Tao Daily"], "ans_acc": 0.1, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.3333333333333333, "ans_recall": 0.1, "path_f1": 0.15384615384615383, "path_precision": 0.3333333333333333, "path_recall": 0.1, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.1}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> geography.river.basin_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg9k2c\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg97p1\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_containedby -> Turkey\n# Answer:\nTurkey"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> Transmural Myocardial Infarction\n# Answer:\nTransmural Myocardial Infarction", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination in ways which appear natural", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> ST segment elevation myocardial infarction\n# Answer:\nST segment elevation myocardial infarction", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nAortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nAbdominal aortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.07n73w_\n# Answer:\nbase.gender.gender_identity.people", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.02_1x1x\n# Answer:\nbase.gender.gender_identity.people"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 9: 1861\n# Answer:\nThe Correspondence of Charles Darwin, Volume 9: 1861", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nCambridgeshire", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nEvolution", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Paleontology\n# Answer:\nPaleontology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nUnited Kingdom"], "ground_truth": ["Evolution by natural selection", "To the members of the Down Friendly Club", "On a remarkable bar of sandstone off Pernambuco", "The Autobiography of Charles Darwin", "Darwin from Insectivorous Plants to Worms", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin for Today", "A student's introduction to Charles Darwin", "Kleinere geologische Abhandlungen", "The Life and Letters of Charles Darwin Volume 1", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "Evolutionary Writings: Including the Autobiographies", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Origins", "Reise eines Naturforschers um die Welt", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 12: 1864", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Tesakneri tsagume\u030c", "A Darwin Selection", "genese\u014ds t\u014dn eid\u014dn", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Charles Darwin's marginalia", "The Structure and Distribution of Coral Reefs", "La vie et la correspondance de Charles Darwin", "Proiskhozhdenie vidov", "Darwin's insects", "Human nature, Darwin's view", "The geology of the voyage of H.M.S. Beagle", "The action of carbonate of ammonia on the roots of certain plants", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "Resa kring jorden", "The voyage of Charles Darwin", "From Darwin's unpublished notebooks", "Darwinism stated by Darwin himself", "Darwin's notebooks on transmutation of species", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Geological Observations on South America", "Part I: Contributions to the Theory of Natural Selection / Part II", "From so simple a beginning", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin", "ontstaan der soorten door natuurlijke teeltkeus", "The Descent of Man, and Selection in Relation to Sex", "Darwin en Patagonia", "The Life of Erasmus Darwin", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Darwin Reader First Edition", "Questions about the breeding of animals", "The Voyage of the Beagle", "Les moyens d'expression chez les animaux", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Opsht\u0323amung fun menshen", "Cartas de Darwin 18251859", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Darwin's journal", "red notebook of Charles Darwin", "Diary of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Works", "The foundations of the Origin of species", "Darwin and Henslow", "Darwin's Ornithological notes", "The principal works", "The Variation of Animals and Plants under Domestication", "H.M.S. Beagle in South America", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Power of Movement in Plants", "Beagle letters", "Rejse om jorden", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Formation of Vegetable Mould through the Action of Worms", "The\u0301orie de l'e\u0301volution", "Diario del Viaje de Un Naturalista Alrededor", "On Natural Selection", "On the tendency of species to form varieties", "Charles Darwin's natural selection", "The Correspondence of Charles Darwin, Volume 14: 1866", "Evolution and natural selection", "South American Geology", "The Correspondence of Charles Darwin, Volume 8: 1860", "On evolution", "The Correspondence of Charles Darwin, Volume 9: 1861", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Essential Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Darwin Darwin", "The living thoughts of Darwin", "Charles Darwin on the routes of male humble bees", "Die geschlechtliche Zuchtwahl", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Motsa ha-minim", "The Life and Letters of Charles Darwin Volume 2", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "More Letters of Charles Darwin", "On the origin of species by means of natural selection", "Les mouvements et les habitudes des plantes grimpantes", "Fertilisation of Orchids", "Charles Darwin's letters", "Insectivorous Plants", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Memorias y epistolario i\u0301ntimo", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin Compendium", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Darwin-Wallace", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "monograph on the sub-class Cirripedia", "Geological Observations on the Volcanic Islands", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Leben und Briefe von Charles Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Volcanic Islands", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 18: 1870", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 15: 1867", "The collected papers of Charles Darwin", "Del Plata a Tierra del Fuego", "The Different Forms of Flowers on Plants of the Same Species", "Metaphysics, Materialism, & the evolution of mind", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Expression of the Emotions in Man and Animals", "El Origin De Las Especies", "The Correspondence of Charles Darwin, Volume 13: 1865", "The education of Darwin", "The portable Darwin", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 10: 1862", "Wu zhong qi yuan", "Darwin on humus and the earthworm", "On the Movements and Habits of Climbing Plants", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Reise um die Welt 1831 - 36", "The Darwin Reader Second Edition", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Orgin of Species", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Evolution", "The Correspondence of Charles Darwin, Volume 7: 1858-1859"], "ans_acc": 0.0392156862745098, "ans_hit": 1, "ans_f1": 0.060120240480961935, "ans_precission": 0.375, "ans_recall": 0.032679738562091505, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.07547169811320754, "path_ans_precision": 1.0, "path_ans_recall": 0.0392156862745098}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> You've Really Got a Hold on Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nGrammy Legend Award", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone"], "ground_truth": ["Just Another Kiss", "Pops, We Love You (disco)", "I've Made Love To You A Thousand Times", "Be Who You Are", "Open", "Ebony Eyes", "Christmas Every Day", "Medley: Never My Love / Never Can Say Goodbye", "You Are Forever", "It's Fantastic", "Deck the Halls", "Come by Here (Kum Ba Ya)", "Be Kind To The Growing Mind (with The Temptations)", "Please Don't Take Your Love (feat. Carlos Santana)", "One Time", "Never My Love / Never Can Say Goodbye", "Tell Me Tomorrow", "You've Really Go a Hold on Me", "Whatcha Gonna Do", "It's Her Turn to Live", "I'll Keep My Light In My Window", "Unless You Do It Again", "You Don't Know What It's Like", "Hold on to Your Love", "If You Wanna Make Love", "Will You Love Me Tomorrow?", "Christmas Everyday", "Why Do Happy Memories Hurt So Bad", "Wedding Song", "And I Love Her", "Let Me Be The Clock", "Little Girl, Little Girl", "Jesus Told Me To Love You", "Be Careful What You Wish For (instrumental)", "More Love", "I Can't Get Enough", "Just to See Her", "Winter Wonderland", "Baby That's Backatcha", "Standing On Jesus", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Ain't That Peculiar", "Heavy On Pride (Light On Love)", "The Track of My Tears", "Why", "Be Careful What You Wish For", "The Love Between Me and My Kids", "Shop Around", "I Can\u2019t Stand to See You Cry (Commercial version)", "Love So Fine", "Everything You Touch", "Noel", "Shoe Soul", "Nearness of You", "I Want You Back", "The Tracks of My Tears", "Just Passing Through", "You Cannot Laugh Alone", "A Tattoo", "The Tracks of My Heart", "Just Like You", "Tracks Of My Tears (Live)", "Aqui Con Tigo (Being With You)", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "Pops, We Love You", "If You Want My Love", "Vitamin U", "I Second That Emotions", "Melody Man", "Santa Claus is Coming to Town", "What's Too Much", "Tell Me Tomorrow (12\\\" extended mix)", "Happy (Love Theme From Lady Sings the Blues)", "Because of You It's the Best It's Ever Been", "Sweet Harmony", "Love Don't Give No Reason", "I Love Your Face", "I'm Glad There Is You", "You Go to My Head", "Hanging on by a Thread", "Ooo Baby Baby", "Who's Sad", "The Agony And The Ecstasy", "Be Kind to the Growing Mind", "Rack Me Back", "We've Saved the Best for Last", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Quiet Storm (single version)", "And I Don't Love You (Larry Levan instrumental dub)", "Quiet Storm", "It's Time to Stop Shoppin' Around", "Walk on By", "I Love The Nearness Of You", "Easy", "Get Ready", "Let Me Be the Clock", "Keep Me", "Double Good Everything", "With Your Love Came", "I'm in the Mood for Love", "Ever Had A Dream", "As You Do", "Quiet Storm (Groove Boutique remix)", "My Guy", "I Have Prayed On It", "Virgin Man", "If You Can Want", "The Family Song", "The Road to Damascus", "Baby Come Close", "Going to a Go-Go", "The Christmas Song", "No Time to Stop Believing", "Can't Fight Love", "Really Gonna Miss You", "Blame It on Love", "There Will Come A Day ( I'm Gonna Happen To You )", "Tears of a Clown", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Train of Thought", "We\u2019ve Come Too Far to End It Now", "Why Are You Running From My Love", "Tea for Two", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "It's a Good Feeling", "Away in the Manger / Coventry Carol", "Time Flies", "Fallin'", "Gang Bangin'", "I\u2019ve Got You Under My Skin", "Tears of a Sweet Free Clown", "You Are So Beautiful (feat. Dave Koz)", "Cruisin", "Did You Know (Berry's Theme)", "Let Your Light Shine On Me", "Fly Me to the Moon (In Other Words)", "Cruisin'", "Tracks of My Tears", "Love Is The Light", "Jasmin", "Crusin", "You're Just My Life (feat. India.Arie)", "Going to a Go Go", "My Girl", "Share It", "Ooo Baby Baby (live)", "Just To See Her Again", "Don't Wanna Be Just Physical", "Love Letters", "When Smokey Sings Tears Of A Clown", "A Child Is Waiting", "I Can't Give You Anything but Love", "When A Woman Cries", "I Know You by Heart", "It's Christmas Time", "Will You Still Love Me Tomorrow", "Wishful Thinking", "Crusin'", "God Rest Ye Merry Gentlemen", "Skid Row", "I've Made Love to You a Thousand Times", "I Am I Am", "Daylight & Darkness", "Just My Soul Responding", "Blame It On Love (Duet with Barbara Mitchell)", "Holly", "You Made Me Feel Love", "Coincidentally", "Christmas Greeting", "I Care About Detroit", "So Bad", "Yester Love", "Tracks of my Tears", "I Like Your Face", "Save Me", "She's Only a Baby Herself", "The Tracks Of My Tears", "I Hear The Children Singing", "Love Brought Us Here", "The Tears of a Clown", "The Tracks of My Tears (live)", "Quiet Storm (Groove Boutique Chill Jazz mix)", "In My Corner", "Mickey's Monkey", "Same Old Love", "He Can Fix Anything", "Don't Know Why", "Our Love Is Here to Stay", "I've Got You Under My Skin", "The Hurt's On You", "Close Encounters of the First Kind", "Te Quiero Como Si No Hubiera Un Manana", "Time After Time", "Rewind", "There Will Come a Day (I'm Gonna Happen to You)", "Going to a Gogo", "Just a Touch Away", "That Place", "Some People Will Do Anything for Love", "Bad Girl", "I Praise & Worship You Father", "Food For Thought", "Gone Forever", "Speak Low", "Fulfill Your Need", "The Tears Of A Clown", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Little Girl Little Girl", "I Am, I Am", "Yes It's You Lady", "You're the One for Me (feat. Joss Stone)", "Driving Thru Life in the Fast Lane", "No\u00ebl", "Take Me Through The Night", "I Second That Emotion", "Love' n Life", "Theme From the Big Time", "Jingle Bells", "Tell Me Tomorrow, Part 1", "Girl I'm Standing There", "Please Come Home for Christmas", "Night and Day", "Sleepless Nights", "Don't Play Another Love Song", "If You Wanna Make Love (Come 'round Here)", "A Silent Partner in a Three-Way Love Affair", "My World", "We Are The Warriors", "I Can't Find", "You Really Got a Hold on Me", "Photograph in My Mind", "Asleep on My Love", "Ebony Eyes (Duet with Rick James)", "Satisfy You", "(It's The) Same Old Love", "And I Don't Love You", "Love Don' Give No Reason (12 Inch Club Mix)", "One Heartbeat", "More Than You Know", "You've Really Got a Hold on Me", "The Way You Do (The Things You Do)", "Come to Me Soon", "Ooh Baby Baby", "Being With You", "Season's Greetings from Smokey Robinson", "Love Bath", "Wanna Know My Mind", "Will You Love Me Tomorrow", "Tears Of A Clown", "You Take Me Away", "It's A Good Night", "The Agony and the Ecstasy", "Girlfriend", "Everything for Christmas", "Mother's Son", "We've Saved The Best For Last (Kenny G with Smokey Robinson)"], "ans_acc": 0.00390625, "ans_hit": 1, "ans_f1": 0.007633587786259542, "ans_precission": 0.16666666666666666, "ans_recall": 0.00390625, "path_f1": 0.023668639053254437, "path_precision": 0.16666666666666666, "path_recall": 0.012738853503184714, "path_ans_f1": 0.007633587786259542, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 0.00390625}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJacqueline Kennedy Onassis", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nbase.popstra.celebrity.friendship", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Presidential Library and Museum -> location.location.street_address -> m.0wjcsjl\n# Answer:\nlocation.location.street_address"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbania", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nMontenegro", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2857142857142857, "ans_precission": 0.16666666666666666, "ans_recall": 1.0, "path_f1": 0.2857142857142857, "path_precision": 0.16666666666666666, "path_recall": 1.0, "path_ans_f1": 0.2857142857142857, "path_ans_precision": 0.16666666666666666, "path_ans_recall": 1.0}
