{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> location.statistical_region.electricity_consumption_per_capita -> g.1245_2gx3\n# Answer:\nlocation.statistical_region.electricity_consumption_per_capita", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nJamaica -> location.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp -> g.12tb6hby7\n# Answer:\nlocation.statistical_region.market_cap_of_listed_companies_as_percent_of_gdp", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\nLouisiana", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Cyclone", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1915 New Orleans hurricane -> meteorology.tropical_cyclone.affected_areas -> Cayman Islands\n# Answer:\nCayman Islands"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.0944j8_ -> government.government_position_held.office_position_or_title -> Speaker of the United States House of Representatives\n# Answer:\nSpeaker of the United States House of Representatives", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specialization_of -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.jurisdiction_of_office -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> common.topic.notable_for -> g.1258v91gm\n# Answer:\nFarmer"], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_d\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_4dk3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> olympics.olympic_participating_country.medals_won -> m.07nnrw4 -> olympics.olympic_medal_honor.medal -> Silver medal\n# Answer:\nSilver medal", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_bsc8\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_f\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nHaiti -> olympics.olympic_participating_country.medals_won -> m.07nnrw4 -> olympics.olympic_medal_honor.olympics -> 1928 Summer Olympics\n# Answer:\n1928 Summer Olympics"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.actor -> Ilyssa Fradin\n# Answer:\nIlyssa Fradin", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4 -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.character_created_by -> Gardner Fox -> film.film_story_contributor.film_story_credits -> Justice League of America\n# Answer:\nJustice League of America", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0c71cxl\n# Answer:\nfictional_universe.fictional_job_title.fictional_characters_with_this_job", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.film -> Batman Begins\n# Answer:\nBatman Begins", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.actor -> Melinda McGraw\n# Answer:\nMelinda McGraw", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nA Cold Christmas", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> people.profession.specializations -> Advocate\n# Answer:\nAdvocate"], "ground_truth": ["Hannah Gunn", "Melinda McGraw", "Ilyssa Fradin"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72v -> sports.sports_team_roster.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.05gmg0p -> sports.sports_team_roster.team -> LSU Tigers men's basketball\n# Answer:\nLSU Tigers men's basketball", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season"], "ground_truth": ["Phoenix Suns", "Boston Celtics", "Los Angeles Lakers", "Cleveland Cavaliers", "Miami Heat", "LSU Tigers men's basketball", "Orlando Magic"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6292134831460674, "ans_precission": 0.7, "ans_recall": 0.5714285714285714, "path_f1": 0.35, "path_precision": 0.7, "path_recall": 0.23333333333333334, "path_ans_f1": 0.6292134831460674, "path_ans_precision": 0.7, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> European American\n# Answer:\nEuropean American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> Scottish people\n# Answer:\nScottish people", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nChandler Muriel Bing", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nAmerican English", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql -> award.award_honor.award_winner -> Patti M. Grant\n# Answer:\nPatti M. Grant", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> common.topic.image -> Map of Scotland within the United Kingdom\n# Answer:\nScottish American"], "ground_truth": ["New Rochelle"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Phu Thai language\n# Answer:\nPhu Thai language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.statistical_region.gdp_growth_rate -> g.11b60tqlwz\n# Answer:\nlocation.statistical_region.gdp_growth_rate", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Cham language\n# Answer:\nCham language", "# Reasoning Path:\nThailand -> location.statistical_region.agriculture_as_percent_of_gdp -> g.11b60t8r6s\n# Answer:\nlocation.statistical_region.agriculture_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nHuman Language"], "ground_truth": ["Akha Language", "Cham language", "Malay, Pattani Language", "Mon Language", "Mlabri Language", "Khmer language", "Nyaw Language", "Vietnamese Language", "Hmong language", "Phu Thai language", "Lao Language", "Thai Language", "Saek language"], "ans_acc": 0.38461538461538464, "ans_hit": 1, "ans_f1": 0.380952380952381, "ans_precission": 0.5, "ans_recall": 0.3076923076923077, "path_f1": 0.4347826086956522, "path_precision": 0.5, "path_recall": 0.38461538461538464, "path_ans_f1": 0.46875, "path_ans_precision": 0.6, "path_ans_recall": 0.38461538461538464}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Internet Rising\n# Answer:\nInternet Rising", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Catfish\n# Answer:\nCatfish", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpnzs0 -> award.award_nomination.award_nominee -> Aaron Sorkin\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpy5d0 -> award.award_nomination.award_nominee -> Andrew Garfield\n# Answer:\nAndrew Garfield", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpy623 -> award.award_nomination.award -> Critics' Choice Movie Award for Best Director\n# Answer:\nCritics' Choice Movie Award for Best Director", "# Reasoning Path:\nThe Social Network -> award.award_winning_work.awards_won -> m.0fpkgmk -> award.award_honor.award -> National Board of Review Award for Best Adapted Screenplay\n# Answer:\nNational Board of Review Award for Best Adapted Screenplay"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.jurisdiction_of_office -> Virginia\n# Answer:\nVirginia", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9m7 -> government.government_position_held.basic_title -> Secretary of State\n# Answer:\nSecretary of State", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9mx -> government.government_position_held.office_position_or_title -> United States Ambassador to France\n# Answer:\nUnited States Ambassador to France", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04sg3t_ -> government.government_position_held.office_position_or_title -> Vice President of the United States\n# Answer:\nVice President of the United States", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9m7 -> government.government_position_held.office_position_or_title -> United States Secretary of State\n# Answer:\nUnited States Secretary of State"], "ground_truth": ["Lawyer", "Architect", "Statesman", "Farmer", "Writer", "Teacher", "Author", "Archaeologist", "Philosopher", "Inventor"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.20000000000000004, "ans_precission": 0.2, "ans_recall": 0.2, "path_f1": 0.20000000000000004, "path_precision": 0.2, "path_recall": 0.2, "path_ans_f1": 0.20000000000000004, "path_ans_precision": 0.2, "path_ans_recall": 0.2}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> base.kwebbase.kwtopic.has_sentences -> In 1839, Darwin published \\\"Journal of Researches into Geology and Natural History of the various countries visited by HMS Beagle (1831-1836)\\\".\n# Answer:\nIn 1839, Darwin published \\\"Journal of Researches into Geology and Natural History of the various countries visited by HMS Beagle (1831-1836)\\\".", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Anne Darwin -> people.person.parents -> Emma Darwin\n# Answer:\nEmma Darwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nCambridgeshire", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nEvolution"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 6", "On evolution", "Volcanic Islands", "The education of Darwin", "Wu zhong qi yuan", "The structure and distribution of coral reefs", "The Autobiography of Charles Darwin (Great Minds Series)", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Origin of Species (Variorum Reprint)", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Rejse om jorden", "Voyage of the Beagle (Dover Value Editions)", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Formation of Vegetable Mould through the Action of Worms", "The descent of man and selection in relation to sex.", "The Variation of Animals and Plants under Domestication", "The Orgin of Species", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 9: 1861", "The Origin of Species (World's Classics)", "Beagle letters", "The autobiography of Charles Darwin", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Darwin's Ornithological notes", "Darwinism stated by Darwin himself", "The foundations of the Origin of species", "Leben und Briefe von Charles Darwin", "Evolution and natural selection", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The living thoughts of Darwin", "Kleinere geologische Abhandlungen", "The Origin of Species", "The Correspondence of Charles Darwin, Volume 7", "The Autobiography of Charles Darwin (Large Print)", "Memorias y epistolario i\u0301ntimo", "The Origin Of Species", "The Correspondence of Charles Darwin, Volume 14: 1866", "The Voyage of the Beagle (Adventure Classics)", "Charles Darwin's natural selection", "The Essential Darwin", "El Origin De Las Especies", "The origin of species", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Origin of Species (Great Books : Learning Channel)", "Resa kring jorden", "The Autobiography of Charles Darwin", "Evolution by natural selection", "Metaphysics, Materialism, & the evolution of mind", "Tesakneri tsagume\u030c", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Darwin on humus and the earthworm", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 11: 1863", "Part I: Contributions to the Theory of Natural Selection / Part II", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 10", "Voyage d'un naturaliste autour du monde", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 1", "Charles Darwin's marginalia", "The voyage of the Beagle.", "The Darwin Reader Second Edition", "monograph on the sub-class Cirripedia", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "H.M.S. Beagle in South America", "Origin of Species (Harvard Classics, Part 11)", "Darwin's journal", "The Voyage of the Beagle (Everyman Paperbacks)", "Voyage of the Beagle (NG Adventure Classics)", "On Natural Selection", "A student's introduction to Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "From Darwin's unpublished notebooks", "The Expression Of The Emotions In Man And Animals", "Darwin and Henslow", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 15", "The Voyage of the Beagle (Unabridged Classics)", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Darwin-Wallace", "Opsht\u0323amung fun menshen", "Charles Darwin's letters", "More Letters of Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Charles Darwin on the routes of male humble bees", "The Origin of Species (Oxford World's Classics)", "The Correspondence of Charles Darwin, Volume 16: 1868", "The structure and distribution of coral reefs.", "The principal works", "Origin of Species (Everyman's University Paperbacks)", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "On a remarkable bar of sandstone off Pernambuco", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 3", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Voyage of the Beagle", "Les r\u00e9cifs de corail, leur structure et leur distribution", "From So Simple a Beginning", "Darwin's notebooks on transmutation of species", "The Autobiography of Charles Darwin (Dodo Press)", "Darwin Darwin", "The Voyage of the Beagle (Great Minds Series)", "The Descent of Man, and Selection in Relation to Sex", "Notebooks on transmutation of species", "The Origin of Species (Collector's Library)", "The Structure And Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 15: 1867", "The expression of the emotions in man and animals", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Reise eines Naturforschers um die Welt", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Life of Erasmus Darwin", "Geological Observations on South America", "The descent of man, and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Expression of the Emotions in Man and Animals", "Diary of the voyage of H.M.S. Beagle", "Die geschlechtliche Zuchtwahl", "The Correspondence of Charles Darwin, Volume 12: 1864", "Les moyens d'expression chez les animaux", "Origin of Species", "The origin of species : complete and fully illustrated", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die fundamente zur entstehung der arten", "Darwin en Patagonia", "Human nature, Darwin's view", "Voyage of the Beagle (Harvard Classics, Part 29)", "The Origin of Species (Mentor)", "Darwin for Today", "Evolution", "On the Movements and Habits of Climbing Plants", "La vie et la correspondance de Charles Darwin", "Motsa ha-minim", "The Correspondence of Charles Darwin, Volume 11", "Les mouvements et les habitudes des plantes grimpantes", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The descent of man, and selection in relation to sex", "La facult\u00e9 motrice dans les plantes", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Darwin Compendium", "The Descent of Man and Selection in Relation to Sex", "The voyage of Charles Darwin", "The Origin of Species (Great Minds Series)", "Origins", "Fertilisation of Orchids", "genese\u014ds t\u014dn eid\u014dn", "Autobiography of Charles Darwin", "The Correspondence of Charles Darwin, Volume 13", "The Autobiography of Charles Darwin, and selected letters", "Voyage of the Beagle", "Works", "The\u0301orie de l'e\u0301volution", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 2", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 12", "The Power of Movement in Plants", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The action of carbonate of ammonia on the roots of certain plants", "The Autobiography Of Charles Darwin", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The Correspondence of Charles Darwin, Volume 9", "Cartas de Darwin 18251859", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "ontstaan der soorten door natuurlijke teeltkeus", "Het uitdrukken van emoties bij mens en dier", "Diario del Viaje de Un Naturalista Alrededor", "Proiskhozhdenie vidov", "Voyage Of The Beagle", "The Correspondence of Charles Darwin, Volume 8", "The Correspondence of Charles Darwin, Volume 5", "The Different Forms of Flowers on Plants of the Same Species", "The Origin of Species (Enriched Classics)", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The collected papers of Charles Darwin", "A Darwin Selection", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Correspondence of Charles Darwin, Volume 14", "Del Plata a Tierra del Fuego", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin", "The Voyage of the Beagle (Mentor)", "The autobiography of Charles Darwin, 1809-1882", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "On the tendency of species to form varieties", "Questions about the breeding of animals", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "On the origin of species by means of natural selection", "The Expression of the Emotions in Man And Animals", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Darwin's insects"], "ans_acc": 0.04205607476635514, "ans_hit": 1, "ans_f1": 0.07860262008733625, "ans_precission": 0.6, "ans_recall": 0.04205607476635514, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.08071748878923767, "path_ans_precision": 1.0, "path_ans_recall": 0.04205607476635514}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nNathan Whitaker", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0vxk_df -> sports.sports_team_roster.team -> New England Patriots\n# Answer:\nNew England Patriots", "# Reasoning Path:\nTim Tebow -> common.topic.notable_for -> g.1255t_dt0\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0sgkpfw -> award.award_nomination.award -> Kids' Choice Award for Favorite Male Athlete\n# Answer:\nKids' Choice Award for Favorite Male Athlete", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.draft -> 1998 NFL draft\n# Answer:\n1998 NFL draft", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8s1x -> location.partial_containment_relationship.partially_contained_by -> Poland\n# Answer:\nPoland", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8t4c -> location.partial_containment_relationship.partially_contained_by -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8sy9 -> location.partial_containment_relationship.partially_contained_by -> Ukraine\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.passes -> Transf\u0103g\u0103r\u0103\u0219an -> location.location.containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8sdn -> location.partial_containment_relationship.partially_contained_by -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8sq_ -> location.partial_containment_relationship.partially_contained_by -> Romania\n# Answer:\nRomania"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> John Clare -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> John Clare -> people.person.place_of_birth -> Helpston\n# Answer:\nHelpston", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTopic"], "ground_truth": ["Bard", "Writer", "Author", "Poet"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.actor -> Michael Fox\n# Answer:\nMichael Fox", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l6zw -> film.performance.actor -> Dr. Smoov\n# Answer:\nDr. Smoov", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.011nc97w -> film.performance.actor -> Michal Paszczyk\n# Answer:\nMichal Paszczyk", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.01xpnt2 -> film.performance.actor -> David Prowse\n# Answer:\nDavid Prowse", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nApple Barrier", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.film -> Blackstar Warrior\n# Answer:\nBlackstar Warrior", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.011nc97w -> film.performance.film -> Lord Vader on retirement\n# Answer:\nLord Vader on retirement"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> tv.tv_actor.guest_roles -> m.09p5f_0 -> tv.tv_guest_role.episodes_appeared_in -> Show #2309\n# Answer:\nShow #2309", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.award -> National Football League Most Valuable Player Award\n# Answer:\nNational Football League Most Valuable Player Award", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nOntario", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber: Never Say Never -> film.film.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.artist -> Ludacris\n# Answer:\nAll Around the World"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpjb -> military.casualties.combatant -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> base.culturalevent.event.entity_involved -> Coalition of the Gulf War\n# Answer:\nCoalition of the Gulf War", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> time.event.includes_event -> Operation Southern Watch\n# Answer:\nIraqi no-fly zones"], "ground_truth": ["Australia", "United States of America", "Argentina", "France", "United Kingdom", "Iraq", "Saudi Arabia"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.17910447761194026, "path_precision": 0.6, "path_recall": 0.10526315789473684, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.09j0xr5 -> tv.regular_tv_appearance.actor -> Debby Ryan\n# Answer:\nDebby Ryan", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gmpx -> tv.regular_tv_appearance.actor -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nMr. Moseby", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl67t -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 3\n# Answer:\nThe Suite Life on Deck - Season 3", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.09j0xr5 -> tv.regular_tv_appearance.character -> Bailey Marie Pickett\n# Answer:\nBailey Marie Pickett", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86 -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Sidekick\n# Answer:\nKids' Choice Award for Favorite TV Sidekick", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gmpx -> tv.regular_tv_appearance.character -> Cody Martin\n# Answer:\nCody Martin"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> government.government.agency -> State Library of Ohio\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.article -> m.03q_wv\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmkl4 -> government.government_position_held.office_holder -> Salmon P. Chase\n# Answer:\nSalmon P. Chase", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nAlabama"], "ground_truth": ["Return J. Meigs, Jr.", "Ted Strickland", "John Kasich"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.borrowing_team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pd_2 -> soccer.football_player_stats.team -> England national under-21 football team\n# Answer:\nEngland national under-21 football team", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvddfb -> soccer.football_player_loan.borrowing_team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.lending_team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pdz2 -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0pxgqz5 -> sports.sports_team_roster.team -> England national football team\n# Answer:\nEngland national football team", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.position -> Midfielder\n# Answer:\nMidfielder"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.administrative_division.country -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.location.containedby -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.administrative_division.first_level_division_of -> Mexico\n# Answer:\nMexico", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.location.containedby -> Greater Mexico City\n# Answer:\nMexico City", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.location.events -> 1968 Summer Olympics\n# Answer:\nMexico City", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> base.aliens.ufo_sighting_location.ufo_sighting_s -> Mexico City skyline UFO\n# Answer:\nMexico City", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.deceased_person.place_of_death -> Mexico City -> location.location.containedby -> South-Central Mexico\n# Answer:\nSouth-Central Mexico", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> book.author.works_written -> Coronado's letter to Mendoza, August 3, 1540 -> book.book.editions -> Coronado's letter to Mendoza, August 3, 1540. The relation of Francis Vasquez de Coronado, captaine generall of the people which were sent in the name of the Emperours Maiestie to the countrey of Cibola newly discouered, which he sent to don Antonio de Mendoca, viceroy of Mexico, of such things as happened in his voyage from the 22. of Aprill in the yeere 1540. Which departed from Culiacan forward, and of such things as hee found in the countrey which he passed\n# Answer:\nCoronado's letter to Mendoza, August 3, 1540"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Reclaiming History: The Assassination of President John F. Kennedy\n# Answer:\nReclaiming History: The Assassination of President John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> location.location.containedby -> Brevard County\n# Answer:\nBrevard County", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nEvent", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> base.culturalevent.event.entity_involved -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Oswald's Tale\n# Answer:\nOswald's Tale", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish Language"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.citytown -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.postal_code -> 760-8521\n# Answer:\n760-8521", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> base.infrastructure.nuclear_power_plant.coolant -> Water\n# Answer:\nWater", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 4 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 4", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 5 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 5"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\nlocation.statistical_region.long_term_unemployment_rate", "# Reasoning Path:\nUnited Kingdom -> periodicals.newspaper_circulation_area.newspapers -> i -> common.topic.notable_types -> Newspaper\n# Answer:\nNewspaper", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography"], "ground_truth": ["Northern Ireland", "Wales", "Scotland", "England"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.administrative_division.first_level_division_of -> United States of America -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> location.administrative_division.first_level_division_of -> United States of America -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_containedby -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> location.location.partially_containedby -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nSabine River", "# Reasoning Path:\nLouisiana -> government.governmental_jurisdiction.agencies -> Louisiana National Guard -> government.government_agency.provides_service -> National security\n# Answer:\nNational security", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> influence.influence_node.influenced -> Hans Urs von Balthasar\n# Answer:\nHans Urs von Balthasar", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> book.author.works_written -> Anti-Pelagian Writings\n# Answer:\nAnti-Pelagian Writings", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> common.topic.image -> Sandro Botticelli 050 -> common.image.appears_in_topic_gallery -> Saint Augustine in His Study\n# Answer:\nSaint Augustine in His Study", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> people.person.profession -> Writer\n# Answer:\nWriter"], "ground_truth": ["Writer", "Philosopher", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nSean Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\nJ. Holiday", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nNeffeteria Pugh", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nPlace of birth", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_types -> Person\n# Answer:\nPerson", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.gender -> Female\n# Answer:\nFemale"], "ground_truth": ["Francine Lons", "Sal Gibson", "Leon Cole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.11b60s4lvy\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.gdp_real -> g.12tb6hbyz\n# Answer:\nlocation.statistical_region.gdp_real", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abbie Hoffman\n# Answer:\nAbbie Hoffman", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.parent_issue -> Minority rights\n# Answer:\nAfrican Americans' rights", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activist_organizations -> SWAPO Party Youth League\n# Answer:\nAnti-Apartheid", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-racism -> base.activism.activism_issue.activists -> Harry Schwarz\n# Answer:\nAnti-racism", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activists -> Abraham Osheroff\n# Answer:\nAfrican Americans' rights", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.parent_issue -> Civil and political rights\n# Answer:\nAfrican Americans' rights", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Atlanta\n# Answer:\nAtlanta"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> book.book_subject.works -> Poe -> common.image.appears_in_topic_gallery -> Thank Heaven! the crisis --The danger, is past, and the lingering illness, is over at last --, and the fever called Living is conquered at last.\n# Answer:\nPoe", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> book.book_subject.works -> Poe -> common.image.appears_in_topic_gallery -> That man is not truly brave who is afraid either to seem or to be, when it suits him, a coward.\n# Answer:\nPoe", "# Reasoning Path:\nEdgar Allan Poe -> book.book_subject.works -> Poe -> common.topic.notable_types -> Book\n# Answer:\nBook", "# Reasoning Path:\nEdgar Allan Poe -> book.book_subject.works -> Poe -> common.image.appears_in_topic_gallery -> I have great faith in fools; My friends call it self-confidence.\n# Answer:\nPoe", "# Reasoning Path:\nEdgar Allan Poe -> book.book_subject.works -> Poe -> common.topic.article -> m.065xxx4\n# Answer:\nPoe", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.source -> The Cask of Amontillado\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong."], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.focus_city_for -> ValuJet Airlines\n# Answer:\nValuJet Airlines", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> zoos.zoo.memberships -> International Marine Animal Trainers' Association\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Omni Atlanta Hotel at CNN Center\n# Answer:\nOmni Atlanta Hotel at CNN Center", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x1g73 -> common.webpage.resource -> Blink-182 postpones more concerts following death of friend DJ AM\n# Answer:\nBlink-182 postpones more concerts following death of friend DJ AM", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x1g73 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> 30135\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.042znwp\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Hyatt Regency Atlanta\n# Answer:\nHyatt Regency Atlanta"], "ground_truth": ["Georgia State Capitol", "Georgia Aquarium", "Arbor Place Mall", "Atlanta Marriott Marquis", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Zoo Atlanta", "Four Seasons Hotel Atlanta", "Margaret Mitchell House & Museum", "Atlanta Jewish Film Festival", "Fernbank Science Center", "Center for Puppetry Arts", "Hyatt Regency Atlanta", "Fernbank Museum of Natural History", "Atlanta History Center", "Peachtree Road Race", "Atlanta Cyclorama & Civil War Museum", "The Tabernacle", "Cobb Energy Performing Arts Centre", "Masquerade", "Atlanta Symphony Orchestra", "World of Coca-Cola", "Underground Atlanta", "Georgia World Congress Center", "Georgia Dome", "Six Flags White Water", "Fox Theatre", "Omni Coliseum", "Woodruff Arts Center", "Variety Playhouse", "CNN Center", "Martin Luther King, Jr. National Historic Site", "Philips Arena", "Turner Field", "Centennial Olympic Park", "Atlanta Ballet"], "ans_acc": 0.1388888888888889, "ans_hit": 1, "ans_f1": 0.22556390977443608, "ans_precission": 0.6, "ans_recall": 0.1388888888888889, "path_f1": 0.14285714285714285, "path_precision": 0.5, "path_recall": 0.08333333333333333, "path_ans_f1": 0.2317880794701987, "path_ans_precision": 0.7, "path_ans_recall": 0.1388888888888889}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Tasmania\n# Answer:\nTasmania", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nBunyip", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> New South Wales\n# Answer:\nNew South Wales", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.folklore.mythical_creature_location.mythical_creature_s -> Bunyip\n# Answer:\nBunyip", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Ashmore and Cartier Islands\n# Answer:\nAshmore and Cartier Islands", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 1]\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> freebase.valuenotation.is_reviewed -> Country of origin -> rdf-schema#range -> Country\n# Answer:\nCountry", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 2]\n# Answer:\nThur 10 June, 2010 [Episode 2]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nMultipart TV episode", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> common.topic.subject_of -> The N Crowd\n# Answer:\nThe N Crowd", "# Reasoning Path:\nCoronation Street -> freebase.valuenotation.is_reviewed -> Country of origin -> rdf-schema#domain -> TV Program\n# Answer:\nCountry of origin", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.article -> m.0j79hlr\n# Answer:\nThursday 10th June 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> common.topic.notable_for -> g.125fj3q_1\n# Answer:\nThursday 10th June 2010"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> base.schemastaging.athlete_extra.coaches -> m.0j1530n -> base.schemastaging.coach_athlete_relationship.coach -> Leon Smith\n# Answer:\nLeon Smith", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAndy Murray -> tennis.tennis_tournament_champion.tennis_titles -> m.0jxs_c6 -> tennis.tennis_tournament_championship.tournament -> Brisbane International\n# Answer:\nBrisbane International", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#domain -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.0w4ftz9 -> freebase.valuenotation.is_reviewed -> Score\n# Answer:\nScore", "# Reasoning Path:\nAndy Murray -> base.schemastaging.athlete_extra.coaches -> m.0j154b5 -> base.schemastaging.coach_athlete_relationship.coach -> Emilio S\u00e1nchez\n# Answer:\nEmilio S\u00e1nchez", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.reverse_property -> People born here\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> tennis.tennis_tournament_champion.tennis_titles -> m.0jxs_c6 -> tennis.tennis_tournament_championship.event_type -> Men's singles\n# Answer:\nMen's singles"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_for -> g.1256mspd6\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Nauru\n# Answer:\nNauru", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency\n# Answer:\nCurrency", "# Reasoning Path:\nAustralian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> m.0h_3m9l\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.internet_tld -> cx\n# Answer:\ncx"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.time_required_to_start_a_business -> g.11b6nzlq6k\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.time_required_to_start_a_business -> g.1hhc3g61h\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.time_required_to_start_a_business -> g.1hhc3vxrs\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nSweden -> location.statistical_region.time_required_to_start_a_business -> g.1hhc3xpfs\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.time_required_to_start_a_business -> g.1hhc3z1n4\n# Answer:\nlocation.statistical_region.time_required_to_start_a_business", "# Reasoning Path:\nSweden -> base.aareas.schema.administrative_area.pertinent_type -> Swedish county -> type.type.domain -> Administrative Areas\n# Answer:\nSwedish county"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> people.person.education -> m.05t88t_ -> education.education.institution -> Blinn College\n# Answer:\nBlinn College", "# Reasoning Path:\nCam Newton -> award.award_nominee.award_nominations -> m.0z23c1d -> award.award_nomination.award -> Best Male College Athlete ESPY Award\n# Answer:\nBest Male College Athlete ESPY Award", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCam Newton -> people.person.parents -> Cecil Newton, Sr. -> people.person.children -> Cecil Newton\n# Answer:\nCecil Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nFrederick -> location.statistical_region.population -> m.0k6l8kn -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population", "# Reasoning Path:\nFrederick -> location.statistical_region.population -> g.11b66fv4wt\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nWashington, D.C."], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmyl -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nHarper Lee -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPerson", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nFemale"], "ground_truth": ["Monroe County High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Abu Simbel temples\n# Answer:\nAbu Simbel temples", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> common.topic.article -> m.09rmp8\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Alaska\n# Answer:\nAlaska", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> business.business_operation.industry -> Travel agency\n# Answer:\nTravel agency", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> organization.organization.phone_number -> m.011llj_0\n# Answer:\norganization.organization.phone_number", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Alberta\n# Answer:\nAlberta", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> common.topic.subjects -> Adventures by Disney - Italy Vacation\n# Answer:\nAdventures by Disney - Italy Vacation", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Amalfi Coast\n# Answer:\nAmalfi Coast", "# Reasoning Path:\nUtah -> travel.travel_destination.tour_operators -> Adventures by Disney -> common.topic.subjects ->       Adventures by Disney - Norway Vacation\n# Answer:\nAdventures by Disney"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> people.person.ethnicity -> White people -> common.topic.notable_types -> Ethnicity\n# Answer:\nEthnicity", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge W. Bush -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> John McCain\n# Answer:\nJohn McCain", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> government.politician.government_positions_held -> m.0b6b7y1\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.profession -> Businessperson\n# Answer:\nBusinessperson", "# Reasoning Path:\nGeorge W. Bush -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.people -> Barbara Pierce Bush\n# Answer:\nBarbara Pierce Bush", "# Reasoning Path:\nGeorge W. Bush -> people.person.ethnicity -> Scotch-Irish American -> people.ethnicity.languages_spoken -> American English\n# Answer:\nAmerican English", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.0115sdhb -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Gene Amondson", "Michael Peroutka", "John Kerry", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_for -> g.125f25_6q\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.education -> m.0n163vb -> education.education.institution -> The Glasgow Academy\n# Answer:\nThe Glasgow Academy", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f -> film.personal_film_appearance.film -> The War of the World: A New History of the 20th Century\n# Answer:\nThe War of the World: A New History of the 20th Century"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nSanta Cruz Canton, Ecuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> common.topic.notable_types -> Island Group\n# Answer:\nIsland Group", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nSanta Cruz Canton, Ecuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Bunnik Tours -> travel.tour_operator.travel_destinations -> Central America\n# Answer:\nCentral America", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney -> travel.tour_operator.travel_destinations -> Abu Simbel temples\n# Answer:\nAbu Simbel temples", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney -> common.topic.subjects -> Adventures by Disney - Ecuador and Galapagos Islands\n# Answer:\nAdventures by Disney - Ecuador and Galapagos Islands"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Carnatic Singer\n# Answer:\nCarnatic Singer", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> common.topic.subject_of -> L\u00e9o Ferr\u00e9\n# Answer:\nL\u00e9o Ferr\u00e9", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Chansonnier\n# Answer:\nChansonnier", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> #thatPower\n# Answer:\n#thatPower", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Child singer\n# Answer:\nChild singer", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> base.descriptive_names.names.descriptive_name -> m.010b9j0d\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Actor -> base.lightweight.profession.professions_similar -> Model\n# Answer:\nModel"], "ground_truth": ["Wait for a Minute", "Turn to You (Mother's Day Dedication)", "Boyfriend", "As Long as You Love Me", "First Dance", "Die in Your Arms", "Beauty And A Beat", "Baby", "Lolly", "Confident", "All That Matters", "Roller Coaster", "Never Say Never", "Eenie Meenie", "Recovery", "Right Here", "Never Let You Go", "Hold Tight", "Heartbreaker", "Bad Day", "Somebody to Love", "Change Me", "Home to Mama", "Live My Life", "Thought Of You", "PYD", "All Bad", "All Around The World", "Pray", "#thatPower", "Bigger"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.04878048780487805, "ans_precission": 0.1, "ans_recall": 0.03225806451612903, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.04878048780487805, "path_ans_precision": 0.1, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_for -> g.125cswvwv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specializations -> General practitioner\n# Answer:\nGeneral practitioner", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Publisher -> common.topic.notable_types -> Editor title\n# Answer:\nEditor title", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.places_lived -> m.03psnjv -> people.place_lived.location -> Pays de la Loire\n# Answer:\nPays de la Loire", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.specializations -> Muckraker\n# Answer:\nMuckraker", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specialization_of -> Healthcare professional\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Mikael Blomkvist\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> location.location.containedby -> Paris\n# Answer:\nParis"], "ground_truth": ["Statesman", "Publisher", "Writer", "Journalist", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5333333333333333, "ans_precission": 0.4, "ans_recall": 0.8, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04stk7b -> location.location_symbol_relationship.symbol -> Apache trout\n# Answer:\nApache trout", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nState flower", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xw9mx -> common.webpage.resource -> 'Idol': Jordin or Blake? And why?\n# Answer:\n'Idol': Jordin or Blake? And why?", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st830 -> location.location_symbol_relationship.symbol -> Cactus wren\n# Answer:\nCactus wren", "# Reasoning Path:\nArizona -> location.location.partially_contains -> Chihuahuan Desert -> location.location.partially_containedby -> New Mexico\n# Answer:\nNew Mexico"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again", "# Reasoning Path:\nRihanna -> broadcast.artist.content -> .977 The Hits Channel -> broadcast.content.artist -> Akon\n# Answer:\nAkon"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> John Tyler, Jr.\n# Answer:\nJohn Tyler, Jr.", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> symbols.name_source.namesakes -> Tyler County\n# Answer:\nTyler County", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> music.artist.track -> Carmel Indiana -> common.topic.notable_for -> g.12ml1w53s\n# Answer:\nCarmel Indiana", "# Reasoning Path:\nGeorge Lopez -> tv.tv_actor.guest_roles -> m.05ns5s_ -> tv.tv_guest_role.episodes_appeared_in -> Polar Bears\n# Answer:\nPolar Bears", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65 -> common.webpage.resource -> Janet Jackson's health derails her tour\n# Answer:\nJanet Jackson's health derails her tour", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0wry0_x -> film.personal_film_appearance.film -> George Lopez: It's Not Me, It's You\n# Answer:\nGeorge Lopez: It's Not Me, It's You", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094hhb3 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.person.place_of_birth -> Uiryeong County\n# Answer:\nUiryeong County", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.administrative_division.country -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.person.nationality -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05t5syf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> business.employer.number_of_employees -> m.0f7q8f_\n# Answer:\nbusiness.employer.number_of_employees", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.geolocation -> m.0239ks3\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> common.topic.article -> m.07gv7d\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.contains -> Daegu Arts University\n# Answer:\nDaegu Arts University", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05ckmy9\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> God in Islam\n# Answer:\nGod in Islam", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nEnd time", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_for -> g.125621qyv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_types -> Belief\n# Answer:\nBelief", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Adha -> common.topic.notable_types -> Holiday\n# Answer:\nHoliday", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Zoroastrianism\n# Answer:\nZoroastrianism"], "ground_truth": ["Islamic view of angels", "Masih ad-Dajjal", "Entering Heaven alive", "Tawhid", "\u1e6c\u016bb\u0101", "Islamic holy books", "Sharia", "Mahdi", "Monotheism", "Prophets in Islam", "Predestination in Islam", "Qiyamah", "God in Islam"], "ans_acc": 0.3076923076923077, "ans_hit": 1, "ans_f1": 0.17391304347826086, "ans_precission": 0.2, "ans_recall": 0.15384615384615385, "path_f1": 0.4067796610169492, "path_precision": 0.6, "path_recall": 0.3076923076923077, "path_ans_f1": 0.4067796610169492, "path_ans_precision": 0.6, "path_ans_recall": 0.3076923076923077}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> type.type.expected_by -> Character\n# Answer:\nCharacter", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades Darker -> book.book.characters -> Anastasia Steele\n# Answer:\nAnastasia Steele", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> film.film.starring -> m.0ydn3r2\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Fictional Character\n# Answer:\nFictional Character", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades Darker -> common.topic.notable_for -> g.12555q0zp\n# Answer:\nFifty Shades Darker", "# Reasoning Path:\nChristian Grey -> common.topic.notable_types -> Film character -> freebase.type_profile.kind -> Portrayal\n# Answer:\nFilm character"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nLaryngeal cancer", "# Reasoning Path:\nGeorge Orwell -> film.film_story_contributor.film_story_credits -> 1984 -> film.film.subjects -> Brainwashing\n# Answer:\nBrainwashing", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler -> people.deceased_person.cause_of_death -> Parkinson's disease\n# Answer:\nParkinson's disease", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nJokes and Jokers", "# Reasoning Path:\nGeorge Orwell -> symbols.name_source.namesakes -> 11020 Orwell -> astronomy.celestial_object.category -> Asteroid\n# Answer:\nAsteroid", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nQuotation", "# Reasoning Path:\nGeorge Orwell -> symbols.name_source.namesakes -> 11020 Orwell -> astronomy.orbital_relationship.orbits -> Sun\n# Answer:\nSun", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> influence.influence_node.influenced_by -> Yevgeny Zamyatin\n# Answer:\nYevgeny Zamyatin"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0w5qpxw -> military.military_command.military_conflict -> The Blitz\n# Answer:\nThe Blitz", "# Reasoning Path:\nAdolf Hitler -> government.political_appointer.appointees -> m.07jrh6q -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.basic_title -> Chancellor\n# Answer:\nChancellor", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.012stfsg -> military.military_command.military_conflict -> Battle of Moscow\n# Answer:\nBattle of Moscow", "# Reasoning Path:\nAdolf Hitler -> government.political_appointer.appointees -> m.09c1k5f -> government.government_position_held.office_holder -> Baldur von Schirach\n# Answer:\nBaldur von Schirach", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.appointed_by -> Paul von Hindenburg\n# Answer:\nPaul von Hindenburg"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.28571428571428564, "path_precision": 0.3, "path_recall": 0.2727272727272727, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> m.01362dph -> music.track_contribution.track -> Alone Again (Naturally)\n# Answer:\nAlone Again (Naturally)", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t -> common.webpage.resource -> Grammys expected to go on with Amy Winehouse, Beyonce\n# Answer:\nGrammys expected to go on with Amy Winehouse, Beyonce", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.image -> Michael Bubl\u00e9, performing at the Borgata Casino in Atlantic City, New Jersey on March 5, 2006 -> common.image.size -> m.01x3652\n# Answer:\nMichael Bubl\u00e9, performing at the Borgata Casino in Atlantic City, New Jersey on March 5, 2006", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nProfession"], "ground_truth": ["Actor", "Singer", "Songwriter"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.location.adjoin_s -> m.02sd6jx -> location.adjoining_relationship.adjoins -> Basehor\n# Answer:\nBasehor", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> common.topic.notable_for -> g.125h3hxdn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.geometry -> m.057rh7c\n# Answer:\n66111"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nKansas City Monarchs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0gggrzr -> sports.sports_team_roster.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> influence.influence_node.influenced -> Steve Hofstetter -> people.person.profession -> Film Producer\n# Answer:\nFilm Producer", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.film_festivals -> 2012 San Francisco International LGBT Film Festival\n# Answer:\n2012 San Francisco International LGBT Film Festival", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> people.profession.specializations -> Film Score Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nAnnie -> media_common.netflix_title.netflix_genres -> Television film\n# Answer:\nTelevision film", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Closed\n# Answer:\nDate Closed", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0jsmxbk -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0k1bktq -> film.film_regional_release_date.film_release_region -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> rdf-schema#range -> Theatrical Composer\n# Answer:\nTheatrical Composer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> owl#inverseOf -> Plays Composed\n# Answer:\nPlays Composed"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09kn0hp -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.games -> m.09kn08r -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.games -> m.07nvlwf -> american_football.player_game_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.rushing -> m.09kn0m1 -> american_football.player_rushing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.games -> m.09kn08r -> american_football.player_game_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09rm3xt -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nNew York", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.place_of_birth -> Hyde Park\n# Answer:\nHyde Park", "# Reasoning Path:\nEleanor Roosevelt -> symbols.name_source.namesakes -> Norvelt, Pennsylvania\n# Answer:\nNorvelt, Pennsylvania", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.parents -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw -> location.religion_percentage.religion -> Hinduism\n# Answer:\nHinduism", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v4rs1\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yb\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp"], "ground_truth": ["Catholicism", "Protestantism", "Islam", "Hinduism"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5333333333333333, "path_precision": 0.4, "path_recall": 0.8, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> common.topic.notable_for -> g.1254z5y4y\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesse James -> film.film.directed_by -> Lloyd Ingraham\n# Answer:\nLloyd Ingraham", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination by firearm", "# Reasoning Path:\nJesse James -> people.person.sibling_s -> m.0pdjg7w -> people.sibling_relationship.sibling -> Fanny Quantrill Samuel\n# Answer:\nFanny Quantrill Samuel", "# Reasoning Path:\nJesse James -> common.topic.notable_for -> g.125d6503n\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Juv\u00e9nal Habyarimana and Cyprien Ntaryamira\n# Answer:\nAssassination of Juv\u00e9nal Habyarimana and Cyprien Ntaryamira", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nMissouri", "# Reasoning Path:\nJesse James -> film.film.directed_by -> Henry King\n# Answer:\nHenry King", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> You never know what's hit you. A gunshot is the perfect way.\n# Answer:\nAssassination"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> celebrities.celebrity.sexual_relationships -> m.0j7fcl_ -> celebrities.romantic_relationship.celebrity -> Ann Rutledge\n# Answer:\nAnn Rutledge", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield", "# Reasoning Path:\nAbraham Lincoln -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> 200 Years of Lincoln -> common.topic.notable_types -> Exhibition\n# Answer:\nExhibition", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> celebrities.celebrity.sexual_relationships -> m.0j7fcl_ -> celebrities.romantic_relationship.relationship_type -> Dated\n# Answer:\nDated", "# Reasoning Path:\nAbraham Lincoln -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> 200 Years of Lincoln -> common.topic.notable_for -> g.125fncfwm\n# Answer:\n200 Years of Lincoln", "# Reasoning Path:\nAbraham Lincoln -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> 200 Years of Lincoln -> exhibitions.exhibition.exhibition_types -> History exhibition\n# Answer:\n200 Years of Lincoln"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> book.author.series_written_or_contributed_to -> Christmas Books -> book.literary_series.works_in_this_series -> The Battle of Life\n# Answer:\nThe Battle of Life", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bob Cratchit -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nFilm character", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.02vdcn_\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nGreat Expectations", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> film.film_character.portrayed_in_films -> m.059wsl3\n# Answer:\nGhost of Christmas Yet to Come", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> The Ghost of Christmas Present with Ebenezer Scrooge\n# Answer:\nThe Ghost of Christmas Present with Ebenezer Scrooge", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nH. G. Wells"], "ground_truth": ["A Christmas Carol (Children's Classics)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Paperback Classics)", "The mystery of Edwin Drood", "A Christmas Carol (Cp 1135)", "A Tale of Two Cities (Enriched Classic)", "Oliver Twist", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Unabridged Classics)", "Bleak house", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Classic Fiction)", "Dombey and Son.", "A Tale of Two Cities (Collected Works of Charles Dickens)", "The Old Curiosity Shop", "A Christmas Carol", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Scholastic Classics)", "A Christmas Carol (Usborne Young Reading)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Penguin Readers, Level 5)", "Sketches by Boz", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (Whole Story)", "The Pickwick papers", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Soundings)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Apple Classics)", "The cricket on the hearth", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (40th Anniversary Edition)", "Martin Chuzzlewit", "Our mutual friend.", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Illustrated Classics)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Classics Illustrated)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Dramascripts)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Collector's Library)", "Great Expectations", "Bleak House", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "The old curiosity shop.", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Prentice Hall Science)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Read & Listen Books)", "Great expectations.", "A Tale of Two Cities (Macmillan Students' Novels)", "David Copperfield.", "A TALE OF TWO CITIES", "A Christmas Carol (The Kennett Library)", "The old curiosity shop", "A Tale of Two Cities (Simple English)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Thornes Classic Novels)", "David Copperfield", "Our mutual friend", "A Tale of Two Cities (Acting Edition)", "Bleak House.", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A CHRISTMAS CAROL", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Saddleback Classics)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Adopted Classic)", "Great Expectations.", "Dombey and son", "A Tale of Two Cities (Progressive English)", "A Christmas Carol (Pacemaker Classics)", "Hard times", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Value Books)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Masterworks)", "Little Dorrit", "A Tale of Two Cities (Penguin Classics)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Konemann Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "Dombey and Son", "Great expectations", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Audio Editions)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (BBC Audio Series)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (R)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Watermill Classics)", "A Christmas Carol (Large Print)", "The Pickwick Papers", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Bantam Classic)", "A Christmas Carol (Classic Collection)"], "ans_acc": 0.04142011834319527, "ans_hit": 1, "ans_f1": 0.06521739130434782, "ans_precission": 0.4, "ans_recall": 0.03550295857988166, "path_f1": 0.23529411764705882, "path_precision": 0.4, "path_recall": 0.16666666666666666, "path_ans_f1": 0.06521739130434782, "path_ans_precision": 0.4, "path_ans_recall": 0.03550295857988166}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> government.government_office_or_title.office_holders -> m.049x6_6\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_x -> government.government_position_held.office_holder -> Mikhail Sergeyevich Gorbachev\n# Answer:\nMikhail Sergeyevich Gorbachev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6yv -> government.government_position_held.office_holder -> Yuri Andropov\n# Answer:\nYuri Andropov", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6z5 -> government.government_position_held.office_holder -> Konstantin Chernenko\n# Answer:\nKonstantin Chernenko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> government.government_office_or_title.office_holders -> m.049x6_k\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nReal Estate Investment Trust", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> olympics.olympic_participating_country.athletes -> m.0k8pr82 -> olympics.olympic_athlete_affiliation.sport -> Track and field athletics\n# Answer:\nTrack and field athletics", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPuerto Rico -> book.book_subject.works -> The Rum Diary -> book.written_work.subjects -> Literary\n# Answer:\nLiterary", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> common.topic.subject_of -> Real Estate\n# Answer:\nReal Estate", "# Reasoning Path:\nPuerto Rico -> olympics.olympic_participating_country.athletes -> m.0k8pr82 -> olympics.olympic_athlete_affiliation.athlete -> Jamele Mason Vegas\n# Answer:\nJamele Mason Vegas", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Hair Club -> organization.organization.geographic_scope -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> music.album.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Like a Brother -> music.album.artist -> Robert Lamm\n# Answer:\nRobert Lamm", "# Reasoning Path:\nCarl Wilson -> music.producer.tracks_produced -> 'Til I Die (alternate mix) -> common.topic.notable_types -> Musical Recording\n# Answer:\nMusical Recording", "# Reasoning Path:\nCarl Wilson -> music.producer.tracks_produced -> All This Is That -> music.recording.artist -> The Beach Boys\n# Answer:\nAll This Is That", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Like a Brother -> music.album.artist -> Gerry Beckley\n# Answer:\nGerry Beckley", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier -> music.recording.releases -> Scotland the Brave\n# Answer:\nScotland the Brave", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> music.album.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> common.topic.notable_types -> Musical Album\n# Answer:\nMusical Album"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.actor -> Edward Mulhare\n# Answer:\nEdward Mulhare", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.characters -> KITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5\n# Answer:\nKITT", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.actor -> Patricia McPherson\n# Answer:\nPatricia McPherson", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.characters -> KITT -> film.film_character.portrayed_in_films -> m.0j7ph8m\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.character -> KITT\n# Answer:\nKITT"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.hud_foreclosure_area.total_residential_addresses -> m.07fz207 -> measurement_unit.dated_integer.source -> United States Department of Housing and Urban Development\n# Answer:\nUnited States Department of Housing and Urban Development", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.us_state.capital -> Nashville\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> m.0hmbn04 -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.people_born_here -> Paul Miller\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Washington Carver -> people.person.places_lived -> m.03ppx0s -> people.place_lived.location -> Tuskegee\n# Answer:\nTuskegee", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> m.0k6vn0t\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nJasper County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.hud_foreclosure_area.total_residential_addresses -> m.07h218k\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nNewton County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> m.0k6vn0l\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_winner.awards_won -> m.0n54qln -> award.award_honor.award -> Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series\n# Answer:\nScreen Actors Guild Award for Outstanding Performance by a Male Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.094k6z_ -> common.webpage.resource -> Stars swarm New York's TriBeCa Film Festival\n# Answer:\nStars swarm New York's TriBeCa Film Festival", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtwp -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_conflict -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04jx9vf\n# Answer:\nmilitary.military_conflict.commanders", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\nmilitary.military_commander.military_commands", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04h_gx9\n# Answer:\nmilitary.military_conflict.commanders", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.0w5rl_8\n# Answer:\nmilitary.military_conflict.commanders", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9lf -> military.military_command.military_conflict -> Manassas Station Operations\n# Answer:\nManassas Station Operations", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9m2 -> military.military_command.military_conflict -> First Battle of Rappahannock Station\n# Answer:\nFirst Battle of Rappahannock Station", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04fvgx7\n# Answer:\nAmerican Civil War"], "ground_truth": ["Second Battle of Bull Run", "American Civil War", "First Battle of Winchester", "Battle of Hoke's Run", "Battle of White Oak Swamp", "First Battle of Rappahannock Station", "Battle of Harpers Ferry", "How Few Remain", "Jackson's Valley Campaign", "Battle of Hancock", "Battle of Front Royal", "Manassas Station Operations", "Battle of Cedar Mountain", "Battle of Chancellorsville", "Romney Expedition", "Battle of Chantilly", "Battle of McDowell", "First Battle of Kernstown", "Battle of Port Republic"], "ans_acc": 0.21052631578947367, "ans_hit": 1, "ans_f1": 0.2962962962962963, "ans_precission": 0.5, "ans_recall": 0.21052631578947367, "path_f1": 0.19178082191780818, "path_precision": 0.7, "path_recall": 0.1111111111111111, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.8, "path_ans_recall": 0.21052631578947367}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> common.topic.article -> m.054w4\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Languages spoken\n# Answer:\nLanguages spoken", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Geographic distribution\n# Answer:\nGeographic distribution", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Included in group(s)\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTopic", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Includes group(s)\n# Answer:\nIncludes group(s)", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.expected_by -> Race\n# Answer:\nEthnicity"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nCommon-law marriage", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Sarah Davenport\n# Answer:\nSarah Davenport", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nBenjamin Franklin\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Elizabeth Douse\n# Answer:\nElizabeth Douse", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nEcton"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\ncommon.image.size", "# Reasoning Path:\nPatrick Swayze -> tv.tv_program_guest.appeared_on -> m.0j7p8gc -> tv.tv_guest_personal_appearance.episode -> Episode 159\n# Answer:\nEpisode 159", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\ncommon.image.size", "# Reasoning Path:\nPatrick Swayze -> tv.tv_program_guest.appeared_on -> m.0j7p8gc -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nCelebrity guest", "# Reasoning Path:\nPatrick Swayze -> broadcast.artist.content -> 1Club.FM: 80s (Lite) -> broadcast.content.genre -> Classic hits\n# Answer:\nClassic hits", "# Reasoning Path:\nPatrick Swayze -> broadcast.artist.content -> 1Club.FM: 80s (Lite) -> common.topic.image -> 1clubfm.jpg\n# Answer:\n1Club.FM: 80s (Lite)", "# Reasoning Path:\nPatrick Swayze -> broadcast.artist.content -> 1Club.FM: 80s (Lite) -> broadcast.content.producer -> 1Club.FM\n# Answer:\n1Club.FM", "# Reasoning Path:\nPatrick Swayze -> broadcast.artist.content -> 1Club.FM: 80s (Lite) -> broadcast.content.location -> Chicago\n# Answer:\n1Club.FM: 80s (Lite)"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Ginevra de' Benci\n# Answer:\nGinevra de' Benci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Painter -> people.profession.specialization_of -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Bacchus\n# Answer:\nBacchus", "# Reasoning Path:\nLeonardo da Vinci -> book.book_subject.works -> Da Vinci\n# Answer:\nDa Vinci", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Stanze di Raffaello Frescoes\n# Answer:\nStanze di Raffaello Frescoes", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci : Experience, Experiment and Design -> exhibitions.exhibition.venues -> m.059xb9w\n# Answer:\nexhibitions.exhibition.venues"], "ground_truth": ["Madonna and Child with St Joseph", "La belle ferronni\u00e8re", "Drapery for a Seated Figure", "Sala delle Asse", "St. Jerome in the Wilderness", "g.12215rxg", "Portrait of a Young Fianc\u00e9e", "Medusa", "Benois Madonna", "The Virgin and Child with St. Anne", "The Holy Infants Embracing", "Madonna of Laroque", "Leda and the Swan", "Portrait of Isabella d'Este", "Portrait of a Musician", "Madonna Litta", "g.1239jd9p", "Head of a Woman", "The Last Supper", "g.121wt37c", "Mona Lisa", "Adoration of the Magi", "g.121yh91r", "Lucan portrait of Leonardo da Vinci", "Ginevra de' Benci", "Salvator Mundi", "St. John the Baptist", "Portrait of a man in red chalk", "The Baptism of Christ", "Bacchus", "Lady with an Ermine", "g.1219sb0g", "Horse and Rider", "The Battle of Anghiari", "g.1224tf0c", "The Virgin and Child with St Anne and St John the Baptist", "Leonardo's horse", "g.1213jb_b", "Madonna of the Carnation", "Annunciation", "g.120vt1gz", "Vitruvian Man", "g.12314dm1", "Virgin of the Rocks", "Madonna of the Yarnwinder"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.1818181818181818, "ans_precission": 0.5, "ans_recall": 0.1111111111111111, "path_f1": 0.18518518518518517, "path_precision": 0.5, "path_recall": 0.11363636363636363, "path_ans_f1": 0.1818181818181818, "path_ans_precision": 0.5, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_k6r2\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nAustria -> location.statistical_region.gni_in_ppp_dollars -> g.11b60sd4t0\n# Answer:\nlocation.statistical_region.gni_in_ppp_dollars", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxk\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc37_h6\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Lower Austria -> base.aareas.schema.administrative_area.administrative_children -> Amstetten District\n# Answer:\nAmstetten District", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nAustrian state", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Burgenland -> common.topic.notable_types -> Administrative Division\n# Answer:\nBurgenland"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Orthomolecular medicine\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Peter McWilliams\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.parent_disease -> Genetic disorder\n# Answer:\nGenetic disorder", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Bryan Murray\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nOncology", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Deborah King\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjftz\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Diana Taylor Dawson\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.containedby -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.ethnicity -> Shakya -> people.ethnicity.people -> Gana sangh kshatriya\n# Answer:\nShakya", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.ethnicity -> Shakya -> common.topic.notable_for -> g.1259sz04r\n# Answer:\nShakya", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.ethnicity -> Shakya -> base.schemastaging.context_name.pronunciation -> g.125_pphgt\n# Answer:\nShakya", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.statistical_region.population -> g.11bc8807jd\n# Answer:\nKapilavastu"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> base.kwebbase.kwtopic.has_sentences -> In 1752, Franklin flew a kite in a thunderstorm and was able to charge up a Leyden jar from sparks produced from the end of the wet hemp string, which he held with a piece of insulating silk. -> base.kwebbase.kwsentence.dates -> m.0c158wk\n# Answer:\nIn 1752, Franklin flew a kite in a thunderstorm and was able to charge up a Leyden jar from sparks produced from the end of the wet hemp string, which he held with a piece of insulating silk.", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Treaty of Amity and Commerce\n# Answer:\nTreaty of Amity and Commerce", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBenjamin Franklin -> base.kwebbase.kwtopic.has_sentences -> In 1752, Franklin flew a kite in a thunderstorm and was able to charge up a Leyden jar from sparks produced from the end of the wet hemp string, which he held with a piece of insulating silk. -> base.kwebbase.kwsentence.previous_sentence -> In 1751 his letters on electricity were published in London as \\\"Experiments and Observations on Electricity\\\" and translated into French.\n# Answer:\nIn 1752, Franklin flew a kite in a thunderstorm and was able to charge up a Leyden jar from sparks produced from the end of the wet hemp string, which he held with a piece of insulating silk.", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nAmerican literature"], "ground_truth": ["Lightning rod", "Bifocals", "Franklin stove", "Glass harmonica"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> Greeley Union Pacific Railroad Depot -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nGreeley -> location.statistical_region.population -> g.11b66krpqz\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> common.topic.article -> m.0qfltq5\n# Answer:\nUniversity of Northern Colorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> film.film_story_contributor.film_story_credits -> The Swan Princess -> film.film.story_by -> Brian Nissen\n# Answer:\nBrian Nissen", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> film.music_contributor.film -> The Queen of Spades\n# Answer:\nThe Queen of Spades", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nBook", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> film.film_story_contributor.film_story_credits -> The Swan Princess -> film.film.story_by -> Richard Rich\n# Answer:\nRichard Rich"], "ground_truth": ["Musician", "Librettist", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nVatican City", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Germany\n# Answer:\nGermany"], "ground_truth": ["Liechtenstein", "Switzerland", "Germany", "Luxembourg", "East Germany", "Austria", "Belgium"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.6292134831460674, "ans_precission": 0.7, "ans_recall": 0.5714285714285714, "path_f1": 0.3529411764705882, "path_precision": 0.3, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6292134831460674, "path_ans_precision": 0.7, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Blues rock\n# Answer:\nBlues rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nShort Film", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.film_character.portrayed_in_films -> m.0h0v4pz -> film.performance.film -> Masterpiece Contemporary: Lennon Naked\n# Answer:\nMasterpiece Contemporary: Lennon Naked", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.directed_by -> Yoko Ono\n# Answer:\nYoko Ono"], "ground_truth": ["Rock music", "Pop music", "Experimental music", "Pop rock", "Experimental rock", "Psychedelic rock", "Blues rock", "Soft rock", "Art rock"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.576923076923077, "ans_precission": 0.6, "ans_recall": 0.5555555555555556, "path_f1": 0.5263157894736842, "path_precision": 0.5, "path_recall": 0.5555555555555556, "path_ans_f1": 0.576923076923077, "path_ans_precision": 0.6, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.office_holder -> Charles S. Thomas\n# Answer:\nCharles S. Thomas", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qppf9 -> government.government_position_held.office_holder -> Charles J. Hughes, Jr.\n# Answer:\nCharles J. Hughes, Jr.", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.jurisdiction_of_office -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wy4l -> government.government_position_held.office_holder -> Ken Salazar\n# Answer:\nKen Salazar", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcqg -> government.government_position_held.office_holder -> Frederick Walker Pitkin\n# Answer:\nFrederick Walker Pitkin", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qppf9 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 0.1, "path_ans_recall": 0.5}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Russia\n# Answer:\nRussia", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> location.country.second_level_divisions -> Kunoy Municipality\n# Answer:\nKunoy Municipality", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Iceland\n# Answer:\nIceland", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.contains -> Arctic Ocean\n# Answer:\nArctic Ocean", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Finland\n# Answer:\nFinland", "# Reasoning Path:\nGreenland -> location.location.partially_contained_by -> m.0wg9h8g -> location.partial_containment_relationship.partially_contained_by -> Arctic\n# Answer:\nArctic"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> m.066ht07\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.location.geolocation -> m.0khstt\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98105\n# Answer:\n98105", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.location.containedby -> Washington\n# Answer:\nWashington"], "ground_truth": ["98188", "98161", "98170", "98181", "98160", "98109", "98158", "98148", "98190", "98195", "98115", "98194", "98177", "98144", "98155", "98126", "98141", "98112", "98119", "98131", "98113", "98138", "98111", "98122", "98145", "98101", "98117", "98165", "98174", "98134", "98133", "98136", "98166", "98121", "98139", "98184", "98127", "98102", "98116", "98107", "98175", "98124", "98103", "98108", "98171", "98154", "98104", "98106", "98146", "98119-4114", "98132", "98178", "98129", "98198", "98191", "98118", "98185", "98105", "98164", "98199", "98114", "98125", "98168"], "ans_acc": 0.07936507936507936, "ans_hit": 1, "ans_f1": 0.13698630136986303, "ans_precission": 0.5, "ans_recall": 0.07936507936507936, "path_f1": 0.13698630136986303, "path_precision": 0.5, "path_recall": 0.07936507936507936, "path_ans_f1": 0.13698630136986303, "path_ans_precision": 0.5, "path_ans_recall": 0.07936507936507936}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nRobsol Pinkett, Jr.", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nAdrienne Banfield-Jones", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Jaden Smith\n# Answer:\nJaden Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nPinkett-Smith", "# Reasoning Path:\nWillow Smith -> common.topic.article -> m.03gq437\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.0bmsb49\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> symbols.namesake.named_after -> Jada Rowland\n# Answer:\nJada Rowland", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Willard Carroll Trey Smith III\n# Answer:\nWillard Carroll Trey Smith III"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Simplified Chinese character\n# Answer:\nSimplified Chinese character", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> book.book_subject.works -> Lacan's Smile -> book.written_work.subjects -> Jacques Lacan\n# Answer:\nLacan's Smile", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> base.schemastaging.context_name.pronunciation -> g.125_plpnh\n# Answer:\nbase.schemastaging.context_name.pronunciation", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Korea\n# Answer:\nKorea", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> The China Story\n# Answer:\nEast Asia"], "ground_truth": ["Simplified Chinese character", "'Phags-pa script", "Chinese characters", "Traditional Chinese characters", "N\u00fcshu script"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.spouse_s -> m.0j4ks8g\n# Answer:\npeople.person.spouse_s", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> common.topic.webpage -> m.09y7_p7 -> common.webpage.resource -> Mark Felt (Deep Throat) dies: A memorial Throat-off\n# Answer:\nMark Felt (Deep Throat) dies: A memorial Throat-off", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Harold Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf3 -> tv.regular_tv_appearance.actor -> Sherman Hemsley\n# Answer:\nSherman Hemsley", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.ceremony -> 16th NAACP Image Awards\n# Answer:\n16th NAACP Image Awards", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.0l5j2k4 -> tv.regular_tv_appearance.actor -> Zara Cully\n# Answer:\nZara Cully", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf3 -> tv.regular_tv_appearance.character -> George Jefferson\n# Answer:\nGeorge Jefferson", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff -> award.award_honor.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tmkvq -> award.award_honor.award -> NAACP Image Award for Outstanding Actor in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actor in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf8 -> tv.regular_tv_appearance.actor -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.award_nominee -> Sherman Hemsley\n# Answer:\nSherman Hemsley"], "ground_truth": ["Berlinda Tolbert", "Franklin Cover", "Sherman Hemsley", "Marla Gibbs", "Mike Evans", "Jay Hammer", "Paul Benedict", "Isabel Sanford", "Damon Evans", "Roxie Roker", "Zara Cully"], "ans_acc": 0.36363636363636365, "ans_hit": 1, "ans_f1": 0.4210526315789474, "ans_precission": 0.5, "ans_recall": 0.36363636363636365, "path_f1": 0.14492753623188406, "path_precision": 0.5, "path_recall": 0.0847457627118644, "path_ans_f1": 0.4210526315789474, "path_ans_precision": 0.5, "path_ans_recall": 0.36363636363636365}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Call\n# Answer:\nSan Francisco Call", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Bay Area Reporter\n# Answer:\nBay Area Reporter", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> California Star\n# Answer:\nCalifornia Star", "# Reasoning Path:\nSan Francisco -> common.topic.webpage -> m.03ll53z -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Dock of the Bay\n# Answer:\nDock of the Bay", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Don't Call it Frisco -> common.topic.notable_for -> g.125gwqscg\n# Answer:\nDon't Call it Frisco", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94107 -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["Bay Area Reporter", "San Francisco Chronicle", "Street Sheet", "Sing Tao Daily", "The Daily Alta California", "San Francisco Bay Guardian", "San Francisco Daily", "San Francisco Bay View", "San Francisco News-Call Bulletin Newspaper", "Dock of the Bay", "San Francisco Call", "Synapse", "San Francisco Bay Times", "The Golden Era", "San Francisco Business Times", "The San Francisco Examiner", "Free Society", "California Star", "AsianWeek", "San Francisco Foghorn"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.3333333333333333, "ans_precission": 0.5, "ans_recall": 0.25, "path_f1": 0.3333333333333333, "path_precision": 0.5, "path_recall": 0.25, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.5, "path_ans_recall": 0.25}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Porak -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> geography.river.basin_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nNorthern Hemisphere", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_containedby -> Turkey\n# Answer:\nTurkey", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aras -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Arpa -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination in ways which appear natural", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> Transmural Myocardial Infarction\n# Answer:\nTransmural Myocardial Infarction", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nAortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Kar\u014dshi\n# Answer:\nKar\u014dshi", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> ST segment elevation myocardial infarction\n# Answer:\nST segment elevation myocardial infarction", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.treatments -> Thrombolytic drug\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nAbdominal aortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.risk_factors -> Tobacco smoking\n# Answer:\nTobacco smoking"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1837-1843", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 9: 1861\n# Answer:\nThe Correspondence of Charles Darwin, Volume 9: 1861", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 8: 1860\n# Answer:\nThe Correspondence of Charles Darwin, Volume 8: 1860", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 10: 1862\n# Answer:\nThe Correspondence of Charles Darwin, Volume 10: 1862", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> History\n# Answer:\nHistory", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_types -> Literary Series\n# Answer:\nLiterary Series", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> base.documentaryeditions.documentary_editions_series.editing_project -> Darwin Correspondence Project\n# Answer:\nDarwin Correspondence Project"], "ground_truth": ["A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "On evolution", "Volcanic Islands", "The education of Darwin", "South American Geology", "Wu zhong qi yuan", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 13: 1865", "Rejse om jorden", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Formation of Vegetable Mould through the Action of Worms", "The Variation of Animals and Plants under Domestication", "The Orgin of Species", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 9: 1861", "Beagle letters", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Darwin's Ornithological notes", "Darwinism stated by Darwin himself", "The foundations of the Origin of species", "Leben und Briefe von Charles Darwin", "Evolution and natural selection", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "The living thoughts of Darwin", "Kleinere geologische Abhandlungen", "Memorias y epistolario i\u0301ntimo", "The Correspondence of Charles Darwin, Volume 14: 1866", "Charles Darwin's natural selection", "El Origin De Las Especies", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Resa kring jorden", "The Autobiography of Charles Darwin", "Evolution by natural selection", "Metaphysics, Materialism, & the evolution of mind", "Tesakneri tsagume\u030c", "Geological Observations on the Volcanic Islands", "Darwin on humus and the earthworm", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 11: 1863", "Part I: Contributions to the Theory of Natural Selection / Part II", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "To the members of the Down Friendly Club", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Essential Darwin", "Charles Darwin's marginalia", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 18: 1870", "monograph on the sub-class Cirripedia", "Charles Darwin", "H.M.S. Beagle in South America", "The Life and Letters of Charles Darwin Volume 2", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Darwin's journal", "On Natural Selection", "A student's introduction to Charles Darwin", "The geology of the voyage of H.M.S. Beagle", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "From Darwin's unpublished notebooks", "Darwin and Henslow", "From so simple a beginning", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Darwin-Wallace", "Opsht\u0323amung fun menshen", "Charles Darwin's letters", "More Letters of Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 16: 1868", "The principal works", "On a remarkable bar of sandstone off Pernambuco", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Voyage of the Beagle", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Darwin's notebooks on transmutation of species", "Darwin Darwin", "The Descent of Man, and Selection in Relation to Sex", "Notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 15: 1867", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Reise eines Naturforschers um die Welt", "The Life of Erasmus Darwin", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "The Expression of the Emotions in Man and Animals", "Diary of the voyage of H.M.S. Beagle", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Evolutionary Writings: Including the Autobiographies", "The Correspondence of Charles Darwin, Volume 12: 1864", "Les moyens d'expression chez les animaux", "Die geschlechtliche Zuchtwahl", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die fundamente zur entstehung der arten", "Darwin en Patagonia", "Human nature, Darwin's view", "Evolution", "Darwin for Today", "On the Movements and Habits of Climbing Plants", "Motsa ha-minim", "La vie et la correspondance de Charles Darwin", "Les mouvements et les habitudes des plantes grimpantes", "La facult\u00e9 motrice dans les plantes", "The Life and Letters of Charles Darwin Volume 1", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Darwin Compendium", "The voyage of Charles Darwin", "Origins", "Fertilisation of Orchids", "genese\u014ds t\u014dn eid\u014dn", "Works", "The\u0301orie de l'e\u0301volution", "Gesammelte kleinere Schriften", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Notes on the fertilization of orchids", "The Power of Movement in Plants", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The action of carbonate of ammonia on the roots of certain plants", "Cartas de Darwin 18251859", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "ontstaan der soorten door natuurlijke teeltkeus", "Het uitdrukken van emoties bij mens en dier", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Diario del Viaje de Un Naturalista Alrededor", "Proiskhozhdenie vidov", "The Different Forms of Flowers on Plants of the Same Species", "Darwin from Insectivorous Plants to Worms", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The collected papers of Charles Darwin", "A Darwin Selection", "Del Plata a Tierra del Fuego", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Structure and Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "On the tendency of species to form varieties", "Questions about the breeding of animals", "On the origin of species by means of natural selection", "Darwin's insects"], "ans_acc": 0.058823529411764705, "ans_hit": 1, "ans_f1": 0.09730668983492616, "ans_precission": 0.7, "ans_recall": 0.05228758169934641, "path_f1": 0.4117647058823529, "path_precision": 1.0, "path_recall": 0.25925925925925924, "path_ans_f1": 0.1111111111111111, "path_ans_precision": 1.0, "path_ans_recall": 0.058823529411764705}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> common.topic.notable_types -> City/Town/Village\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglt -> tv.tv_guest_role.episodes_appeared_in -> Episode #30\n# Answer:\nEpisode #30", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> common.topic.webpage -> m.09y7_p7 -> common.webpage.resource -> Mark Felt (Deep Throat) dies: A memorial Throat-off\n# Answer:\nMark Felt (Deep Throat) dies: A memorial Throat-off", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nOur Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> common.topic.webpage -> m.09y7_p7 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> music.album.releases -> Greatest Hits Vol. 2\n# Answer:\nGreatest Hits Vol. 2", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> You've Really Got a Hold on Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> You've Really Got a Hold on Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Blame It on Love & All the Greatest Hits -> music.album.album_content_type -> Compilation album\n# Answer:\nBlame It on Love & All the Greatest Hits", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> common.topic.article -> m.02py0ln\n# Answer:\nGreatest Hits, Vol. 2", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> I've Been Good to You\n# Answer:\nI've Been Good to You", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Blame It on Love & All the Greatest Hits -> music.album.release_type -> Album\n# Answer:\nBlame It on Love & All the Greatest Hits", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nAlan Motley", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> music.album.artist -> The Miracles\n# Answer:\nThe Miracles"], "ground_truth": ["Photograph in My Mind", "Why Do Happy Memories Hurt So Bad", "Be Careful What You Wish For", "One Heartbeat", "One Time", "So Bad", "Love Bath", "You've Really Go a Hold on Me", "We Are The Warriors", "Tears Of A Clown", "Christmas Every Day", "You're Just My Life (feat. India.Arie)", "If You Wanna Make Love", "Ooo Baby Baby", "Gone Forever", "Blame It on Love", "Keep Me", "You Go to My Head", "Christmas Everyday", "Please Don't Take Your Love (feat. Carlos Santana)", "Aqui Con Tigo (Being With You)", "Going to a Gogo", "Fulfill Your Need", "The Tracks of My Tears", "Jesus Told Me To Love You", "More Than You Know", "I Can't Get Enough", "Be Careful What You Wish For (instrumental)", "There Will Come A Day ( I'm Gonna Happen To You )", "You Made Me Feel Love", "Ever Had A Dream", "Because of You It's the Best It's Ever Been", "I've Made Love to You a Thousand Times", "Did You Know (Berry's Theme)", "Driving Thru Life in the Fast Lane", "Going to a Go-Go", "Can't Fight Love", "Tears of a Clown", "Walk on By", "Close Encounters of the First Kind", "Just Like You", "That Place", "Sleepless Nights", "Nearness of You", "Vitamin U", "Santa Claus is Coming to Town", "Quiet Storm (single version)", "Tea for Two", "Pops, We Love You", "Be Kind to the Growing Mind", "Will You Love Me Tomorrow", "Pops, We Love You (disco)", "I Love Your Face", "Get Ready", "The Tears Of A Clown", "You Cannot Laugh Alone", "Really Gonna Miss You", "Tracks of my Tears", "Be Who You Are", "Love Letters", "Tell Me Tomorrow, Part 1", "My Guy", "I Know You by Heart", "I Hear The Children Singing", "Shoe Soul", "It's A Good Night", "Christmas Greeting", "Why Are You Running From My Love", "I Can't Find", "Little Girl, Little Girl", "It's Time to Stop Shoppin' Around", "Love Don' Give No Reason (12 Inch Club Mix)", "You Are Forever", "Going to a Go Go", "Food For Thought", "A Silent Partner in a Three-Way Love Affair", "Come to Me Soon", "Let Your Light Shine On Me", "Wishful Thinking", "Just Passing Through", "Cruisin", "Te Quiero Como Si No Hubiera Un Manana", "If You Want My Love", "The Tracks of My Heart", "Tell Me Tomorrow", "You're the One for Me (feat. Joss Stone)", "Love' n Life", "Whatcha Gonna Do", "You Are So Beautiful (feat. Dave Koz)", "More Love", "Quiet Storm (Groove Boutique Chill Jazz mix)", "I've Got You Under My Skin", "Love Brought Us Here", "Tracks Of My Tears (Live)", "Fly Me to the Moon (In Other Words)", "Cruisin'", "Let Me Be the Clock", "I'm in the Mood for Love", "Everything for Christmas", "Night and Day", "It's Her Turn to Live", "Medley: Never My Love / Never Can Say Goodbye", "Girl I'm Standing There", "And I Don't Love You", "A Child Is Waiting", "Sweet Harmony", "Just My Soul Responding", "Bad Girl", "Will You Love Me Tomorrow?", "Daylight & Darkness", "Season's Greetings from Smokey Robinson", "Quiet Storm", "Never My Love / Never Can Say Goodbye", "It's Christmas Time", "With Your Love Came", "I Care About Detroit", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Being With You", "Don't Play Another Love Song", "And I Love Her", "Quiet Storm (Groove Boutique remix)", "Open", "There Will Come a Day (I'm Gonna Happen to You)", "Heavy On Pride (Light On Love)", "You Take Me Away", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "I\u2019ve Got You Under My Skin", "Mickey's Monkey", "My Girl", "Jasmin", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Our Love Is Here to Stay", "Rewind", "The Family Song", "Save Me", "It's a Good Feeling", "Be Kind To The Growing Mind (with The Temptations)", "Just Another Kiss", "Ooh Baby Baby", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Love Don't Give No Reason", "Girlfriend", "Virgin Man", "Please Come Home for Christmas", "Rack Me Back", "Little Girl Little Girl", "Take Me Through The Night", "Tears of a Sweet Free Clown", "Love So Fine", "(It's The) Same Old Love", "Baby Come Close", "I Second That Emotion", "I Love The Nearness Of You", "Holly", "The Hurt's On You", "Double Good Everything", "Coincidentally", "The Christmas Song", "Let Me Be The Clock", "No\u00ebl", "Share It", "Unless You Do It Again", "You Don't Know What It's Like", "Mother's Son", "Crusin", "I Second That Emotions", "Will You Still Love Me Tomorrow", "I've Made Love To You A Thousand Times", "The Way You Do (The Things You Do)", "Train of Thought", "Just to See Her", "Everything You Touch", "Asleep on My Love", "When Smokey Sings Tears Of A Clown", "She's Only a Baby Herself", "The Track of My Tears", "Love Is The Light", "Standing On Jesus", "Time After Time", "Deck the Halls", "The Tears of a Clown", "In My Corner", "Wedding Song", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "We've Saved the Best for Last", "A Tattoo", "I'll Keep My Light In My Window", "As You Do", "Easy", "I Want You Back", "The Tracks of My Tears (live)", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Hold on to Your Love", "Tell Me Tomorrow (12\\\" extended mix)", "Away in the Manger / Coventry Carol", "The Tracks Of My Tears", "Ooo Baby Baby (live)", "Melody Man", "And I Don't Love You (Larry Levan instrumental dub)", "Gang Bangin'", "The Agony and the Ecstasy", "Tracks of My Tears", "Don't Know Why", "What's Too Much", "God Rest Ye Merry Gentlemen", "Same Old Love", "Happy (Love Theme From Lady Sings the Blues)", "Blame It On Love (Duet with Barbara Mitchell)", "I Have Prayed On It", "Shop Around", "The Love Between Me and My Kids", "Winter Wonderland", "Why", "Theme From the Big Time", "Just To See Her Again", "I Can\u2019t Stand to See You Cry (Commercial version)", "Yester Love", "If You Can Want", "Just a Touch Away", "I Am I Am", "Ebony Eyes", "Ebony Eyes (Duet with Rick James)", "Come by Here (Kum Ba Ya)", "The Agony And The Ecstasy", "No Time to Stop Believing", "Hanging on by a Thread", "Jingle Bells", "Some People Will Do Anything for Love", "I'm Glad There Is You", "Speak Low", "I Am, I Am", "Time Flies", "Fallin'", "If You Wanna Make Love (Come 'round Here)", "Yes It's You Lady", "Who's Sad", "He Can Fix Anything", "Don't Wanna Be Just Physical", "Ain't That Peculiar", "You Really Got a Hold on Me", "Satisfy You", "I Can't Give You Anything but Love", "I Like Your Face", "My World", "I Praise & Worship You Father", "The Road to Damascus", "We\u2019ve Come Too Far to End It Now", "Noel", "Wanna Know My Mind", "Skid Row", "Baby That's Backatcha", "You've Really Got a Hold on Me", "It's Fantastic", "Crusin'", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "When A Woman Cries"], "ans_acc": 0.0078125, "ans_hit": 1, "ans_f1": 0.01532567049808429, "ans_precission": 0.4, "ans_recall": 0.0078125, "path_f1": 0.02259887005649718, "path_precision": 0.1, "path_recall": 0.012738853503184714, "path_ans_f1": 0.01532567049808429, "path_ans_precision": 0.4, "path_ans_recall": 0.0078125}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> location.location.containedby -> Brevard County\n# Answer:\nBrevard County", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\nBloomington Kennedy High School", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> Praise From a Future Generation: The Assassination of John F. Kennedy and the First Generation Critics of the Warren Report\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> film.film_location.featured_in_films -> Marooned\n# Answer:\nMarooned", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nAssassination of John F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> book.author.works_written -> A Nation of Immigrants -> book.written_work.previous_in_series -> Profiles in Courage\n# Answer:\nProfiles in Courage"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> language.language_dialect.language -> Serbo-Croatian Language\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language -> language.human_language.countries_spoken_in -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbania", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nIndo-European languages", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> time.event.locations -> Czech Republic\n# Answer:\nCzech Republic"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
