{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_m\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\nFlorida", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6f6_n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\nLouisiana", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> common.topic.notable_types -> Tropical Cyclone\n# Answer:\nTropical Cyclone", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.12tb6fszp\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Cuba\n# Answer:\nCuba"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.office_position_or_title -> Governor of Tennessee\n# Answer:\nGovernor of Tennessee", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specialization_of -> Criminal defense lawyer\n# Answer:\nCriminal defense lawyer", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j5sk8 -> government.government_position_held.jurisdiction_of_office -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04469y8 -> government.government_position_held.basic_title -> President\n# Answer:\nPresident", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specialization_of -> Agriculturalist\n# Answer:\nAgriculturalist", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> common.topic.subjects -> Sullivan & Galleshaw, LLP\n# Answer:\nLawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician"], "ground_truth": ["United States Representative", "Speaker of the United States House of Representatives", "Governor of Tennessee"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.25, "path_precision": 0.2, "path_recall": 0.3333333333333333, "path_ans_f1": 0.30769230769230765, "path_ans_precision": 0.2, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_3vpv\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_4dk3\n# Answer:\nlocation.statistical_region.energy_use_per_capita", "# Reasoning Path:\nHaiti -> olympics.olympic_participating_country.medals_won -> m.07nnrw4 -> olympics.olympic_medal_honor.medal -> Silver medal\n# Answer:\nSilver medal", "# Reasoning Path:\nHaiti -> olympics.olympic_participating_country.medals_won -> m.07nnrw4 -> olympics.olympic_medal_honor.olympics -> 1928 Summer Olympics\n# Answer:\n1928 Summer Olympics"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.22222222222222224, "path_precision": 0.14285714285714285, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.actor -> Ilyssa Fradin\n# Answer:\nIlyssa Fradin", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4 -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.film -> Batman Begins\n# Answer:\nBatman Begins", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.actor -> Melinda McGraw\n# Answer:\nMelinda McGraw", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4 -> film.performance.actor -> Hannah Gunn\n# Answer:\nHannah Gunn", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> base.descriptive_names.names.descriptive_name -> m.0101pjk9\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> fictional_universe.fictional_job_title.fictional_characters_with_this_job -> m.0c71cxl\n# Answer:\nfictional_universe.fictional_job_title.fictional_characters_with_this_job"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72b -> sports.sports_team_roster.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04q9532 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0j2dtly -> sports.sports_team_roster.team -> Boston Celtics\n# Answer:\nBoston Celtics", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcbdr -> basketball.basketball_player_stats.season -> 1995\u201396 NBA season\n# Answer:\n1995\u201396 NBA season", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.04ynxq7 -> sports.sports_award.award -> NBA All-Star Game Most Valuable Player Award\n# Answer:\nNBA All-Star Game Most Valuable Player Award", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc3w -> sports.sports_award.season -> 1992\u201393 NBA season\n# Answer:\n1992\u201393 NBA season"], "ground_truth": ["Miami Heat", "Los Angeles Lakers", "LSU Tigers men's basketball", "Phoenix Suns", "Orlando Magic", "Cleveland Cavaliers", "Boston Celtics"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.3, "path_precision": 0.6, "path_recall": 0.2, "path_ans_f1": 0.5, "path_ans_precision": 0.6, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> European American\n# Answer:\nEuropean American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> Scottish people\n# Answer:\nScottish people", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Chandler Muriel Bing\n# Answer:\nChandler Muriel Bing", "# Reasoning Path:\nJay Leno -> film.actor.film -> m.02t9_1z -> film.performance.film -> Americathon\n# Answer:\nAmericathon", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09wfyz1\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0cqc3ql -> award.award_honor.award_winner -> Patti M. Grant\n# Answer:\nPatti M. Grant", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> fictional_universe.ethnicity_in_fiction.characters_of_this_ethnicity -> Mutt Williams\n# Answer:\nMutt Williams"], "ground_truth": ["New Rochelle"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> common.topic.notable_types -> Human Language\n# Answer:\nHuman Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language -> language.human_language.main_country -> Vietnam\n# Answer:\nVietnam", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Vietnamese Language", "Saek language", "Thai Language", "Mlabri Language", "Phu Thai language", "Lao Language", "Mon Language", "Khmer language", "Cham language", "Nyaw Language", "Hmong language", "Malay, Pattani Language", "Akha Language"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.2181818181818182, "ans_precission": 0.375, "ans_recall": 0.15384615384615385, "path_f1": 0.3157894736842105, "path_precision": 0.5, "path_recall": 0.23076923076923078, "path_ans_f1": 0.33707865168539325, "path_ans_precision": 0.625, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0dlskcl -> award.award_nomination.ceremony -> 37th People's Choice Awards\n# Answer:\n37th People's Choice Awards", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Internet Rising\n# Answer:\nInternet Rising", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.subject_of -> Uolala\n# Answer:\nUolala", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0dlskcl -> award.award_nomination.award -> People's Choice Award for Favorite Dramatic Movie\n# Answer:\nPeople's Choice Award for Favorite Dramatic Movie", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0_rfq07 -> award.award_nomination.award -> NAACP Image Award for Outstanding Supporting Actor in a Motion Picture\n# Answer:\nNAACP Image Award for Outstanding Supporting Actor in a Motion Picture", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpnkz_ -> award.award_nomination.award -> Satellite Award for Best Actor \u2013 Motion Picture Drama\n# Answer:\nSatellite Award for Best Actor \u2013 Motion Picture Drama", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> film.film_subject.films -> Catfish\n# Answer:\nCatfish"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04j5sl4 -> government.government_position_held.jurisdiction_of_office -> Virginia\n# Answer:\nVirginia", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9m7 -> government.government_position_held.basic_title -> Secretary of State\n# Answer:\nSecretary of State", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9m7 -> government.government_position_held.appointed_by -> George Washington\n# Answer:\nGeorge Washington", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.03fx8bt -> government.government_position_held.basic_title -> President\n# Answer:\nPresident"], "ground_truth": ["Lawyer", "Writer", "Author", "Architect", "Farmer", "Archaeologist", "Statesman", "Inventor", "Teacher", "Philosopher"], "ans_acc": 0.3, "ans_hit": 1, "ans_f1": 0.20000000000000004, "ans_precission": 0.2, "ans_recall": 0.2, "path_f1": 0.3, "path_precision": 0.3, "path_recall": 0.3, "path_ans_f1": 0.3, "path_ans_precision": 0.3, "path_ans_recall": 0.3}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin\n# Answer:\nThe Correspondence of Charles Darwin", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nEvolution", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Paleontology\n# Answer:\nPaleontology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Charles Darwin: The Power of Place\n# Answer:\nCharles Darwin: The Power of Place", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nCambridgeshire", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> England\n# Answer:\nEngland"], "ground_truth": ["The expression of the emotions in man and animals", "monograph on the sub-class Cirripedia", "Darwin and Henslow", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Origin of Species", "Voyage of the Beagle", "The collected papers of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Insectivorous Plants", "Notebooks on transmutation of species", "The Darwin Reader Second Edition", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Voyage of the Beagle (Adventure Classics)", "The Origin of Species (Mentor)", "Gesammelte kleinere Schriften", "The Structure and Distribution of Coral Reefs", "Die fundamente zur entstehung der arten", "The Essential Darwin", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 4", "Darwin-Wallace", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Voyage of the Beagle (Harvard Classics, Part 29)", "red notebook of Charles Darwin", "Darwin en Patagonia", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "The expression of the emotions in man and animals.", "The Variation of Animals and Plants under Domestication", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "On Natural Selection", "Questions about the breeding of animals", "The Correspondence of Charles Darwin, Volume 7", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The voyage of Charles Darwin", "Opsht\u0323amung fun menshen", "Voyage of the Beagle (NG Adventure Classics)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Beagle letters", "Volcanic Islands", "The Correspondence of Charles Darwin, Volume 13", "To the members of the Down Friendly Club", "The Correspondence of Charles Darwin, Volume 5", "On the Movements and Habits of Climbing Plants", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Diary of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 10", "The Correspondence of Charles Darwin, Volume 9: 1861", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The Autobiography of Charles Darwin (Dodo Press)", "Voyage of the Beagle (Dover Value Editions)", "The Correspondence of Charles Darwin, Volume 14", "Human nature, Darwin's view", "The voyage of the Beagle.", "On the tendency of species to form varieties", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Origin of Species (World's Classics)", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 2", "The Voyage of the Beagle", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 15", "Voyage d'un naturaliste autour du monde", "Les moyens d'expression chez les animaux", "The Correspondence of Charles Darwin, Volume 12", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Origin of Species (Everyman's University Paperbacks)", "The Correspondence of Charles Darwin, Volume 9", "The structure and distribution of coral reefs", "Wu zhong qi yuan", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "A student's introduction to Charles Darwin", "Kleinere geologische Abhandlungen", "Charles Darwin's marginalia", "The autobiography of Charles Darwin", "The Autobiography Of Charles Darwin", "The Structure And Distribution of Coral Reefs", "The Correspondence of Charles Darwin, Volume 12: 1864", "Memorias y epistolario i\u0301ntimo", "Motsa ha-minim", "Leben und Briefe von Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Del Plata a Tierra del Fuego", "The Origin of Species (Great Books : Learning Channel)", "From so simple a beginning", "The Different Forms of Flowers on Plants of the Same Species", "The Origin of Species (Great Minds Series)", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Reise eines Naturforschers um die Welt", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Orgin of Species", "Evolution and natural selection", "The geology of the voyage of H.M.S. Beagle", "Darwin on humus and the earthworm", "Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The origin of species : complete and fully illustrated", "Charles Darwin", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 8: 1860", "On evolution", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Correspondence of Charles Darwin, Volume 11", "The portable Darwin", "La vie et la correspondance de Charles Darwin", "Proiskhozhdenie vidov", "The structure and distribution of coral reefs.", "H.M.S. Beagle in South America", "Darwinism stated by Darwin himself", "The autobiography of Charles Darwin, 1809-1882", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "From Darwin's unpublished notebooks", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 16: 1868", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Origins", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 17: 1869", "Fertilisation of Orchids", "The Life of Erasmus Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 11: 1863", "Voyage Of The Beagle", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Descent of Man and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Voyage of the Beagle (Great Minds Series)", "Resa kring jorden", "ontstaan der soorten door natuurlijke teeltkeus", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "Darwin Darwin", "The Correspondence of Charles Darwin, Volume 6", "On a remarkable bar of sandstone off Pernambuco", "The descent of man, and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 8", "The Correspondence of Charles Darwin, Volume 14: 1866", "The action of carbonate of ammonia on the roots of certain plants", "The Origin of Species (Collector's Library)", "The education of Darwin", "The Autobiography of Charles Darwin", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The living thoughts of Darwin", "Origin of Species (Harvard Classics, Part 11)", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Charles Darwin on the routes of male humble bees", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "The Origin Of Species", "Monographs of the fossil Lepadidae and the fossil Balanidae", "On the origin of species by means of natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The Voyage of the Beagle (Mentor)", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The foundations of the Origin of species", "La facult\u00e9 motrice dans les plantes", "The Descent of Man, and Selection in Relation to Sex", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The principal works", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Correspondence of Charles Darwin, Volume 1", "The origin of species", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 3", "The Voyage of the Beagle (Everyman Paperbacks)", "More Letters of Charles Darwin", "Charles Darwin's letters", "From So Simple a Beginning", "Metaphysics, Materialism, & the evolution of mind", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Rejse om jorden", "The Autobiography of Charles Darwin, and selected letters", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Evolution", "Notes on the fertilization of orchids", "Works", "El Origin De Las Especies", "Darwin's notebooks on transmutation of species", "Darwin's journal", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Geological Observations on South America", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Origin of Species", "The\u0301orie de l'e\u0301volution", "A Darwin Selection", "Charles Darwin's natural selection", "Darwin's Ornithological notes", "Tesakneri tsagume\u030c", "The Autobiography of Charles Darwin (Great Minds Series)", "Reise um die Welt 1831 - 36", "The Autobiography of Charles Darwin (Large Print)", "Die geschlechtliche Zuchtwahl", "The Origin of Species (Oxford World's Classics)", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Expression of the Emotions in Man And Animals", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The descent of man and selection in relation to sex.", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Autobiography of Charles Darwin", "The Voyage of the Beagle (Unabridged Classics)", "Evolution by natural selection", "Darwin for Today", "The descent of man, and selection in relation to sex"], "ans_acc": 0.04205607476635514, "ans_hit": 1, "ans_f1": 0.07758620689655173, "ans_precission": 0.5, "ans_recall": 0.04205607476635514, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.08071748878923767, "path_ans_precision": 1.0, "path_ans_recall": 0.04205607476635514}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nNathan Whitaker", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0sgkpfw -> award.award_nomination.award -> Kids' Choice Award for Favorite Male Athlete\n# Answer:\nKids' Choice Award for Favorite Male Athlete", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.article -> m.0j4d5g4\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0sgkpfw -> freebase.valuenotation.has_no_value -> Nominated work\n# Answer:\nNominated work"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nPerson", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps -> sports.sports_award.award -> NFL Pro Bowl Most Valuable Player Award\n# Answer:\nNFL Pro Bowl Most Valuable Player Award"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8t4c -> location.partial_containment_relationship.partially_contained_by -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8s1x -> location.partial_containment_relationship.partially_contained_by -> Poland\n# Answer:\nPoland", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_contained_by -> m.0wg8sdn -> location.partial_containment_relationship.partially_contained_by -> Czech Republic\n# Answer:\nCzech Republic"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTopic", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> people.person.profession -> Novelist\n# Answer:\nNovelist", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Percy Bysshe Shelley -> people.person.quotations -> Life may change, but it may fly not; Hope may vanish, but can die not; Truth be veiled, but still it burneth; Love repulsed, -- but it returneth.\n# Answer:\nPercy Bysshe Shelley"], "ground_truth": ["Bard", "Author", "Poet", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.actor -> Abraham Benrubi\n# Answer:\nAbraham Benrubi", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.010wvb1d -> film.performance.film -> Robot Chicken: Star Wars Episode III\n# Answer:\nRobot Chicken: Star Wars Episode III", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.actor -> Michael Fox\n# Answer:\nMichael Fox", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nStar Wars Episode II: Attack of the Clones", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> fictional_universe.fictional_setting.contains -> Apple Barrier\n# Answer:\nApple Barrier", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.film -> Blackstar Warrior\n# Answer:\nBlackstar Warrior", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.written_work.author -> Judy Blundell\n# Answer:\nJudy Blundell", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.book.genre -> Speculative fiction\n# Answer:\nSpeculative fiction", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> book.book.characters -> Obi-Wan Kenobi\n# Answer:\nObi-Wan Kenobi"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nPerson", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps -> sports.sports_award.season -> 2004 NFL season\n# Answer:\n2004 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate/Time", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.award -> Super Bowl Most Valuable Player Award\n# Answer:\nSuper Bowl Most Valuable Player Award"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Ontario\n# Answer:\nOntario", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nArea codes 519 and 226", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber's Believe -> film.film.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJustin Bieber -> film.producer.film -> Justin Bieber: Never Say Never -> film.film.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nJuno Awards of 2014", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.010lkp2z -> award.award_honor.award -> Billboard Music Award for Top Social Artist\n# Answer:\nBillboard Music Award for Top Social Artist", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nWinning work"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> military.military_conflict.combatants -> m.059q_5c\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.04fvd6y -> military.military_combatant_group.combatants -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpjb -> military.casualties.combatant -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> military.military_conflict.combatants -> m.05t6dmh\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Battle of 73 Easting -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> base.culturalevent.event.entity_involved -> Iraqi Army\n# Answer:\nIraqi Army"], "ground_truth": ["Australia", "United States of America", "France", "Saudi Arabia", "Argentina", "United Kingdom", "Iraq"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.7547169811320756, "ans_precission": 0.8, "ans_recall": 0.7142857142857143, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.7547169811320756, "path_ans_precision": 0.8, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gmpx -> tv.regular_tv_appearance.actor -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nMr. Moseby", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl67t -> award.award_nomination.award_nominee -> Cole Sprouse\n# Answer:\nCole Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gmpx -> tv.regular_tv_appearance.character -> Cody Martin\n# Answer:\nCody Martin", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86 -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Sidekick\n# Answer:\nKids' Choice Award for Favorite TV Sidekick", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gmpx -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 3\n# Answer:\nThe Suite Life on Deck - Season 3", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nLondon Tipton", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgl67t -> award.award_nomination.ceremony -> 2010 Kids' Choice Awards\n# Answer:\n2010 Kids' Choice Awards"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nGeorge Voinovich", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.notable_for -> g.125h3hwcp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\n111th United States Congress", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nSherrod Brown", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_holder -> Mike DeWine\n# Answer:\nMike DeWine", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nAlabama", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.tropical_cyclone_season -> 1982 Atlantic hurricane season\n# Answer:\nTropical Storm Chris", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Tropical Storm Chris -> meteorology.tropical_cyclone.affected_areas -> Arkansas\n# Answer:\nArkansas"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.borrowing_team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pd_2 -> soccer.football_player_stats.team -> England national under-21 football team\n# Answer:\nEngland national under-21 football team", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvddfb -> soccer.football_player_loan.borrowing_team -> Preston North End F.C.\n# Answer:\nPreston North End F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.lending_team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pdz2 -> soccer.football_player_stats.team -> A.C. Milan\n# Answer:\nA.C. Milan", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvddfb -> soccer.football_player_loan.lending_team -> Manchester United F.C.\n# Answer:\nManchester United F.C.", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Tim Thomerson\n# Answer:\nTim Thomerson", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0k6n97c\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado National Forest -> location.location.containedby -> Arizona\n# Answer:\nArizona", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Armando Favela\n# Answer:\nArmando Favela", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.people_born_here -> Bill Harrison\n# Answer:\nBill Harrison", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.statistical_region.population -> m.0k6n97l\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site -> common.topic.article -> m.04668xn\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado Historic Site -> location.location.containedby -> Sandoval County\n# Answer:\nSandoval County"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03lfq4h -> people.sibling_relationship.sibling -> Kathleen Cavendish\n# Answer:\nKathleen Cavendish", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.02_wjwf -> people.sibling_relationship.sibling -> Ted Kennedy\n# Answer:\nTed Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> people.person.sibling_s -> m.03hs9fc -> people.sibling_relationship.sibling -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> book.book_subject.works -> The Death of a President\n# Answer:\nThe Death of a President", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Assassination of John F. Kennedy -> common.topic.notable_types -> Event\n# Answer:\nEvent", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> common.topic.notable_for -> g.125583hnb\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> film.film_location.featured_in_films -> Marooned\n# Answer:\nMarooned", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Bloomington Kennedy High School -> education.educational_institution.school_type -> State school\n# Answer:\nState school", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> Cape Canaveral -> common.topic.article -> m.0f76p\n# Answer:\ncommon.topic.article"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.citytown -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> common.topic.notable_types -> Location\n# Answer:\nLocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.postal_code -> 760-8521\n# Answer:\n760-8521"], "ground_truth": ["Okuma", "Japan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nChild pornography", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nGary Glitter"], "ground_truth": ["Wales", "Northern Ireland", "England", "Scotland"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.75, "ans_precission": 0.75, "ans_recall": 0.75, "path_f1": 0.5, "path_precision": 0.375, "path_recall": 0.75, "path_ans_f1": 0.75, "path_ans_precision": 0.75, "path_ans_recall": 0.75}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_containedby -> Mississippi\n# Answer:\nMississippi", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_containedby -> Arkansas\n# Answer:\nArkansas", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nSabine River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Mississippi River -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8_r5\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nDenham Springs", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmv2\n# Answer:\nlocation.location.partially_contained_by"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> influence.influence_node.influenced -> Hans Urs von Balthasar\n# Answer:\nHans Urs von Balthasar", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> influence.influence_node.influenced -> Albert Camus\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Blaise Pascal -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> S\u00f8ren Kierkegaard -> influence.influence_node.influenced -> Alasdair MacIntyre\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Physician", "Philosopher", "Writer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.children -> Sean Cole\n# Answer:\nSean Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\nJ. Holiday", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nNeffeteria Pugh", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nPlace of birth", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> Mama -> music.recording.artist -> Ghostface Killah\n# Answer:\nGhostface Killah", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.releases -> Definition of Real\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nCountry of nationality", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nPlace of birth"], "ground_truth": ["Leon Cole", "Sal Gibson", "Francine Lons"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.41379310344827586, "path_precision": 0.3, "path_recall": 0.6666666666666666, "path_ans_f1": 0.41379310344827586, "path_ans_precision": 0.3, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nLibya", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nGovernmental Body", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 0.1, "ans_recall": 0.5, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Atlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> base.usnationalparks.us_national_park.state -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.film -> Alpha Man: The Brotherhood of MLK\n# Answer:\nAlpha Man: The Brotherhood of MLK", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Montgomery\n# Answer:\nMontgomery", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0_714v2 -> film.personal_film_appearance.film -> Alice Walker: Beauty in Truth\n# Answer:\nAlice Walker: Beauty in Truth", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.011j_4sh -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Dexter Avenue Baptist Church -> location.location.containedby -> Alabama\n# Answer:\nAlabama"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> media_common.quotation.subjects -> Christians and Christianity\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime. -> common.topic.image -> Edgar Allan Poe signature\n# Answer:\nA strong argument for the religion of Christ is this -- that offences against Charity are about the only ones which men on their death-beds can be made -- not to understand -- but to feel -- as crime.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> media_common.quotation.source -> The Cask of Amontillado\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> A wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong. -> common.topic.article -> m.05chc0k\n# Answer:\nA wrong is unredressed when retribution overtakes its redresser. It is equally unredressed when the avenger fails to make himself felt as such to him who has done the wrong.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment. -> media_common.quotation.subjects -> Soul\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment. -> common.topic.image -> Edgar Allan Poe 2\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment. -> media_common.quotation.subjects -> Meaning of life\n# Answer:\nAfter reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment.", "# Reasoning Path:\nEdgar Allan Poe -> people.person.quotations -> After reading all that has been written, and after thinking all that can be thought, on the topics of God and the soul, the man who has a right to say that he thinks at all, will find himself face to face with the conclusion that, on these topics, the most profound thought is that which can be the least easily distinguished from the most superficial sentiment. -> media_common.quotation.subjects -> God\n# Answer:\nGod"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> World of Coca-Cola\n# Answer:\nWorld of Coca-Cola", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Omni Atlanta Hotel at CNN Center\n# Answer:\nOmni Atlanta Hotel at CNN Center", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nZoo", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.serves -> Hyatt Regency Atlanta\n# Answer:\nHyatt Regency Atlanta", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> aviation.airport.focus_city_for -> ValuJet Airlines\n# Answer:\nValuJet Airlines", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09x1g73 -> common.webpage.resource -> Blink-182 postpones more concerts following death of friend DJ AM\n# Answer:\nBlink-182 postpones more concerts following death of friend DJ AM", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> location.location.containedby -> 30135\n# Answer:\nArbor Place Mall", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Arbor Place Mall -> business.shopping_center.address -> m.042znwp\n# Answer:\nArbor Place Mall"], "ground_truth": ["Woodruff Arts Center", "Atlanta Cyclorama & Civil War Museum", "Four Seasons Hotel Atlanta", "Fernbank Museum of Natural History", "Atlanta Symphony Orchestra", "CNN Center", "Masquerade", "World of Coca-Cola", "Underground Atlanta", "Six Flags White Water", "Georgia World Congress Center", "Hyatt Regency Atlanta", "The Tabernacle", "Cobb Energy Performing Arts Centre", "Jimmy Carter Library and Museum", "Fernbank Science Center", "Georgia Aquarium", "Georgia Dome", "Atlanta Jewish Film Festival", "Turner Field", "Atlanta History Center", "Centennial Olympic Park", "Georgia State Capitol", "Center for Puppetry Arts", "Six Flags Over Georgia", "Arbor Place Mall", "Margaret Mitchell House & Museum", "Variety Playhouse", "Philips Arena", "Atlanta Ballet", "Zoo Atlanta", "Omni Coliseum", "Fox Theatre", "Martin Luther King, Jr. National Historic Site", "Peachtree Road Race", "Atlanta Marriott Marquis"], "ans_acc": 0.1388888888888889, "ans_hit": 1, "ans_f1": 0.22556390977443608, "ans_precission": 0.6, "ans_recall": 0.1388888888888889, "path_f1": 0.14634146341463414, "path_precision": 0.6, "path_recall": 0.08333333333333333, "path_ans_f1": 0.27586206896551724, "path_ans_precision": 0.8, "path_ans_recall": 0.16666666666666666}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Tasmania\n# Answer:\nTasmania", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nBunyip", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Ashmore and Cartier Islands\n# Answer:\nAshmore and Cartier Islands", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.folklore.mythical_creature_location.mythical_creature_s -> Bunyip\n# Answer:\nBunyip", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Warwick -> location.location.containedby -> Australia\n# Answer:\nAustralia"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nThe N Crowd", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> common.topic.subject_of -> The N Crowd\n# Answer:\nThe N Crowd", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> tv.tv_genre.programs -> Gavin & Stacey\n# Answer:\nGavin & Stacey", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Thursday 10th June 2010 -> tv.multipart_tv_episode.episodes -> Thur 10 June, 2010 [Episode 1]\n# Answer:\nThur 10 June, 2010 [Episode 1]", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Chivalric romance -> book.book_subject.works -> Sir Gawain and the Green Knight\n# Answer:\nSir Gawain and the Green Knight", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> common.topic.subject_of -> Albrecht Behmel\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy -> comic_books.comic_book_genre.comic_book_series_in_this_genre -> Little Wansa\n# Answer:\nLittle Wansa", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_types -> Multipart TV episode\n# Answer:\nMultipart TV episode", "# Reasoning Path:\nCoronation Street -> tv.tv_program.seasons -> Coronation Street - Season 0 -> tv.tv_series_season.episodes -> I Am Ken Eternal\n# Answer:\nI Am Ken Eternal"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> people.person.place_of_birth -> Glasgow\n# Answer:\nGlasgow", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.0w4ftz9 -> freebase.valuenotation.is_reviewed -> Score\n# Answer:\nScore", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#domain -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.0w4ftz9 -> tennis.tennis_match.match_format -> Men's singles\n# Answer:\nMen's singles", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> rdf-schema#range -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.0w4ftz9 -> freebase.valuenotation.is_reviewed -> Date\n# Answer:\nDate", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nOpenCyc", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> m.0h_3m9l\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.internet_tld -> cx\n# Answer:\ncx", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> m.03j_zjk\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.resource -> m.0blsygc\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl -> common.webpage.resource -> m.0bjddkm\n# Answer:\ncommon.webpage.resource"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.currency -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> m.0nfs4_f -> measurement_unit.dated_money_value.source -> GNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nGNI per capita in PPP dollars, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\nlocation.statistical_region.part_time_employment_percent", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\nlocation.statistical_region.part_time_employment_percent"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> people.person.parents -> Jackie Newton -> common.topic.notable_for -> g.125dzwcd6\n# Answer:\nJackie Newton"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> base.aareas.schema.administrative_area.administrative_parent -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer -> common.topic.notable_types -> Author\n# Answer:\nAuthor"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nUniversity of Oxford", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Matar Un Ruisenor -> book.book_edition.book -> To Kill a Mockingbird\n# Answer:\nMatar Un Ruisenor", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Place of birth\n# Answer:\nPlace of birth", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History\n# Answer:\nHistory", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.author -> Thomas G. Alexander\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> book.book_subject.works -> Bound for Canaan -> book.written_work.author -> Margaret Blair Young\n# Answer:\nMargaret Blair Young", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_for -> g.125fjpyr7\n# Answer:\nBlossoms of faith", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> common.topic.notable_for -> g.125dtp7bg\n# Answer:\nUtah, the right place", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nLieutenant Governor of Utah", "# Reasoning Path:\nUtah -> book.book_subject.works -> Utah, the right place -> book.written_work.subjects -> History of the Americas\n# Answer:\nHistory of the Americas", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_types -> Book\n# Answer:\nBook"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nSiblings", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nHenry A. Crumpton", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointee -> Dan Mozena\n# Answer:\nDan Mozena", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointee -> Clifford Sobel\n# Answer:\nClifford Sobel", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.0cscnd7 -> film.personal_film_appearance.film -> A Moment in History: The Inauguration of Barack Obama\n# Answer:\nA Moment in History: The Inauguration of Barack Obama", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nDirector of the Bureau of Counterterrorism", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079pxt1 -> people.appointment.appointed_role -> United States Ambassador to Angola\n# Answer:\nUnited States Ambassador to Angola", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q3lx -> people.appointment.appointed_role -> United States Ambassador to Brazil\n# Answer:\nUnited States Ambassador to Brazil"], "ground_truth": ["Ralph Nader", "Michael Peroutka", "Gene Amondson", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nAyaan Hirsi Ali", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.spouse -> Sue Douglas\n# Answer:\nSue Douglas", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> freebase.valuenotation.has_value -> Location of ceremony\n# Answer:\nLocation of ceremony", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5 -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> freebase.valuenotation.has_value -> To\n# Answer:\nTo", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f -> film.personal_film_appearance.film -> The War of the World: A New History of the 20th Century\n# Answer:\nThe War of the World: A New History of the 20th Century", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The House of Rothschild: Money's Prophets, 1798-1848 -> book.book.editions -> The house of Rothschild\n# Answer:\nThe house of Rothschild", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The House of Rothschild: Money's Prophets, 1798-1848 -> common.topic.article -> m.06_rmpg\n# Answer:\ncommon.topic.article"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> San Crist\u00f3bal Canton\n# Answer:\nSan Crist\u00f3bal Canton", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.contains -> Puerto Baquerizo Moreno\n# Answer:\nPuerto Baquerizo Moreno", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.nearby_airports -> San Crist\u00f3bal Airport\n# Answer:\nSan Crist\u00f3bal Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nSanta Cruz Canton, Ecuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park -> protected_sites.protected_site.iucn_category -> National park\n# Answer:\nNational park", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nSanta Cruz Canton, Ecuador"], "ground_truth": ["Ecuador", "Pacific Ocean", "Gal\u00e1pagos Province"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> common.topic.subject_of -> L\u00e9o Ferr\u00e9\n# Answer:\nL\u00e9o Ferr\u00e9", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Carnatic Singer\n# Answer:\nCarnatic Singer", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> common.topic.subject_of -> Alan Motley\n# Answer:\nAlan Motley", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Wait for a Minute\n# Answer:\nWait for a Minute", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> people.profession.specializations -> Chansonnier\n# Answer:\nChansonnier", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Singer -> base.lightweight.profession.specialization_of -> Musicians and Singers\n# Answer:\nSinger", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Dancer -> people.profession.specialization_of -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0n1ykxp -> award.award_honor.honored_for -> Baby\n# Answer:\nBaby", "# Reasoning Path:\nJustin Bieber -> people.person.profession -> Actor -> base.lightweight.profession.professions_similar -> Model\n# Answer:\nModel"], "ground_truth": ["Recovery", "All Around The World", "Never Let You Go", "Eenie Meenie", "Turn to You (Mother's Day Dedication)", "Roller Coaster", "Bad Day", "As Long as You Love Me", "Bigger", "Heartbreaker", "Somebody to Love", "Confident", "Live My Life", "Change Me", "Hold Tight", "Lolly", "Boyfriend", "Die in Your Arms", "Thought Of You", "Beauty And A Beat", "Home to Mama", "Wait for a Minute", "First Dance", "Right Here", "#thatPower", "Baby", "Pray", "Never Say Never", "All Bad", "PYD", "All That Matters"], "ans_acc": 0.06451612903225806, "ans_hit": 1, "ans_f1": 0.0975609756097561, "ans_precission": 0.2, "ans_recall": 0.06451612903225806, "path_f1": 0.04878048780487805, "path_precision": 0.1, "path_recall": 0.03225806451612903, "path_ans_f1": 0.0975609756097561, "path_ans_precision": 0.2, "path_ans_recall": 0.06451612903225806}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specializations -> General practitioner\n# Answer:\nGeneral practitioner", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_for -> g.125cswvwv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specialization_of -> Healthcare professional\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specializations -> Anesthesiologist\n# Answer:\nAnesthesiologist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> people.profession.specializations -> Army Doctor\n# Answer:\nArmy Doctor", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> media_common.quotation_subject.quotations_about_this_subject -> A doctor, like anyone else who has to deal with human beings, each of them unique, cannot be a scientist; he is either, like the surgeon, a craftsman, or, like the physician and the psychologist, an artist. This means that in order to be a good doctor a man must also have a good character, that is to say, whatever weaknesses and foibles he may have, he must love his fellow human beings in the concrete and desire their good before his own.\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> location.location.containedby -> Paris\n# Answer:\nParis", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> base.schemastaging.context_name.pronunciation -> g.125_qqq2r\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau"], "ground_truth": ["Writer", "Statesman", "Journalist", "Physician", "Publisher"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.48, "ans_precission": 0.4, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.symbol -> Mountain tree frog\n# Answer:\nMountain tree frog", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nPetrified wood", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st6lz -> location.location_symbol_relationship.Kind_of_symbol -> State Amphibian\n# Answer:\nState Amphibian", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04st830 -> location.location_symbol_relationship.symbol -> Cactus wren\n# Answer:\nCactus wren", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403h9 -> location.religion_percentage.religion -> Pentecostalism\n# Answer:\nPentecostalism", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.Kind_of_symbol -> State fossil\n# Answer:\nState fossil", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xw9mx -> common.webpage.resource -> 'Idol': Jordin or Blake? And why?\n# Answer:\n'Idol': Jordin or Blake? And why?", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xw9mx -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09y4z3n -> common.webpage.resource -> Exit interview with 'Big Brother 10''s April Dowling\n# Answer:\nExit interview with 'Big Brother 10''s April Dowling", "# Reasoning Path:\nArizona -> location.statistical_region.religions -> m.04403hm -> location.religion_percentage.religion -> Episcopal Church\n# Answer:\nEpiscopal Church"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.perpetrator -> Jay-Z\n# Answer:\nJay-Z", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.role -> Vocals\n# Answer:\nVocals"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Episcopal Church\n# Answer:\nEpiscopal Church", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> John Tyler, Jr.\n# Answer:\nJohn Tyler, Jr.", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.religion -> Deism\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Alice Tyler\n# Answer:\nJohn Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.us_president.vice_president -> John Tyler -> people.person.children -> Anne Contesse Tyler\n# Answer:\nAnne Contesse Tyler", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nPresident"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0wry0_x -> film.personal_film_appearance.film -> George Lopez: It's Not Me, It's You\n# Answer:\nGeorge Lopez: It's Not Me, It's You", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\ncommon.webpage.resource", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs8ydd -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs8ydd -> film.personal_film_appearance.film -> President Barack Obama: The Man and His Journey\n# Answer:\nPresident Barack Obama: The Man and His Journey", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65 -> common.webpage.resource -> Janet Jackson's health derails her tour\n# Answer:\nJanet Jackson's health derails her tour", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094cr65 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0cs3c3w -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.containedby -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.person.place_of_birth -> Uiryeong County\n# Answer:\nUiryeong County", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05ckmy9\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> people.person.nationality -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.location.geolocation -> m.0239ks3\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Toyota Group\n# Answer:\nToyota Group", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> m.05t5syf\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSamsung Group -> organization.organization.founders -> Lee Byung-chul -> organization.organization_founder.organizations_founded -> Samsung C&T Corporation\n# Answer:\nSamsung C&T Corporation", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nMahaka Media", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu -> location.statistical_region.population -> g.11b7tcbqv4\n# Answer:\nDaegu"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> God in Islam\n# Answer:\nGod in Islam", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nEnd time", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.includes -> Judaism\n# Answer:\nJudaism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Anglicanism\n# Answer:\nAnglicanism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.holidays -> Eid al-Adha -> common.topic.notable_types -> Holiday\n# Answer:\nHoliday", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Catholicism\n# Answer:\nCatholicism"], "ground_truth": ["Masih ad-Dajjal", "Islamic view of angels", "Tawhid", "\u1e6c\u016bb\u0101", "Monotheism", "Predestination in Islam", "Qiyamah", "Islamic holy books", "Sharia", "Entering Heaven alive", "God in Islam", "Prophets in Islam", "Mahdi"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.2608695652173913, "ans_precission": 0.3, "ans_recall": 0.23076923076923078, "path_f1": 0.33333333333333337, "path_precision": 0.6, "path_recall": 0.23076923076923078, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.6, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey\n# Answer:\nJamie Dornan"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Fyodor Dostoyevsky -> people.deceased_person.cause_of_death -> Epilepsy\n# Answer:\nEpilepsy", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.person.nationality -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Arthur Koestler -> people.deceased_person.cause_of_death -> Parkinson's disease\n# Answer:\nParkinson's disease", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> people.deceased_person.cause_of_death -> Laryngeal cancer\n# Answer:\nLaryngeal cancer", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Fyodor Dostoyevsky -> people.deceased_person.cause_of_death -> Emphysema\n# Answer:\nEmphysema", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Fyodor Dostoyevsky -> freebase.valuenotation.is_reviewed -> Date of death\n# Answer:\nDate of death", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> media_common.quotation.subjects -> Jokes and Jokers\n# Answer:\nJokes and Jokers", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Fyodor Dostoyevsky -> common.topic.notable_types -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> influence.influence_node.influenced_by -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nH. G. Wells"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0w5qpxw -> military.military_command.military_conflict -> The Blitz\n# Answer:\nThe Blitz", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.office_position_or_title -> Chancellor of Germany\n# Answer:\nChancellor of Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.052bwvx -> military.military_command.military_conflict -> Siege of Budapest\n# Answer:\nSiege of Budapest", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.office_position_or_title -> F\u00fchrer\n# Answer:\nF\u00fchrer", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.012stfsg -> military.military_command.military_conflict -> Battle of Moscow\n# Answer:\nBattle of Moscow", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.basic_title -> F\u00fchrer\n# Answer:\nF\u00fchrer"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.3636363636363636, "ans_precission": 0.2222222222222222, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.2222222222222222, "path_recall": 0.18181818181818182, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t -> common.webpage.resource -> Grammys expected to go on with Amy Winehouse, Beyonce\n# Answer:\nGrammys expected to go on with Amy Winehouse, Beyonce", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> g.11b7_lvdf2\n# Answer:\nmusic.artist.track_contributions", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.09w699t -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.webpage -> m.03lpqjt -> common.webpage.category -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.track_contributions -> m.01362dph -> music.track_contribution.track -> Alone Again (Naturally)\n# Answer:\nAlone Again (Naturally)", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nProfession"], "ground_truth": ["Singer", "Songwriter", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.30769230769230765, "ans_precission": 0.2, "ans_recall": 0.6666666666666666, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66101 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66110 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66101 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nKansas City Monarchs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.team -> Montreal Royals\n# Answer:\nMontreal Royals", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> freebase.valuenotation.is_reviewed -> Team\n# Answer:\nTeam", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Bob Ferguson\n# Answer:\nBob Ferguson", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> freebase.valuenotation.has_value -> Number\n# Answer:\nNumber", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxm4r -> sports.sports_team_roster.position -> Shortstop\n# Answer:\nShortstop"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> people.profession.specializations -> Film Score Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Closed\n# Answer:\nDate Closed", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Opened\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Director\n# Answer:\nDirector", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Country of origin -> rdf-schema#domain -> Play\n# Answer:\nPlay", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Country of origin -> rdf-schema#range -> Country\n# Answer:\nCountry of origin", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Country of origin -> type.property.expected_type -> Country\n# Answer:\nCountry of origin"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.09tckd5 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07919ln -> american_football.player_passing_statistics.season -> 2008 NFL season\n# Answer:\n2008 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.place_of_birth -> Hyde Park\n# Answer:\nHyde Park", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.parents -> Franklin D. Roosevelt\n# Answer:\nFranklin D. Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> book.written_work.subjects -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Franklin D. Roosevelt, Jr. -> freebase.valuenotation.has_no_value -> Children\n# Answer:\nChildren", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nBook Edition", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> John Aspinwall Roosevelt -> people.person.profession -> Soldier\n# Answer:\nSoldier", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf2_w -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\nlocation.statistical_region.military_expenditure_percent_gdp", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Catholicism", "Protestantism", "Hinduism", "Islam"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.3333333333333333, "ans_recall": 0.75, "path_f1": 0.42857142857142855, "path_precision": 0.3333333333333333, "path_recall": 0.6, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nMissouri", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Juv\u00e9nal Habyarimana and Cyprien Ntaryamira\n# Answer:\nAssassination of Juv\u00e9nal Habyarimana and Cyprien Ntaryamira", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nJesse James -> people.person.sibling_s -> m.0pdjg7w -> people.sibling_relationship.sibling -> Fanny Quantrill Samuel\n# Answer:\nFanny Quantrill Samuel", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> media_common.quotation_subject.quotations_about_this_subject -> You never know what's hit you. A gunshot is the perfect way.\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.disaster2.tactic.attacks_of_this_form -> m.065tkfr\n# Answer:\nbase.disaster2.tactic.attacks_of_this_form", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Anna Lindh assassination\n# Answer:\nAnna Lindh assassination", "# Reasoning Path:\nJesse James -> people.person.sibling_s -> m.0pdjf8n -> people.sibling_relationship.sibling -> John Thomas Samuel\n# Answer:\nJohn Thomas Samuel", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of Akhmad Kadyrov\n# Answer:\nAssassination of Akhmad Kadyrov"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> common.topic.notable_types -> Film character\n# Answer:\nFilm character", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> book.book_character.appears_in_book -> Great Expectations\n# Answer:\nGreat Expectations", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.02vdcn_\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced_by -> H. G. Wells\n# Answer:\nH. G. Wells", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> common.topic.image -> The Ghost of Christmas Present with Ebenezer Scrooge\n# Answer:\nThe Ghost of Christmas Present with Ebenezer Scrooge", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> film.film_character.portrayed_in_films -> m.02tbg1c\n# Answer:\nGhost of Christmas Present", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Alphonse Daudet -> influence.influence_node.influenced_by -> \u00c9mile Zola\n# Answer:\n\u00c9mile Zola", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Abel Magwitch -> tv.tv_character.appeared_in_tv_program -> m.0j7c9jh\n# Answer:\nAbel Magwitch"], "ground_truth": ["A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Puffin Choice)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Apple Classics)", "The mystery of Edwin Drood", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Scholastic Classics)", "A TALE OF TWO CITIES", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Classics for Young Adults and Adults)", "Great Expectations.", "A Christmas Carol (Usborne Young Reading)", "Dombey and son", "A CHRISTMAS CAROL", "Bleak House", "A Christmas Carol (Penguin Readers, Level 2)", "A Christmas Carol (Aladdin Classics)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "Great expectations", "Our mutual friend", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Green Integer, 50)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Read & Listen Books)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Classics Illustrated)", "The Pickwick papers", "A Christmas Carol (Classic, Picture, Ladybird)", "David Copperfield.", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Thornes Classic Novels)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Dramascripts S.)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Take Part)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (Paperback Classics)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Christmas Carol (Audio Editions)", "Martin Chuzzlewit", "Oliver Twist", "The Old Curiosity Shop", "The old curiosity shop", "A Tale of Two Cities (Saddleback Classics)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "The Pickwick Papers", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (BBC Audio Series)", "Our mutual friend.", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (40th Anniversary Edition)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "The Mystery of Edwin Drood", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Classic Retelling)", "A Christmas Carol (Dramascripts Classic Texts)", "A Tale of Two Cities (Oxford Playscripts)", "Hard times", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Bantam Classic)", "Bleak House.", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Great Stories)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Value Books)", "Bleak house", "A Tale Of Two Cities (Adult Classics in Audio)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Classic Books on Cassettes Collection)", "David Copperfield", "A Tale of Two Cities (Large Print Edition)", "Dombey and Son", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Pacemaker Classics)", "Sketches by Boz", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Courage Literary Classics)", "Little Dorrit", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol", "A Christmas Carol (Watermill Classics)", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Pacemaker Classics)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Large Print)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Illustrated Junior Library)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Simple English)", "A Christmas Carol (Pacemaker Classic)", "The cricket on the hearth", "A Christmas Carol. (Lernmaterialien)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Everyman Paperbacks)", "A Christmas Carol (Puffin Classics)", "Great Expectations", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Unabridged Classics)", "Dombey and Son.", "Great expectations.", "A Tale of Two Cities (Collector's Library)", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "The old curiosity shop.", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Tor Classics)", "A Christmas Carol (Radio Theatre)", "A Christmas Carol (Soundings)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (R)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Young Reading Series 2)"], "ans_acc": 0.04142011834319527, "ans_hit": 1, "ans_f1": 0.06349206349206349, "ans_precission": 0.3, "ans_recall": 0.03550295857988166, "path_f1": 0.16323529411764706, "path_precision": 0.3, "path_recall": 0.11212121212121212, "path_ans_f1": 0.06349206349206349, "path_ans_precision": 0.3, "path_ans_recall": 0.03550295857988166}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_holder -> Vladimir Ivashko\n# Answer:\nVladimir Ivashko", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_6 -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_x -> government.government_position_held.office_holder -> Mikhail Sergeyevich Gorbachev\n# Answer:\nMikhail Sergeyevich Gorbachev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_position_or_title -> General Secretary of the Communist Party of the Soviet Union\n# Answer:\nGeneral Secretary of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n58th Army", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_x -> government.government_position_held.governmental_body -> Central Committee of the Communist Party of the Soviet Union\n# Answer:\nCentral Committee of the Communist Party of the Soviet Union", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1991 Soviet coup d'\u00e9tat attempt -> base.culturalevent.event.entity_involved -> Vladimir Kryuchkov\n# Answer:\nVladimir Kryuchkov", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> Chechens\n# Answer:\nChechens"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.11b6ddwl64\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Real Estate Investment Trust\n# Answer:\nReal Estate Investment Trust", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_1y\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.geographic_scope -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.gdp_deflator_change -> g.124x8g_5d\n# Answer:\nlocation.statistical_region.gdp_deflator_change", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> common.topic.subject_of -> Real Estate\n# Answer:\nReal Estate", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> business.business_operation.industry -> Retail\n# Answer:\nRetail", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> DDR Corp. -> freebase.valuenotation.is_reviewed -> Subject of\n# Answer:\nSubject of", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> Frontpoint Security Solutions -> organization.organization.legal_structure -> Limited liability company\n# Answer:\nFrontpoint Security Solutions"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> A Scottish Soldier -> music.recording.releases -> Scotland the Brave\n# Answer:\nScotland the Brave", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_for -> g.126t2bpvr\n# Answer:\nAul Lang Syne", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Ballard of Glencoe -> music.recording.releases -> Scotland the Brave\n# Answer:\nScotland the Brave", "# Reasoning Path:\nCarl Wilson -> music.artist.track -> Aul Lang Syne -> common.topic.notable_types -> Musical Recording\n# Answer:\nMusical Recording"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.actor -> Edward Mulhare\n# Answer:\nEdward Mulhare", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nVoice", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nKnight Rider - Season 3", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m0 -> tv.regular_tv_appearance.character -> Devon Miles\n# Answer:\nDevon Miles", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.character -> Michael Knight\n# Answer:\nMichael Knight", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w93kp -> common.webpage.resource -> Ratings: 'Knight Rider' continues to stall\n# Answer:\nRatings: 'Knight Rider' continues to stall"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> m.0hmbn04 -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.first_level_division_of -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nNewton County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> m.0k6vn0t\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nJasper County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> m.0k6vn0l\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> people.profession.specialization_of -> Scientist\n# Answer:\nScientist", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nMoon", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Botanist -> base.descriptive_names.names.descriptive_name -> m.0102h0md\n# Answer:\nbase.descriptive_names.names.descriptive_name"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtwp -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actor in a Comedy Series", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.0949nlk -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> common.topic.webpage -> m.0949nlk -> common.webpage.resource -> Name-dropping is one of pop music's latest trends\n# Answer:\nName-dropping is one of pop music's latest trends", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.ceremony -> 52nd Primetime Emmy Awards\n# Answer:\n52nd Primetime Emmy Awards"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_conflict -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04jx9vf\n# Answer:\nmilitary.military_conflict.commanders", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\nmilitary.military_commander.military_commands", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04h_gx9\n# Answer:\nmilitary.military_conflict.commanders", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.049y34w -> military.military_command.military_combatant -> Confederate States of America\n# Answer:\nConfederate States of America", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9lf -> military.military_command.military_conflict -> Manassas Station Operations\n# Answer:\nManassas Station Operations", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.commanders -> m.04fvgx7\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> base.culturalevent.event.entity_involved -> Confederate States of America\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Seven Days Battles -> military.military_conflict.military_personnel_involved -> George B. McClellan\n# Answer:\nSeven Days Battles", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> visual_art.art_subject.artwork_on_the_subject -> The Kearsarge and the Alabama\n# Answer:\nAmerican Civil War"], "ground_truth": ["First Battle of Kernstown", "American Civil War", "Battle of Hoke's Run", "Battle of Cedar Mountain", "Battle of White Oak Swamp", "Battle of Port Republic", "Second Battle of Bull Run", "Romney Expedition", "Battle of Harpers Ferry", "How Few Remain", "Jackson's Valley Campaign", "Battle of Hancock", "Manassas Station Operations", "Battle of McDowell", "Battle of Front Royal", "First Battle of Rappahannock Station", "Battle of Chantilly", "First Battle of Winchester", "Battle of Chancellorsville"], "ans_acc": 0.15789473684210525, "ans_hit": 1, "ans_f1": 0.23999999999999996, "ans_precission": 0.5, "ans_recall": 0.15789473684210525, "path_f1": 0.13186813186813187, "path_precision": 0.6, "path_recall": 0.07407407407407407, "path_ans_f1": 0.2576687116564417, "path_ans_precision": 0.7, "path_ans_recall": 0.15789473684210525}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\ncommon.image.size", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\ncommon.image.size"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0mt\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.spouse_s -> m.0j4k0ll\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Sarah Davenport\n# Answer:\nSarah Davenport", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Hannah Cole\n# Answer:\nHannah Cole", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.place_of_birth -> Ecton\n# Answer:\nEcton", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nCommon-law marriage", "# Reasoning Path:\nBenjamin Franklin -> people.person.parents -> Josiah Franklin -> people.person.children -> Anne Harris\n# Answer:\nAnne Harris", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\ncommon.image.size", "# Reasoning Path:\nPatrick Swayze -> tv.tv_program_guest.appeared_on -> m.0j7p8gc -> tv.tv_guest_personal_appearance.episode -> Episode 159\n# Answer:\nEpisode 159", "# Reasoning Path:\nPatrick Swayze -> tv.tv_program_guest.appeared_on -> m.0j7p8gc -> tv.tv_guest_personal_appearance.appearance_type -> Celebrity guest\n# Answer:\nCelebrity guest", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\ncommon.image.size"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Ginevra de' Benci\n# Answer:\nGinevra de' Benci", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Raphael Cartoons\n# Answer:\nRaphael Cartoons", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Painter -> people.profession.specialization_of -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> people.person.profession -> Sculptor -> people.profession.specialization_of -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> visual_art.visual_artist.art_series -> Stanze di Raffaello Frescoes\n# Answer:\nStanze di Raffaello Frescoes", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> influence.influence_node.influenced -> Parmigianino\n# Answer:\nParmigianino", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Raphael -> base.kwebbase.kwtopic.has_sentences -> It was here that his first official work was recorded in a document dated December 10th 1500, which stated that Raphael, now aged 17 and a young \\\"master\\\", was commissioned to help with the painting of an altarpiece, to be completed by September 13, 1502.\n# Answer:\nRaphael"], "ground_truth": ["Madonna of the Yarnwinder", "Virgin of the Rocks", "Leonardo's horse", "Bacchus", "Leda and the Swan", "g.120vt1gz", "The Baptism of Christ", "g.1219sb0g", "Mona Lisa", "Sala delle Asse", "g.1224tf0c", "Portrait of a Musician", "Drapery for a Seated Figure", "g.1239jd9p", "Vitruvian Man", "The Battle of Anghiari", "Portrait of a man in red chalk", "Salvator Mundi", "Portrait of Isabella d'Este", "The Virgin and Child with St Anne and St John the Baptist", "The Virgin and Child with St. Anne", "The Holy Infants Embracing", "Horse and Rider", "Lady with an Ermine", "St. Jerome in the Wilderness", "Madonna of Laroque", "Adoration of the Magi", "g.12215rxg", "St. John the Baptist", "g.1213jb_b", "g.12314dm1", "Madonna of the Carnation", "g.121yh91r", "Medusa", "The Last Supper", "Benois Madonna", "Ginevra de' Benci", "Madonna and Child with St Joseph", "Annunciation", "La belle ferronni\u00e8re", "Portrait of a Young Fianc\u00e9e", "Head of a Woman", "Madonna Litta", "Lucan portrait of Leonardo da Vinci", "g.121wt37c"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1090909090909091, "ans_precission": 0.3, "ans_recall": 0.06666666666666667, "path_f1": 0.11111111111111109, "path_precision": 0.3, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1090909090909091, "path_ans_precision": 0.3, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyf\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.12cp_k6r2\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxk\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_391x\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc37_h6\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp", "# Reasoning Path:\nAustria -> location.statistical_region.health_expenditure_as_percent_of_gdp -> g.1hhc385fk\n# Answer:\nlocation.statistical_region.health_expenditure_as_percent_of_gdp"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Sandy Gallin\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Orthomolecular medicine\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Diana Taylor Dawson\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.medical_specialties -> Oncology\n# Answer:\nOncology", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Anthony Casso\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjftz\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Acne\n# Answer:\nAcne", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.disease.parent_disease -> Complications of pregnancy\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Gautam Buddha University -> location.location.containedby -> Uttar Pradesh\n# Answer:\nUttar Pradesh", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Buddhist meditation\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Gautam Buddha University -> location.location.containedby -> Greater Noida\n# Answer:\nGreater Noida", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Gautam Buddha University -> organization.organization.headquarters -> m.0jvbt5c\n# Answer:\nGautam Buddha University", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> book.book_subject.works -> India Old and New, with a Memorial Address\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Gautam Buddha University -> location.location.containedby -> India\n# Answer:\nIndia", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Samadhi\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.practices -> Vipassan\u0101\n# Answer:\nBuddhism", "# Reasoning Path:\nGautama Buddha -> people.person.quotations -> 'He insulted me, he cheated me, he beat me, he robbed me' -- those who are free of resentful thoughts surely find peace. -> media_common.quotation.subjects -> Peace\n# Answer:\n'He insulted me, he cheated me, he beat me, he robbed me' -- those who are free of resentful thoughts surely find peace."], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> Treaty of Amity and Commerce\n# Answer:\nTreaty of Amity and Commerce", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nAmerican literature", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\ncommon.image.size", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\n:Library and information science"], "ground_truth": ["Bifocals", "Glass harmonica", "Lightning rod", "Franklin stove"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.46153846153846156, "ans_precission": 0.3333333333333333, "ans_recall": 0.75, "path_f1": 0.46153846153846156, "path_precision": 0.3333333333333333, "path_recall": 0.75, "path_ans_f1": 0.46153846153846156, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.country.first_level_divisions -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> Greeley Union Pacific Railroad Depot -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> common.topic.article -> m.0qfltq5\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> education.educational_institution.sports_teams -> Northern Colorado Bears men's basketball\n# Answer:\nUniversity of Northern Colorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_for -> g.12599cm43\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_for -> g.1259l_93p\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nBook", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Detski\u012d al'bom -> common.topic.notable_for -> g.12578_08h\n# Answer:\nDetski\u012d al'bom", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Detski\u012d al'bom -> common.topic.notable_types -> Book\n# Answer:\nBook"], "ground_truth": ["Musician", "Librettist", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Slovakia\n# Answer:\nSlovakia", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0103v73t -> education.education.institution -> University of Washington\n# Answer:\nUniversity of Washington", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.student -> Sigurd Burckhardt\n# Answer:\nSigurd Burckhardt", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f -> education.education.institution -> University of Wisconsin\u2013Green Bay\n# Answer:\nUniversity of Wisconsin\u2013Green Bay", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0102xvg7 -> education.education.degree -> PhD\n# Answer:\nPhD"], "ground_truth": ["Switzerland", "Austria", "East Germany", "Germany", "Belgium", "Luxembourg", "Liechtenstein"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.5, "ans_recall": 0.42857142857142855, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.5, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Lennon Legend: The Very Best of John Lennon -> film.film.genre -> Music\n# Answer:\nMusic", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> .997 Radiostorm Oldies -> broadcast.content.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 181-greatoldies -> broadcast.content.genre -> Oldies\n# Answer:\nOldies", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> 1Club.FM: 70s (Lite) -> broadcast.content.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Lennon Legend: The Very Best of John Lennon -> film.film.genre -> Documentary film\n# Answer:\nDocumentary film", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Lennon Legend: The Very Best of John Lennon -> film.film.runtime -> m.0cs7h_d\n# Answer:\nLennon Legend: The Very Best of John Lennon", "# Reasoning Path:\nJohn Lennon -> film.director.film -> Apotheosis -> film.film.genre -> Short Film\n# Answer:\nShort Film"], "ground_truth": ["Psychedelic rock", "Soft rock", "Pop music", "Experimental rock", "Rock music", "Experimental music", "Pop rock", "Blues rock", "Art rock"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.5, "ans_recall": 0.3333333333333333, "path_f1": 0.3157894736842105, "path_precision": 0.3, "path_recall": 0.3333333333333333, "path_ans_f1": 0.4, "path_ans_precision": 0.5, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpc0p -> government.government_position_held.office_holder -> Floyd K. Haskell\n# Answer:\nFloyd K. Haskell", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpg7j -> government.government_position_held.office_holder -> Thomas M. Bowen\n# Answer:\nThomas M. Bowen", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.office_holder -> Charles S. Thomas\n# Answer:\nCharles S. Thomas", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcqg -> government.government_position_held.office_holder -> Frederick Walker Pitkin\n# Answer:\nFrederick Walker Pitkin", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpc0p -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpg7j -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcqg -> government.government_position_held.basic_title -> Governor\n# Answer:\nGovernor", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.010f0mcq -> government.government_position_held.office_holder -> Bernie Buescher\n# Answer:\nBernie Buescher", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpp0t -> government.government_position_held.basic_title -> Senator\n# Answer:\nSenator"], "ground_truth": ["Mark Udall", "Michael Bennet"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Russia\n# Answer:\nRussia", "# Reasoning Path:\nGreenland -> location.location.partially_contained_by -> m.0wg9h8g -> location.partial_containment_relationship.partially_contained_by -> Arctic\n# Answer:\nArctic", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partially_contains -> Iceland\n# Answer:\nIceland", "# Reasoning Path:\nGreenland -> location.location.partially_containedby -> Arctic -> location.location.partiallycontains -> m.0wg9h96\n# Answer:\nlocation.location.partiallycontains", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.location.contains -> Faroe Islands\n# Answer:\nFaroe Islands", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> location.statistical_region.population -> g.11b7vv2xt_\n# Answer:\nNordic countries"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> m.066ht07\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_for -> g.1256x4nvs\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> location.location.containedby -> Washington\n# Answer:\nWashington", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\nlocation.statistical_region.population"], "ground_truth": ["98139", "98117", "98191", "98122", "98144", "98106", "98118", "98125", "98105", "98119", "98121", "98195", "98141", "98146", "98158", "98124", "98131", "98107", "98160", "98190", "98171", "98174", "98198", "98126", "98134", "98188", "98129", "98116", "98194", "98138", "98165", "98132", "98127", "98101", "98113", "98185", "98104", "98178", "98168", "98164", "98111", "98133", "98114", "98199", "98119-4114", "98148", "98161", "98177", "98155", "98108", "98166", "98112", "98145", "98170", "98184", "98175", "98102", "98181", "98103", "98109", "98136", "98154", "98115"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.0821917808219178, "ans_precission": 0.3, "ans_recall": 0.047619047619047616, "path_f1": 0.0821917808219178, "path_precision": 0.3, "path_recall": 0.047619047619047616, "path_ans_f1": 0.0821917808219178, "path_ans_precision": 0.3, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.children -> Jaden Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nPinkett-Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Jaden Smith\n# Answer:\nJaden Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.0bmsb49\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> music.genre.subgenre -> Quiet Storm\n# Answer:\nContemporary R&B", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_t7\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.spouse_s -> m.02kp6m3\n# Answer:\npeople.person.spouse_s", "# Reasoning Path:\nWillow Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Willard Carroll Trey Smith III\n# Answer:\nWillard Carroll Trey Smith III", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_t2\n# Answer:\nJada Pinkett Smith"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Simplified Chinese character\n# Answer:\nSimplified Chinese character", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Korea\n# Answer:\nKorea", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> book.book_subject.works -> The China Story\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> base.schemastaging.context_name.pronunciation -> g.125_l82wv\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.0104b7h1 -> education.education.degree -> PhD\n# Answer:\nPhD", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02wn10k -> education.education.major_field_of_study -> Spanish Language\n# Answer:\nSpanish Language", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Empire of Japan\n# Answer:\nEast Asia"], "ground_truth": ["Simplified Chinese character", "Chinese characters", "'Phags-pa script", "Traditional Chinese characters", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3, "ans_recall": 0.6, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Harold Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.type_of_union -> Marriage\n# Answer:\nMarriage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Edward Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq -> people.marriage.location_of_ceremony -> The Mission Inn Hotel & Spa\n# Answer:\nThe Mission Inn Hotel & Spa", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> freebase.valuenotation.has_value -> Place of death\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.children -> Harold Nixon\n# Answer:\nHarold Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Hannah Milhous Nixon -> people.person.children -> Arthur Nixon\n# Answer:\nHannah Milhous Nixon", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> people.person.parents -> Francis A. Nixon -> people.person.children -> Edward Nixon\n# Answer:\nEdward Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11764705882352941, "path_precision": 0.1, "path_recall": 0.14285714285714285, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0v4v237 -> award.award_honor.award_winner -> Larry Harris\n# Answer:\nLarry Harris", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_y5jsh -> award.award_nomination.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0v4v237 -> award.award_honor.award -> Primetime Emmy Award for Outstanding Videotape Editing - Series\n# Answer:\nPrimetime Emmy Award for Outstanding Videotape Editing - Series", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shttt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nPrimetime Emmy Award for Outstanding Lead Actress in a Comedy Series", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.ceremony -> 16th NAACP Image Awards\n# Answer:\n16th NAACP Image Awards", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtx3 -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_y5jsh -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> A Friend in Need\n# Answer:\nA Friend in Need", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award -> NAACP Image Award for Outstanding Actress in a Comedy Series\n# Answer:\nNAACP Image Award for Outstanding Actress in a Comedy Series"], "ground_truth": ["Marla Gibbs", "Zara Cully", "Sherman Hemsley", "Paul Benedict", "Jay Hammer", "Damon Evans", "Franklin Cover", "Berlinda Tolbert", "Isabel Sanford", "Roxie Roker", "Mike Evans"], "ans_acc": 0.18181818181818182, "ans_hit": 1, "ans_f1": 0.22641509433962265, "ans_precission": 0.3, "ans_recall": 0.18181818181818182, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.22641509433962265, "path_ans_precision": 0.3, "path_ans_recall": 0.18181818181818182}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The San Francisco Examiner\n# Answer:\nThe San Francisco Examiner", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.periodical.first_issue_date -> m.02npn31\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Don't Call it Frisco -> common.topic.notable_for -> g.125gwqscg\n# Answer:\nDon't Call it Frisco", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> location.postal_code.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian -> book.publication.contents -> m.0znkynv\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Don't Call it Frisco -> book.written_work.author -> Herb Caen\n# Answer:\nDon't Call it Frisco", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> book.written_work.subjects -> California\n# Answer:\nCalifornia", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> book.written_work.subjects -> Suspense\n# Answer:\nSuspense"], "ground_truth": ["The San Francisco Examiner", "Bay Area Reporter", "San Francisco Daily", "The Golden Era", "Street Sheet", "Synapse", "Sing Tao Daily", "San Francisco Bay Guardian", "Free Society", "The Daily Alta California", "AsianWeek", "San Francisco News-Call Bulletin Newspaper", "San Francisco Foghorn", "California Star", "San Francisco Chronicle", "San Francisco Business Times", "Dock of the Bay", "San Francisco Bay Times", "San Francisco Bay View", "San Francisco Call"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.21818181818181814, "ans_precission": 0.4, "ans_recall": 0.15, "path_f1": 0.21818181818181814, "path_precision": 0.4, "path_recall": 0.15, "path_ans_f1": 0.21818181818181814, "path_ans_precision": 0.4, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> geography.river.basin_countries -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg9k2c\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aras -> location.location.partially_containedby -> Azerbaijan\n# Answer:\nAzerbaijan", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_contained_by -> m.0wg97p1\n# Answer:\nAkhurian River", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Akhurian River -> location.location.partially_containedby -> Turkey\n# Answer:\nTurkey", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.partially_contains -> Aghstafa -> location.location.containedby -> Asia\n# Answer:\nAsia"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> Transmural Myocardial Infarction\n# Answer:\nTransmural Myocardial Infarction", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nAssassination in ways which appear natural", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> ST segment elevation myocardial infarction\n# Answer:\nST segment elevation myocardial infarction", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nAortic aneurysm", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> people.cause_of_death.parent_cause_of_death -> Kar\u014dshi\n# Answer:\nKar\u014dshi", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.includes_diseases -> Subendocardial Myocardial Infarction\n# Answer:\nSubendocardial Myocardial Infarction", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.gender.gender_identity.people -> m.07n73w_\n# Answer:\nbase.gender.gender_identity.people"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1837-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1837-1843", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 9: 1861\n# Answer:\nThe Correspondence of Charles Darwin, Volume 9: 1861", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> common.topic.notable_for -> g.1258m_lcy\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> History\n# Answer:\nHistory", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nEvolution", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Science\n# Answer:\nScience", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Paleontology\n# Answer:\nPaleontology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiologist"], "ground_truth": ["monograph on the sub-class Cirripedia", "Darwin and Henslow", "Darwin from Insectivorous Plants to Worms", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The collected papers of Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Insectivorous Plants", "Notebooks on transmutation of species", "The Darwin Reader Second Edition", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Gesammelte kleinere Schriften", "The Structure and Distribution of Coral Reefs", "Die fundamente zur entstehung der arten", "The Essential Darwin", "The Power of Movement in Plants", "Darwin-Wallace", "genese\u014ds t\u014dn eid\u014dn", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "red notebook of Charles Darwin", "Darwin en Patagonia", "The Variation of Animals and Plants under Domestication", "On Natural Selection", "Questions about the breeding of animals", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "Geological Observations on the Volcanic Islands", "The voyage of Charles Darwin", "Opsht\u0323amung fun menshen", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Beagle letters", "Volcanic Islands", "To the members of the Down Friendly Club", "On the Movements and Habits of Climbing Plants", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Diary of the voyage of H.M.S. Beagle", "The Correspondence of Charles Darwin, Volume 9: 1861", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Human nature, Darwin's view", "On the tendency of species to form varieties", "South American Geology", "The Darwin Reader First Edition", "The Voyage of the Beagle", "Het uitdrukken van emoties bij mens en dier", "Voyage d'un naturaliste autour du monde", "Les moyens d'expression chez les animaux", "Wu zhong qi yuan", "The Life and Letters of Charles Darwin Volume 2", "Motsa ha-minim", "A student's introduction to Charles Darwin", "Kleinere geologische Abhandlungen", "Charles Darwin's marginalia", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Correspondence of Charles Darwin, Volume 12: 1864", "Memorias y epistolario i\u0301ntimo", "Leben und Briefe von Charles Darwin", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Del Plata a Tierra del Fuego", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "From so simple a beginning", "The Different Forms of Flowers on Plants of the Same Species", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Reise eines Naturforschers um die Welt", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Orgin of Species", "Evolution and natural selection", "The geology of the voyage of H.M.S. Beagle", "Darwin on humus and the earthworm", "Darwin", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Charles Darwin", "Darwin's insects", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Correspondence of Charles Darwin, Volume 8: 1860", "On evolution", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The portable Darwin", "La vie et la correspondance de Charles Darwin", "Proiskhozhdenie vidov", "H.M.S. Beagle in South America", "Darwinism stated by Darwin himself", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "From Darwin's unpublished notebooks", "The Correspondence of Charles Darwin, Volume 16: 1868", "Origins", "Darwin Compendium", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Expression of the Emotions in Man and Animals", "The Correspondence of Charles Darwin, Volume 17: 1869", "Fertilisation of Orchids", "The Life of Erasmus Darwin", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 11: 1863", "Les mouvements et les habitudes des plantes grimpantes", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Resa kring jorden", "ontstaan der soorten door natuurlijke teeltkeus", "Darwin Darwin", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 14: 1866", "The action of carbonate of ammonia on the roots of certain plants", "The education of Darwin", "The Autobiography of Charles Darwin", "The living thoughts of Darwin", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Charles Darwin on the routes of male humble bees", "Evolutionary Writings: Including the Autobiographies", "Monographs of the fossil Lepadidae and the fossil Balanidae", "On the origin of species by means of natural selection", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The foundations of the Origin of species", "La facult\u00e9 motrice dans les plantes", "The Descent of Man, and Selection in Relation to Sex", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The principal works", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Formation of Vegetable Mould through the Action of Worms", "More Letters of Charles Darwin", "Charles Darwin's letters", "Metaphysics, Materialism, & the evolution of mind", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "Rejse om jorden", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "Evolution", "Notes on the fertilization of orchids", "Works", "El Origin De Las Especies", "Darwin's notebooks on transmutation of species", "Darwin's journal", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The\u0301orie de l'e\u0301volution", "A Darwin Selection", "Charles Darwin's natural selection", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Darwin's Ornithological notes", "Tesakneri tsagume\u030c", "Reise um die Welt 1831 - 36", "Die geschlechtliche Zuchtwahl", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Life and Letters of Charles Darwin Volume 1", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Evolution by natural selection", "Darwin for Today"], "ans_acc": 0.0457516339869281, "ans_hit": 1, "ans_f1": 0.07142857142857142, "ans_precission": 0.4, "ans_recall": 0.0392156862745098, "path_f1": 0.25806451612903225, "path_precision": 1.0, "path_recall": 0.14814814814814814, "path_ans_f1": 0.0875, "path_ans_precision": 1.0, "path_ans_recall": 0.0457516339869281}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nEnd of the Road: How Money Became Worthless", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nOur Nixon", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nThe American Film Institute Salute to James Cagney", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nJohnny Grant", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.film -> Sicko\n# Answer:\nSicko"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> music.album.releases -> Greatest Hits Vol. 2\n# Answer:\nGreatest Hits Vol. 2", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> You've Really Got a Hold on Me\n# Answer:\nYou've Really Got a Hold on Me", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> common.topic.article -> m.02py0ln\n# Answer:\nGreatest Hits, Vol. 2", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Greatest Hits, Vol. 2 -> music.album.artist -> The Miracles\n# Answer:\nThe Miracles", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> After All\n# Answer:\nAfter All", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Blame It on Love & All the Greatest Hits -> music.album.album_content_type -> Compilation album\n# Answer:\nBlame It on Love & All the Greatest Hits", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Come Spy with Me\n# Answer:\nCome Spy with Me", "# Reasoning Path:\nSmokey Robinson -> music.artist.album -> Blame It on Love & All the Greatest Hits -> music.album.release_type -> Album\n# Answer:\nBlame It on Love & All the Greatest Hits", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nGrammy Legend Award"], "ground_truth": ["Season's Greetings from Smokey Robinson", "Sweet Harmony", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "I Can't Give You Anything but Love", "Our Love Is Here to Stay", "Come by Here (Kum Ba Ya)", "Melody Man", "Aqui Con Tigo (Being With You)", "A Silent Partner in a Three-Way Love Affair", "Did You Know (Berry's Theme)", "I'm Glad There Is You", "Everything You Touch", "Same Old Love", "Train of Thought", "Food For Thought", "I've Made Love To You A Thousand Times", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "I Have Prayed On It", "With Your Love Came", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Why Do Happy Memories Hurt So Bad", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "We Are The Warriors", "You're the One for Me (feat. Joss Stone)", "What's Too Much", "You're Just My Life (feat. India.Arie)", "The Christmas Song", "No Time to Stop Believing", "More Than You Know", "I'll Keep My Light In My Window", "Tears Of A Clown", "If You Can Want", "More Love", "Ebony Eyes (Duet with Rick James)", "Whatcha Gonna Do", "Save Me", "Really Gonna Miss You", "Tell Me Tomorrow (12\\\" extended mix)", "Just Like You", "Wishful Thinking", "I Want You Back", "Christmas Everyday", "Because of You It's the Best It's Ever Been", "Gone Forever", "I've Made Love to You a Thousand Times", "Jingle Bells", "Shoe Soul", "Love Letters", "Be Kind To The Growing Mind (with The Temptations)", "God Rest Ye Merry Gentlemen", "Jesus Told Me To Love You", "Being With You", "Can't Fight Love", "If You Wanna Make Love (Come 'round Here)", "Tears of a Clown", "I've Got You Under My Skin", "You've Really Go a Hold on Me", "Just My Soul Responding", "Tell Me Tomorrow, Part 1", "Pops, We Love You (disco)", "The Agony and the Ecstasy", "Asleep on My Love", "Just Another Kiss", "One Time", "My Girl", "A Tattoo", "I Am, I Am", "Some People Will Do Anything for Love", "Keep Me", "Little Girl, Little Girl", "Tracks of my Tears", "Quiet Storm (Groove Boutique remix)", "I Love The Nearness Of You", "Don't Play Another Love Song", "Standing On Jesus", "Photograph in My Mind", "Cruisin", "Tracks Of My Tears (Live)", "Hold on to Your Love", "Come to Me Soon", "Why", "Will You Still Love Me Tomorrow", "Be Kind to the Growing Mind", "Ever Had A Dream", "Love Is The Light", "I Know You by Heart", "I Am I Am", "You Are Forever", "Love So Fine", "Tell Me Tomorrow", "And I Don't Love You", "Tracks of My Tears", "Love' n Life", "A Child Is Waiting", "My Guy", "Vitamin U", "There Will Come A Day ( I'm Gonna Happen To You )", "Quiet Storm", "I'm in the Mood for Love", "Everything for Christmas", "Girl I'm Standing There", "Pops, We Love You", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Gang Bangin'", "Will You Love Me Tomorrow?", "You Go to My Head", "Love Don' Give No Reason (12 Inch Club Mix)", "You Don't Know What It's Like", "It's a Good Feeling", "We've Saved the Best for Last", "Please Don't Take Your Love (feat. Carlos Santana)", "Share It", "Time Flies", "Let Me Be The Clock", "Hanging on by a Thread", "Mother's Son", "I Like Your Face", "And I Love Her", "Wanna Know My Mind", "I Praise & Worship You Father", "She's Only a Baby Herself", "Blame It on Love", "Yester Love", "Christmas Every Day", "Be Careful What You Wish For", "You've Really Got a Hold on Me", "It's Christmas Time", "No\u00ebl", "Heavy On Pride (Light On Love)", "(It's The) Same Old Love", "You Are So Beautiful (feat. Dave Koz)", "Fly Me to the Moon (In Other Words)", "Yes It's You Lady", "Daylight & Darkness", "I Hear The Children Singing", "He Can Fix Anything", "We\u2019ve Come Too Far to End It Now", "Noel", "Deck the Halls", "Cruisin'", "Shop Around", "I Second That Emotion", "I Can't Find", "Little Girl Little Girl", "Tears of a Sweet Free Clown", "You Really Got a Hold on Me", "It's Time to Stop Shoppin' Around", "I Can\u2019t Stand to See You Cry (Commercial version)", "Unless You Do It Again", "Baby That's Backatcha", "Medley: Never My Love / Never Can Say Goodbye", "Baby Come Close", "Close Encounters of the First Kind", "If You Wanna Make Love", "The Road to Damascus", "Mickey's Monkey", "Driving Thru Life in the Fast Lane", "You Take Me Away", "You Cannot Laugh Alone", "Wedding Song", "Just to See Her", "Never My Love / Never Can Say Goodbye", "I Can't Get Enough", "Walk on By", "Going to a Go-Go", "Going to a Go Go", "Sleepless Nights", "Santa Claus is Coming to Town", "The Tears Of A Clown", "Be Careful What You Wish For (instrumental)", "And I Don't Love You (Larry Levan instrumental dub)", "Away in the Manger / Coventry Carol", "Christmas Greeting", "Bad Girl", "It's Her Turn to Live", "So Bad", "Winter Wonderland", "Going to a Gogo", "Virgin Man", "Just To See Her Again", "Ooo Baby Baby", "It's A Good Night", "The Tracks of My Tears", "Satisfy You", "Te Quiero Como Si No Hubiera Un Manana", "The Way You Do (The Things You Do)", "The Tracks Of My Tears", "Coincidentally", "There Will Come a Day (I'm Gonna Happen to You)", "Love Brought Us Here", "Just Passing Through", "Fallin'", "Time After Time", "Don't Wanna Be Just Physical", "In My Corner", "Who's Sad", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Tea for Two", "Skid Row", "I Care About Detroit", "I Second That Emotions", "I\u2019ve Got You Under My Skin", "Theme From the Big Time", "The Tracks of My Tears (live)", "Girlfriend", "My World", "The Family Song", "Don't Know Why", "Fulfill Your Need", "One Heartbeat", "Crusin", "Blame It On Love (Duet with Barbara Mitchell)", "Let Your Light Shine On Me", "The Tracks of My Heart", "Will You Love Me Tomorrow", "Why Are You Running From My Love", "Ooo Baby Baby (live)", "When A Woman Cries", "Speak Low", "I Love Your Face", "Crusin'", "Love Don't Give No Reason", "Quiet Storm (single version)", "If You Want My Love", "Ooh Baby Baby", "Love Bath", "You Made Me Feel Love", "The Tears of a Clown", "Just a Touch Away", "Be Who You Are", "Open", "Ain't That Peculiar", "Night and Day", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "As You Do", "Get Ready", "Let Me Be the Clock", "Jasmin", "Quiet Storm (Groove Boutique Chill Jazz mix)", "The Hurt's On You", "Rewind", "Holly", "It's Fantastic", "Take Me Through The Night", "Rack Me Back", "When Smokey Sings Tears Of A Clown", "Easy", "Double Good Everything", "The Love Between Me and My Kids", "The Track of My Tears", "The Agony And The Ecstasy", "Happy (Love Theme From Lady Sings the Blues)", "Please Come Home for Christmas", "That Place", "Ebony Eyes", "Nearness of You"], "ans_acc": 0.0078125, "ans_hit": 1, "ans_f1": 0.015228426395939087, "ans_precission": 0.3, "ans_recall": 0.0078125, "path_f1": 0.02259887005649718, "path_precision": 0.1, "path_recall": 0.012738853503184714, "path_ans_f1": 0.015228426395939087, "path_ans_precision": 0.3, "path_ans_recall": 0.0078125}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> people.person.parents -> Jacqueline Kennedy Onassis\n# Answer:\nJacqueline Kennedy Onassis", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.friendship -> m.0645k94\n# Answer:\nbase.popstra.celebrity.friendship", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.dated -> m.063t10x\n# Answer:\nbase.popstra.celebrity.dated", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> Irish American -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJohn F. Kennedy -> people.person.ethnicity -> White people -> medicine.risk_factor.diseases -> Breast cancer\n# Answer:\nWhite people", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Presidential Library and Museum -> location.location.street_address -> m.0wjcsjl\n# Answer:\nlocation.location.street_address", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Jr. -> base.popstra.celebrity.dated -> m.0645kcr\n# Answer:\nbase.popstra.celebrity.dated", "# Reasoning Path:\nJohn F. Kennedy -> symbols.name_source.namesakes -> John F. Kennedy Presidential Library and Museum -> travel.tourist_attraction.near_travel_destination -> Dorchester\n# Answer:\nDorchester"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Montenegrin language\n# Answer:\nMontenegrin language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> language.language_dialect.language -> Serbo-Croatian Language\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Torlakian dialect -> common.topic.image -> Torlak\n# Answer:\nTorlakian dialect", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.main_country -> Albania\n# Answer:\nAlbania", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Montenegro\n# Answer:\nMontenegro", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.sports -> Basketball\n# Answer:\nBasketball", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language -> language.human_language.countries_spoken_in -> Albania\n# Answer:\nAlbanian language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
