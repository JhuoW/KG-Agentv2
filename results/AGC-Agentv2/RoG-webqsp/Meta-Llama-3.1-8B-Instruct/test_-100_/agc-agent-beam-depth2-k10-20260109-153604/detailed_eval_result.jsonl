{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.statistical_region.prevalence_of_undernourisment -> g.1hhc47p4n\n# Answer:\nlocation.statistical_region.prevalence_of_undernourisment"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8571428571428571, "ans_precission": 0.75, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> people.person.places_lived -> m.03phtbg -> people.place_lived.location -> North Carolina\n# Answer:\nNorth Carolina", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_profile.kind -> Title\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specialization_of -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specializations -> Tenant farmer\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> At 28, Polk was elected to the North Carolina State House of Representatives, where he served two years. -> base.kwebbase.kwsentence.next_sentence -> He opposed the old-style politicians who supported land speculators and bankers, and was a follower of Andrew Jackson, a popular military hero.\n# Answer:\nAt 28, Polk was elected to the North Carolina State House of Representatives, where he served two years.", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Lawyer -> people.profession.specializations -> Criminal defense lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> freebase.type_hints.included_types -> Person\n# Answer:\nPolitician", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specializations -> Grazier\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specialization_of -> Agriculturalist\n# Answer:\nFarmer", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Farmer -> people.profession.specializations -> Cowboy\n# Answer:\nFarmer"], "ground_truth": ["United States Representative", "Governor of Tennessee", "Speaker of the United States House of Representatives"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> m.0nf5p6x -> measurement_unit.dated_kgoe.source -> Energy use per capita, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nEnergy use per capita, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> m.0nf5p73 -> measurement_unit.dated_kgoe.source -> Energy use per capita, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nEnergy use per capita, World Development Indicators and Global Development Finance, World Bank"], "ground_truth": ["French", "Haitian Creole"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 0.7142857142857143, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.7272727272727273, "path_ans_precision": 0.5714285714285714, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> comic_books.comic_book_character.regular_featured_appearances -> Birds of Prey -> comic_books.comic_book_series.featured_characters -> Black Canary\n# Answer:\nBirds of Prey", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nYvonne Craig", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4 -> film.performance.actor -> Hannah Gunn\n# Answer:\nHannah Gunn", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0c04kpn -> film.performance.actor -> Melinda McGraw\n# Answer:\nMelinda McGraw", "# Reasoning Path:\nBarbara Gordon -> comic_books.comic_book_character.regular_featured_appearances -> Birds of Prey -> common.topic.article -> m.02vhn8\n# Answer:\nBirds of Prey", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0y54_x4 -> film.performance.film -> The Dark Knight\n# Answer:\nThe Dark Knight", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0wyc2dy -> tv.regular_tv_appearance.actor -> Danielle Judovits\n# Answer:\nDanielle Judovits", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0cgn08z -> tv.regular_tv_appearance.actor -> Dina Meyer\n# Answer:\nDina Meyer", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.actor -> Ilyssa Fradin\n# Answer:\nIlyssa Fradin", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.041w0vy -> film.performance.film -> Batman Begins\n# Answer:\nBatman Begins"], "ground_truth": ["Hannah Gunn", "Ilyssa Fradin", "Melinda McGraw"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0_r5_7f -> sports.sports_team_roster.team -> Phoenix Suns\n# Answer:\nPhoenix Suns", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.05gmg0p -> sports.sports_team_roster.team -> LSU Tigers men's basketball\n# Answer:\nLSU Tigers men's basketball", "# Reasoning Path:\nShaquille O'Neal\n# Answer:\nPhoenix Suns", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.0_r5_7f -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.04fw72l -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065p_r0 -> base.popstra.dated.participant -> Holly Robinson Peete\n# Answer:\nHolly Robinson Peete", "# Reasoning Path:\nShaquille O'Neal -> sports.pro_athlete.teams -> m.05gmg0p -> sports.sports_team_roster.position -> Center\n# Answer:\nCenter", "# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065q96t -> base.popstra.dated.participant -> Beverly DAngelo\n# Answer:\nBeverly DAngelo", "# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065pt05 -> base.popstra.dated.participant -> Inga Newson\n# Answer:\nInga Newson"], "ground_truth": ["Phoenix Suns", "Miami Heat", "LSU Tigers men's basketball", "Boston Celtics", "Orlando Magic", "Cleveland Cavaliers", "Los Angeles Lakers"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.4137931034482759, "ans_precission": 0.4, "ans_recall": 0.42857142857142855, "path_f1": 0.15, "path_precision": 0.3, "path_recall": 0.1, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.3, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New Rochelle\n# Answer:\nNew Rochelle", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.honored_for -> The Tonight Show with Jay Leno\n# Answer:\nThe Tonight Show with Jay Leno", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.included_in_group -> European American\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nEmmy Award for Outstanding Variety, Music or Comedy Series", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> common.topic.webpage -> m.09yqnh7\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.included_in_group -> Scottish people\n# Answer:\nScottish people", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0r66n6w -> award.award_honor.honored_for -> Jay Leno's Garage\n# Answer:\nJay Leno's Garage", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nPeople's Choice Award for Favorite Late Night Talk Show Host"], "ground_truth": ["New Rochelle"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Thai Language\n# Answer:\nThai Language", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\nlocation.statistical_region.gross_savings_as_percent_of_gdp"], "ground_truth": ["Akha Language", "Thai Language", "Hmong language", "Malay, Pattani Language", "Phu Thai language", "Mon Language", "Lao Language", "Mlabri Language", "Saek language", "Nyaw Language", "Vietnamese Language", "Cham language", "Khmer language"], "ans_acc": 0.07692307692307693, "ans_hit": 1, "ans_f1": 0.13793103448275862, "ans_precission": 0.6666666666666666, "ans_recall": 0.07692307692307693, "path_f1": 0.125, "path_precision": 0.3333333333333333, "path_recall": 0.07692307692307693, "path_ans_f1": 0.125, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.07692307692307693}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.writer.film -> Moneyball\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.film_story_contributor.film_story_credits -> A Few Good Men\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.writer.film -> A Few Good Men\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.actor.film -> m.0dgm3mp\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.writer.film -> Charlie Wilson's War\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.actor.film -> m.0dgm3n3\n# Answer:\nfilm.actor.film", "# Reasoning Path:\nThe Social Network -> award.award_nominated_work.award_nominations -> m.0fpny_l -> award.award_nomination.award_nominee -> David Fincher\n# Answer:\nDavid Fincher", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.film_story_contributor.film_story_credits -> Malice\n# Answer:\nAaron Sorkin", "# Reasoning Path:\nThe Social Network -> film.film.written_by -> Aaron Sorkin -> film.actor.film -> m.063hwv1\n# Answer:\nfilm.actor.film"], "ground_truth": ["Tyler Winklevoss", "Cameron Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> James Madison -> influence.influence_node.influenced_by -> Montesquieu\n# Answer:\nJames Madison", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9m7\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> James Madison -> influence.influence_node.influenced_by -> David Hume\n# Answer:\nJames Madison", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> James Madison -> government.politician.government_positions_held -> m.04mmb0x\n# Answer:\nJames Madison", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> government.politician.government_positions_held -> m.04mm9mx\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nThomas Jefferson\n# Answer:\nJames Madison", "# Reasoning Path:\nThomas Jefferson -> influence.influence_node.influenced -> James Madison -> organization.organization_founder.organizations_founded -> Democratic-Republican Party\n# Answer:\nJames Madison", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Lawyer -> freebase.notability_hints.notable_for -> Profession\n# Answer:\nLawyer"], "ground_truth": ["Lawyer", "Statesman", "Author", "Philosopher", "Archaeologist", "Architect", "Farmer", "Writer", "Teacher", "Inventor"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.3, "ans_recall": 0.2, "path_f1": 0.24, "path_precision": 0.3, "path_recall": 0.2, "path_ans_f1": 0.24, "path_ans_precision": 0.3, "path_ans_recall": 0.2}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> common.topic.subject_of -> Science\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland"], "ground_truth": ["Origin of Species (Harvard Classics, Part 11)", "Notebooks on transmutation of species", "Voyage of the Beagle (Dover Value Editions)", "The Correspondence of Charles Darwin, Volume 12", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Variation of Animals and Plants under Domestication", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 5", "vari\u00eberen der huisdieren en cultuurplanten", "The Autobiography of Charles Darwin, and selected letters", "The Correspondence of Charles Darwin, Volume 7", "The Essential Darwin", "The expression of the emotions in man and animals.", "Rejse om jorden", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 2", "The origin of species : complete and fully illustrated", "Les mouvements et les habitudes des plantes grimpantes", "The Autobiography of Charles Darwin [EasyRead Edition]", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 9: 1861", "genese\u014ds t\u014dn eid\u014dn", "The Autobiography Of Charles Darwin", "The collected papers of Charles Darwin", "The Voyage of the Beagle (Unabridged Classics)", "On evolution", "Darwin's notebooks on transmutation of species", "The Origin of Species (Great Minds Series)", "The Origin Of Species", "The Structure and Distribution of Coral Reefs", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Metaphysics, Materialism, & the evolution of mind", "Origin of Species", "The Voyage of the Beagle (Adventure Classics)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 6", "The Autobiography of Charles Darwin (Dodo Press)", "The Correspondence of Charles Darwin, Volume 9", "The Autobiography of Charles Darwin (Large Print)", "Wu zhong qi yuan", "Tesakneri tsagume\u030c", "The autobiography of Charles Darwin, 1809-1882", "H.M.S. Beagle in South America", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Volcanic Islands", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Correspondence of Charles Darwin, Volume 8", "More Letters of Charles Darwin", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 17: 1869", "Reise um die Welt 1831 - 36", "Voyage of the Beagle", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin en Patagonia", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Formation of Vegetable Mould through the Action of Worms", "The voyage of Charles Darwin", "To the members of the Down Friendly Club", "Charles Darwin's letters", "Darwin's insects", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Diary of the voyage of H.M.S. Beagle", "Gesammelte kleinere Schriften", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Correspondence of Charles Darwin, Volume 4", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The action of carbonate of ammonia on the roots of certain plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Origin of Species", "The Origin of Species (Great Books : Learning Channel)", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Origins", "Het uitdrukken van emoties bij mens en dier", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Charles Darwin's marginalia", "Notes on the fertilization of orchids", "Reise eines Naturforschers um die Welt", "On Natural Selection", "Resa kring jorden", "Memorias y epistolario i\u0301ntimo", "The Voyage of the Beagle (Everyman Paperbacks)", "The Correspondence of Charles Darwin, Volume 13", "The Voyage of the Beagle", "On a remarkable bar of sandstone off Pernambuco", "Darwin for Today", "Charles Darwin", "Voyage Of The Beagle", "Diario del Viaje de Un Naturalista Alrededor", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The expression of the emotions in man and animals", "Darwin", "The living thoughts of Darwin", "Die fundamente zur entstehung der arten", "The voyage of the Beagle.", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Darwin and Henslow", "La vie et la correspondance de Charles Darwin", "red notebook of Charles Darwin", "The Expression of the Emotions in Man and Animals", "La facult\u00e9 motrice dans les plantes", "Leben und Briefe von Charles Darwin", "Questions about the breeding of animals", "The Autobiography of Charles Darwin", "Charles Darwin on the routes of male humble bees", "The portable Darwin", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 15", "The Darwin Reader Second Edition", "The descent of man and selection in relation to sex.", "Evolution", "The Descent of Man, and Selection in Relation to Sex", "Autobiography of Charles Darwin", "The Origin of Species (Variorum Reprint)", "Evolution by natural selection", "The Correspondence of Charles Darwin, Volume 11", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 18: 1870", "The Expression of the Emotions in Man And Animals", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Les moyens d'expression chez les animaux", "Kleinere geologische Abhandlungen", "From So Simple a Beginning", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 10", "Die geschlechtliche Zuchtwahl", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Structure And Distribution of Coral Reefs", "Darwin Darwin", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwin-Wallace", "Insectivorous Plants", "On the Movements and Habits of Climbing Plants", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 10: 1862", "On the tendency of species to form varieties", "The Expression Of The Emotions In Man And Animals", "The Origin of Species (Oxford World's Classics)", "From Darwin's unpublished notebooks", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "The\u0301orie de l'e\u0301volution", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Different Forms of Flowers on Plants of the Same Species", "The descent of man, and selection in relation to sex", "The Origin of Species (Mentor)", "Voyage of the Beagle (Harvard Classics, Part 29)", "The origin of species", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The education of Darwin", "Fertilisation of Orchids", "Darwin Compendium", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Charles Darwin's natural selection", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 1", "The principal works", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Correspondence of Charles Darwin, Volume 14", "Works", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The Descent of Man and Selection in Relation to Sex", "Evolution and natural selection", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The Darwin Reader First Edition", "The geology of the voyage of H.M.S. Beagle", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Voyage of the Beagle (Mentor)", "The Origin of Species (Collector's Library)", "Motsa ha-minim", "The Orgin of Species", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 14: 1866", "Darwin on humus and the earthworm", "Voyage d'un naturaliste autour du monde", "Darwin's Ornithological notes", "monograph on the sub-class Cirripedia", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The descent of man, and selection in relation to sex.", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Origin of Species (Everyman's University Paperbacks)", "A student's introduction to Charles Darwin", "Beagle letters", "The structure and distribution of coral reefs", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 13: 1865", "El Origin De Las Especies", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Power of Movement in Plants", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The foundations of the Origin of species", "Proiskhozhdenie vidov", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Voyage of the Beagle (NG Adventure Classics)", "Darwin's journal", "A Darwin Selection", "The Origin of Species (World's Classics)", "The autobiography of Charles Darwin", "Darwinism stated by Darwin himself"], "ans_acc": 0.037383177570093455, "ans_hit": 1, "ans_f1": 0.042735042735042736, "ans_precission": 0.25, "ans_recall": 0.02336448598130841, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.07207207207207207, "path_ans_precision": 1.0, "path_ans_recall": 0.037383177570093455}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0j5dkn6 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> book.written_work.author -> Nathan Whitaker\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.team -> New York Jets\n# Answer:\nNew York Jets", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_for -> g.12595kjnj\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.04nb7z0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> award.award_nominee.award_nominations -> m.0z43czv -> award.award_nomination.award -> Best Male College Athlete ESPY Award\n# Answer:\nBest Male College Athlete ESPY Award"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.position_s -> Quarterback -> sports.sports_position.sport -> American football\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j4z5bh -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.position_s -> Quarterback -> base.ontologies.ontology_instance.equivalent_instances -> m.09klhys\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.position_s -> Quarterback -> common.topic.webpage -> m.09x5hdh\n# Answer:\nQuarterback", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Profession -> type.property.schema -> Person\n# Answer:\nProfession", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Sports played -> rdf-schema#domain -> Athlete\n# Answer:\nSports played", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.position_s -> Quarterback -> common.topic.webpage -> m.09ycth_\n# Answer:\nQuarterback"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.geolocation -> m.02_lmf5\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Ukraine\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Poland\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Hoverla -> location.location.containedby -> Ukraine\n# Answer:\nUkraine", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.partially_contained_by -> m.0wg9jws\n# Answer:\nBabia G\u00f3ra"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> music.lyricist.lyrics_written -> Auld Lang Syne\n# Answer:\nAuld Lang Syne", "# Reasoning Path:\nRobert Burns -> music.lyricist.lyrics_written -> Ae Fond Kiss\n# Answer:\nAe Fond Kiss", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> music.lyricist.lyrics_written -> To a Mouse\n# Answer:\nTo a Mouse", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb -> influence.influence_node.influenced_by -> Charlotte Bront\u00eb\n# Answer:\nEmily Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> influence.influence_node.influenced_by -> Emily Bront\u00eb\n# Answer:\nAnne Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb -> influence.influence_node.influenced_by -> Walter Scott\n# Answer:\nEmily Bront\u00eb", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Emily Bront\u00eb -> influence.influence_node.influenced -> Anne Bront\u00eb\n# Answer:\nEmily Bront\u00eb"], "ground_truth": ["Poet", "Writer", "Bard", "Author"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.4285714285714285, "ans_precission": 0.3, "ans_recall": 0.75, "path_f1": 0.4285714285714285, "path_precision": 0.3, "path_recall": 0.75, "path_ans_f1": 0.4285714285714285, "path_ans_precision": 0.3, "path_ans_recall": 0.75}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader\n# Answer:\nMichael Fox", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.actor -> Michael Fox\n# Answer:\nMichael Fox", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmz\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0113l35j -> film.performance.film -> Blackstar Warrior\n# Answer:\nBlackstar Warrior", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmm -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films_dubbed -> m.04dmkc9 -> film.dubbing_performance.actor -> Reiner Sch\u00f6ne\n# Answer:\nReiner Sch\u00f6ne", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films_dubbed -> m.04dmkc9 -> film.dubbing_performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nStar Wars Episode III: Revenge of the Sith", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02nwtmm -> film.performance.film -> Star Wars Episode V: The Empire Strikes Back\n# Answer:\nStar Wars Episode V: The Empire Strikes Back", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> book.book.characters -> Obi-Wan Kenobi\n# Answer:\nPath to Truth"], "ground_truth": ["Hayden Christensen"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9xs\n# Answer:\nsports.sports_award_winner.awards", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b\n# Answer:\nsports.sports_award_winner.awards", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kt4ps\n# Answer:\nsports.sports_award_winner.awards", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.games -> m.09kmwr0 -> american_football.player_game_statistics.team -> Indianapolis Colts\n# Answer:\nIndianapolis Colts", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.games -> m.09kmwr0 -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Profession -> owl#inverseOf -> People With This Profession\n# Answer:\nProfession", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.games -> m.07vz720\n# Answer:\namerican_football.football_player.games", "# Reasoning Path:\nPeyton Manning -> american_football.football_player.games -> m.07z21p4 -> american_football.player_game_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Profession -> rdf-schema#domain -> Person\n# Answer:\nProfession", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Sports played -> owl#inverseOf -> Athlete\n# Answer:\nSports played"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber\n# Answer:\nJustin Bieber", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0gwhmhm -> award.award_honor.ceremony -> Juno Awards of 2011\n# Answer:\nJuno Awards of 2011", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0gwhmhm -> award.award_honor.award -> Juno Fan Choice Award\n# Answer:\nJuno Fan Choice Award"], "ground_truth": ["Canada"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z973l -> military.military_combatant_group.combatants -> Iraq\n# Answer:\nIraq", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.commanders -> m.04q65_7 -> military.military_command.military_commander -> Margaret Thatcher\n# Answer:\nMargaret Thatcher", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.commanders -> m.0b6sk0b -> military.military_command.military_commander -> Mustafa Tlass\n# Answer:\nMustafa Tlass", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> United Kingdom\n# Answer:\nUnited Kingdom", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.commanders -> m.062tbhk -> military.military_command.military_commander -> Hosni Mubarak\n# Answer:\nHosni Mubarak", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Invasion of Kuwait -> military.military_conflict.combatants -> m.04yxs82\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Invasion of Kuwait -> military.military_conflict.combatants -> m.0bhdrfq\n# Answer:\nmilitary.military_conflict.combatants"], "ground_truth": ["Saudi Arabia", "Australia", "United States of America", "United Kingdom", "Argentina", "France", "Iraq"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.588235294117647, "ans_precission": 0.5, "ans_recall": 0.7142857142857143, "path_f1": 0.11940298507462686, "path_precision": 0.4, "path_recall": 0.07017543859649122, "path_ans_f1": 0.588235294117647, "path_ans_precision": 0.5, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0gcb7pp -> tv.regular_tv_appearance.actor -> Matthew Timmons\n# Answer:\nMatthew Timmons", "# Reasoning Path:\nThe Suite Life on Deck -> common.topic.notable_for -> g.125d5v3hd\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0gcb7pp -> tv.regular_tv_appearance.character -> Woody Fink\n# Answer:\nWoody Fink", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gtjm -> tv.regular_tv_appearance.actor -> Dylan Sprouse\n# Answer:\nDylan Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.actor -> Phill Lewis\n# Answer:\nPhill Lewis", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award_nominee -> Dylan Sprouse\n# Answer:\nDylan Sprouse", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0b_gtjm -> tv.regular_tv_appearance.character -> Zack Martin\n# Answer:\nZack Martin", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86\n# Answer:\naward.award_nominated_work.award_nominations", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0gcb7pp -> tv.regular_tv_appearance.seasons -> The Suite Life on Deck - Season 2\n# Answer:\nThe Suite Life on Deck - Season 2", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nMr. Moseby"], "ground_truth": ["Brenda Song"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> government.governmental_body.members -> m.05glyvj\n# Answer:\ngovernment.governmental_body.members", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government_positions -> Ohio State Senator -> government.government_office_or_title.governmental_body_if_any -> Ohio Senate\n# Answer:\nOhio State Senator", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government -> Government of Ohio -> common.topic.image -> The Rhodes State Office Tower adjacent to the Statehouse\n# Answer:\nGovernment of Ohio", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.government_positions -> Ohio State Senator -> common.topic.notable_for -> g.125c1jtfp\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhl0 -> government.government_position_held.office_holder -> Mike DeWine\n# Answer:\nMike DeWine", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhr5 -> government.government_position_held.office_holder -> Harold Hitz Burton\n# Answer:\nHarold Hitz Burton", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhrh -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhrh -> government.government_position_held.office_holder -> A. Victor Donahey\n# Answer:\nA. Victor Donahey", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0bfmhr5 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate"], "ground_truth": ["Ted Strickland", "John Kasich", "Return J. Meigs, Jr."], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham\n# Answer:\nDavid Beckham"], "ground_truth": ["LA Galaxy"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.capital_of_administrative_division.capital_of -> m.0jvvl78\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Spain\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> base.aareas.schema.administrative_area.administrative_children -> Castile and Le\u00f3n\n# Answer:\nCastile and Le\u00f3n", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.containedby -> Province of Salamanca\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> base.aareas.schema.administrative_area.administrative_children -> Navarre\n# Answer:\nNavarre", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.first_level_divisions -> Castile and Le\u00f3n\n# Answer:\nCastile and Le\u00f3n", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> base.aareas.schema.administrative_area.administrative_children -> Aragon\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.administrative_divisions -> Castile and Le\u00f3n\n# Answer:\nCastile and Le\u00f3n", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain -> location.country.first_level_divisions -> Andalusia\n# Answer:\nAndalusia"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.04dmfx8 -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.04dmfx8 -> government.government_position_held.governmental_body -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.02kbhl6 -> government.government_position_held.district_represented -> Massachusetts\n# Answer:\nMassachusetts", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.02kbhl6 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nUnited States Senate", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.02hz5r6 -> freebase.valuenotation.is_reviewed -> Officeholder\n# Answer:\nOfficeholder", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nParty", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.government_positions_held -> m.02hz5r6 -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nlocation.location.geolocation", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> common.topic.image -> The Fukushima 1 NPP -> common.image.size -> m.0k127w\n# Answer:\nThe Fukushima 1 NPP", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 3\n# Answer:\nFukushima I \u2013 3", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> common.topic.notable_for -> g.1257z60_h\n# Answer:\nFukushima I \u2013 2", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> base.infrastructure.nuclear_power_plant.reactor_type -> Boiling water reactor\n# Answer:\nFukushima I \u2013 2"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> location.country.first_level_divisions -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> location.country.first_level_divisions -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> base.aareas.schema.administrative_area.administrative_children -> North East England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.country.first_level_divisions -> County Tyrone\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> location.country.first_level_divisions -> East of England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> base.aareas.schema.administrative_area.administrative_children -> Yorkshire and the Humber\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.country.first_level_divisions -> County Antrim\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England -> base.aareas.schema.administrative_area.administrative_children -> East Midlands\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland -> location.country.first_level_divisions -> County Londonderry\n# Answer:\nNorthern Ireland"], "ground_truth": ["Wales", "Scotland", "England", "Northern Ireland"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.2727272727272727, "path_precision": 0.3, "path_recall": 0.25, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wjpmvd -> location.partial_containment_relationship.partially_contains -> Bayou Bartholomew\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8_zq -> location.partial_containment_relationship.partially_contains -> Mississippi River\n# Answer:\nMississippi River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8_r5\n# Answer:\nlocation.location.partially_contained_by", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__5 -> location.partial_containment_relationship.partially_contains -> Tickfaw River\n# Answer:\nTickfaw River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> location.location.partially_contained_by -> m.0wjpmvd\n# Answer:\nlocation.location.partially_contained_by", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_contained_by -> m.0wg8__h\n# Answer:\nlocation.location.partially_contained_by", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nDenham Springs", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.partially_containedby -> Mississippi\n# Answer:\nAmite River"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> religion.religious_leader.religious_leadership -> m.0dgvxmz -> religion.religious_organization_leadership.role -> Doctor of the Church\n# Answer:\nDoctor of the Church", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Thomas Aquinas -> influence.influence_node.influenced_by -> Anselm of Canterbury\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced_by -> Cicero -> influence.influence_node.influenced_by -> Aristotle\n# Answer:\nCicero", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Thomas Aquinas -> influence.influence_node.influenced_by -> Albertus Magnus\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Thomas Aquinas -> influence.influence_node.influenced -> Duns Scotus\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced_by -> Cicero -> influence.influence_node.influenced_by -> Plato\n# Answer:\nCicero", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Thomas Aquinas -> influence.influence_node.influenced_by -> Al-Kindi\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Thomas Aquinas -> people.person.profession -> Philosopher\n# Answer:\nThomas Aquinas"], "ground_truth": ["Philosopher", "Physician", "Writer"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666666, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> people.person.spouse_s -> m.0n9hknn\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> common.topic.notable_for -> g.125dlnswt\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nYvonne Cole", "# Reasoning Path:\nKeyshia Cole -> award.award_nominee.award_nominations -> m.0zbhlnq -> award.award_nomination.ceremony -> American Music Awards of 2006\n# Answer:\nAmerican Music Awards of 2006", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Yvonne Cole -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nYvonne Cole"], "ground_truth": ["Sal Gibson", "Leon Cole", "Francine Lons"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.4444444444444444, "ans_precission": 0.3333333333333333, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.2222222222222222, "path_recall": 0.6666666666666666, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government -> Government of Egypt\n# Answer:\nGovernment of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc469t1\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> Shura Council\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\nlocation.statistical_region.size_of_armed_forces", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> common.topic.notable_types -> Governmental Body\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Shura Council -> common.topic.notable_types -> Governmental Body\n# Answer:\nShura Council", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nParliament of Egypt"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Mojola Agbebi\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King III -> people.person.place_of_birth -> Montgomery\n# Answer:\nMontgomery", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Atlanta\n# Answer:\nAtlanta", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Ann Nixon Cooper\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> film.person_or_entity_appearing_in_film.films -> m.0gc6p8z -> film.personal_film_appearance.film -> The Greatest Speeches of All Time: Vol. 1\n# Answer:\nThe Greatest Speeches of All Time: Vol. 1", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.containedby -> Georgia\n# Answer:\nGeorgia", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> people.place_of_interment.interred_here -> Benjamin F. Ward\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> location.location.geolocation -> m.0wmyhzk\n# Answer:\nMartin Luther King, Jr. National Historic Site"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminster Hall and Burying Ground"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta History Center -> exhibitions.exhibition_venue.exhibitions_at_this_venue -> m.05by1nb\n# Answer:\nAtlanta History Center", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.0542hwn -> common.webpage.category -> Topic Webpage\n# Answer:\nTopic Webpage", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Six Flags Over Georgia -> amusement_parks.park.rides -> Dahlonega Mine Train\n# Answer:\nDahlonega Mine Train", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\ntravel.transport_terminus.travel_destinations_served", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.0542hwn -> common.webpage.resource -> Visitor Information\n# Answer:\nVisitor Information", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.09w0bqz -> common.webpage.resource -> Exclusive: 'Heroes' Geeks Out Over Seth Green, Breckin Meyer\n# Answer:\nExclusive: 'Heroes' Geeks Out Over Seth Green, Breckin Meyer", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta History Center -> location.location.containedby -> Fulton County\n# Answer:\nAtlanta History Center"], "ground_truth": ["CNN Center", "Margaret Mitchell House & Museum", "Martin Luther King, Jr. National Historic Site", "Six Flags Over Georgia", "Jimmy Carter Library and Museum", "Fernbank Museum of Natural History", "Georgia Dome", "Arbor Place Mall", "Six Flags White Water", "Fox Theatre", "Atlanta Jewish Film Festival", "World of Coca-Cola", "Georgia World Congress Center", "Atlanta Marriott Marquis", "Atlanta Cyclorama & Civil War Museum", "Omni Coliseum", "Atlanta Symphony Orchestra", "Variety Playhouse", "Turner Field", "Georgia State Capitol", "Woodruff Arts Center", "Fernbank Science Center", "The Tabernacle", "Centennial Olympic Park", "Cobb Energy Performing Arts Centre", "Masquerade", "Atlanta History Center", "Philips Arena", "Center for Puppetry Arts", "Atlanta Ballet", "Underground Atlanta", "Hyatt Regency Atlanta", "Georgia Aquarium", "Four Seasons Hotel Atlanta", "Zoo Atlanta", "Peachtree Road Race"], "ans_acc": 0.08333333333333333, "ans_hit": 1, "ans_f1": 0.09999999999999999, "ans_precission": 0.5, "ans_recall": 0.05555555555555555, "path_f1": 0.14634146341463414, "path_precision": 0.6, "path_recall": 0.08333333333333333, "path_ans_f1": 0.18750000000000003, "path_ans_precision": 0.6, "path_ans_recall": 0.1111111111111111}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nElectoral district of South Brisbane", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> people.person.education -> m.04hrrdm -> education.education.institution -> University of Queensland\n# Answer:\nUniversity of Queensland", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2015\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.basic_title -> Premier\n# Answer:\nPremier", "# Reasoning Path:\nAnna Bligh -> people.person.education -> m.0n13s3z -> education.education.institution -> Miami State High School\n# Answer:\nMiami State High School", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.education -> m.04hrrdm -> education.education.major_field_of_study -> Social science\n# Answer:\nSocial science"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street\n# Answer:\nPeter Whalley", "# Reasoning Path:\nCoronation Street -> tv.tv_program.recurring_writers -> m.0x0dtzm -> tv.tv_program_writer_relationship.writer -> Peter Whalley\n# Answer:\nPeter Whalley", "# Reasoning Path:\nCoronation Street -> tv.tv_program.recurring_writers -> m.0w_xm47 -> tv.tv_program_writer_relationship.writer -> Phil Collinson\n# Answer:\nPhil Collinson", "# Reasoning Path:\nCoronation Street -> tv.tv_program.recurring_writers -> m.0w_sxrf -> tv.tv_program_writer_relationship.writer -> Debbie Oates\n# Answer:\nDebbie Oates", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> tv.tv_character.appeared_in_tv_episodes -> m.06zxzgq\n# Answer:\ntv.tv_character.appeared_in_tv_episodes", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.notable_for -> g.1255d7dvc\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy-drama -> common.topic.notable_for -> g.1258t625w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.subject_of -> Albrecht Behmel\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Comedy-drama -> film.film_subject.films -> Desire Street\n# Answer:\nComedy-drama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Soap opera -> common.topic.webpage -> m.09yqnf1\n# Answer:\ncommon.topic.webpage"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.reverse_property -> People born here\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.expected_type -> Location\n# Answer:\nLocation", "# Reasoning Path:\nAndy Murray -> tennis.tennis_tournament_champion.tennis_titles -> m.0_z96sx -> tennis.tennis_tournament_championship.tournament -> Canadian Open\n# Answer:\nCanadian Open", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#domain -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> tennis.tennis_player.matches_won -> m.0lzf1z3 -> tennis.tennis_match.match_format -> Men's singles\n# Answer:\nMen's singles", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> tennis.tennis_tournament_champion.tennis_titles -> m.0_yybn4 -> tennis.tennis_tournament_championship.tournament -> Queen's Club Championships\n# Answer:\nQueen's Club Championships", "# Reasoning Path:\nAndy Murray\n# Answer:\nPlace of birth"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_types -> Currency\n# Answer:\nCurrency", "# Reasoning Path:\nAustralian dollar -> common.topic.notable_for -> g.1256mspd6\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAustralian dollar\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> m.03j_zjk\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.internet_tld -> cx\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> m.0h_3m9l\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> g.11bymn224r\n# Answer:\nChristmas Island"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_zt00\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.12tb6fsx6\n# Answer:\nlocation.statistical_region.gni_per_capita_in_ppp_dollars"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.team -> Auburn Tigers football\n# Answer:\nAuburn Tigers football", "# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0z23kt0 -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nQuarterback"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.125, "path_recall": 0.3333333333333333, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nUniversity of Alabama", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.places_lived -> m.03ppx3r -> people.place_lived.location -> Monroeville\n# Answer:\nMonroeville", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.place_of_birth -> Monroeville -> location.location.people_born_here -> Marie Rudisill\n# Answer:\nMarie Rudisill", "# Reasoning Path:\nHarper Lee -> people.person.places_lived -> m.04hd8bj -> people.place_lived.location -> Alabama\n# Answer:\nAlabama", "# Reasoning Path:\nHarper Lee -> people.person.place_of_birth -> Monroeville -> location.location.people_born_here -> Bert Nettles\n# Answer:\nBert Nettles", "# Reasoning Path:\nHarper Lee -> people.person.place_of_birth -> Monroeville -> location.citytown.postal_codes -> 36460\n# Answer:\nMonroeville", "# Reasoning Path:\nHarper Lee -> people.person.place_of_birth -> Monroeville -> location.location.people_born_here -> Allison Moorer\n# Answer:\nMonroeville", "# Reasoning Path:\nHarper Lee -> people.person.place_of_birth -> Monroeville -> location.statistical_region.population -> m.0hk5bf2\n# Answer:\nMonroeville"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0_rc0x7 -> government.government_position_held.office_holder -> Stuart Reid\n# Answer:\nStuart Reid", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.0_rc0x7 -> government.government_position_held.basic_title -> Member of the Legislative Assembly\n# Answer:\nMember of the Legislative Assembly", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.04kcmn6 -> government.government_position_held.office_holder -> Heber Manning Wells\n# Answer:\nHeber Manning Wells", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.04kcmn6 -> government.government_position_held.office_position_or_title -> Governor of Utah\n# Answer:\nGovernor of Utah", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nUtah -> government.governmental_jurisdiction.governing_officials -> m.010flwmg -> government.government_position_held.office_position_or_title -> Lieutenant Governor of Utah\n# Answer:\nLieutenant Governor of Utah"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> freebase.valuenotation.has_value -> Siblings\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.us_president.vice_president -> Dick Cheney -> people.person.nationality -> United States of America\n# Answer:\nDick Cheney", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> government.election_campaign.election -> United States presidential election, 2004\n# Answer:\nUnited States presidential election, 2004", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.election -> United States presidential election, 2000\n# Answer:\nUnited States presidential election, 2000", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2004 -> common.topic.notable_for -> g.1257w3www\n# Answer:\nGeorge W. Bush presidential campaign, 2004", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q61y -> people.appointment.appointee -> William E. Todd\n# Answer:\nWilliam E. Todd", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.07p2gdn -> people.appointment.appointee -> John G. Grimes\n# Answer:\nJohn G. Grimes", "# Reasoning Path:\nGeorge W. Bush -> government.politician.election_campaigns -> George W. Bush presidential campaign, 2000 -> government.election_campaign.party -> Republican Party\n# Answer:\nGeorge W. Bush presidential campaign, 2000", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.079q61y -> people.appointment.appointed_role -> United States Ambassador to Brunei Darussalam\n# Answer:\nUnited States Ambassador to Brunei Darussalam", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.07t428_ -> people.appointment.appointee -> Robin R. Sanders\n# Answer:\nRobin R. Sanders"], "ground_truth": ["Gene Amondson", "John Kerry", "Michael Peroutka", "Ralph Nader"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0pdtjg5\n# Answer:\npeople.person.spouse_s", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57\n# Answer:\npeople.person.spouse_s", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> Colossus: The Rise and Fall of the American Empire -> book.written_work.author -> Dennis Feltham Jones\n# Answer:\nDennis Feltham Jones", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nmc7 -> film.personal_film_appearance.film -> The Ascent of Money\n# Answer:\nThe Ascent of Money", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The Cash Nexus: Money and Power in the Modern World, 1700-2000 -> book.written_work.subjects -> Economics\n# Answer:\nThe Cash Nexus: Money and Power in the Modern World, 1700-2000", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The Cash Nexus: Money and Power in the Modern World, 1700-2000 -> book.written_work.subjects -> Politics\n# Answer:\nThe Cash Nexus: Money and Power in the Modern World, 1700-2000", "# Reasoning Path:\nNiall Ferguson -> book.author.works_written -> The Cash Nexus: Money and Power in the Modern World, 1700-2000 -> common.topic.notable_types -> Book\n# Answer:\nThe Cash Nexus: Money and Power in the Modern World, 1700-2000", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0pdthbn -> film.personal_film_appearance.film -> Valentino's Ghost\n# Answer:\nValentino's Ghost", "# Reasoning Path:\nNiall Ferguson -> film.person_or_entity_appearing_in_film.films -> m.0h4nq4f -> film.personal_film_appearance.film -> The War of the World: A New History of the 20th Century\n# Answer:\nThe War of the World: A New History of the 20th Century"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Gal\u00e1pagos National Park -> location.location.geolocation -> m.0clvbn7\n# Answer:\nGal\u00e1pagos National Park", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> geography.island.body_of_water -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Floreana Island -> symbols.namesake.named_after -> Juan Jos\u00e9 Flores\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Santa Cruz Island -> geography.island.body_of_water -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Bartolom\u00e9 Island -> location.location.geolocation -> m.0dg7tbw\n# Answer:\nBartolom\u00e9 Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney - Ecuador: Amazon and Galapagos Islands -> travel.tour_operator.travel_destinations -> Santa Fe Island\n# Answer:\nAdventures by Disney - Ecuador: Amazon and Galapagos Islands", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nSanta Cruz Canton, Ecuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Floreana Island -> location.location.events -> Action off Charles Island\n# Answer:\nFloreana Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney - Ecuador and Galapagos Islands -> travel.tour_operator.travel_destinations -> Santa Cruz Island\n# Answer:\nAdventures by Disney - Ecuador and Galapagos Islands", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tour_operators -> Adventures by Disney - Ecuador and Galapagos Islands -> travel.tour_operator.travel_destinations -> Quito\n# Answer:\nAdventures by Disney - Ecuador and Galapagos Islands"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.631578947368421, "ans_precission": 0.6, "ans_recall": 0.6666666666666666, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.631578947368421, "path_ans_precision": 0.6, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber\n# Answer:\nNasri", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.award -> MTV Europe Music Voices Award\n# Answer:\nMTV Europe Music Voices Award", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0115qhzk -> award.award_honor.ceremony -> 2011 MTV Europe Music Awards\n# Answer:\n2011 MTV Europe Music Awards", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> music.composition.composer -> Nasri\n# Answer:\nNasri", "# Reasoning Path:\nJustin Bieber -> music.lyricist.lyrics_written -> All Around The World -> common.topic.article -> m.0j_3rq4\n# Answer:\nAll Around The World"], "ground_truth": ["Pray", "All Bad", "Somebody to Love", "Turn to You (Mother's Day Dedication)", "Right Here", "#thatPower", "First Dance", "Home to Mama", "Eenie Meenie", "Change Me", "Boyfriend", "Die in Your Arms", "Confident", "Hold Tight", "Lolly", "As Long as You Love Me", "Beauty And A Beat", "All That Matters", "Wait for a Minute", "Live My Life", "PYD", "Bad Day", "All Around The World", "Recovery", "Heartbreaker", "Never Say Never", "Never Let You Go", "Roller Coaster", "Thought Of You", "Bigger", "Baby"], "ans_acc": 0.03225806451612903, "ans_hit": 1, "ans_f1": 0.05555555555555555, "ans_precission": 0.2, "ans_recall": 0.03225806451612903, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.05970149253731343, "path_ans_precision": 0.4, "path_ans_recall": 0.03225806451612903}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_for -> g.125b587vz\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_for -> g.125cswvwv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> people.profession.specialization_of -> Politician\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.deceased_person.place_of_death -> Paris -> film.film_subject.films -> Am\u00e9lie\n# Answer:\nParis", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.corresponding_type -> Author\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> people.profession.specialization_of -> Official\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.deceased_person.place_of_death -> Paris -> film.film_subject.films -> An American in Paris\n# Answer:\nParis", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.deceased_person.place_of_death -> Paris -> location.administrative_division.capital -> m.0jvvlcg\n# Answer:\nParis", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> people.profession.specializations -> Muckraker\n# Answer:\nJournalist"], "ground_truth": ["Journalist", "Publisher", "Statesman", "Writer", "Physician"], "ans_acc": 0.4, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.5, "ans_recall": 0.4, "path_f1": 0.48, "path_precision": 0.6, "path_recall": 0.4, "path_ans_f1": 0.48, "path_ans_precision": 0.6, "path_ans_recall": 0.4}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> common.topic.notable_for -> g.125flk9rn\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nArizona\n# Answer:\n793 Arizona", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09x4hq1 -> common.webpage.resource -> Mike Tyson allegedly hits photographer\n# Answer:\nMike Tyson allegedly hits photographer", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09x4hq1 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xtp66 -> common.webpage.resource -> Hitting the post-Grammy parties with Dierks Bentley\n# Answer:\nHitting the post-Grammy parties with Dierks Bentley", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w2m5s -> common.webpage.resource -> Exclusive: 'Grey's Anatomy' promotes Jessica Capshaw\n# Answer:\nExclusive: 'Grey's Anatomy' promotes Jessica Capshaw", "# Reasoning Path:\nArizona -> symbols.name_source.namesakes -> 793 Arizona -> astronomy.star_system_body.star_system -> Solar System\n# Answer:\n793 Arizona", "# Reasoning Path:\nArizona -> symbols.name_source.namesakes -> 793 Arizona -> astronomy.astronomical_discovery.discovery_site -> Lowell Observatory\n# Answer:\nLowell Observatory", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09xtp66 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nArizona -> common.topic.webpage -> m.09w2m5s -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index"], "ground_truth": ["Saguaro"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.country -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nBeyonc\u00e9 Knowles", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.0wkb5zg -> music.track_contribution.track -> We All Want Love\n# Answer:\nWe All Want Love", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.064_ltw -> base.popstra.infidelity.participant -> Kanye West\n# Answer:\nKanye West", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.0v2yr61 -> music.track_contribution.track -> Drunk on Love\n# Answer:\nDrunk on Love", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.0wkb5zg -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.0v2yr61 -> music.track_contribution.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nIf I Never See Your Face Again", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.perpetrator -> Jay-Z\n# Answer:\nJay-Z"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison\n# Answer:\nWilliam Henry Harrison", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmvhw\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\ncommon.image.size", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\ncommon.image.size"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Alan Muraoka\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Richard S\u00e1nchez\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> location.location.people_born_here -> Mike Montgomery\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills -> common.topic.image -> Ryan Braun\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094k4jf -> common.webpage.resource -> George Lopez will host Latin Grammys\n# Answer:\nGeorge Lopez will host Latin Grammys", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094k4jf -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.0948qtd -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0pw9ccp -> film.personal_film_appearance.film -> Loco Comedy Jam: Vol. 1\n# Answer:\nLoco Comedy Jam: Vol. 1", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.094hhb3 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nEntertainment Weekly annotation index", "# Reasoning Path:\nGeorge Lopez -> film.person_or_entity_appearing_in_film.films -> m.0pw9ccp -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nHim/Herself"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.03l9ynf\n# Answer:\norganization.organization.headquarters", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.010f4dpq\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.010b9m82\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Toyota Group\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Sikhism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Jehovah's Witnesses\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Moses and Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> religion.belief.belief_of -> Christianity\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> base.ontologies.ontology_instance.equivalent_instances -> m.07nfc1g\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> I, the Sun\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism -> book.book_subject.works -> Beside Still Waters\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> God in Islam -> common.topic.article -> m.09kb_7\n# Answer:\nGod in Islam"], "ground_truth": ["Tawhid", "Islamic holy books", "Entering Heaven alive", "Monotheism", "Mahdi", "God in Islam", "Masih ad-Dajjal", "Sharia", "Predestination in Islam", "Islamic view of angels", "\u1e6c\u016bb\u0101", "Qiyamah", "Prophets in Islam"], "ans_acc": 0.15384615384615385, "ans_hit": 1, "ans_f1": 0.26277372262773724, "ans_precission": 0.9, "ans_recall": 0.15384615384615385, "path_f1": 0.25806451612903225, "path_precision": 0.8, "path_recall": 0.15384615384615385, "path_ans_f1": 0.25806451612903225, "path_ans_precision": 0.8, "path_ans_recall": 0.15384615384615385}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nJamie Dornan", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> film.film.starring -> m.0ydn3r2\n# Answer:\nfilm.film.starring", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> book.book.characters -> Anastasia Steele\n# Answer:\nFifty Shades of Grey", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades Darker -> book.book.characters -> Anastasia Steele\n# Answer:\nFifty Shades Darker", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> book.book.characters -> Carrick Grey\n# Answer:\nCarrick Grey", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades Darker -> book.written_work.author -> E. L. James\n# Answer:\nE. L. James", "# Reasoning Path:\nChristian Grey -> book.book_character.appears_in_book -> Fifty Shades of Grey -> book.book.characters -> Bob Adams\n# Answer:\nBob Adams"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.19999999999999998, "ans_precission": 0.1111111111111111, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.19999999999999998, "path_ans_precision": 0.1111111111111111, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.place_of_death -> London -> location.administrative_division.country -> United Kingdom\n# Answer:\nLondon", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.place_of_death -> London -> organization.organization_scope.organizations_with_this_scope -> Box UK\n# Answer:\nLondon", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.place_of_death -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> location.administrative_division.country -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nAdolf Hitler -> people.person.nationality -> German Reich\n# Answer:\nGerman Reich", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> base.aareas.schema.administrative_area.administrative_parent -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nAdolf Hitler -> people.deceased_person.place_of_death -> Berlin -> location.administrative_division.first_level_division_of -> Germany\n# Answer:\nGermany"], "ground_truth": ["Nazi Germany"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Jazz -> music.compositional_form.subforms -> Vocal jazz\n# Answer:\nJazz", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Jazz -> music.genre.subgenre -> Swing music\n# Answer:\nJazz", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Traditional pop music -> music.genre.parent_genre -> Pop music\n# Answer:\nTraditional pop music", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Jazz -> common.topic.subjects -> Sylvia Brooks\n# Answer:\nJazz", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Easy listening -> music.genre.parent_genre -> Parlour music\n# Answer:\nEasy listening", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.origin -> Burnaby -> location.capital_of_administrative_division.capital_of -> m.0r5fw61\n# Answer:\nBurnaby", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Jazz -> music.genre.subgenre -> Kansas City jazz\n# Answer:\nJazz", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Traditional pop music -> music.genre.parent_genre -> Arabesque\n# Answer:\nTraditional pop music", "# Reasoning Path:\nMichael Bubl\u00e9 -> music.artist.genre -> Traditional pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nTraditional pop music"], "ground_truth": ["Actor", "Songwriter", "Singer"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas Speedway\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.postal_code.country -> United States of America\n# Answer:\n66111", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.geometry -> m.057rh7c\n# Answer:\n66111", "# Reasoning Path:\nKansas Speedway -> base.nascar.nascar_venue.nascar_races_held_here -> Kansas Lottery 300 -> common.topic.article -> m.0dp7br\n# Answer:\ncommon.topic.article", "# Reasoning Path:\nKansas City -> location.citytown.postal_codes -> 66101 -> location.postal_code.country -> United States of America\n# Answer:\n66101"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.5454545454545454, "path_ans_precision": 0.375, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.team -> Los Angeles Bulldogs\n# Answer:\nLos Angeles Bulldogs", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Langley Wakeman Collyer\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgj2z -> sports.sports_team_roster.position -> Running back\n# Answer:\nRunning back", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0ncxlxp -> sports.sports_team_roster.team -> Kansas City Monarchs\n# Answer:\nKansas City Monarchs", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> UCLA Bruins football\n# Answer:\nUCLA Bruins football", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Monk Eastman\n# Answer:\nMonk Eastman", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> location.location.containedby -> Brooklyn\n# Answer:\nBrooklyn", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> people.place_of_interment.interred_here -> Irving Lehman\n# Answer:\nIrving Lehman", "# Reasoning Path:\nJackie Robinson\n# Answer:\nLos Angeles Bulldogs", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> location.location.containedby -> 11208\n# Answer:\nCypress Hills Cemetery"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0w4c1m6\n# Answer:\nfilm.film.release_date_s", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0lhvp68\n# Answer:\nfilm.film.release_date_s", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Date Opened\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> film.film.release_date_s -> m.0nc_4m7\n# Answer:\nfilm.film.release_date_s", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Date Opened\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> freebase.valuenotation.has_value -> Producing Company\n# Answer:\nProducing Company", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Theater production venue relationship\n# Answer:\nTheater production venue relationship", "# Reasoning Path:\nAnnie\n# Answer:\nDate Opened", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nLouisiana State University", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07z250c -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07td0n0 -> american_football.player_passing_statistics.team -> Oakland Raiders\n# Answer:\nOakland Raiders", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07mmjx4 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07td0n0 -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nJaMarcus Russell -> american_football.football_player.passing -> m.07z250c -> american_football.player_passing_statistics.season -> 2009 NFL season\n# Answer:\n2009 NFL season"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.2222222222222222, "ans_precission": 0.125, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> The Autobiography of Eleanor Roosevelt -> book.written_work.subjects -> 20th century\n# Answer:\nThe Autobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> The Autobiography of Eleanor Roosevelt -> book.written_work.subjects -> United States of America\n# Answer:\nThe Autobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> book.book_edition.book -> The Autobiography of Eleanor Roosevelt\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> The Autobiography of Eleanor Roosevelt -> book.written_work.subjects -> Woman\n# Answer:\nThe Autobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4444444444444445, "ans_precission": 0.2857142857142857, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.03xf301 -> location.religion_percentage.religion -> Protestantism\n# Answer:\nProtestantism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szk2 -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.064szjw\n# Answer:\nlocation.statistical_region.religions", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\nlocation.statistical_region.official_development_assistance", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\nlocation.statistical_region.official_development_assistance"], "ground_truth": ["Catholicism", "Protestantism", "Hinduism", "Islam"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.5, "ans_recall": 0.5, "path_f1": 0.3636363636363636, "path_precision": 0.3333333333333333, "path_recall": 0.4, "path_ans_f1": 0.4, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 0.5}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.parent_cause_of_death -> Murder\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> people.cause_of_death.includes_causes_of_death -> Assassination by firearm\n# Answer:\nAssassination by firearm", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> g.11bc87r407\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Assassination -> base.fight.crime_type.crimes_of_this_type -> Assassination of James A. Garfield\n# Answer:\nAssassination", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.citytown.postal_codes -> 64503\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> base.militaryinfiction.location_in_fiction.contains -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph -> location.statistical_region.population -> m.066hwtb\n# Answer:\nSaint Joseph"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.04j60k7\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0446bdb\n# Answer:\ngovernment.politician.government_positions_held", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.jurisdiction_of_office -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> government.politician.party -> m.03gjfyn -> government.political_party_tenure.party -> National Union Party\n# Answer:\nNational Union Party", "# Reasoning Path:\nAbraham Lincoln -> government.politician.government_positions_held -> m.0bqspr2 -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives", "# Reasoning Path:\nAbraham Lincoln -> government.politician.party -> m.03ld2ph -> government.political_party_tenure.party -> Whig Party\n# Answer:\nWhig Party", "# Reasoning Path:\nAbraham Lincoln -> government.politician.party -> m.03gjfyh -> government.political_party_tenure.party -> Republican Party\n# Answer:\nRepublican Party", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nKentucky", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nIllinois", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nSpringfield"], "ground_truth": ["Andrew Johnson", "Hannibal Hamlin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens\n# Answer:\nChristmas Books", "# Reasoning Path:\nCharles Dickens -> book.author.series_written_or_contributed_to -> Christmas Books -> book.literary_series.works_in_this_series -> The Battle of Life\n# Answer:\nChristmas Books", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Elizabeth Gaskell\n# Answer:\nElizabeth Gaskell", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Adelaide Anne Procter\n# Answer:\nAdelaide Anne Procter", "# Reasoning Path:\nCharles Dickens -> book.author.series_written_or_contributed_to -> Christmas Books -> common.topic.notable_types -> Literary Series\n# Answer:\nChristmas Books", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> book.written_work.author -> Wilkie Collins\n# Answer:\nWilkie Collins", "# Reasoning Path:\nCharles Dickens -> book.author.works_edited -> A House to Let -> common.topic.notable_for -> g.1255hfpzv\n# Answer:\nA House to Let"], "ground_truth": ["A Tale of Two Cities (Macmillan Students' Novels)", "A Tale of Two Cities (Isis Clear Type Classic)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Longman Fiction)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Tor Classics)", "Our mutual friend.", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Masterworks)", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol", "Oliver Twist", "A Tale of Two Cities (Silver Classics)", "The cricket on the hearth", "Martin Chuzzlewit", "The old curiosity shop", "A Tale of Two Cities (Cyber Classics)", "The old curiosity shop.", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Read & Listen Books)", "Great Expectations", "Bleak House.", "A Christmas Carol (Watermill Classics)", "A Tale of Two Cities", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Large Print Edition)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol (Take Part)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Classic Fiction)", "Great expectations", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Penguin Readers, Level 2)", "Great Expectations.", "A Christmas Carol (Aladdin Classics)", "Dombey and Son.", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Simple English)", "The Pickwick papers", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Tale of Two Cities (Acting Edition)", "A Tale of Two Cities (Longman Classics, Stage 2)", "Bleak House", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Oxford Bookworms Library)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Piccolo Books)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "Dombey and son", "A Tale of Two Cities (Collected Works of Charles Dickens)", "The Old Curiosity Shop", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Penguin Classics)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Large Print)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A CHRISTMAS CAROL", "The Mystery of Edwin Drood", "Great expectations.", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Compact English Classics)", "A Tale of Two Cities (Wordsworth Classics)", "A Tale of Two Cities (Dramatized)", "A Tale Of Two Cities (Adult Classics)", "Sketches by Boz", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Soundings)", "A Tale of Two Cities (Clear Print)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Acting Edition)", "A Christmas Carol (New Longman Literature)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Courage Literary Classics)", "A Tale of Two Cities (Penguin Popular Classics)", "Our mutual friend", "A Christmas Carol (Classic, Picture, Ladybird)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Gollancz Children's Classics)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Ladybird Classics)", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Classic Collection)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (R)", "A Christmas Carol (Pacemaker Classic)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Illustrated Classics)", "Dombey and Son", "A Christmas Carol (Saddleback Classics)", "Hard times", "The mystery of Edwin Drood", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Limited Editions)", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "Little Dorrit", "A Tale of Two Cities (Bantam Classic)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Illustrated Junior Library)", "David Copperfield.", "Bleak house", "A Tale of Two Cities (Cassette (1 Hr).)", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Soundings)", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Great Stories)", "The Pickwick Papers", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Cover to Cover)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "David Copperfield", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Christmas Carol (Children's Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Nelson Graded Readers)", "A Tale of Two Cities (Paperback Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Illustrated Classics)", "A Christmas Carol (Puffin Classics)", "A Christmas Carol (Audio Editions)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Children's Theatre Playscript)"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union\n# Answer:\nVyacheslav Molotov", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.04hzvwh\n# Answer:\ngovernment.governmental_jurisdiction.governing_officials", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.0c5f3d3 -> government.government_position_held.office_holder -> Vyacheslav Molotov\n# Answer:\nVyacheslav Molotov", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.0c5f1pl\n# Answer:\ngovernment.governmental_jurisdiction.governing_officials", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.0c5f3d3 -> government.government_position_held.basic_title -> Foreign Minister\n# Answer:\nForeign Minister", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> government.government_office_or_title.office_holders -> m.049x6zw\n# Answer:\ngovernment.government_office_or_title.office_holders", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> government.government_office_or_title.office_holders -> m.049x6zj\n# Answer:\ngovernment.government_office_or_title.office_holders", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> common.topic.image -> Joseph Stalin, first General Secretary\n# Answer:\nJoseph Stalin, first General Secretary", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.government_positions -> General Secretary of the Communist Party of the Soviet Union -> government.government_office_or_title.office_holders -> m.049x6z5\n# Answer:\ngovernment.government_office_or_title.office_holders"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.internet_users_percent_population -> m.0nf8lq8 -> measurement_unit.dated_percentage.source -> Internet users as percentage of population, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nInternet users as percentage of population, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nPuerto Rico -> location.statistical_region.internet_users_percent_population -> m.0nf8ln9 -> measurement_unit.dated_percentage.source -> Internet users as percentage of population, World Development Indicators and Global Development Finance, World Bank\n# Answer:\nInternet users as percentage of population, World Development Indicators and Global Development Finance, World Bank", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> New Progressive Party of Puerto Rico -> organization.organization.founders -> Luis A. Ferr\u00e9\n# Answer:\nNew Progressive Party of Puerto Rico", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> New Progressive Party of Puerto Rico -> common.topic.webpage -> m.04yvx62\n# Answer:\ncommon.topic.webpage", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> New Progressive Party of Puerto Rico -> common.topic.notable_for -> g.125gbg_vw\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nPuerto Rico -> organization.organization_scope.organizations_with_this_scope -> New Progressive Party of Puerto Rico -> common.topic.webpage -> m.0h7tp58\n# Answer:\ncommon.topic.webpage"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> music.album.genre -> Rock music\n# Answer:\nYoungblood", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> common.topic.notable_for -> g.1258tnqny\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> music.album.genre -> Rock and roll\n# Answer:\nYoungblood", "# Reasoning Path:\nCarl Wilson -> music.artist.album -> Youngblood -> music.album.genre -> Pop music\n# Answer:\nYoungblood"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.42857142857142855, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.2857142857142857, "path_recall": 1.0, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.2857142857142857, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nDavid Hasselhoff", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.character -> Michael Knight\n# Answer:\nMichael Knight", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.characters -> KITT -> tv.tv_character.appeared_in_tv_program -> m.03lj4m5\n# Answer:\nKITT", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d1z -> tv.regular_tv_appearance.actor -> Rebecca Holden\n# Answer:\nRebecca Holden", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.characters -> KITT -> film.film_character.portrayed_in_films -> m.0j7ph8m\n# Answer:\nfilm.film_character.portrayed_in_films", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nKnight Rider - Season 4", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.character -> KITT\n# Answer:\nKITT", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.0kv9d1z -> tv.regular_tv_appearance.character -> April Curtis\n# Answer:\nApril Curtis", "# Reasoning Path:\nKnight Rider\n# Answer:\nDavid Hasselhoff"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> m.0k6l5d0 -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> m.0hmbm_p -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> m.0k6l5ct -> measurement_unit.dated_integer.source -> United States Census Bureau, Population\n# Answer:\nUnited States Census Bureau, Population"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver\n# Answer:\nMissouri", "# Reasoning Path:\nGeorge Washington Carver -> people.person.places_lived -> m.03ppx0s\n# Answer:\npeople.person.places_lived", "# Reasoning Path:\nGeorge Washington Carver -> people.person.places_lived -> m.03prs0h\n# Answer:\npeople.person.places_lived", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> George Washington Carver National Monument -> location.location.containedby -> Missouri\n# Answer:\nMissouri", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> astronomy.extraterrestrial_location.on_celestial_object -> Moon\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> George Washington Carver National Monument -> location.location.containedby -> Newton County\n# Answer:\nNewton County", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> George Washington Carver National Monument -> base.usnris.nris_listing.significance_level -> National\n# Answer:\nGeorge Washington Carver National Monument"], "ground_truth": ["Diamond"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q2m7 -> base.popstra.dated.participant -> Sarah Jessica Parker\n# Answer:\nSarah Jessica Parker", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nArlington", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nTo", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.ceremony -> 52nd Primetime Emmy Awards\n# Answer:\n52nd Primetime Emmy Awards", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtxt -> award.award_nomination.nominated_for -> Spin City\n# Answer:\nSpin City", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.ceremony -> 51st Primetime Emmy Awards\n# Answer:\n51st Primetime Emmy Awards", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtxt -> award.award_nomination.ceremony -> 49th Primetime Emmy Awards\n# Answer:\n49th Primetime Emmy Awards"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.11111111111111112, "path_precision": 0.1, "path_recall": 0.125, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.combatants -> m.04fv9n6\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.combatants -> m.04fv9n1\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.military_personnel_involved -> Jubal Early\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> First Battle of Bull Run -> military.military_conflict.military_personnel_involved -> Jubal Early\n# Answer:\nFirst Battle of Bull Run", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.combatants -> m.04fv9mx\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.commanders -> m.04fv9mr\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War -> military.military_conflict.combatants -> m.03z98d_\n# Answer:\nmilitary.military_conflict.combatants", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> military.military_conflict.military_personnel_involved -> William E. Starke\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> First Battle of Bull Run -> military.military_conflict.military_personnel_involved -> Joseph E. Johnston\n# Answer:\nFirst Battle of Bull Run", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> First Battle of Bull Run -> military.military_conflict.combatants -> m.04fvc8s\n# Answer:\nmilitary.military_conflict.combatants"], "ground_truth": ["First Battle of Winchester", "Romney Expedition", "Battle of Harpers Ferry", "Battle of McDowell", "Battle of White Oak Swamp", "Battle of Front Royal", "How Few Remain", "Manassas Station Operations", "Battle of Port Republic", "First Battle of Rappahannock Station", "Second Battle of Bull Run", "Jackson's Valley Campaign", "Battle of Cedar Mountain", "Battle of Hancock", "First Battle of Kernstown", "Battle of Chantilly", "Battle of Hoke's Run", "American Civil War", "Battle of Chancellorsville"], "ans_acc": 0.10526315789473684, "ans_hit": 1, "ans_f1": 0.09677419354838708, "ans_precission": 0.6, "ans_recall": 0.05263157894736842, "path_f1": 0.13397129186602869, "path_precision": 0.7, "path_recall": 0.07407407407407407, "path_ans_f1": 0.1830065359477124, "path_ans_precision": 0.7, "path_ans_recall": 0.10526315789473684}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai -> common.image.size -> m.02bgrp_\n# Answer:\nMaasai", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai women and children -> common.image.size -> m.03qy0z9\n# Answer:\nMaasai women and children", "# Reasoning Path:\nMaasai people -> common.topic.image -> Maasai-jump -> common.image.size -> m.02br_p7\n# Answer:\nMaasai-jump"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.spouse -> Deborah Read\n# Answer:\nDeborah Read", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nCommon-law marriage", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nSpouse", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> American literature\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nFrom", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nTo", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> British American\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.size -> m.02bc9fn\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Deborah Read"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.14285714285714288, "path_precision": 0.1, "path_recall": 0.25, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\nPatrick Swayze 2006", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\nSwayze2"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4, "path_precision": 0.25, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> Godbey School of Art\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Mona Lisa -> exhibitions.exhibit.exhibitions_displayed_in -> Mona Lisa by Leonardo da Vinci\n# Answer:\nMona Lisa", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> common.topic.subject_of -> BRS Custom Painting\n# Answer:\nBRS Custom Painting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> common.topic.article -> m.0jnzf\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Annunciation -> visual_art.artwork.art_genre -> History painting\n# Answer:\nAnnunciation", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.media -> Tempera\n# Answer:\nThe Last Supper"], "ground_truth": ["The Battle of Anghiari", "St. Jerome in the Wilderness", "g.1213jb_b", "Leonardo's horse", "g.12314dm1", "Bacchus", "Medusa", "Benois Madonna", "g.121wt37c", "Madonna of Laroque", "Adoration of the Magi", "La belle ferronni\u00e8re", "g.120vt1gz", "g.1224tf0c", "Mona Lisa", "Annunciation", "Madonna of the Yarnwinder", "Leda and the Swan", "Vitruvian Man", "g.12215rxg", "Lucan portrait of Leonardo da Vinci", "Horse and Rider", "Ginevra de' Benci", "g.1239jd9p", "The Holy Infants Embracing", "Portrait of a man in red chalk", "St. John the Baptist", "Lady with an Ermine", "Madonna of the Carnation", "g.1219sb0g", "Madonna and Child with St Joseph", "Head of a Woman", "Drapery for a Seated Figure", "Portrait of a Young Fianc\u00e9e", "The Baptism of Christ", "Portrait of a Musician", "Portrait of Isabella d'Este", "The Last Supper", "Madonna Litta", "Salvator Mundi", "Virgin of the Rocks", "g.121yh91r", "The Virgin and Child with St Anne and St John the Baptist", "The Virgin and Child with St. Anne", "Sala delle Asse"], "ans_acc": 0.06666666666666667, "ans_hit": 1, "ans_f1": 0.1142857142857143, "ans_precission": 0.4, "ans_recall": 0.06666666666666667, "path_f1": 0.11650485436893203, "path_precision": 0.4, "path_recall": 0.06818181818181818, "path_ans_f1": 0.1142857142857143, "path_ans_precision": 0.4, "path_ans_recall": 0.06666666666666667}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_4ld_\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_rxgx\n# Answer:\nlocation.statistical_region.co2_emissions_per_capita"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.4, "path_ans_precision": 0.25, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> people.cause_of_death.parent_cause_of_death -> Assassination in ways which appear natural\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> In My Own Words -> common.topic.notable_for -> g.1jmcbzgs1\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Deborah King\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.profession -> Politician -> base.descriptive_names.names.descriptive_name -> m.0105znh0\n# Answer:\nbase.descriptive_names.names.descriptive_name", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> La razo\u0301n de mi vida y otros escritos -> common.topic.notable_for -> g.1jmcbynqp\n# Answer:\nLa razo\u0301n de mi vida y otros escritos", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Peter McWilliams\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> book.author.works_written -> In My Own Words -> common.topic.notable_types -> Book\n# Answer:\nIn My Own Words", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.disease.notable_people_with_this_condition -> Karen Hardy\n# Answer:\nCancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Sakya Muni Buddha Gaya Temple -> location.location.containedby -> Singapore\n# Answer:\nSakya Muni Buddha Gaya Temple", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Sakya Muni Buddha Gaya Temple -> common.topic.article -> m.03q772\n# Answer:\nSakya Muni Buddha Gaya Temple"], "ground_truth": ["Nepal"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.notable_types -> Invention\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> law.invention.inventor -> Prokop Divi\u0161\n# Answer:\nProkop Divi\u0161", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.notable_for -> g.125fblhtl\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> common.topic.notable_for -> g.12590cjb6\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod -> book.book_subject.works -> Stealing God's Thunder: Benjamin Franklin's Lightning Rod and the Invention of America\n# Answer:\nStealing God's Thunder: Benjamin Franklin's Lightning Rod and the Invention of America", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.notable_types -> Product category\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nIndependent", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.notable_for -> g.1259st24m\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis"], "ground_truth": ["Lightning rod", "Franklin stove", "Glass harmonica", "Bifocals"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> Colorado\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> location.location.containedby -> United States of America\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> common.topic.article -> m.01_k7l\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> education.educational_institution.sports_teams -> Northern Colorado Bears football\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> common.topic.article -> m.0qfltq5\n# Answer:\nUniversity of Northern Colorado", "# Reasoning Path:\nGreeley -> location.location.contains -> University of Northern Colorado -> education.educational_institution.sports_teams -> Northern Colorado Bears men's basketball\n# Answer:\nUniversity of Northern Colorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_for -> g.1259l_93p\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.artist.contribution -> m.0yqpt3d -> music.recording_contribution.album -> Anna Karenina\n# Answer:\nAnna Karenina", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Eugene Onegin -> music.composition.includes -> Kuda, kuda v\u00ef udalilis\n# Answer:\nEugene Onegin", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Eugene Onegin -> book.written_work.author -> Aleksandr Pushkin\n# Answer:\nEugene Onegin", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.image -> Pietro Metastasio\n# Answer:\nLibrettist"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.country.languages_spoken -> Bavarian Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f -> education.education.major_field_of_study -> Philosophy\n# Answer:\nPhilosophy", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 1896 Summer Olympics\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGerman Language\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> location.country.official_language -> French\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> education.field_of_study.students_majoring -> m.0127rg0f -> education.education.student -> Lowell Vizenor\n# Answer:\nLowell Vizenor", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics\n# Answer:\nAustria"], "ground_truth": ["Switzerland", "Germany", "Liechtenstein", "Austria", "East Germany", "Luxembourg", "Belgium"], "ans_acc": 0.2857142857142857, "ans_hit": 1, "ans_f1": 0.40579710144927533, "ans_precission": 0.7, "ans_recall": 0.2857142857142857, "path_f1": 0.3870967741935483, "path_precision": 0.6, "path_recall": 0.2857142857142857, "path_ans_f1": 0.3870967741935483, "path_ans_precision": 0.6, "path_ans_recall": 0.2857142857142857}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> music.genre.parent_genre -> Folk music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music -> common.topic.notable_types -> Musical genre\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Rock music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> people.person.profession -> Musician -> people.profession.specializations -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock -> music.genre.subgenre -> Post-rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> music.group_member.membership -> m.01tqrc7 -> music.group_membership.role -> Vocals\n# Answer:\nVocals", "# Reasoning Path:\nJohn Lennon -> people.person.profession -> Singer-songwriter\n# Answer:\nSinger-songwriter", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> music.genre.parent_genre -> Rock and roll\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music -> common.topic.notable_types -> Musical genre\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> people.person.profession -> Musician -> people.profession.specializations -> Singer-songwriter\n# Answer:\nSinger-songwriter"], "ground_truth": ["Soft rock", "Pop music", "Art rock", "Pop rock", "Experimental rock", "Psychedelic rock", "Rock music", "Blues rock", "Experimental music"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.42857142857142855, "ans_precission": 0.6, "ans_recall": 0.3333333333333333, "path_f1": 0.42857142857142855, "path_precision": 0.6, "path_recall": 0.3333333333333333, "path_ans_f1": 0.42857142857142855, "path_ans_precision": 0.6, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nWayne Allard", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpc0p -> government.government_position_held.office_holder -> Floyd K. Haskell\n# Answer:\nFloyd K. Haskell", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpg7j -> government.government_position_held.office_holder -> Thomas M. Bowen\n# Answer:\nThomas M. Bowen", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.09qpg7j -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nUnited States Senator", "# Reasoning Path:\nColorado -> location.location.partially_contains -> San Luis Valley -> location.location.contains -> Great Sand Dunes National Park and Preserve\n# Answer:\nSan Luis Valley", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 3rd Regiment Colorado Volunteer Cavalry -> military.military_unit.armed_force -> Union Army\n# Answer:\n3rd Regiment Colorado Volunteer Cavalry", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> military.military_unit.armed_force -> Union Army\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> location.location.partially_contains -> San Luis Valley -> location.location.contains -> Zapata Falls\n# Answer:\nSan Luis Valley", "# Reasoning Path:\nColorado -> location.location.partially_contains -> San Luis Valley -> location.location.partially_contained_by -> m.0wg938l\n# Answer:\nlocation.location.partially_contained_by"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98104\n# Answer:\n98104", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98121\n# Answer:\n98121", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Ballard -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Capitol Hill -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nSeattle-Tacoma-Bellevue, WA Metropolitan Statistical Area", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11x1f57h0\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Ballard -> location.location.containedby -> King County\n# Answer:\nBallard", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11x1h8nw8\n# Answer:\nlocation.statistical_region.population", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Ballard -> common.topic.notable_types -> Neighborhood\n# Answer:\nBallard"], "ground_truth": ["98125", "98104", "98194", "98181", "98113", "98122", "98108", "98198", "98190", "98127", "98146", "98103", "98141", "98132", "98199", "98129", "98105", "98170", "98116", "98161", "98115", "98185", "98121", "98158", "98111", "98138", "98171", "98102", "98136", "98134", "98175", "98119", "98145", "98178", "98177", "98118", "98166", "98124", "98117", "98188", "98114", "98101", "98119-4114", "98144", "98109", "98154", "98164", "98126", "98195", "98139", "98148", "98168", "98133", "98174", "98165", "98106", "98112", "98107", "98191", "98160", "98155", "98184", "98131"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.0821917808219178, "ans_precission": 0.3, "ans_recall": 0.047619047619047616, "path_f1": 0.0821917808219178, "path_precision": 0.3, "path_recall": 0.047619047619047616, "path_ans_f1": 0.0821917808219178, "path_ans_precision": 0.3, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nCaroline Bright", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Willard Christopher Smith, Sr.\n# Answer:\nWillard Christopher Smith, Sr.", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.spouse_s -> m.04j69gn\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Hip hop music -> music.genre.parent_genre -> East Coast hip hop\n# Answer:\nHip hop music", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.sibling_s -> m.04j69lq\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> common.topic.webpage -> m.0h24fj5 -> common.webpage.resource -> Official Website\n# Answer:\nOfficial Website", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.spouse_s -> m.02kp6m3\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Hip hop music -> music.genre.subgenre -> East Coast hip hop\n# Answer:\nHip hop music", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> base.schemastaging.music_genre_concept.artists -> Yves Bole\n# Answer:\nYves Bole"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese characters\n# Answer:\nTraditional Chinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Simplified Chinese character\n# Answer:\nSimplified Chinese character", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> China\n# Answer:\nChina", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02t0c71\n# Answer:\neducation.field_of_study.students_majoring", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Taiwan\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.containedby -> Asia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> education.field_of_study.students_majoring -> m.02wn10k\n# Answer:\neducation.field_of_study.students_majoring", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> location.location.contains -> Mongolia\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> common.topic.article -> m.02qnv\n# Answer:\ncommon.topic.article"], "ground_truth": ["Chinese characters", "'Phags-pa script", "Simplified Chinese character", "Traditional Chinese characters", "N\u00fcshu script"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.3, "ans_recall": 0.6, "path_f1": 0.4, "path_precision": 0.3, "path_recall": 0.6, "path_ans_f1": 0.4, "path_ans_precision": 0.3, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.02h98gq\n# Answer:\npeople.person.spouse_s", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nfilm.person_or_entity_appearing_in_film.films", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb\n# Answer:\ntv.tv_actor.guest_roles", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0bvqgc7\n# Answer:\nfilm.person_or_entity_appearing_in_film.films", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh\n# Answer:\ntv.tv_actor.guest_roles", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k\n# Answer:\nfilm.person_or_entity_appearing_in_film.films", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nThe Future of the GOP"], "ground_truth": ["Pat Nixon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf8 -> tv.regular_tv_appearance.actor -> Isabel Sanford\n# Answer:\nIsabel Sanford", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf8 -> tv.regular_tv_appearance.character -> Louise Jefferson\n# Answer:\nLouise Jefferson", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf3 -> tv.regular_tv_appearance.actor -> Sherman Hemsley\n# Answer:\nSherman Hemsley", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> Lionel Cries Uncle\n# Answer:\nLionel Cries Uncle", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdn -> tv.regular_tv_appearance.actor -> Franklin Cover\n# Answer:\nFranklin Cover", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkf3 -> tv.regular_tv_appearance.character -> George Jefferson\n# Answer:\nGeorge Jefferson", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> Lionel the Playboy\n# Answer:\nLionel the Playboy", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> tv.tv_series_season.episodes -> A Friend in Need\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 1 -> common.topic.notable_for -> g.1259dkfrc\n# Answer:\nThe Jeffersons - Season 1", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 5 -> tv.tv_series_season.episodes -> George Finds a Father\n# Answer:\nGeorge Finds a Father"], "ground_truth": ["Mike Evans", "Jay Hammer", "Isabel Sanford", "Damon Evans", "Roxie Roker", "Marla Gibbs", "Berlinda Tolbert", "Franklin Cover", "Paul Benedict", "Sherman Hemsley", "Zara Cully"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0.28571428571428564, "ans_precission": 0.3, "ans_recall": 0.2727272727272727, "path_f1": 0.08695652173913043, "path_precision": 0.3, "path_recall": 0.05084745762711865, "path_ans_f1": 0.28571428571428564, "path_ans_precision": 0.3, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco\n# Answer:\nSan Francisco Chronicle", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Chronicle\n# Answer:\nSan Francisco Chronicle", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Daily Alta California\n# Answer:\nThe Daily Alta California", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> Herb Caen's San Francisco: 1976-1991\n# Answer:\nHerb Caen's San Francisco: 1976-1991", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek -> book.newspaper.owner -> Pan Asia Venture Capital Corporation\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek -> common.topic.notable_types -> Newspaper\n# Answer:\nAsianWeek"], "ground_truth": ["Free Society", "Dock of the Bay", "AsianWeek", "San Francisco Call", "San Francisco Foghorn", "San Francisco Bay Times", "Bay Area Reporter", "Sing Tao Daily", "The Golden Era", "San Francisco Daily", "San Francisco Chronicle", "The Daily Alta California", "San Francisco Business Times", "California Star", "Synapse", "San Francisco Bay Guardian", "Street Sheet", "The San Francisco Examiner", "San Francisco Bay View", "San Francisco News-Call Bulletin Newspaper"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.2542372881355932, "ans_precission": 0.8333333333333334, "ans_recall": 0.15, "path_f1": 0.24489795918367346, "path_precision": 0.6666666666666666, "path_recall": 0.15, "path_ans_f1": 0.24489795918367346, "path_ans_precision": 0.6666666666666666, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fsr3\n# Answer:\nlocation.statistical_region.trade_balance_as_percent_of_gdp"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8, "ans_precission": 0.6666666666666666, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.3333333333333333, "path_recall": 1.0, "path_ans_f1": 0.5, "path_ans_precision": 0.3333333333333333, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Alcohol abuse\n# Answer:\nAlcohol abuse", "# Reasoning Path:\nRandy Savage -> people.deceased_person.place_of_death -> Seminole -> common.topic.notable_for -> g.1255tk51t\n# Answer:\nSeminole", "# Reasoning Path:\nRandy Savage -> people.deceased_person.place_of_death -> Seminole -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nSeminole", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Kidney Stone\n# Answer:\nKidney Stone", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\ncommon.topic.notable_for", "# Reasoning Path:\nRandy Savage -> people.deceased_person.place_of_death -> Seminole -> location.statistical_region.population -> m.0k6nnn6\n# Answer:\nSeminole", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nKidney cancer", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> base.schemastaging.context_name.pronunciation -> m.011sf4j6\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.place_of_death -> Seminole -> location.statistical_region.population -> g.11x1h5tsg\n# Answer:\nSeminole"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin's big book -> book.written_work.author -> Stephen Jay Gould\n# Answer:\nDarwin's big book", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> common.topic.subject_of -> Science\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Natural history -> common.topic.notable_types -> Literature Subject\n# Answer:\nNatural history", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin's big book -> book.written_work.subjects -> Evolution\n# Answer:\nDarwin's big book", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland"], "ground_truth": ["Notebooks on transmutation of species", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The Correspondence of Charles Darwin, Volume 11: 1863", "The Variation of Animals and Plants under Domestication", "vari\u00eberen der huisdieren en cultuurplanten", "The Essential Darwin", "Rejse om jorden", "ontstaan der soorten door natuurlijke teeltkeus", "Les mouvements et les habitudes des plantes grimpantes", "Del Plata a Tierra del Fuego", "The Correspondence of Charles Darwin, Volume 9: 1861", "genese\u014ds t\u014dn eid\u014dn", "The collected papers of Charles Darwin", "On evolution", "Darwin's notebooks on transmutation of species", "The Structure and Distribution of Coral Reefs", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "Metaphysics, Materialism, & the evolution of mind", "Geological Observations on South America", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "Wu zhong qi yuan", "Tesakneri tsagume\u030c", "H.M.S. Beagle in South America", "Volcanic Islands", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "More Letters of Charles Darwin", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 17: 1869", "Darwin from Insectivorous Plants to Worms", "Reise um die Welt 1831 - 36", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Darwin en Patagonia", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "The Formation of Vegetable Mould through the Action of Worms", "The voyage of Charles Darwin", "To the members of the Down Friendly Club", "Charles Darwin's letters", "Darwin's insects", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Diary of the voyage of H.M.S. Beagle", "Gesammelte kleinere Schriften", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The action of carbonate of ammonia on the roots of certain plants", "Part I: Contributions to the Theory of Natural Selection / Part II", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Origins", "Het uitdrukken van emoties bij mens en dier", "Charles Darwin's marginalia", "Notes on the fertilization of orchids", "Reise eines Naturforschers um die Welt", "On Natural Selection", "Resa kring jorden", "Memorias y epistolario i\u0301ntimo", "The Voyage of the Beagle", "On a remarkable bar of sandstone off Pernambuco", "Darwin for Today", "Charles Darwin", "Diario del Viaje de Un Naturalista Alrededor", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Darwin", "The living thoughts of Darwin", "Die fundamente zur entstehung der arten", "The Correspondence of Charles Darwin, Volume 8: 1860", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Darwin and Henslow", "La vie et la correspondance de Charles Darwin", "red notebook of Charles Darwin", "La facult\u00e9 motrice dans les plantes", "The Expression of the Emotions in Man and Animals", "Leben und Briefe von Charles Darwin", "The Autobiography of Charles Darwin", "Charles Darwin on the routes of male humble bees", "Questions about the breeding of animals", "The portable Darwin", "Cartas de Darwin 18251859", "The Darwin Reader Second Edition", "South American Geology", "Evolution", "The Descent of Man, and Selection in Relation to Sex", "Evolution by natural selection", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Correspondence of Charles Darwin, Volume 18: 1870", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Les moyens d'expression chez les animaux", "Kleinere geologische Abhandlungen", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "Die geschlechtliche Zuchtwahl", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "Darwin Darwin", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Darwin-Wallace", "Insectivorous Plants", "On the Movements and Habits of Climbing Plants", "The Correspondence of Charles Darwin, Volume 10: 1862", "On the tendency of species to form varieties", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "From Darwin's unpublished notebooks", "The Life and Letters of Charles Darwin Volume 2", "The\u0301orie de l'e\u0301volution", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Different Forms of Flowers on Plants of the Same Species", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The education of Darwin", "Fertilisation of Orchids", "Darwin Compendium", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Charles Darwin's natural selection", "The Life of Erasmus Darwin", "The principal works", "The Correspondence of Charles Darwin, Volume 12: 1864", "Works", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Life and Letters of Charles Darwin Volume 1", "Evolution and natural selection", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Darwin Reader First Edition", "The geology of the voyage of H.M.S. Beagle", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Motsa ha-minim", "The Orgin of Species", "From so simple a beginning", "The Correspondence of Charles Darwin, Volume 14: 1866", "Darwin on humus and the earthworm", "Voyage d'un naturaliste autour du monde", "Darwin's Ornithological notes", "monograph on the sub-class Cirripedia", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "Evolutionary Writings: Including the Autobiographies", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Geological Observations on the Volcanic Islands", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "A student's introduction to Charles Darwin", "Beagle letters", "On the origin of species by means of natural selection", "The Correspondence of Charles Darwin, Volume 13: 1865", "El Origin De Las Especies", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Power of Movement in Plants", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The foundations of the Origin of species", "Proiskhozhdenie vidov", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Darwin's journal", "A Darwin Selection", "Darwinism stated by Darwin himself"], "ans_acc": 0.032679738562091505, "ans_hit": 1, "ans_f1": 0.02547770700636943, "ans_precission": 0.5, "ans_recall": 0.013071895424836602, "path_f1": 0.13793103448275862, "path_precision": 1.0, "path_recall": 0.07407407407407407, "path_ans_f1": 0.06329113924050632, "path_ans_precision": 1.0, "path_ans_recall": 0.032679738562091505}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15\n# Answer:\nfilm.person_or_entity_appearing_in_film.films", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.0gmhxmh -> tv.tv_guest_role.episodes_appeared_in -> The Impossible Astronaut\n# Answer:\nThe Impossible Astronaut", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0w2rt04 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nArchive Footage", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0w2rt04 -> film.personal_film_appearance.film -> The Hoax\n# Answer:\nThe Hoax", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglt -> tv.tv_guest_role.episodes_appeared_in -> Episode #30\n# Answer:\nEpisode #30", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglh -> tv.tv_guest_role.episodes_appeared_in -> Johnny Grant\n# Answer:\nJohnny Grant"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4, "ans_precission": 0.25, "ans_recall": 1.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Girl -> music.composition.lyricist -> Ronald White\n# Answer:\nMy Girl", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Guy\n# Answer:\nMy Guy", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Girl -> music.composition.composer -> Ronald White\n# Answer:\nMy Girl", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> I Second That Emotion -> music.recording.artist -> Tammy Wynette\n# Answer:\nI Second That Emotion", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> I Second That Emotion -> music.composition.recordings -> I Second That Emotions\n# Answer:\nI Second That Emotion", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> My Girl -> common.topic.notable_for -> g.125gs6kmp\n# Answer:\nMy Girl", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> I Second That Emotion -> music.recording.artist -> The Manhattan Transfer\n# Answer:\nI Second That Emotion", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Who's Lovin' You -> music.recording.artist -> The Jackson 5\n# Answer:\nWho's Lovin' You", "# Reasoning Path:\nSmokey Robinson -> music.lyricist.lyrics_written -> Who's Lovin' You -> music.composition.recordings -> Who's Lovin' You (live)\n# Answer:\nWho's Lovin' You", "# Reasoning Path:\nSmokey Robinson -> music.featured_artist.recordings -> The Tracks of My Tears -> music.recording.artist -> Stevie Wonder\n# Answer:\nThe Tracks of My Tears"], "ground_truth": ["Blame It On Love (Duet with Barbara Mitchell)", "I Hear The Children Singing", "Hold on to Your Love", "You're Just My Life (feat. India.Arie)", "It's Christmas Time", "Quiet Storm (Groove Boutique Chill Jazz mix)", "I Am, I Am", "Be Careful What You Wish For", "Double Good Everything", "Save Me", "Going to a Go-Go", "The Love Between Me and My Kids", "You Go to My Head", "Baby Come Close", "And I Don't Love You (Larry Levan instrumental dub)", "A Silent Partner in a Three-Way Love Affair", "I've Made Love To You A Thousand Times", "I Like Your Face", "Let Me Be the Clock", "I Can\u2019t Stand to See You Cry (Commercial version)", "Love Is The Light", "Open", "Christmas Greeting", "Keep Me", "Yes It's You Lady", "Away in the Manger / Coventry Carol", "Wishful Thinking", "Fly Me to the Moon (In Other Words)", "Ebony Eyes (Duet with Rick James)", "Pops, We Love You (disco)", "Why", "The Tracks Of My Tears", "Gang Bangin'", "Tracks of My Tears", "I Can't Find", "Girl I'm Standing There", "Will You Still Love Me Tomorrow", "With Your Love Came", "You Take Me Away", "Shop Around", "Let Your Light Shine On Me", "Santa Claus is Coming to Town", "A Child Is Waiting", "Sleepless Nights", "Tell Me Tomorrow (12\\\" extended mix)", "Mickey's Monkey", "Gone Forever", "The Agony and the Ecstasy", "Ebony Eyes", "You Cannot Laugh Alone", "Come by Here (Kum Ba Ya)", "Fulfill Your Need", "The Agony And The Ecstasy", "Driving Thru Life in the Fast Lane", "Shoe Soul", "Holly", "Don't Play Another Love Song", "Love So Fine", "Walk on By", "Coincidentally", "(It's The) Same Old Love", "Little Girl Little Girl", "More Than You Know", "I Praise & Worship You Father", "Share It", "She's Only a Baby Herself", "Some People Will Do Anything for Love", "What's Too Much", "It's Fantastic", "Don't Wanna Be Just Physical", "Going to a Go Go", "Quiet Storm (single version)", "The Tears Of A Clown", "Why Do Happy Memories Hurt So Bad", "More Love", "Tracks Of My Tears (Live)", "The Christmas Song", "Tracks of my Tears", "I Am I Am", "The Family Song", "Be Who You Are", "The Tracks of My Tears (live)", "If You Wanna Make Love", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "You Made Me Feel Love", "In My Corner", "Can't Fight Love", "Who's Sad", "Our Love Is Here to Stay", "I Can't Give You Anything but Love", "You've Really Go a Hold on Me", "You Are Forever", "You Really Got a Hold on Me", "Going to a Gogo", "Cruisin", "Yester Love", "I Second That Emotion", "Tea for Two", "Night and Day", "Really Gonna Miss You", "Don't Know Why", "Season's Greetings from Smokey Robinson", "Hanging on by a Thread", "And I Don't Love You", "Crusin", "Be Careful What You Wish For (instrumental)", "I Love Your Face", "Christmas Everyday", "Please Come Home for Christmas", "I Second That Emotions", "I'm in the Mood for Love", "If You Wanna Make Love (Come 'round Here)", "Medley: Never My Love / Never Can Say Goodbye", "Just My Soul Responding", "That Place", "You've Really Got a Hold on Me", "Skid Row", "Food For Thought", "My World", "Baby That's Backatcha", "Easy", "Blame It on Love", "Ooo Baby Baby", "Speak Low", "Satisfy You", "Tell Me Tomorrow", "Love Don' Give No Reason (12 Inch Club Mix)", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "I Know You by Heart", "Ooh Baby Baby", "Nearness of You", "Love' n Life", "Take Me Through The Night", "Just Another Kiss", "One Time", "I\u2019ve Got You Under My Skin", "Whatcha Gonna Do", "We've Saved the Best for Last", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "You Don't Know What It's Like", "I've Made Love to You a Thousand Times", "Jasmin", "Aqui Con Tigo (Being With You)", "We Are The Warriors", "Te Quiero Como Si No Hubiera Un Manana", "Pops, We Love You", "It's a Good Feeling", "My Girl", "Just a Touch Away", "You're the One for Me (feat. Joss Stone)", "I Have Prayed On It", "Will You Love Me Tomorrow", "When A Woman Cries", "So Bad", "As You Do", "Unless You Do It Again", "Vitamin U", "Love Brought Us Here", "Everything You Touch", "Time After Time", "I Can't Get Enough", "Tears of a Clown", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Ain't That Peculiar", "No\u00ebl", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Asleep on My Love", "Melody Man", "Just Like You", "I'm Glad There Is You", "Bad Girl", "Just To See Her Again", "Wanna Know My Mind", "Love Don't Give No Reason", "Just to See Her", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "The Way You Do (The Things You Do)", "Standing On Jesus", "It's A Good Night", "Be Kind To The Growing Mind (with The Temptations)", "Jingle Bells", "He Can Fix Anything", "Quiet Storm (Groove Boutique remix)", "Fallin'", "The Tears of a Clown", "Girlfriend", "Winter Wonderland", "Daylight & Darkness", "It's Time to Stop Shoppin' Around", "You Are So Beautiful (feat. Dave Koz)", "Noel", "The Track of My Tears", "And I Love Her", "If You Can Want", "There Will Come A Day ( I'm Gonna Happen To You )", "Photograph in My Mind", "Cruisin'", "Wedding Song", "A Tattoo", "My Guy", "Love Bath", "Sweet Harmony", "I've Got You Under My Skin", "Please Don't Take Your Love (feat. Carlos Santana)", "Mother's Son", "Crusin'", "Happy (Love Theme From Lady Sings the Blues)", "I Love The Nearness Of You", "Little Girl, Little Girl", "Just Passing Through", "Virgin Man", "Will You Love Me Tomorrow?", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Never My Love / Never Can Say Goodbye", "I Care About Detroit", "Tell Me Tomorrow, Part 1", "Jesus Told Me To Love You", "Everything for Christmas", "One Heartbeat", "Theme From the Big Time", "Tears of a Sweet Free Clown", "We\u2019ve Come Too Far to End It Now", "The Hurt's On You", "It's Her Turn to Live", "Deck the Halls", "Heavy On Pride (Light On Love)", "Ever Had A Dream", "Quiet Storm", "No Time to Stop Believing", "Rewind", "Because of You It's the Best It's Ever Been", "Tears Of A Clown", "Be Kind to the Growing Mind", "Close Encounters of the First Kind", "Come to Me Soon", "Love Letters", "Time Flies", "Rack Me Back", "The Tracks of My Tears", "Ooo Baby Baby (live)", "I'll Keep My Light In My Window", "Train of Thought", "I Want You Back", "Same Old Love", "There Will Come a Day (I'm Gonna Happen to You)", "Did You Know (Berry's Theme)", "If You Want My Love", "God Rest Ye Merry Gentlemen", "Let Me Be The Clock", "Being With You", "The Road to Damascus", "Christmas Every Day", "Get Ready", "The Tracks of My Heart", "When Smokey Sings Tears Of A Clown", "Why Are You Running From My Love"], "ans_acc": 0.03125, "ans_hit": 1, "ans_f1": 0.05288007554296506, "ans_precission": 0.8, "ans_recall": 0.02734375, "path_f1": 0.33291298865069363, "path_precision": 0.8, "path_recall": 0.21019108280254778, "path_ans_f1": 0.06015037593984962, "path_ans_precision": 0.8, "path_ans_recall": 0.03125}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> government.political_party_tenure.party -> Democratic Party\n# Answer:\nDemocratic Party", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> freebase.valuenotation.is_reviewed -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nJohn F. Kennedy -> government.politician.party -> m.03gjk7n -> freebase.valuenotation.is_reviewed -> Party\n# Answer:\nParty"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.04h_ngf -> location.adjoining_relationship.adjoins -> Republic of Kosovo\n# Answer:\nRepublic of Kosovo", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.046cspc -> location.adjoining_relationship.adjoins -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.05g_617 -> location.adjoining_relationship.adjoins -> Republic of Macedonia\n# Answer:\nRepublic of Macedonia"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
