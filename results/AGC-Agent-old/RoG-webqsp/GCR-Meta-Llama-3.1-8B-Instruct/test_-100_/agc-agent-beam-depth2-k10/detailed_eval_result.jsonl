{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.countries_spoken_in -> Costa Rica\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language -> language.human_language.region -> Americas\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.article -> m.014294\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English -> common.topic.notable_types -> Human Language\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Florida\n# Answer:\n1812 Louisiana hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1900 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Haiti\n# Answer:\n1900 Galveston hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1915 Galveston hurricane -> meteorology.tropical_cyclone.affected_areas -> Haiti\n# Answer:\n1915 Galveston hurricane", "# Reasoning Path:\nJamaica -> meteorology.cyclone_affected_area.cyclones -> 1812 Louisiana hurricane -> meteorology.tropical_cyclone.affected_areas -> Louisiana\n# Answer:\n1812 Louisiana hurricane"], "ground_truth": ["Jamaican Creole English Language", "Jamaican English"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> common.image.appears_in_topic_gallery -> United States presidential election, 1844\n# Answer:\nUnited States presidential election, 1844", "# Reasoning Path:\nJames K. Polk -> people.person.places_lived -> m.03phtbg -> people.place_lived.location -> North Carolina\n# Answer:\nm.03phtbg", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> After some skirmishes, the Americans provoked the Mexicans to retaliate at Matamoras. -> base.kwebbase.kwsentence.previous_sentence -> Mexico had always refused to recognize Texan independence and thus had no diplomatic relations with the U.S.  Polk sent troops to the Texas-Mexico border, which he claimed was along the Rio Grande, a hundred miles further into Mexico than Mexico had ever allowed.\n# Answer:\nAfter some skirmishes, the Americans provoked the Mexicans to retaliate at Matamoras.", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> After some skirmishes, the Americans provoked the Mexicans to retaliate at Matamoras. -> base.kwebbase.kwsentence.next_sentence -> When the first American soldiers were killed, Polk got Congressional support for declaring war.\n# Answer:\nAfter some skirmishes, the Americans provoked the Mexicans to retaliate at Matamoras.", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> America's success owed much to his vigor and unflagging attention to detail. -> base.kwebbase.kwsentence.previous_sentence -> When the first American soldiers were killed, Polk got Congressional support for declaring war.\n# Answer:\nAmerica's success owed much to his vigor and unflagging attention to detail.", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> And he believed that state powers should hold sway and that the powers of the President and Congress should be limited. -> base.kwebbase.kwsentence.previous_sentence -> Like Jackson, Polk believed there was no need for a national bank.\n# Answer:\nAnd he believed that state powers should hold sway and that the powers of the President and Congress should be limited.", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> America's success owed much to his vigor and unflagging attention to detail. -> base.kwebbase.kwsentence.next_sentence -> The final treaty gave the United States all the  Mexican territory Polk wanted, including California.\n# Answer:\nAmerica's success owed much to his vigor and unflagging attention to detail.", "# Reasoning Path:\nJames K. Polk -> base.kwebbase.kwtopic.has_sentences -> And he believed that state powers should hold sway and that the powers of the President and Congress should be limited. -> base.kwebbase.kwsentence.next_sentence -> In 1835, Jackson rewarded Polk by making him Speaker of the House.\n# Answer:\nAnd he believed that state powers should hold sway and that the powers of the President and Congress should be limited."], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Cuba\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.main_country -> Puerto Rico\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.countries_spoken_in -> Turks and Caicos Islands\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole -> language.human_language.region -> Americas\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French -> language.human_language.countries_spoken_in -> France\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole -> language.human_language.countries_spoken_in -> Dominican Republic\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.statistical_region.energy_use_per_capita -> g.1245_1j97\n# Answer:\ng.1245_1j97"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t8hv2 -> tv.regular_tv_appearance.actor -> Yvonne Craig\n# Answer:\nm.02t8hv2", "# Reasoning Path:\nBarbara Gordon -> common.topic.notable_for -> g.12596ymdk\n# Answer:\ng.12596ymdk", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> common.topic.notable_types -> Profession\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.03ljbs2 -> tv.regular_tv_appearance.actor -> Angie Harmon\n# Answer:\nm.03ljbs2", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.actor -> Melissa Gilbert\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Lawyer -> common.topic.notable_types -> Profession\n# Answer:\nLawyer", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> film.film_character.portrayed_in_films -> m.012hbb3h\n# Answer:\nDetective", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.02t91b7 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.02t91b7", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nPolitician", "# Reasoning Path:\nBarbara Gordon -> fictional_universe.fictional_character.occupation -> Detective -> book.book_subject.works -> A Cold Christmas\n# Answer:\nDetective"], "ground_truth": ["Melinda McGraw", "Hannah Gunn", "Ilyssa Fradin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065p_r0 -> base.popstra.dated.participant -> Holly Robinson Peete\n# Answer:\nm.065p_r0", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz -> common.topic.notable_types -> Musical Recording\n# Answer:\n(I Know I Got) Skillz", "# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065py4q -> base.popstra.dated.participant -> Arnetta Yardbourgh\n# Answer:\nm.065py4q", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz -> common.topic.notable_for -> g.125h3k00m\n# Answer:\n(I Know I Got) Skillz", "# Reasoning Path:\nShaquille O'Neal -> base.popstra.celebrity.dated -> m.065pt05 -> base.popstra.dated.participant -> Inga Newson\n# Answer:\nm.065pt05", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz (feat. Def Jef) -> music.recording.canonical_version -> (I Know I Got) Skillz\n# Answer:\n(I Know I Got) Skillz (feat. Def Jef)", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (So U Wanna Be) Hardcore -> common.topic.notable_types -> Musical Recording\n# Answer:\n(So U Wanna Be) Hardcore", "# Reasoning Path:\nShaquille O'Neal -> award.award_nominee.award_nominations -> m.08s6_hx -> award.award_nomination.ceremony -> 18th Golden Raspberry Awards\n# Answer:\nm.08s6_hx", "# Reasoning Path:\nShaquille O'Neal -> award.award_nominee.award_nominations -> m.0_81p9h -> award.award_nomination.award_nominee -> Adam Sandler\n# Answer:\nm.0_81p9h", "# Reasoning Path:\nShaquille O'Neal -> music.artist.track -> (I Know I Got) Skillz -> common.topic.notable_for -> g.1yg94jdvq\n# Answer:\n(I Know I Got) Skillz"], "ground_truth": ["LSU Tigers men's basketball", "Orlando Magic", "Cleveland Cavaliers", "Boston Celtics", "Los Angeles Lakers", "Miami Heat", "Phoenix Suns"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Italian American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nItalian American", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> David Brenner -> influence.influence_node.influenced -> David Letterman\n# Answer:\nDavid Brenner", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Alan King -> tv.tv_actor.guest_roles -> m.0bmlfv3\n# Answer:\nAlan King", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Bob Newhart -> influence.influence_node.influenced -> Conan O'Brien\n# Answer:\nBob Newhart", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.04fk_g9 -> award.award_honor.award -> Emmy Award for Outstanding Variety, Music or Comedy Series\n# Answer:\nm.04fk_g9", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Alan King -> tv.tv_actor.guest_roles -> m.0bmlfx4\n# Answer:\nAlan King", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Alan King -> film.actor.film -> m.010qyzmw\n# Answer:\nAlan King", "# Reasoning Path:\nJay Leno -> influence.influence_node.influenced_by -> Bob Newhart -> influence.influence_node.influenced -> David Steinberg\n# Answer:\nBob Newhart", "# Reasoning Path:\nJay Leno -> people.person.ethnicity -> Scottish American -> people.ethnicity.geographic_distribution -> United States of America\n# Answer:\nScottish American", "# Reasoning Path:\nJay Leno -> award.award_winner.awards_won -> m.0n2qyfk -> award.award_honor.award -> People's Choice Award for Favorite Late Night Talk Show Host\n# Answer:\nm.0n2qyfk"], "ground_truth": ["New Rochelle"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> common.topic.notable_types -> Country\n# Answer:\nCountry", "# Reasoning Path:\nThailand -> location.location.containedby -> Asia -> freebase.valuenotation.is_reviewed -> Time zone(s)\n# Answer:\nAsia", "# Reasoning Path:\nThailand -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nThailand -> location.location.containedby -> Southeast Asia -> organization.organization_scope.organizations_with_this_scope -> Asian Couture Federation\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6f6yg\n# Answer:\ng.12tb6f6yg", "# Reasoning Path:\nThailand -> location.location.containedby -> Asia -> base.locations.continents.countries_within -> Laos\n# Answer:\nAsia", "# Reasoning Path:\nThailand -> location.location.containedby -> Asia -> location.location.time_zones -> Thailand Time Zone\n# Answer:\nAsia", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.12tb6gh2z\n# Answer:\ng.12tb6gh2z", "# Reasoning Path:\nThailand -> location.location.containedby -> Southeast Asia -> location.location.contains -> Laos\n# Answer:\nSoutheast Asia", "# Reasoning Path:\nThailand -> location.statistical_region.gross_savings_as_percent_of_gdp -> g.1hhc37pvk\n# Answer:\ng.1hhc37pvk"], "ground_truth": ["Nyaw Language", "Cham language", "Mlabri Language", "Saek language", "Phu Thai language", "Malay, Pattani Language", "Vietnamese Language", "Lao Language", "Akha Language", "Mon Language", "Hmong language", "Khmer language", "Thai Language"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-106", "prediction": ["# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social networking service\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Austin Cameron\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> common.topic.notable_types -> Award-Winning Work\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.sibling_s -> m.0jsfpwr\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.subject_of -> Krishna Mali\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Social networking service -> common.topic.notable_types -> Website Category\n# Answer:\nSocial networking service", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Paul Scolnick\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Facebook -> internet.website.category -> Social Network (Consumer)\n# Answer:\nFacebook", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> influence.influence_node.influenced -> Remek Elek\n# Answer:\nMark Zuckerberg", "# Reasoning Path:\nThe Social Network -> film.film.subjects -> Mark Zuckerberg -> people.person.sibling_s -> m.0jsfpx1\n# Answer:\nMark Zuckerberg"], "ground_truth": ["Cameron Winklevoss", "Tyler Winklevoss"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> common.topic.notable_types -> Profession\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> fictional_universe.character_occupation.characters_with_this_occupation -> Indiana Jones\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> fictional_universe.fictional_character.children -> Patsy Jefferson\n# Answer:\nPatsy Jefferson", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist -> people.profession.specialization_of -> Scientist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> common.topic.notable_types -> Book -> freebase.type_hints.included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect -> common.topic.notable_types -> Profession\n# Answer:\nArchitect"], "ground_truth": ["Philosopher", "Inventor", "Archaeologist", "Lawyer", "Statesman", "Architect", "Teacher", "Author", "Farmer", "Writer"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.26666666666666666, "ans_precission": 0.4, "ans_recall": 0.2, "path_f1": 0.26666666666666666, "path_precision": 0.4, "path_recall": 0.2, "path_ans_f1": 0.26666666666666666, "path_ans_precision": 0.4, "path_ans_recall": 0.2}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_children -> Cambridgeshire\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> book.written_work.subjects -> Biology\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> B. F. Skinner -> influence.influence_node.influenced -> Willard Van Orman Quine\n# Answer:\nB. F. Skinner", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> education.field_of_study.subdisciplines -> Evolution\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.book_subject.works -> Darwin -> common.topic.notable_types -> Book\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.country -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> influence.influence_node.influenced -> B. F. Skinner -> influence.influence_node.influenced_by -> William James\n# Answer:\nB. F. Skinner"], "ground_truth": ["The Voyage of the Beagle (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 8: 1860", "Voyage of the Beagle (Dover Value Editions)", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 9: 1861", "Wu zhong qi yuan", "Leben und Briefe von Charles Darwin", "The Correspondence of Charles Darwin, Volume 10", "The Origin of Species (Oxford World's Classics)", "Gesammelte kleinere Schriften", "The Correspondence of Charles Darwin, Volume 3", "Cartas de Darwin 18251859", "On the Movements and Habits of Climbing Plants", "Motsa ha-minim", "The Autobiography of Charles Darwin, and selected letters", "The Autobiography of Charles Darwin", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 13: 1865", "The geology of the voyage of H.M.S. Beagle", "The\u0301orie de l'e\u0301volution", "From So Simple a Beginning", "Origin of Species (Harvard Classics, Part 11)", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The voyage of the Beagle.", "The descent of man, and selection in relation to sex.", "The Correspondence of Charles Darwin, Volume 15: 1867", "The action of carbonate of ammonia on the roots of certain plants", "The Different Forms of Flowers on Plants of the Same Species", "Voyage Of The Beagle", "Diary of the voyage of H.M.S. Beagle", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The Expression Of The Emotions In Man And Animals", "Opsht\u0323amung fun menshen", "The Correspondence of Charles Darwin, Volume 14", "The principal works", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The origin of species : complete and fully illustrated", "The Correspondence of Charles Darwin, Volume 12", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Proiskhozhdenie vidov", "The Origin of Species (Enriched Classics)", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "genese\u014ds t\u014dn eid\u014dn", "Voyage of the Beagle (Harvard Classics, Part 29)", "Darwinism stated by Darwin himself", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "Les r\u00e9cifs de corail, leur structure et leur distribution", "Die geschlechtliche Zuchtwahl", "The Autobiography of Charles Darwin [EasyRead Edition]", "The Voyage of the Beagle", "Les moyens d'expression chez les animaux", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Evolution and natural selection", "Tesakneri tsagume\u030c", "Memorias y epistolario i\u0301ntimo", "Origins", "Fertilisation of Orchids", "The structure and distribution of coral reefs.", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "A Darwin Selection", "Reise eines Naturforschers um die Welt", "The Origin of Species (Collector's Library)", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin's letters", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "Reise um die Welt 1831 - 36", "The Darwin Reader Second Edition", "The Correspondence of Charles Darwin, Volume 6", "The collected papers of Charles Darwin", "Darwin on humus and the earthworm", "The Origin of Species (Great Minds Series)", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "The Structure And Distribution of Coral Reefs", "vari\u00eberen der huisdieren en cultuurplanten", "The Correspondence of Charles Darwin, Volume 13", "Darwin for Today", "Rejse om jorden", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Correspondence of Charles Darwin, Volume 5", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The voyage of Charles Darwin", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The portable Darwin", "The Autobiography of Charles Darwin (Great Minds Series)", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin's Ornithological notes", "On evolution", "Part I: Contributions to the Theory of Natural Selection / Part II", "On the origin of species by means of natural selection", "Darwin-Wallace", "The Expression of the Emotions in Man and Animals", "The origin of species", "The Origin of Species (Great Books : Learning Channel)", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Correspondence of Charles Darwin, Volume 7", "The Structure and Distribution of Coral Reefs", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 16: 1868", "Notebooks on transmutation of species", "La facult\u00e9 motrice dans les plantes", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "On the tendency of species to form varieties", "The Correspondence of Charles Darwin, Volume 11", "The Correspondence of Charles Darwin, Volume 2", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "The Correspondence of Charles Darwin, Volume 9", "Autobiography of Charles Darwin", "Metaphysics, Materialism, & the evolution of mind", "The Correspondence of Charles Darwin, Volume 8", "The Voyage of the Beagle (Adventure Classics)", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "Evolution by natural selection", "monograph on the sub-class Cirripedia", "Volcanic Islands", "Beagle letters", "The expression of the emotions in man and animals", "Darwin en Patagonia", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "The autobiography of Charles Darwin, 1809-1882", "The Correspondence of Charles Darwin, Volume 1", "Charles Darwin's natural selection", "The education of Darwin", "The Power of Movement in Plants", "Origin of Species (Everyman's University Paperbacks)", "The Autobiography of Charles Darwin (Large Print)", "Voyage of the Beagle (NG Adventure Classics)", "Voyage of the Beagle", "To the members of the Down Friendly Club", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "The descent of man and selection in relation to sex.", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "The Origin of Species (World's Classics)", "Notes on the fertilization of orchids", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Del Plata a Tierra del Fuego", "More Letters of Charles Darwin", "Origin of Species", "The Orgin of Species", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "ontstaan der soorten door natuurlijke teeltkeus", "The Voyage of the Beagle (Unabridged Classics)", "Darwin's journal", "The Variation of Animals and Plants under Domestication", "The Origin Of Species", "Darwin and Henslow", "The Origin of Species (Variorum Reprint)", "The Formation of Vegetable Mould through the Action of Worms", "The Correspondence of Charles Darwin, Volume 15", "The foundations of the Origin of species", "Charles Darwin on the routes of male humble bees", "A student's introduction to Charles Darwin", "The Autobiography Of Charles Darwin", "Human nature, Darwin's view", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Expression of the Emotions in Man And Animals", "The Origin of Species", "On Natural Selection", "The descent of man, and selection in relation to sex", "Les mouvements et les habitudes des plantes grimpantes", "The Autobiography of Charles Darwin (Dodo Press)", "Kleinere geologische Abhandlungen", "From Darwin's unpublished notebooks", "Diario del Viaje de Un Naturalista Alrededor", "The Darwin Reader First Edition", "Darwin", "The autobiography of Charles Darwin", "Questions about the breeding of animals", "Het uitdrukken van emoties bij mens en dier", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Voyage d'un naturaliste autour du monde", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 12: 1864", "The Voyage of the Beagle (Mentor)", "Die fundamente zur entstehung der arten", "Darwin's insects", "Darwin Compendium", "Charles Darwin's marginalia", "The Voyage of the Beagle (Everyman Paperbacks)", "Evolution", "The expression of the emotions in man and animals.", "The Descent of Man, and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Resa kring jorden", "The Descent of Man and Selection in Relation to Sex", "The Correspondence of Charles Darwin, Volume 11: 1863", "Darwin's notebooks on transmutation of species", "The Correspondence of Charles Darwin, Volume 4", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Essential Darwin", "El Origin De Las Especies", "Darwin Darwin", "La vie et la correspondance de Charles Darwin", "From so simple a beginning", "Works", "The structure and distribution of coral reefs", "The Origin of Species (Mentor)", "The living thoughts of Darwin", "Charles Darwin", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "H.M.S. Beagle in South America"], "ans_acc": 0.04205607476635514, "ans_hit": 1, "ans_f1": 0.0091324200913242, "ans_precission": 0.2, "ans_recall": 0.004672897196261682, "path_f1": 0.12903225806451613, "path_precision": 1.0, "path_recall": 0.06896551724137931, "path_ans_f1": 0.08071748878923767, "path_ans_precision": 1.0, "path_ans_recall": 0.04205607476635514}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_profile.equivalent_topic -> board member\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Topic\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> book.author.works_written -> Through My Eyes -> common.topic.notable_types -> Book\n# Answer:\nThrough My Eyes", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nOrganization leader", "# Reasoning Path:\nTim Tebow -> people.person.profession -> Athlete -> people.profession.specializations -> American football player\n# Answer:\nAthlete", "# Reasoning Path:\nTim Tebow -> people.person.profession -> Athlete -> common.topic.notable_types -> Profession\n# Answer:\nAthlete", "# Reasoning Path:\nTim Tebow -> people.person.profession -> Athlete -> people.profession.specializations -> Alpine skier\n# Answer:\nAthlete", "# Reasoning Path:\nTim Tebow -> people.person.profession -> Athlete -> people.profession.specializations -> Amateur Boxing\n# Answer:\nAthlete", "# Reasoning Path:\nTim Tebow -> common.topic.notable_types -> Organization leader -> freebase.type_hints.included_types -> Person\n# Answer:\nOrganization leader"], "ground_truth": ["Florida Gators football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> type.type.expected_by -> Football Player\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kcv8b -> sports.sports_award.season -> 2006 NFL season\n# Answer:\nm.04kcv8b", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_8 -> sports.sports_award.season -> 2003 NFL season\n# Answer:\nm.04kg9_8", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.04kg9_j -> sports.sports_award.season -> 2004 NFL season\n# Answer:\nm.04kg9_j"], "ground_truth": ["Denver Broncos"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Ukraine\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Armenia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Central European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Babia G\u00f3ra -> location.location.containedby -> Europe\n# Answer:\nBabia G\u00f3ra", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> base.locations.continents.countries_within -> Czech Republic\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Georgia\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.passes -> Buz\u0103u Pass -> common.topic.notable_types -> Mountain pass\n# Answer:\nBuz\u0103u Pass", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.time_zones -> Eastern European Time Zone\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> geography.mountain_range.mountains -> Brebeneskul -> common.topic.notable_types -> Location\n# Answer:\nBrebeneskul", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe -> location.location.partially_contains -> Russia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> William Wordsworth -> common.topic.notable_types -> Author\n# Answer:\nWilliam Wordsworth", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Artwork -> freebase.type_profile.published -> Published\n# Answer:\nArtwork", "# Reasoning Path:\nRobert Burns -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Cardiovascular disease\n# Answer:\nMale", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author -> type.type.properties -> Book editions edited\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author -> type.type.properties -> Book editions published\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author -> common.topic.subject_of -> Abhishek Ghosh\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> William Wordsworth -> influence.influence_node.peers -> m.02nc24x\n# Answer:\nWilliam Wordsworth", "# Reasoning Path:\nRobert Burns -> influence.influence_node.influenced -> Anne Bront\u00eb -> common.topic.notable_types -> Author\n# Answer:\nAnne Bront\u00eb"], "ground_truth": ["Bard", "Writer", "Poet", "Author"], "ans_acc": 0.25, "ans_hit": 1, "ans_f1": 0.2727272727272727, "ans_precission": 0.3, "ans_recall": 0.25, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.3333333333333333, "path_ans_precision": 0.5, "path_ans_recall": 0.25}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Jake Lloyd\n# Answer:\nm.02sg5rn", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.film -> Star Wars Episode III: Revenge of the Sith\n# Answer:\nm.02sg5ry", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.film -> Star Wars Episode I: The Phantom Menace\n# Answer:\nm.02sg5rn", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Moment of Truth -> common.topic.notable_types -> Book\n# Answer:\nThe Moment of Truth", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.film -> Star Wars Episode II: Attack of the Clones\n# Answer:\nm.02sg5s6", "# Reasoning Path:\nDarth Vader -> fictional_universe.fictional_character.appears_in_these_fictional_universes -> Star Wars Expanded Universe -> common.topic.notable_types -> Comic Book Fictional Universe\n# Answer:\nStar Wars Expanded Universe", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> Path to Truth -> common.topic.notable_types -> Book\n# Answer:\nPath to Truth", "# Reasoning Path:\nDarth Vader -> book.book_character.appears_in_book -> The Changing of the Guard -> common.topic.notable_types -> Book\n# Answer:\nThe Changing of the Guard"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> type.type.domain -> American football\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> type.type.expected_by -> Player\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.schema -> Person\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> common.topic.notable_types -> American football player -> type.type.expected_by -> Football Player\n# Answer:\nAmerican football player", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Country of nationality -> type.property.expected_type -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth", "# Reasoning Path:\nPeyton Manning -> freebase.valuenotation.is_reviewed -> Gender -> type.property.schema -> Person\n# Answer:\nGender"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> common.topic.notable_types -> Country\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> common.topic.notable_types -> City/Town/Village\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.administrative_divisions -> Ontario\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Canada\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> base.wikipedia_infobox.settlement.area_code -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> English Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx", "# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada -> location.country.languages_spoken -> Spanish Language\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London -> location.location.containedby -> Area codes 519 and 226\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> freebase.valuenotation.has_no_value -> Winning work\n# Answer:\nm.0102z0vx"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.country -> United States of America\n# Answer:\nThree Kings", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.country -> Australia\n# Answer:\nThree Kings", "# Reasoning Path:\nGulf War -> film.film_subject.films -> A Just War? -> film.film.country -> Sweden\n# Answer:\nA Just War?", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Bravo Two Zero -> film.film.country -> United Kingdom\n# Answer:\nBravo Two Zero", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> France\n# Answer:\nm.03z96c5", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> media_common.netflix_title.netflix_genres -> Action/Adventure\n# Answer:\nThree Kings", "# Reasoning Path:\nGulf War -> time.event.includes_event -> 1991 uprisings in Iraq -> common.topic.notable_types -> Military Conflict\n# Answer:\n1991 uprisings in Iraq", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Air engagements of the Gulf War -> common.topic.notable_types -> Military Conflict\n# Answer:\nAir engagements of the Gulf War", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.genre -> Action Film\n# Answer:\nThree Kings", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nm.03z96c5"], "ground_truth": ["Iraq", "Australia", "Saudi Arabia", "Argentina", "United States of America", "France", "United Kingdom"], "ans_acc": 0.7142857142857143, "ans_hit": 1, "ans_f1": 0.11764705882352941, "ans_precission": 0.1, "ans_recall": 0.14285714285714285, "path_f1": 0.14925373134328357, "path_precision": 0.5, "path_recall": 0.08771929824561403, "path_ans_f1": 0.6521739130434783, "path_ans_precision": 0.6, "path_ans_recall": 0.7142857142857143}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.character -> London Tipton\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nm.040p0ym", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ys -> tv.regular_tv_appearance.character -> Mr. Moseby\n# Answer:\nm.040p0ys", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Show\n# Answer:\nm.0sgkxsb", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.actor -> Erin Cardillo\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyvs -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Actor\n# Answer:\nm.0sgkyvs", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.05v3ngr -> tv.regular_tv_appearance.character -> Emma Tutweiller\n# Answer:\nm.05v3ngr", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkz86 -> award.award_nomination.award -> Kids' Choice Award for Favorite TV Sidekick\n# Answer:\nm.0sgkz86", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.episodes -> A London Carol -> common.topic.article -> m.0j94h3g\n# Answer:\nA London Carol", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkxsb -> award.award_nomination.ceremony -> 2011 Kids' Choice Awards\n# Answer:\nm.0sgkxsb"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.office_holder -> George Voinovich\n# Answer:\nm.05kfccr", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_children -> Highland County -> common.topic.notable_types -> US County\n# Answer:\nHighland County", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0gm5f32", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_holder -> Sherrod Brown\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kg_6s -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.05kg_6s", "# Reasoning Path:\nOhio -> base.aareas.schema.administrative_area.administrative_children -> Highland County -> location.location.containedby -> United States of America\n# Answer:\nHighland County", "# Reasoning Path:\nOhio -> meteorology.cyclone_affected_area.cyclones -> Hurricane Bob -> meteorology.tropical_cyclone.affected_areas -> Alabama\n# Answer:\nHurricane Bob", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.05kfccr -> government.government_position_held.legislative_sessions -> 106th United States Congress\n# Answer:\nm.05kfccr"], "ground_truth": ["Return J. Meigs, Jr.", "John Kasich", "Ted Strickland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.team -> Paris Saint-Germain F.C.\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> base.popstra.celebrity.canoodled -> m.0649s64 -> base.popstra.canoodled.participant -> Rebecca Loos\n# Answer:\nm.0649s64", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0j2zzj_", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.currency -> United States Dollar\n# Answer:\nm.0k4ytw5", "# Reasoning Path:\nDavid Beckham -> base.popstra.celebrity.canoodled -> m.0652v4p -> base.popstra.canoodled.participant -> Elsa Pataky\n# Answer:\nm.0652v4p", "# Reasoning Path:\nDavid Beckham -> base.popstra.celebrity.canoodled -> m.0652v7x -> base.popstra.canoodled.participant -> Rebecca Loos\n# Answer:\nm.0652v7x", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0qzkj58 -> base.schemastaging.athlete_salary.currency -> Pound sterling\n# Answer:\nm.0qzkj58", "# Reasoning Path:\nDavid Beckham -> tv.tv_actor.guest_roles -> m.0_z851f -> tv.tv_guest_role.special_performance_type -> Him/Herself\n# Answer:\nm.0_z851f"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> common.topic.notable_types -> City/Town/Village\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> common.topic.notable_types -> Author -> common.topic.notable_types -> Profession\n# Answer:\nAuthor", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Hidalgo\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7t8559g\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> common.topic.notable_types -> Author -> type.type.expected_by -> author\n# Answer:\nAuthor", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Abraham Zacuto\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.location.people_born_here -> Alfonso XI of Castile\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7tbpl3m\n# Answer:\nSalamanca", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> symbols.name_source.namesakes -> Coronado -> location.location.containedby -> San Diego County\n# Answer:\nCoronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Salamanca -> location.statistical_region.population -> g.11b7vzj2hj\n# Answer:\nSalamanca"], "ground_truth": ["Salamanca"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> visual_art.artwork.artist -> Elaine de Kooning\n# Answer:\nElaine de Kooning", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_hints.included_types -> Topic\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> freebase.type_profile.equivalent_topic -> President of the United States\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> Artwork -> freebase.type_profile.published -> Published\n# Answer:\nArtwork", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.properties -> President number\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> Owner\n# Answer:\nUS President", "# Reasoning Path:\nJohn F. Kennedy -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.location.containedby -> Asia\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.geolocation -> m.0clv1h_\n# Answer:\nm.0clv1h_", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Fukushima Prefecture\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> Japan\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 1 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 1", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> common.topic.image -> Okuma town office\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Okuma -> location.location.containedby -> T\u014dhoku region\n# Answer:\nOkuma", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> base.infrastructure.nuclear_power_station.reactors -> Fukushima I \u2013 2 -> common.topic.notable_types -> Structure\n# Answer:\nFukushima I \u2013 2"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> base.aareas.schema.administrative_area.administrative_children -> Anguilla\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> location.location.partially_containedby -> Western Europe\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom, with Dependencies and Territories -> common.topic.notable_types -> Country\n# Answer:\nUnited Kingdom, with Dependencies and Territories", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.offense -> Child pornography\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.12cp_jvpx\n# Answer:\ng.12cp_jvpx", "# Reasoning Path:\nUnited Kingdom -> base.popstra.location.arrestee -> m.0ghc35h -> base.popstra.arrest.arrested_person -> Gary Glitter\n# Answer:\nm.0ghc35h", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc390lx\n# Answer:\ng.1hhc390lx", "# Reasoning Path:\nUnited Kingdom -> location.statistical_region.long_term_unemployment_rate -> g.1hhc3_l16\n# Answer:\ng.1hhc3_l16"], "ground_truth": ["Wales", "Scotland", "England", "Northern Ireland"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.image -> Timezoneswest\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.02fqx0\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone -> common.topic.article -> m.0j7mzvm\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Bartholomew -> common.topic.notable_types -> River\n# Answer:\nBayou Bartholomew", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> common.topic.notable_types -> River\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partiallycontains -> m.0wg8__0 -> location.partial_containment_relationship.partially_contains -> Sabine River\n# Answer:\nm.0wg8__0", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> location.location.containedby -> United States of America\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Amite River -> geography.river.cities -> Denham Springs\n# Answer:\nAmite River", "# Reasoning Path:\nLouisiana -> location.location.partially_contains -> Bayou Macon -> common.topic.notable_types -> River\n# Answer:\nBayou Macon"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> common.image.appears_in_topic_gallery -> Allegorical interpretations of Genesis\n# Answer:\nAllegorical interpretations of Genesis", "# Reasoning Path:\nAugustine of Hippo -> common.image.appears_in_topic_gallery -> Austin Preparatory School\n# Answer:\nAustin Preparatory School", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.profession -> Philosopher\n# Answer:\nAlasdair MacIntyre", "# Reasoning Path:\nAugustine of Hippo -> common.image.appears_in_topic_gallery -> Berbers -> people.ethnicity.languages_spoken -> Spain\n# Answer:\nBerbers", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> A thing is not necessarily true because badly uttered, nor false because spoken magnificently. -> common.topic.notable_types -> Quotation\n# Answer:\nA thing is not necessarily true because badly uttered, nor false because spoken magnificently.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Albert Camus -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nAlbert Camus", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> Charity is no substitute for justice withheld. -> common.topic.notable_types -> Quotation\n# Answer:\nCharity is no substitute for justice withheld.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alvin Plantinga -> influence.influence_node.influenced -> John Frame\n# Answer:\nAlvin Plantinga", "# Reasoning Path:\nAugustine of Hippo -> people.person.quotations -> By faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity. -> common.topic.notable_types -> Quotation\n# Answer:\nBy faithfulness we are collected and wound up into unity within ourselves, whereas we had been scattered abroad in multiplicity.", "# Reasoning Path:\nAugustine of Hippo -> influence.influence_node.influenced -> Alasdair MacIntyre -> people.person.nationality -> United Kingdom\n# Answer:\nAlasdair MacIntyre"], "ground_truth": ["Philosopher", "Writer", "Physician"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Parents\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> people.person.children -> Sean Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> music.featured_artist.recordings -> #1 Fan -> music.recording.featured_artists -> J. Holiday\n# Answer:\n#1 Fan", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Parents\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> common.topic.notable_types -> Person\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> people.person.children -> Neffeteria Pugh\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> common.topic.notable_types -> Person\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole -> freebase.valuenotation.has_value -> Date of birth\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nFrancine Lons"], "ground_truth": ["Francine Lons", "Leon Cole", "Sal Gibson"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.7659574468085106, "ans_precission": 0.9, "ans_recall": 0.6666666666666666, "path_f1": 0.7659574468085106, "path_precision": 0.9, "path_recall": 0.6666666666666666, "path_ans_f1": 0.7659574468085106, "path_ans_precision": 0.9, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> France\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government -> government.form_of_government.countries -> Libya\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Portugal\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.12cp_k6sh\n# Answer:\ng.12cp_k6sh", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system -> government.form_of_government.countries -> Syria\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> Parliament of Egypt -> government.governmental_body.component_bodies -> House of Representatives\n# Answer:\nParliament of Egypt", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3_4cn\n# Answer:\ng.1hhc3_4cn", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.offices_positions -> Speaker of the House of Representatives\n# Answer:\nHouse of Representatives", "# Reasoning Path:\nEgypt -> location.statistical_region.size_of_armed_forces -> g.1hhc3f_h6\n# Answer:\ng.1hhc3f_h6", "# Reasoning Path:\nEgypt -> government.governmental_jurisdiction.government_bodies -> House of Representatives -> government.governmental_body.body_this_is_a_component_of -> Parliament of Egypt\n# Answer:\nHouse of Representatives"], "ground_truth": ["Semi-presidential system", "Provisional government"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> common.topic.notable_types -> Review\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> African American\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.events -> Assassination of Martin Luther King, Jr.\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis -> location.location.containedby -> United States of America\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial -> book.written_work.subjects -> African American\n# Answer:\nCountdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.next_in_series -> Selma Revisited: from Violent Racism to Reflective Compassion Part  2\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song -> common.topic.notable_types -> Short Non-fiction\n# Answer:\nCountdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> A Writer's Journey to Selma, Alabama -> book.written_work.subjects -> Academy Awards\n# Answer:\nA Writer's Journey to Selma, Alabama", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial -> book.written_work.part_of_series -> Countdown of 10 Amazing Moments from the Year 2011\n# Answer:\nCountdown of 10 Amazing Moments from the Year 2011: No. 4 the Martin Luther King Jr. Memorial"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Novelist\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore -> freebase.valuenotation.is_reviewed -> Place of birth\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.gender -> Female\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Albrecht Behmel -> influence.influence_node.influenced_by -> Friedrich Nietzsche\n# Answer:\nAlbrecht Behmel", "# Reasoning Path:\nEdgar Allan Poe -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Fictional Character\n# Answer:\nFilm character", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Playwright\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> freebase.valuenotation.has_value -> Education\n# Answer:\nAgatha Christie", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Alan Moore -> freebase.valuenotation.is_reviewed -> Children\n# Answer:\nAlan Moore", "# Reasoning Path:\nEdgar Allan Poe -> influence.influence_node.influenced -> Agatha Christie -> people.person.profession -> Poet\n# Answer:\nAgatha Christie"], "ground_truth": ["Baltimore"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> common.topic.notable_types -> Zoo\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.containedby -> United States of America\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Men's rings\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium -> location.location.containedby -> Georgia\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> Georgia\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Dome -> location.location.events -> Gymnastics at the 1996 Summer Olympics \u2013 Women's artistic team all-around\n# Answer:\nGeorgia Dome", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> location.location.containedby -> United States of America\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> location.location.nearby_airports -> Hartsfield\u2013Jackson Atlanta International Airport -> travel.transport_terminus.travel_destinations_served -> m.052zwlt\n# Answer:\nHartsfield\u2013Jackson Atlanta International Airport", "# Reasoning Path:\nAtlanta -> common.topic.webpage -> m.03ldb41 -> common.webpage.resource -> m.0blf53m\n# Answer:\nm.03ldb41"], "ground_truth": ["The Tabernacle", "Masquerade", "Fox Theatre", "Atlanta Marriott Marquis", "Georgia Aquarium", "Six Flags Over Georgia", "Six Flags White Water", "Centennial Olympic Park", "Georgia Dome", "Fernbank Science Center", "Atlanta Ballet", "Arbor Place Mall", "Hyatt Regency Atlanta", "Philips Arena", "Four Seasons Hotel Atlanta", "World of Coca-Cola", "Center for Puppetry Arts", "Turner Field", "Zoo Atlanta", "Omni Coliseum", "Variety Playhouse", "CNN Center", "Martin Luther King, Jr. National Historic Site", "Jimmy Carter Library and Museum", "Cobb Energy Performing Arts Centre", "Atlanta Jewish Film Festival", "Woodruff Arts Center", "Fernbank Museum of Natural History", "Atlanta Symphony Orchestra", "Georgia State Capitol", "Underground Atlanta", "Georgia World Congress Center", "Atlanta History Center", "Margaret Mitchell House & Museum", "Atlanta Cyclorama & Civil War Museum", "Peachtree Road Race"], "ans_acc": 0.05555555555555555, "ans_hit": 1, "ans_f1": 0.10169491525423728, "ans_precission": 0.6, "ans_recall": 0.05555555555555555, "path_f1": 0.10169491525423728, "path_precision": 0.6, "path_recall": 0.05555555555555555, "path_ans_f1": 0.14634146341463414, "path_ans_precision": 0.6, "path_ans_recall": 0.08333333333333333}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> common.image.appears_in_topic_gallery -> Premier of Queensland -> government.government_office_or_title.jurisdiction -> Queensland\n# Answer:\nPremier of Queensland", "# Reasoning Path:\nAnna Bligh -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nQueensland state election, 2009", "# Reasoning Path:\nAnna Bligh -> common.image.appears_in_topic_gallery -> Premier of Queensland -> government.government_office_or_title.office_holders -> m.0cr301h\n# Answer:\nPremier of Queensland", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2009\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.appears_in_topic_gallery -> Queensland state election, 2015\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> common.topic.image -> Anna Bligh crop -> common.image.size -> m.05t654b\n# Answer:\nAnna Bligh crop", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr320w", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr301h -> government.government_position_held.basic_title -> Premier\n# Answer:\nm.0cr301h", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0cr320w -> government.government_position_held.district_represented -> Electoral district of South Brisbane\n# Answer:\nm.0cr320w"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-3", "prediction": ["# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> tv.tv_program_creator.programs_created -> Inspector Morse\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> people.person.gender -> Male\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> base.webvideo.internet_video_genre.series -> Electric City\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> tv.tv_program_creator.programs_created -> The War of Darkie Pilbeam\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.program_creator -> Tony Warren -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ken Barlow\n# Answer:\nTony Warren", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.subject_of -> Albrecht Behmel\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.multipart_episodes -> Fri 31 Dec 2010 -> common.topic.notable_for -> g.1255pdp4f\n# Answer:\nFri 31 Dec 2010", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> base.webvideo.internet_video_genre.series -> $5 Cover\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> base.webvideo.internet_video_genre.series -> 2009 A True Story\n# Answer:\nDrama", "# Reasoning Path:\nCoronation Street -> tv.tv_program.genre -> Drama -> common.topic.subject_of -> Vivacity\n# Answer:\nDrama"], "ground_truth": ["William Roache"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-31", "prediction": ["# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> freebase.type_profile.equivalent_topic -> Tennis player\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> type.type.properties -> Career Losses (doubles)\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> rdf-schema#range -> Location\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> common.topic.article -> m.0dg0pnd\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> type.type.properties -> Career Losses (singles)\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.expected_type -> Location\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> common.topic.notable_types -> Tennis Player -> type.type.properties -> Career Prize Money\n# Answer:\nTennis Player", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Place of birth -> type.property.schema -> Person\n# Answer:\nPlace of birth", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Country of nationality -> rdf-schema#range -> Country\n# Answer:\nCountry of nationality", "# Reasoning Path:\nAndy Murray -> freebase.valuenotation.is_reviewed -> Date of birth -> type.property.expected_type -> Date/Time\n# Answer:\nDate of birth"], "ground_truth": ["2005"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-32", "prediction": ["# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4pgdn\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> base.ontologies.ontology_instance.equivalent_instances -> m.09knr56 -> base.ontologies.ontology_instance_mapping.ontology -> OpenCyc\n# Answer:\nm.09knr56", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Bunyip\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.country.official_language -> English Language\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> location.statistical_region.gdp_nominal_per_capita -> g.1hhc4y0zl\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Australia -> base.mystery.cryptid_observation_location.cryptid_s_occurring_here -> Yowie\n# Answer:\nAustralia", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Cocos (Keeling) Islands -> location.location.time_zones -> Cocos Islands\u00a0Time Zone\n# Answer:\nCocos (Keeling) Islands", "# Reasoning Path:\nAustralian dollar -> finance.currency.countries_used -> Christmas Island -> location.statistical_region.population -> g.11bymn224r\n# Answer:\nChristmas Island", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gf -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gf", "# Reasoning Path:\nAustralian dollar -> common.topic.webpage -> m.04lt3gl -> common.webpage.category -> Topic Webpage\n# Answer:\nm.04lt3gl"], "ground_truth": ["AUD"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.11b60n70x2\n# Answer:\ng.11b60n70x2", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.12cp_k2s4\n# Answer:\ng.12cp_k2s4", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22ll\n# Answer:\ng.1245_22ll", "# Reasoning Path:\nSweden -> location.statistical_region.gni_per_capita_in_ppp_dollars -> g.1245_22zj\n# Answer:\ng.1245_22zj", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc378pv\n# Answer:\ng.1hhc378pv", "# Reasoning Path:\nSweden -> location.statistical_region.part_time_employment_percent -> g.1hhc37hbq\n# Answer:\ng.1hhc37hbq"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.25, "ans_precission": 0.14285714285714285, "ans_recall": 1.0, "path_f1": 0.25, "path_precision": 0.14285714285714285, "path_recall": 1.0, "path_ans_f1": 0.25, "path_ans_precision": 0.14285714285714285, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> common.topic.notable_for -> g.125dysc88\n# Answer:\ng.125dysc88", "# Reasoning Path:\nCam Newton -> common.topic.notable_types -> American football player -> type.type.expected_by -> Player\n# Answer:\nAmerican football player", "# Reasoning Path:\nCam Newton -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Athlete\n# Answer:\nAmerican football player", "# Reasoning Path:\nCam Newton -> common.topic.notable_types -> American football player -> people.profession.specialization_of -> Football player\n# Answer:\nAmerican football player", "# Reasoning Path:\nCam Newton -> common.topic.notable_types -> American football player -> type.type.expected_by -> Football Player\n# Answer:\nAmerican football player", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> common.topic.notable_types -> American football player -> type.type.expected_by -> Football Players\n# Answer:\nAmerican football player", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.position -> Quarterback\n# Answer:\nm.04nb7yn", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0hpgnsj -> freebase.valuenotation.has_value -> Number\n# Answer:\nm.0hpgnsj", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.04nb7yn -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nm.04nb7yn"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.15384615384615383, "path_precision": 0.1, "path_recall": 0.3333333333333333, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> common.topic.notable_types -> US County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.rent50_1 -> m.05gcgl3\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11b66h2c0w\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States of America\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.people_born_here -> Alfred G. Mayer -> common.topic.notable_types -> Author\n# Answer:\nAlfred G. Mayer", "# Reasoning Path:\nFrederick -> location.statistical_region.population -> g.11b66fv4wt\n# Answer:\ng.11b66fv4wt", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Maryland\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County -> location.statistical_region.population -> g.11x1cfntl\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland -> location.location.containedby -> United States, with Territories\n# Answer:\nMaryland"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nm.0lwxmy1", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> University of Oxford\n# Answer:\nm.0lwxmy9", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.04hx138 -> education.education.institution -> University of Alabama\n# Answer:\nm.04hx138", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Country of nationality\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_types -> Book Edition\n# Answer:\nHarper Lee's To Kill a Mockingbird", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Date of birth\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.place_of_birth -> Los Angeles\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> freebase.valuenotation.is_reviewed -> Parents\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> symbols.name_source.namesakes -> Harper Seven Beckham -> people.person.gender -> Female\n# Answer:\nHarper Seven Beckham", "# Reasoning Path:\nHarper Lee -> book.author.book_editions_published -> Harper Lee's To Kill a Mockingbird -> common.topic.notable_for -> g.125920htw\n# Answer:\nHarper Lee's To Kill a Mockingbird"], "ground_truth": ["Monroe County High School"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> common.topic.notable_types -> Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> time.time_zone.day_dst_ends -> First Sunday in November\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.containedby -> Contiguous United States -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nContiguous United States", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone -> base.schemastaging.context_name.pronunciation -> g.125_rngwj\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> book.book_subject.works -> Red -> common.topic.notable_types -> Book\n# Answer:\nRed", "# Reasoning Path:\nUtah -> location.location.containedby -> Southwestern United States -> location.location.containedby -> United States of America\n# Answer:\nSouthwestern United States", "# Reasoning Path:\nUtah -> location.location.containedby -> Contiguous United States -> location.location.time_zones -> Central Time Zone\n# Answer:\nContiguous United States", "# Reasoning Path:\nUtah -> location.location.containedby -> Mountain States -> common.topic.notable_types -> Region\n# Answer:\nMountain States", "# Reasoning Path:\nUtah -> location.location.containedby -> Contiguous United States -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nContiguous United States", "# Reasoning Path:\nUtah -> book.book_subject.works -> Blossoms of faith -> common.topic.notable_types -> Book\n# Answer:\nBlossoms of faith"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-38", "prediction": ["# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> type.type.expected_by -> President\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> type.type.expected_by -> Owner\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> type.type.expected_by -> US Presidents\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> type.type.properties -> President number\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> freebase.type_profile.kind -> Significant\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> type.type.properties -> Vice president\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointed_role -> Director of the Bureau of Counterterrorism\n# Answer:\nm.010p95d2", "# Reasoning Path:\nGeorge W. Bush -> common.topic.notable_types -> US President -> freebase.type_profile.kind -> Title\n# Answer:\nUS President", "# Reasoning Path:\nGeorge W. Bush -> film.person_or_entity_appearing_in_film.films -> m.010l29pk -> film.personal_film_appearance.type_of_appearance -> Him/Herself\n# Answer:\nm.010l29pk", "# Reasoning Path:\nGeorge W. Bush -> people.appointer.appointment_made -> m.010p95d2 -> people.appointment.appointee -> Henry A. Crumpton\n# Answer:\nm.010p95d2"], "ground_truth": ["Michael Peroutka", "Gene Amondson", "Ralph Nader", "John Kerry"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.parents -> Ayaan Hirsi Ali\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> people.person.gender -> Male\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_types -> Academic -> type.type.expected_by -> Person\n# Answer:\nAcademic", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Country of nationality\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_types -> Academic -> freebase.type_profile.strict_included_types -> Person\n# Answer:\nAcademic", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j4jq57 -> people.marriage.spouse -> Ayaan Hirsi Ali\n# Answer:\nm.0j4jq57", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_types -> Academic -> type.type.expected_by -> Academic advisees\n# Answer:\nAcademic", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_types -> Academic -> type.type.properties -> Academic advisees\n# Answer:\nAcademic", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Thomas Ferguson -> freebase.valuenotation.has_value -> Place of birth\n# Answer:\nThomas Ferguson", "# Reasoning Path:\nNiall Ferguson -> common.topic.notable_types -> Academic -> type.type.expected_by -> Academic advisors\n# Answer:\nAcademic"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Pacific Ocean\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.administrative_division.country -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.containedby -> Americas\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> location.location.containedby -> Ecuador\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> base.aareas.schema.administrative_area.administrative_children -> Gal\u00e1pagos Province\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province -> common.topic.article -> m.03660h\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean -> location.location.containedby -> World Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador -> location.location.primarily_containedby -> South America\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> travel.travel_destination.tourist_attractions -> Baltra Island -> location.location.containedby -> Santa Cruz Canton, Ecuador\n# Answer:\nBaltra Island"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.recording.featured_artists -> Big Sean\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Somebody to Love\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> music.album.releases -> As Long As You Love Me (remixes)\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> All Around the World -> music.recording.featured_artists -> Ludacris\n# Answer:\nAll Around the World", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.1257jn46q\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My World -> award.award_nominated_work.award_nominations -> m.0tkc3tj\n# Answer:\nMy World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.role -> Vocals\n# Answer:\nm.0rqp4h0", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> As Long as You Love Me -> common.topic.notable_for -> g.125ddwtp0\n# Answer:\nAs Long as You Love Me", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My World -> award.award_nominated_work.award_nominations -> m.0tkqqgg\n# Answer:\nMy World", "# Reasoning Path:\nJustin Bieber -> award.award_winner.awards_won -> m.0102z0vx -> award.award_honor.ceremony -> Juno Awards of 2014\n# Answer:\nm.0102z0vx"], "ground_truth": ["Beauty And A Beat", "All That Matters", "Baby", "Bad Day", "First Dance", "Never Say Never", "Confident", "Roller Coaster", "Bigger", "Die in Your Arms", "Hold Tight", "All Around The World", "Eenie Meenie", "Wait for a Minute", "All Bad", "Somebody to Love", "Home to Mama", "Boyfriend", "Never Let You Go", "Right Here", "Thought Of You", "PYD", "Live My Life", "As Long as You Love Me", "Heartbreaker", "Lolly", "Recovery", "Pray", "#thatPower", "Change Me", "Turn to You (Mother's Day Dedication)"], "ans_acc": 0.0967741935483871, "ans_hit": 1, "ans_f1": 0.1142857142857143, "ans_precission": 0.5, "ans_recall": 0.06451612903225806, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.16666666666666666, "path_ans_precision": 0.6, "path_ans_recall": 0.0967741935483871}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.notable_types -> Profession\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> common.topic.notable_types -> Profession\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> common.topic.notable_types -> Profession\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> common.topic.subjects -> Ciro Pellegrino\n# Answer:\nJournalist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> common.topic.article -> m.016fc9\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician -> freebase.type_hints.included_types -> Topic\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Physician -> common.topic.notable_types -> Profession\n# Answer:\nPhysician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman -> base.descriptive_names.names.descriptive_name -> m.0101fnw4\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> symbols.name_source.namesakes -> Champs-\u00c9lys\u00e9es \u2013 Clemenceau -> location.location.containedby -> Paris\n# Answer:\nChamps-\u00c9lys\u00e9es \u2013 Clemenceau", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Journalist -> fictional_universe.character_occupation.characters_with_this_occupation -> Acetylene Lamp\n# Answer:\nJournalist"], "ground_truth": ["Statesman", "Journalist", "Publisher", "Writer", "Physician"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6461538461538462, "ans_precission": 0.7, "ans_recall": 0.6, "path_f1": 0.6461538461538462, "path_precision": 0.7, "path_recall": 0.6, "path_ans_f1": 0.6461538461538462, "path_ans_precision": 0.7, "path_ans_recall": 0.6}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.equivalent_topic -> U.S. state\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hzd2r_ -> location.location_symbol_relationship.symbol -> Ditat Deus\n# Answer:\nm.0hzd2r_", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.Kind_of_symbol -> State flower\n# Answer:\nm.0hz8v06", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.04l0cxp -> location.location_symbol_relationship.symbol -> Petrified wood\n# Answer:\nm.04l0cxp", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> type.type.expected_by -> Indiana\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> type.type.expected_by -> State\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hzd2r_ -> location.location_symbol_relationship.Kind_of_symbol -> Motto\n# Answer:\nm.0hzd2r_", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> type.type.expected_by -> state\n# Answer:\nUS State", "# Reasoning Path:\nArizona -> common.topic.notable_types -> US State -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nUS State"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.location.containedby -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> location.administrative_division.first_level_division_of -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish -> base.aareas.schema.administrative_area.administrative_parent -> Barbados\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv1mttmr\n# Answer:\ng.11bv1mttmr", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.location.contains -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> g.11bv383dbd\n# Answer:\ng.11bv383dbd", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> location.country.administrative_divisions -> Saint Michael Parish\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados -> common.topic.notable_types -> Country\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> base.popstra.celebrity.infidelity_participant -> m.063y0bl -> base.popstra.infidelity.victim -> Beyonc\u00e9 Knowles\n# Answer:\nm.063y0bl", "# Reasoning Path:\nRihanna -> music.artist.track_contributions -> m.010_ydxv -> music.track_contribution.track -> If I Never See Your Face Again\n# Answer:\nm.010_ydxv"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-46", "prediction": ["# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.02bk8zt\n# Answer:\nm.02bk8zt", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.office_position_or_title -> President of the United States\n# Answer:\nm.03fx87q", "# Reasoning Path:\nWilliam Henry Harrison -> common.image.size -> m.03qtjkt\n# Answer:\nm.03qtjkt", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.office_position_or_title -> United States Senator\n# Answer:\nm.0bfmkg2", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.basic_title -> President\n# Answer:\nm.03fx87q", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj -> government.government_position_held.basic_title -> Governor\n# Answer:\nm.04j5skj", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.03fx87q -> government.government_position_held.district_represented -> United States of America\n# Answer:\nm.03fx87q", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.04j5skj -> government.government_position_held.jurisdiction_of_office -> Indiana Territory\n# Answer:\nm.04j5skj", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.district_represented -> Ohio\n# Answer:\nm.0bfmkg2", "# Reasoning Path:\nWilliam Henry Harrison -> government.politician.government_positions_held -> m.0bfmkg2 -> government.government_position_held.governmental_body -> United States Senate\n# Answer:\nm.0bfmkg2"], "ground_truth": ["1841-03-04"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> Musical Recording\n# Answer:\nMusical Recording", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.country_of_origin -> United States of America -> location.country.administrative_divisions -> Washington, D.C.\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Episode\n# Answer:\nTV Episode", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.country_of_origin -> United States of America -> base.locations.countries.states_provinces_within -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Program -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nTV Program", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.country_of_origin -> United States of America -> location.country.administrative_divisions -> California\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Lopez -> tv.tv_program.country_of_origin -> United States of America -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nUnited States of America", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Program -> freebase.type_hints.included_types -> Topic\n# Answer:\nTV Program", "# Reasoning Path:\nGeorge Lopez -> common.topic.webpage -> m.03lppm1 -> common.webpage.resource -> m.0bjrztw\n# Answer:\nm.03lppm1", "# Reasoning Path:\nGeorge Lopez -> common.topic.notable_types -> TV Program -> type.type.expected_by -> TV programs Created\n# Answer:\nTV Program"], "ground_truth": ["Mission Hills"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> business.employer.number_of_employees -> m.0f7q8f_\n# Answer:\nm.0f7q8f_", "# Reasoning Path:\nSamsung Group -> common.topic.article -> m.07gv7d\n# Answer:\nm.07gv7d", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_khq\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> common.topic.article -> m.0h6dr\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_kp0\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Mahaka Media\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> base.descriptive_names.names.descriptive_name -> m.0105_lnf\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Textron\n# Answer:\nConglomerate", "# Reasoning Path:\nSamsung Group -> business.business_operation.industry -> Conglomerate -> organization.organization_type.organizations_of_this_type -> Time Warner\n# Answer:\nConglomerate"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Anglicanism\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> religion.religion.beliefs -> End time\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Catholicism\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.notable_for -> g.1q69mrtxz\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> religion.belief.belief_of -> Christianity\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive -> common.topic.article -> m.02wvcg8\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books -> common.topic.notable_for -> g.125621qyv\n# Answer:\nIslamic holy books", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> God in Islam -> common.topic.article -> m.09kb_7\n# Answer:\nGod in Islam", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> base.schemastaging.context_name.pronunciation -> g.125_r5my9\n# Answer:\nAbrahamic religions", "# Reasoning Path:\nIslam -> religion.religion.is_part_of -> Abrahamic religions -> common.topic.notable_for -> g.125cc3_fl\n# Answer:\nAbrahamic religions"], "ground_truth": ["Qiyamah", "Islamic view of angels", "Tawhid", "Predestination in Islam", "Sharia", "Masih ad-Dajjal", "Monotheism", "Mahdi", "Entering Heaven alive", "Prophets in Islam", "God in Islam", "Islamic holy books", "\u1e6c\u016bb\u0101"], "ans_acc": 0.23076923076923078, "ans_hit": 1, "ans_f1": 0.34710743801652894, "ans_precission": 0.7, "ans_recall": 0.23076923076923078, "path_f1": 0.34710743801652894, "path_precision": 0.7, "path_recall": 0.23076923076923078, "path_ans_f1": 0.34710743801652894, "path_ans_precision": 0.7, "path_ans_recall": 0.23076923076923078}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.actor -> Jamie Dornan\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0ydn3r2 -> film.performance.film -> Fifty Shades of Grey\n# Answer:\nm.0ydn3r2", "# Reasoning Path:\nChristian Grey -> common.topic.notable_for -> g.125czvn3w\n# Answer:\ng.125czvn3w", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Abdominal aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Kidney cancer\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> base.skosbase.vocabulary_equivalent_topic.equivalent_concept -> Males\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> common.topic.notable_for -> g.1255p_jgc\n# Answer:\nMale", "# Reasoning Path:\nChristian Grey -> fictional_universe.fictional_character.gender -> Male -> medicine.risk_factor.diseases -> Actinic keratosis\n# Answer:\nMale"], "ground_truth": ["Jamie Dornan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.2222222222222222, "path_precision": 0.125, "path_recall": 1.0, "path_ans_f1": 0.2222222222222222, "path_ans_precision": 0.125, "path_ans_recall": 1.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Albert Camus\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> common.topic.notable_types -> Author -> fictional_universe.character_occupation.characters_with_this_occupation -> Kilgore Trout\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Anton Chekhov\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.notable_people_with_this_condition -> Franz Kafka\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> people.cause_of_death.parent_cause_of_death -> Infectious disease\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Antibiotics\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> common.topic.notable_types -> Author -> people.profession.specialization_of -> Writer\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> common.topic.notable_types -> Author -> common.topic.subject_of -> Kiersten Fay\n# Answer:\nAuthor", "# Reasoning Path:\nGeorge Orwell -> people.person.quotations -> A dirty joke is a sort of mental rebellion. -> common.topic.notable_types -> Quotation\n# Answer:\nA dirty joke is a sort of mental rebellion.", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis -> medicine.disease.treatments -> Isoniazid\n# Answer:\nTuberculosis"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> common.topic.notable_types -> Military Commander -> type.type.domain -> Military\n# Answer:\nMilitary Commander", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_for -> g.1254zbtvv\n# Answer:\ng.1254zbtvv", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_types -> Military Commander -> common.topic.article -> m.0dgb763\n# Answer:\nMilitary Commander", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_types -> Military Commander -> base.descriptive_names.names.descriptive_name -> m.010dyd62\n# Answer:\nMilitary Commander", "# Reasoning Path:\nAdolf Hitler -> common.topic.notable_types -> Military Commander -> base.descriptive_names.names.descriptive_name -> m.011qyh10\n# Answer:\nMilitary Commander", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wf140 -> common.webpage.resource -> Another Megan Fox letter -- and this one sings her praises!\n# Answer:\nm.09wf140", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wgn1j -> common.webpage.resource -> Charles Manson: 40 years later, the movie about him you have to see\n# Answer:\nm.09wgn1j", "# Reasoning Path:\nAdolf Hitler -> common.topic.webpage -> m.09wqfb3 -> common.webpage.resource -> Erykah Badu teases 'New Amerykah Part Two' with trippy, funky clip\n# Answer:\nm.09wqfb3"], "ground_truth": ["Nazi Germany"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> common.topic.notable_types -> Profession\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> type.type.expected_by -> Artist\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.specialization_of -> Musician\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.specialization_of -> Musician\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_hints.included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> people.profession.corresponding_type -> Musical Artist\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter -> common.topic.notable_types -> Profession\n# Answer:\nSongwriter"], "ground_truth": ["Songwriter", "Actor", "Singer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> location.location.containedby -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> common.topic.notable_types -> City/Town/Village -> freebase.type_profile.strict_included_types -> Location\n# Answer:\nCity/Town/Village", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas City, MO-KS Metropolitan Statistical Area\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> 66111 -> location.location.containedby -> Wyandotte County\n# Answer:\n66111", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas -> base.locations.states_and_provences.country -> United States of America\n# Answer:\nKansas", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.us_county.hud_county_place -> Kansas City\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County -> location.statistical_region.co2_emissions_mobile -> m.045l26t\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County -> location.location.containedby -> Kansas\n# Answer:\nWyandotte County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.4444444444444445, "path_precision": 0.4, "path_recall": 0.5, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.domain -> Baseball\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.notable_types -> Cemetery\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> influence.influence_node.influenced -> Steve Hofstetter -> common.topic.notable_types -> Musical Artist\n# Answer:\nSteve Hofstetter", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Baseball Almanac ID\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.expected_by -> Player\n# Answer:\nBaseball Player", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Film character -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nJackie Robinson -> people.deceased_person.place_of_burial -> Cypress Hills Cemetery -> common.topic.article -> m.03vbmy\n# Answer:\nCypress Hills Cemetery", "# Reasoning Path:\nJackie Robinson -> common.topic.notable_types -> Baseball Player -> type.type.properties -> Bats\n# Answer:\nBaseball Player"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-59", "prediction": ["# Reasoning Path:\nAnnie -> film.film.film_set_decoration_by -> Archie D'Amico\n# Answer:\nArchie D'Amico", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Choreographer\n# Answer:\nChoreographer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Lyricist -> type.property.expected_type -> Theatrical Lyricist\n# Answer:\nLyricist", "# Reasoning Path:\nAnnie -> film.film.film_set_decoration_by -> Marvin March -> people.person.place_of_birth -> United States of America\n# Answer:\nMarvin March", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Lyricist -> rdf-schema#domain -> Play\n# Answer:\nLyricist", "# Reasoning Path:\nAnnie -> film.film.film_set_decoration_by -> Marvin March -> award.award_nominee.award_nominations -> m.09p9wjv\n# Answer:\nMarvin March", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Lyricist -> rdf-schema#range -> Theatrical Lyricist\n# Answer:\nLyricist", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> type.property.expected_type -> Theatrical Composer\n# Answer:\nComposer", "# Reasoning Path:\nAnnie -> freebase.valuenotation.is_reviewed -> Composer -> freebase.type_hints.included_types -> Topic\n# Answer:\nComposer"], "ground_truth": ["1976-08-10"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0, "path_recall": 0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> common.topic.notable_types -> American football player -> type.type.properties -> Games\n# Answer:\nAmerican football player", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Alabama\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.hud_foreclosure_area.bls_unemployment_rate -> m.07ghzrz\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.education -> m.03gkqtp -> education.education.institution -> Louisiana State University\n# Answer:\nm.03gkqtp", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b66g7msm\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> common.topic.notable_types -> American football player -> type.type.properties -> 40-yard dash time\n# Answer:\nAmerican football player", "# Reasoning Path:\nJaMarcus Russell -> common.topic.notable_types -> American football player -> common.topic.notable_types -> Profession\n# Answer:\nAmerican football player", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.location.containedby -> Area code 251\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> common.topic.notable_types -> American football player -> type.type.properties -> Football Almanac ID\n# Answer:\nAmerican football player", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile -> location.statistical_region.population -> g.11b7tn_fd2\n# Answer:\nMobile"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.aareas.schema.administrative_area.administrative_parent -> New York City\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.administrative_division.country -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Autobiography of Eleanor Roosevelt -> common.topic.notable_types -> Book Edition\n# Answer:\nAutobiography of Eleanor Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Anna Roosevelt Halsted -> people.deceased_person.place_of_death -> New York City\n# Answer:\nAnna Roosevelt Halsted", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Courage in a dangerous world -> common.topic.notable_types -> Publication\n# Answer:\nCourage in a dangerous world", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> United States of America\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> book.author.book_editions_published -> Eleanor and Harry -> common.topic.notable_types -> Book\n# Answer:\nEleanor and Harry", "# Reasoning Path:\nEleanor Roosevelt -> people.person.children -> Elliott Roosevelt -> common.topic.notable_types -> Politician\n# Answer:\nElliott Roosevelt", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nManhattan"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.11b60ptk2z\n# Answer:\ng.11b60ptk2z", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_4m6h\n# Answer:\ng.1245_4m6h", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.12cp_j7n1\n# Answer:\ng.12cp_j7n1", "# Reasoning Path:\nIndonesia -> location.statistical_region.military_expenditure_percent_gdp -> g.1245_67l9\n# Answer:\ng.1245_67l9", "# Reasoning Path:\nIndonesia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.11b60v4rs1\n# Answer:\ng.11b60v4rs1", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc378kt\n# Answer:\ng.1hhc378kt", "# Reasoning Path:\nIndonesia -> location.statistical_region.official_development_assistance -> g.1hhc38d0l\n# Answer:\ng.1hhc38d0l", "# Reasoning Path:\nIndonesia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.12tb6fdlc\n# Answer:\ng.12tb6fdlc", "# Reasoning Path:\nIndonesia -> location.statistical_region.trade_balance_as_percent_of_gdp -> g.1hhc37cr6\n# Answer:\ng.1hhc37cr6"], "ground_truth": ["Protestantism", "Catholicism", "Islam", "Hinduism"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> common.topic.notable_types -> Film -> freebase.type_profile.published -> Published\n# Answer:\nFilm", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Film -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Film -> freebase.type_hints.included_types -> Topic\n# Answer:\nFilm", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Organization founder -> freebase.type_profile.equivalent_topic -> Founder\n# Answer:\nOrganization founder", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Film character -> freebase.type_profile.published -> Published\n# Answer:\nFilm character", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Kansas\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Organization founder -> type.type.properties -> Organizations founded\n# Answer:\nOrganization founder", "# Reasoning Path:\nJesse James -> common.topic.notable_types -> Film character -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nFilm character", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> base.aareas.schema.administrative_area.administrative_children -> Missouri\n# Answer:\nUnited States of America", "# Reasoning Path:\nJesse James -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Kansas\n# Answer:\nUnited States of America"], "ground_truth": ["Firearm"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> visual_art.artwork.artist -> Adalbert J. Volck\n# Answer:\nAdalbert J. Volck", "# Reasoning Path:\nAbraham Lincoln -> visual_art.artwork.artist -> Alexander Gardner\n# Answer:\nAlexander Gardner", "# Reasoning Path:\nAbraham Lincoln -> visual_art.artwork.artist -> Alexander Hessler\n# Answer:\nAlexander Hessler", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pgr_5 -> people.place_lived.location -> Kentucky\n# Answer:\nm.03pgr_5", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.03pn4x_ -> people.place_lived.location -> Springfield\n# Answer:\nm.03pn4x_", "# Reasoning Path:\nAbraham Lincoln -> people.person.places_lived -> m.04hdfss -> people.place_lived.location -> Illinois\n# Answer:\nm.04hdfss"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Coffee with Dickens -> common.topic.notable_types -> Book\n# Answer:\nCoffee with Dickens", "# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Drood: A Novel -> common.topic.notable_types -> Book\n# Answer:\nDrood: A Novel", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Edward Murdstone -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nEdward Murdstone", "# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Coffee with Dickens -> common.topic.article -> m.063st5y\n# Answer:\nCoffee with Dickens", "# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Coffee with Dickens -> book.written_work.part_of_series -> Coffee with... Biographies\n# Answer:\nCoffee with Dickens", "# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Drood: A Novel -> book.written_work.subjects -> The Mystery of Edwin Drood\n# Answer:\nDrood: A Novel", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ebenezer Scrooge -> common.topic.notable_for -> g.125702yyh\n# Answer:\nEbenezer Scrooge", "# Reasoning Path:\nCharles Dickens -> book.book_subject.works -> Drood: A Novel -> book.book.genre -> Fiction\n# Answer:\nDrood: A Novel", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ebenezer Scrooge -> film.film_character.portrayed_in_films -> m.011cd1r7\n# Answer:\nEbenezer Scrooge", "# Reasoning Path:\nCharles Dickens -> influence.influence_node.influenced -> Aldous Huxley -> influence.influence_node.influenced -> Anthony Burgess\n# Answer:\nAldous Huxley"], "ground_truth": ["A Tale of Two Cities (Everyman Paperbacks)", "A Tale of Two Cities (Dramatized)", "A Christmas Carol (Soundings)", "Bleak house", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Ultimate Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Pacemaker Classics)", "A Tale of Two Cities (Naxos AudioBooks)", "A Tale of Two Cities (40th Anniversary Edition)", "Bleak House.", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (Gollancz Children's Classics)", "Dombey and Son", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Family Classics)", "A Tale of Two Cities (The Classic Collection)", "A Christmas Carol (Acting Edition)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Illustrated Junior Library)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Cover to Cover Classics)", "A Christmas Carol (Chrysalis Children's Classics Series)", "A Christmas Carol (The Kennett Library)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Read & Listen Books)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "Great Expectations.", "A Tale of Two Cities (Masterworks)", "A CHRISTMAS CAROL", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Classics Illustrated)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "A Tale of Two Cities (Classics Illustrated)", "Our mutual friend.", "A Tale of Two Cities (Tor Classics)", "The old curiosity shop", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "Dombey and son", "A Christmas Carol (Pacemaker Classic)", "Martin Chuzzlewit", "A Tale of Two Cities (BBC Audio Series)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Green Integer, 50)", "A Christmas Carol (Reissue)", "A Tale of Two Cities (Cyber Classics)", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Bantam Classic)", "Oliver Twist", "A Tale of Two Cities (Student's Novels)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Christmas Carol (Pacemaker Classics)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Oxford Playscripts)", "The old curiosity shop.", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Thornes Classic Novels)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Everyman's Library Classics)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Cp 1135)", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Macmillan Students' Novels)", "A Christmas Carol (Ladybird Children's Classics)", "A Christmas Carol (Children's Classics)", "Great expectations.", "A Tale of Two Cities (Adopted Classic)", "The cricket on the hearth", "Great Expectations", "A Tale Of Two Cities (Adult Classics in Audio)", "Bleak House", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (R)", "A Christmas Carol (Classics for Young Adults and Adults)", "The Mystery of Edwin Drood", "The Pickwick Papers", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Young Reading Series 2)", "A Tale of Two Cities (Isis Clear Type Classic)", "Hard times", "A Tale of Two Cities (Signet Classics)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Classic Fiction)", "A Tale of Two Cities (Saddleback Classics)", "Great expectations", "A Tale of Two Cities (Enriched Classic)", "A Christmas Carol (Cover to Cover)", "Sketches by Boz", "A Tale of Two Cities", "Dombey and Son.", "David Copperfield", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale Of Two Cities (Adult Classics)", "A Christmas Carol (Take Part)", "A Tale of Two Cities (Unabridged Classics)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Classic Retelling)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol", "A Christmas Carol (Children's Theatre Playscript)", "A Christmas Carol (Watermill Classics)", "The Pickwick papers", "A Christmas Carol (Clear Print)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Bookcassette(r) Edition)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Puffin Classics)", "A Christmas Carol (Penguin Student Editions)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (Tor Classics)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Ladybird Classics)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Tale of Two Cities (Dramascripts S.)", "A Tale of Two Cities (Konemann Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Large Print)", "A Christmas Carol (Through the Magic Window Series)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Apple Classics)", "A Tale of Two Cities (Soundings)", "The mystery of Edwin Drood", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Compact English Classics)", "Little Dorrit", "The Old Curiosity Shop", "A Tale of Two Cities (10 Cassettes)", "A Christmas Carol (Whole Story)", "A Christmas Carol (Classic Collection)", "David Copperfield.", "A Christmas Carol (Illustrated Classics)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Christmas Carol (Audio Editions)", "Our mutual friend", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Penguin Popular Classics)", "A Christmas Carol (Classic Fiction)", "A Tale of Two Cities (Penguin Classics)"], "ans_acc": 0.023668639053254437, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.059574468085106386, "path_precision": 0.1, "path_recall": 0.04242424242424243, "path_ans_f1": 0.042328042328042326, "path_ans_precision": 0.2, "path_ans_recall": 0.023668639053254437}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> common.topic.notable_types -> Military Conflict\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> common.topic.notable_types -> Event\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1991 Soviet coup d'\u00e9tat attempt -> common.topic.notable_types -> Military Conflict\n# Answer:\n1991 Soviet coup d'\u00e9tat attempt", "# Reasoning Path:\nSoviet Union -> military.military_combatant.casualties -> m.04mk8w1 -> military.casualties.military_conflict -> Winter War\n# Answer:\nm.04mk8w1", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> military.military_conflict.combatants -> m.064ykvf\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1940\u201344 insurgency in Chechnya -> base.culturalevent.event.entity_involved -> 58th Army\n# Answer:\n1940\u201344 insurgency in Chechnya", "# Reasoning Path:\nSoviet Union -> location.location.events -> 1977 Moscow bombings -> common.topic.article -> m.05b0fkx\n# Answer:\n1977 Moscow bombings", "# Reasoning Path:\nSoviet Union -> military.military_combatant.casualties -> m.04mk8w1 -> military.casualties.type_of_casualties -> Military - Wounded\n# Answer:\nm.04mk8w1", "# Reasoning Path:\nSoviet Union -> location.location.contains -> Almaty Tower -> location.location.containedby -> Almaty\n# Answer:\nAlmaty Tower", "# Reasoning Path:\nSoviet Union -> location.location.contains -> Almaty Tower -> location.location.street_address -> m.0wf986w\n# Answer:\nAlmaty Tower"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Netherlands\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_formerly_used -> Cuba\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> American Samoa\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> location.country.first_level_divisions -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.countries_used -> Bonaire\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.administrative_area.administrative_children -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar -> finance.currency.target_of_exchange -> m.03x8_16\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States, with Territories -> base.aareas.schema.earth.sovereign_domain.sovereign_state -> United States of America\n# Answer:\nUnited States, with Territories", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> base.mystery.cryptid.area_of_occurrence -> Mexico\n# Answer:\nChupacabra", "# Reasoning Path:\nPuerto Rico -> base.mystery.cryptid_area_of_occurrence.cryptid_s_found_here -> Chupacabra -> base.folklore.mythical_creature.similar_mythical_creature_s -> m.05d0ybv\n# Answer:\nChupacabra"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Air crescent sign\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> medicine.risk_factor.diseases -> Ptosis\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.treatments -> Radiation therapy\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Composer -> type.type.properties -> Works Composed\n# Answer:\nComposer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Blumer's shelf\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> common.topic.webpage -> m.09v_zjv\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer -> medicine.disease.symptoms -> Bone pain\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor -> common.topic.notable_types -> Disease or medical condition\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Composer -> people.profession.specialization_of -> Musician\n# Answer:\nComposer", "# Reasoning Path:\nCarl Wilson -> common.topic.notable_types -> Musical Album -> type.type.domain -> Music\n# Answer:\nMusical Album"], "ground_truth": ["Brain tumor", "Lung cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.actor -> David Hasselhoff\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.special_performance_type -> Voice\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.actor -> Patricia McPherson\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 4\n# Answer:\nm.03lj4m5", "# Reasoning Path:\nKnight Rider -> common.topic.webpage -> m.09w0_mw -> common.webpage.in_index -> Entertainment Weekly annotation index\n# Answer:\nm.09w0_mw", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.02h9cb0 -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.02h9cb0", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4lk -> tv.regular_tv_appearance.seasons -> Knight Rider - Season 3\n# Answer:\nm.03lj4lk", "# Reasoning Path:\nKnight Rider -> tv.tv_program.seasons -> Knight Rider - Season 0 -> common.topic.notable_for -> g.125fby47j\n# Answer:\nKnight Rider - Season 0"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.location.containedby -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> freebase.valuenotation.is_reviewed -> Minimum wage\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Tennessee\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee -> location.administrative_division.country -> United States of America\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11b66dwnl4\n# Answer:\ng.11b66dwnl4", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.administrative_division.second_level_division_of -> United States of America\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1ddsd6\n# Answer:\ng.11x1ddsd6", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County -> location.location.containedby -> Nashville-Davidson--Murfreesboro--Franklin, TN Metropolitan Statistical Area\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.statistical_region.population -> g.11x1fr8dg\n# Answer:\ng.11x1fr8dg", "# Reasoning Path:\nBrentwood -> location.location.contains -> Alexander Smith House -> common.topic.notable_types -> Location\n# Answer:\nAlexander Smith House"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.geolocation -> m.0kdmvf\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11b66mljn1\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_types -> Extraterrestrial location\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1chmhk\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Missouri\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.statistical_region.population -> g.11x1ct3wq\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Jasper County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> symbols.name_source.namesakes -> Carver -> common.topic.notable_for -> g.1255959wd\n# Answer:\nCarver", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Diamond -> location.location.containedby -> Newton County\n# Answer:\nDiamond", "# Reasoning Path:\nGeorge Washington Carver -> people.person.profession -> Inventor -> people.profession.specialization_of -> Scientist\n# Answer:\nInventor"], "ground_truth": ["Diamond"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.spouse -> Tracy Pollan\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Aquinnah Kathleen Fox -> common.topic.notable_types -> Person\n# Answer:\nAquinnah Kathleen Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.profession -> Actor\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Esm\u00e9 Annabelle Fox -> people.person.parents -> Tracy Pollan\n# Answer:\nEsm\u00e9 Annabelle Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> people.marriage.location_of_ceremony -> Arlington\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvttg -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nm.07nvttg", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Michael Fox -> people.person.sibling_s -> m.0j217jw\n# Answer:\nSam Michael Fox", "# Reasoning Path:\nMichael J. Fox -> people.person.spouse_s -> m.02kkmrn -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.02kkmrn", "# Reasoning Path:\nMichael J. Fox -> award.award_nominee.award_nominations -> m.07nvtvt -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series\n# Answer:\nm.07nvtvt"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.33333333333333326, "path_precision": 0.3, "path_recall": 0.375, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nm.04yvq68", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nm.04fv9nd", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> g.11bcf3yybd\n# Answer:\ng.11bcf3yybd", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Foot cavalry\n# Answer:\nFoot cavalry", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of Port Republic\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of McDowell\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Northern Virginia Campaign -> common.topic.notable_types -> Military Conflict\n# Answer:\nNorthern Virginia Campaign", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> First Battle of Winchester\n# Answer:\nJackson's Valley Campaign", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain -> time.event.included_in_event -> Northern Virginia Campaign\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> military.military_conflict.military_personnel_involved -> Adolph von Steinwehr\n# Answer:\nJackson's Valley Campaign"], "ground_truth": ["Romney Expedition", "American Civil War", "Battle of White Oak Swamp", "First Battle of Kernstown", "Battle of Chancellorsville", "Battle of Cedar Mountain", "Battle of Front Royal", "How Few Remain", "Jackson's Valley Campaign", "Battle of Hancock", "Battle of Chantilly", "Battle of Hoke's Run", "Battle of Harpers Ferry", "Manassas Station Operations", "Battle of McDowell", "Battle of Port Republic", "First Battle of Rappahannock Station", "Second Battle of Bull Run", "First Battle of Winchester"], "ans_acc": 0.3157894736842105, "ans_hit": 1, "ans_f1": 0.17391304347826086, "ans_precission": 0.5, "ans_recall": 0.10526315789473684, "path_f1": 0.16216216216216217, "path_precision": 0.3, "path_recall": 0.1111111111111111, "path_ans_f1": 0.43523316062176165, "path_ans_precision": 0.7, "path_ans_recall": 0.3157894736842105}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> freebase.type_profile.equivalent_topic -> Ethnic group\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Kenya\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_for -> g.1256fv3pv\n# Answer:\ng.1256fv3pv", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Geographic distribution\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Included in group(s)\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> language.human_language.countries_spoken_in -> Tanzania\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> base.rosetta.languoid.parent -> Ongamo-Maa Group\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> common.topic.notable_types -> Ethnicity -> type.type.properties -> Includes group(s)\n# Answer:\nEthnicity", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language -> common.topic.article -> m.064klf\n# Answer:\nMaasai Language"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6153846153846153, "ans_precission": 0.4444444444444444, "ans_recall": 1.0, "path_f1": 0.6153846153846153, "path_precision": 0.4444444444444444, "path_recall": 1.0, "path_ans_f1": 0.6153846153846153, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.notable_types -> Deceased Person\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> Spouse\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> common.topic.image -> WilliamFranklin\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Francis Folger Franklin -> common.topic.notable_types -> Deceased Person\n# Answer:\nFrancis Folger Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> To\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.gender -> Male\n# Answer:\nWilliam Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> common.topic.notable_types -> Deceased Person\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> people.marriage.type_of_union -> Common-law marriage\n# Answer:\nm.0j4kb46", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Sarah Franklin Bache -> people.person.children -> Benjamin Franklin Bache\n# Answer:\nSarah Franklin Bache", "# Reasoning Path:\nBenjamin Franklin -> people.person.spouse_s -> m.0j4kb46 -> freebase.valuenotation.is_reviewed -> From\n# Answer:\nm.0j4kb46"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.domain -> Film\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.expected_by -> Actor\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> base.descriptive_names.names.descriptive_name -> m.010h52df\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> base.descriptive_names.names.descriptive_name -> m.010h52h5\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Patrick Swayze 2006 -> common.image.size -> m.0291zyw\n# Answer:\nPatrick Swayze 2006", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> base.descriptive_names.names.descriptive_name -> m.0118g900\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.expected_by -> Actor Name\n# Answer:\nFilm actor", "# Reasoning Path:\nPatrick Swayze -> common.topic.image -> Swayze2 -> common.image.size -> m.03sznp2\n# Answer:\nSwayze2", "# Reasoning Path:\nPatrick Swayze -> common.topic.notable_types -> Film actor -> type.type.expected_by -> Film\n# Answer:\nFilm actor"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> book.book_subject.works -> Raphael and his age\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing -> common.topic.subject_of -> Corporate Art Painttwits Style\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Leonardo da Vinci and the Art of Sculpture: Inspiration and Invention\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting -> people.profession.people_with_this_profession -> Sandro Botticelli\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Adoration of the Magi -> visual_art.artwork.art_subject -> Biblical Magi\n# Answer:\nAdoration of the Magi", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Antonio da Correggio -> common.topic.notable_for -> g.125c82t9n\n# Answer:\nAntonio da Correggio", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bernardino Luini -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nBernardino Luini", "# Reasoning Path:\nLeonardo da Vinci -> influence.influence_node.influenced -> Bob Kane -> comic_books.comic_book_creator.characters_created -> Bruce Wayne\n# Answer:\nBob Kane", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper -> visual_art.artwork.art_subject -> Jesus Christ\n# Answer:\nThe Last Supper"], "ground_truth": ["Leda and the Swan", "g.12314dm1", "Lucan portrait of Leonardo da Vinci", "Adoration of the Magi", "Horse and Rider", "The Virgin and Child with St Anne and St John the Baptist", "Salvator Mundi", "The Virgin and Child with St. Anne", "Leonardo's horse", "Medusa", "Madonna of the Yarnwinder", "Lady with an Ermine", "Portrait of Isabella d'Este", "Virgin of the Rocks", "Sala delle Asse", "The Baptism of Christ", "g.12215rxg", "Bacchus", "Madonna and Child with St Joseph", "La belle ferronni\u00e8re", "Madonna of the Carnation", "Head of a Woman", "g.120vt1gz", "Annunciation", "g.121yh91r", "Ginevra de' Benci", "Portrait of a Young Fianc\u00e9e", "Portrait of a Musician", "The Battle of Anghiari", "g.1239jd9p", "g.1213jb_b", "Madonna Litta", "Vitruvian Man", "Madonna of Laroque", "Mona Lisa", "g.1219sb0g", "g.121wt37c", "St. Jerome in the Wilderness", "St. John the Baptist", "Portrait of a man in red chalk", "The Holy Infants Embracing", "g.1224tf0c", "Benois Madonna", "Drapery for a Seated Figure", "The Last Supper"], "ans_acc": 0.044444444444444446, "ans_hit": 1, "ans_f1": 0.07272727272727272, "ans_precission": 0.2, "ans_recall": 0.044444444444444446, "path_f1": 0.07407407407407407, "path_precision": 0.2, "path_recall": 0.045454545454545456, "path_ans_f1": 0.07272727272727272, "path_ans_precision": 0.2, "path_ans_recall": 0.044444444444444446}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna -> common.topic.notable_types -> City/Town/Village\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> travel.travel_destination.tour_operators -> Adventures by Disney\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna -> common.topic.notable_types -> City/Town/Village\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_1hyf\n# Answer:\ng.1245_1hyf", "# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Carinthia -> common.topic.notable_types -> Administrative Division\n# Answer:\nCarinthia", "# Reasoning Path:\nAustria -> location.statistical_region.co2_emissions_per_capita -> g.1245_2gxk\n# Answer:\ng.1245_2gxk", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Burgenland -> base.aareas.schema.administrative_area.administrative_area_type -> Austrian state\n# Answer:\nBurgenland", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Carinthia -> location.location.partially_contains -> Coglians\n# Answer:\nCarinthia"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Major depression\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Ovarian cancer\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Anemia\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> medicine.risk_factor.diseases -> Bacterial arthritis\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04d7nvm\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> medicine.risk_factor.diseases -> Lupus erythematosus\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.person.gender -> Female -> common.topic.notable_types -> Literature Subject\n# Answer:\nFemale", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> book.book_subject.works -> A Time to Mend\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer -> base.bioventurist.bv_medical_condition.therapeutics -> m.04yjf_c\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer -> medicine.symptom.symptom_of -> HIV/AIDS\n# Answer:\nCervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> type.type.properties -> Religious Leadership\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> freebase.type_hints.included_types -> Person\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Abhiyaza\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_for -> g.12582gf_5\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> base.descriptive_names.names.descriptive_name -> m.011mcpf3\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> common.topic.notable_types -> City/Town/Village\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> freebase.type_hints.included_types -> Topic\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> Nanda\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> common.topic.notable_types -> Religious Leader -> base.descriptive_names.names.descriptive_name -> m.011mcpk6\n# Answer:\nReligious Leader", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu -> location.location.people_born_here -> R\u0101hula\n# Answer:\nKapilavastu"], "ground_truth": ["Nepal"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.notable_types -> Invention\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove -> common.topic.article -> m.029mn3\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> medicine.medical_treatment.used_to_treat -> Presbyopia\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals -> common.topic.image -> A bifocal corrective eyeglasses lens\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> government.politician.party -> m.012zbkk5 -> government.political_party_tenure.party -> Independent\n# Answer:\nm.012zbkk5", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_trwk\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> common.topic.image -> Glass\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica -> music.performance_role.track_performances -> m.010_ty__\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> BenFranklinDuplessis -> common.image.size -> m.02fglp1\n# Answer:\nBenFranklinDuplessis", "# Reasoning Path:\nBenjamin Franklin -> common.topic.image -> Benjamin Franklin by Jean-Baptiste Greuze -> common.image.appears_in_topic_gallery -> :Library and information science\n# Answer:\nBenjamin Franklin by Jean-Baptiste Greuze"], "ground_truth": ["Franklin stove", "Bifocals", "Glass harmonica", "Lightning rod"], "ans_acc": 0.75, "ans_hit": 1, "ans_f1": 0.7241379310344827, "ans_precission": 0.7, "ans_recall": 0.75, "path_f1": 0.7241379310344827, "path_precision": 0.7, "path_recall": 0.75, "path_ans_f1": 0.7241379310344827, "path_ans_precision": 0.7, "path_ans_recall": 0.75}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> common.topic.notable_types -> College/University\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> common.topic.notable_types -> US County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.contains -> Aims Community College -> common.topic.notable_types -> College/University\n# Answer:\nAims Community College", "# Reasoning Path:\nGreeley -> location.location.contains -> Bank of Colorado Arena -> common.topic.notable_types -> Sports Facility\n# Answer:\nBank of Colorado Arena", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> education.educational_institution.faculty -> m.0k7sk9c\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado -> location.location.containedby -> United States of America\n# Answer:\nColorado", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America -> location.location.containedby -> United States, with Territories\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.people_born_here -> Alyssa Peterson -> common.topic.notable_types -> Military Person\n# Answer:\nAlyssa Peterson", "# Reasoning Path:\nGreeley -> location.location.contains -> Academy of Natural Therapy -> education.university.domestic_tuition -> m.0k7gb23\n# Answer:\nAcademy of Natural Therapy", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County -> location.location.adjoin_s -> m.03jpsq_\n# Answer:\nWeld County"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.properties -> Works Composed\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Music\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> people.profession.specialization_of -> Writer\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> music.group_member.instruments_played -> Piano\n# Answer:\nPiano", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> common.topic.notable_types -> Profession\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Librettist -> common.topic.notable_types -> Profession\n# Answer:\nLibrettist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> book.author.works_written -> Andante cantabile from quartet in D major, op. 11 -> common.topic.notable_types -> Book\n# Answer:\nAndante cantabile from quartet in D major, op. 11", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> base.descriptive_names.names.descriptive_name -> m.01260py_\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician -> people.profession.specializations -> Composer\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer -> type.type.expected_by -> Arranger\n# Answer:\nComposer"], "ground_truth": ["Librettist", "Musician", "Composer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.5, "path_precision": 0.4, "path_recall": 0.6666666666666666, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> base.aareas.schema.administrative_area.administrative_parent -> Earth\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Belgium\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Liechtenstein\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.location.containedby -> Europe\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.local_name.locale -> Switzerland\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium -> olympics.olympic_participating_country.olympics_participated_in -> 1900 Summer Olympics\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.location.containedby -> Central Europe\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein -> location.location.containedby -> Eurasia\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> base.rosetta.languoid.local_name -> German, Standard -> base.rosetta.rosetta_document.document_class -> Universal Declaration of Human Rights\n# Answer:\nGerman, Standard", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria -> location.location.containedby -> Eurasia\n# Answer:\nAustria"], "ground_truth": ["Switzerland", "Germany", "Luxembourg", "Austria", "Belgium", "East Germany", "Liechtenstein"], "ans_acc": 0.5714285714285714, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.6, "ans_recall": 0.42857142857142855, "path_f1": 0.5, "path_precision": 0.6, "path_recall": 0.42857142857142855, "path_ans_f1": 0.6990291262135921, "path_ans_precision": 0.9, "path_ans_recall": 0.5714285714285714}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> common.topic.notable_types -> Musical genre\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music -> music.genre.subgenre -> Math rock\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Pop rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> common.topic.notable_types -> Musical genre\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.recordings -> Chasing a Feeling\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock and roll\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> broadcast.artist.content -> Big R Radio - Soft Hits -> common.topic.image -> Big R radio\n# Answer:\nBig R Radio - Soft Hits", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock -> music.genre.parent_genre -> Rock music\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock -> music.genre.parent_genre -> Progressive rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock -> music.genre.subgenre -> Post-rock\n# Answer:\nArt rock"], "ground_truth": ["Blues rock", "Psychedelic rock", "Soft rock", "Pop rock", "Experimental rock", "Rock music", "Pop music", "Art rock", "Experimental music"], "ans_acc": 0.5555555555555556, "ans_hit": 1, "ans_f1": 0.48648648648648646, "ans_precission": 0.9, "ans_recall": 0.3333333333333333, "path_f1": 0.48648648648648646, "path_precision": 0.9, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6870229007633587, "path_ans_precision": 0.9, "path_ans_recall": 0.5555555555555556}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> freebase.valuenotation.has_no_value -> To\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> Basic title\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.02_wstr -> government.government_position_held.office_holder -> Wayne Allard\n# Answer:\nm.02_wstr", "# Reasoning Path:\nColorado -> location.location.partially_contains -> Colorado River -> geography.river.basin_countries -> Mexico\n# Answer:\nColorado River", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.legislative_sessions -> 111th United States Congress\n# Answer:\nm.05kfbwl", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> freebase.valuenotation.is_reviewed -> District represented (if position is district-related)\n# Answer:\nm.05kh0nn", "# Reasoning Path:\nColorado -> military.military_unit_place_of_origin.military_units -> 1st Colorado Cavalry Regiment -> common.topic.notable_types -> Military unit\n# Answer:\n1st Colorado Cavalry Regiment", "# Reasoning Path:\nColorado -> location.location.partially_contains -> Colorado River -> common.topic.notable_types -> River\n# Answer:\nColorado River"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.30769230769230765, "path_precision": 0.2, "path_recall": 0.6666666666666666, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Europe\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Capital Region of Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark -> location.location.contains -> Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries -> common.topic.notable_types -> Region\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> location.statistical_region.gdp_real -> g.1hhc37psk\n# Answer:\ng.1hhc37psk", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.location.containedby -> Nordic countries\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.form_of_government -> Constitutional monarchy\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> North America -> location.location.containedby -> Americas\n# Answer:\nNorth America", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark -> location.country.administrative_divisions -> Central Denmark Region\n# Answer:\nDenmark"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_types -> Postal Code\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_types -> Postal Code\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98103 -> common.topic.notable_types -> Postal Code\n# Answer:\n98103", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98101 -> common.topic.notable_for -> g.125h4fcxl\n# Answer:\n98101", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> common.topic.notable_types -> Neighborhood\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11b66b70n7\n# Answer:\ng.11b66b70n7", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98102 -> common.topic.notable_for -> g.125f2tsfn\n# Answer:\n98102", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Alki Point -> location.neighborhood.neighborhood_of -> Seattle-Tacoma-Bellevue, WA Metropolitan Statistical Area\n# Answer:\nAlki Point", "# Reasoning Path:\nSeattle -> location.place_with_neighborhoods.neighborhoods -> Arbor Heights -> common.topic.notable_types -> Neighborhood\n# Answer:\nArbor Heights", "# Reasoning Path:\nSeattle -> location.statistical_region.population -> g.11btt54h7d\n# Answer:\ng.11btt54h7d"], "ground_truth": ["98181", "98154", "98104", "98141", "98106", "98109", "98195", "98103", "98119-4114", "98126", "98160", "98188", "98185", "98175", "98108", "98194", "98164", "98101", "98191", "98158", "98139", "98171", "98112", "98177", "98131", "98144", "98146", "98102", "98114", "98129", "98174", "98161", "98116", "98134", "98145", "98113", "98199", "98133", "98166", "98148", "98117", "98136", "98115", "98132", "98119", "98111", "98168", "98178", "98127", "98105", "98170", "98190", "98155", "98198", "98122", "98184", "98124", "98121", "98118", "98107", "98165", "98138", "98125"], "ans_acc": 0.047619047619047616, "ans_hit": 1, "ans_f1": 0.08695652173913042, "ans_precission": 0.5, "ans_recall": 0.047619047619047616, "path_f1": 0.08695652173913042, "path_precision": 0.5, "path_recall": 0.047619047619047616, "path_ans_f1": 0.08695652173913042, "path_ans_precision": 0.5, "path_ans_recall": 0.047619047619047616}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Adrienne Banfield-Jones\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> people.person.parents -> Robsol Pinkett, Jr.\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> common.topic.image -> Pinkett-Smith\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_ss\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.parents -> Caroline Bright\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Contemporary R&B -> broadcast.genre.content -> #Musik.Main on RauteMusik.FM\n# Answer:\n#Musik.Main on RauteMusik.FM", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance music -> common.topic.notable_types -> Media genre\n# Answer:\nDance music", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Pinkett Smith -> tv.tv_actor.guest_roles -> m.09nq_sy\n# Answer:\nJada Pinkett Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith -> people.person.children -> Jaden Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> music.artist.genre -> Dance-pop -> common.topic.notable_types -> Musical genre\n# Answer:\nDance-pop"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.6666666666666666, "path_precision": 0.5, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> common.topic.notable_types -> Human Language -> freebase.type_profile.equivalent_topic -> Human language\n# Answer:\nHuman Language", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> common.topic.notable_types -> Language Writing System\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.region -> East Asia -> common.topic.notable_types -> Region\n# Answer:\nEast Asia", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_types -> Language Writing System\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.notable_types -> Language Writing System\n# Answer:\nN\u00fcshu script", "# Reasoning Path:\nChinese language -> common.topic.notable_types -> Human Language -> freebase.type_profile.kind -> Definition\n# Answer:\nHuman Language", "# Reasoning Path:\nChinese language -> common.topic.notable_types -> Human Language -> type.type.expected_by -> Language\n# Answer:\nHuman Language", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters -> language.language_writing_system.languages -> Chinese, Hakka Language\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> 'Phags-pa script -> common.topic.notable_for -> g.1258512fl\n# Answer:\n'Phags-pa script", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> N\u00fcshu script -> common.topic.article -> m.014lbq\n# Answer:\nN\u00fcshu script"], "ground_truth": ["N\u00fcshu script", "'Phags-pa script", "Traditional Chinese characters", "Simplified Chinese character", "Chinese characters"], "ans_acc": 0.6, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 0.6, "ans_recall": 0.6, "path_f1": 0.6, "path_precision": 0.6, "path_recall": 0.6, "path_ans_f1": 0.6, "path_ans_precision": 0.6, "path_ans_recall": 0.6}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> common.topic.notable_types -> Organization leader\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.parents -> Pat Nixon\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Tricia Nixon Cox -> people.person.ethnicity -> White American\n# Answer:\nTricia Nixon Cox", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.parents -> Pat Nixon\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.spouse_s -> m.0j4k1q4\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Julie Nixon Eisenhower -> people.person.employment_history -> m.0k0dcyp\n# Answer:\nJulie Nixon Eisenhower", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsgl5 -> tv.tv_guest_role.episodes_appeared_in -> The Future of the GOP\n# Answer:\nm.09nsgl5", "# Reasoning Path:\nRichard Nixon -> tv.tv_actor.guest_roles -> m.09nsglb -> tv.tv_guest_role.episodes_appeared_in -> The American Film Institute Salute to James Cagney\n# Answer:\nm.09nsglb"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.23529411764705882, "path_precision": 0.2, "path_recall": 0.2857142857142857, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.award_nominee -> Sherman Hemsley\n# Answer:\nm.07nvvbj", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmd5r -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nm.0zbmd5r", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtpx -> award.award_nomination.award_nominee -> Isabel Sanford\n# Answer:\nm.07shtpx", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmd5r -> award.award_nomination.award_nominee -> Sherman Hemsley\n# Answer:\nm.0zbmd5r", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmd5r -> freebase.valuenotation.is_reviewed -> Award\n# Answer:\nm.0zbmd5r", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmd5r -> award.award_nomination.award -> TV Land Favorite Cantankerous Couple Award\n# Answer:\nm.0zbmd5r", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_yczwx -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nm.0_yczwx", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07nvvbj -> award.award_nomination.ceremony -> 36th Primetime Emmy Awards\n# Answer:\nm.07nvvbj", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07shtpx -> award.award_nomination.award -> Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series\n# Answer:\nm.07shtpx", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.seasons -> The Jeffersons - Season 0 -> tv.tv_series_season.episodes -> Entertainment Tonight: Whatever Happened To...\n# Answer:\nThe Jeffersons - Season 0"], "ground_truth": ["Mike Evans", "Paul Benedict", "Roxie Roker", "Isabel Sanford", "Jay Hammer", "Berlinda Tolbert", "Zara Cully", "Franklin Cover", "Sherman Hemsley", "Marla Gibbs", "Damon Evans"], "ans_acc": 0.2727272727272727, "ans_hit": 1, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0.14492753623188406, "path_precision": 0.5, "path_recall": 0.0847457627118644, "path_ans_f1": 0.3529411764705882, "path_ans_precision": 0.5, "path_ans_recall": 0.2727272727272727}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco News-Call Bulletin Newspaper -> common.topic.notable_types -> Newspaper\n# Answer:\nSan Francisco News-Call Bulletin Newspaper", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek -> common.topic.notable_types -> Newspaper\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco News-Call Bulletin Newspaper -> book.newspaper.headquarters -> m.0j1grx7\n# Answer:\nSan Francisco News-Call Bulletin Newspaper", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Bay Area Reporter -> book.periodical.language -> English Language\n# Answer:\nBay Area Reporter", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1906 -> common.topic.notable_types -> Book\n# Answer:\n1906", "# Reasoning Path:\nSan Francisco -> location.citytown.postal_codes -> 94101 -> common.topic.notable_types -> Postal Code\n# Answer:\n94101", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 1st to Die -> common.topic.notable_types -> Book\n# Answer:\n1st to Die", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> AsianWeek -> book.periodical.final_issue_date -> m.0h_nn62\n# Answer:\nAsianWeek", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Bay Area Reporter -> common.topic.article -> m.0f83d3\n# Answer:\nBay Area Reporter", "# Reasoning Path:\nSan Francisco -> book.book_subject.works -> 2nd Chance -> common.topic.notable_types -> Book\n# Answer:\n2nd Chance"], "ground_truth": ["Synapse", "The Golden Era", "San Francisco Business Times", "Bay Area Reporter", "Free Society", "Sing Tao Daily", "San Francisco Chronicle", "AsianWeek", "California Star", "Street Sheet", "Dock of the Bay", "San Francisco Daily", "The San Francisco Examiner", "San Francisco Call", "San Francisco Bay View", "San Francisco Bay Guardian", "San Francisco News-Call Bulletin Newspaper", "The Daily Alta California", "San Francisco Bay Times", "San Francisco Foghorn"], "ans_acc": 0.15, "ans_hit": 1, "ans_f1": 0.24, "ans_precission": 0.6, "ans_recall": 0.15, "path_f1": 0.24, "path_precision": 0.6, "path_recall": 0.15, "path_ans_f1": 0.24, "path_ans_precision": 0.6, "path_ans_recall": 0.15}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Europe\n# Answer:\nEarth", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Belgium\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> location.location.containedby -> Northern Hemisphere\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.aareas.schema.administrative_area.administrative_parent -> Earth -> base.locations.planets.continents_within -> Asia\n# Answer:\nEarth", "# Reasoning Path:\nArmenia -> location.location.containedby -> Roman Catholic Archdiocese of Manizales -> common.topic.notable_types -> Religious Jurisdiction\n# Answer:\nRoman Catholic Archdiocese of Manizales", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Denmark\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> base.locations.continents.countries_within -> Azerbaijan\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia -> organization.organization_scope.organizations_with_this_scope -> Commonwealth of Independent States\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe -> base.locations.continents.countries_within -> Georgia\n# Answer:\nEurope"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Amphetamine\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Systemic lupus erythematosus\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aneurysm\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> heart attack\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.disease.causes -> Aortic dissection\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Coronary artery disease\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.person.gender -> Male -> medicine.risk_factor.diseases -> Aortic aneurysm\n# Answer:\nMale", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> medicine.symptom.symptom_of -> Myocardial Ischemia\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w0hpl\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack -> common.topic.webpage -> m.09w4lwz\n# Answer:\nheart attack"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.888888888888889, "path_precision": 0.8, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> media_common.literary_genre.books_in_this_genre -> On the origin of species by means of natural selection\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.country.capital -> London\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Biology -> people.professional_field.professions_in_this_field -> Biologist\n# Answer:\nBiology", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> base.aareas.schema.administrative_area.administrative_parent -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> common.topic.notable_types -> Book -> freebase.type_hints.included_types -> Topic\n# Answer:\nBook", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> Great Britain -> location.location.contains -> London\n# Answer:\nGreat Britain", "# Reasoning Path:\nCharles Darwin -> book.written_work.subjects -> England -> location.administrative_division.first_level_division_of -> United Kingdom\n# Answer:\nEngland", "# Reasoning Path:\nCharles Darwin -> exhibitions.exhibition_subject.exhibitions_created_about_this_subject -> Darwin -> common.topic.notable_for -> g.1254zbncs\n# Answer:\nDarwin", "# Reasoning Path:\nCharles Darwin -> common.topic.notable_types -> Book -> freebase.type_profile.strict_included_types -> Written Work\n# Answer:\nBook"], "ground_truth": ["The Correspondence of Charles Darwin, Volume 8: 1860", "The Life and Letters of Charles Darwin Volume 1", "Geological Observations on South America", "The Correspondence of Charles Darwin, Volume 9: 1861", "Wu zhong qi yuan", "Leben und Briefe von Charles Darwin", "Gesammelte kleinere Schriften", "Cartas de Darwin 18251859", "Motsa ha-minim", "On the Movements and Habits of Climbing Plants", "The Autobiography of Charles Darwin", "On a remarkable bar of sandstone off Pernambuco", "The Correspondence of Charles Darwin, Volume 13: 1865", "The geology of the voyage of H.M.S. Beagle", "The\u0301orie de l'e\u0301volution", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Correspondence of Charles Darwin, Volume 15: 1867", "The action of carbonate of ammonia on the roots of certain plants", "The Different Forms of Flowers on Plants of the Same Species", "Diary of the voyage of H.M.S. Beagle", "Opsht\u0323amung fun menshen", "The principal works", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "genese\u014ds t\u014dn eid\u014dn", "Die geschlechtliche Zuchtwahl", "Darwinism stated by Darwin himself", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Voyage of the Beagle", "Les moyens d'expression chez les animaux", "Tesakneri tsagume\u030c", "Memorias y epistolario i\u0301ntimo", "Evolution and natural selection", "Origins", "Fertilisation of Orchids", "The Correspondence of Charles Darwin, Volume 10: 1862", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "A Darwin Selection", "Reise eines Naturforschers um die Welt", "red notebook of Charles Darwin", "The Correspondence of Charles Darwin, Volume 14: 1866", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 18: 1870", "Charles Darwin's letters", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "Reise um die Welt 1831 - 36", "The Darwin Reader Second Edition", "The collected papers of Charles Darwin", "Darwin on humus and the earthworm", "Darwin from Insectivorous Plants to Worms", "South American Geology", "vari\u00eberen der huisdieren en cultuurplanten", "Rejse om jorden", "Darwin for Today", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "The Life and Letters of Charles Darwin Volume 2", "The Correspondence of Charles Darwin, Volume 17: 1869", "The voyage of Charles Darwin", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The portable Darwin", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Darwin's Ornithological notes", "On evolution", "Part I: Contributions to the Theory of Natural Selection / Part II", "On the origin of species by means of natural selection", "Darwin-Wallace", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Expression of the Emotions in Man and Animals", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Structure and Distribution of Coral Reefs", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 16: 1868", "Notebooks on transmutation of species", "La facult\u00e9 motrice dans les plantes", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "On the tendency of species to form varieties", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Metaphysics, Materialism, & the evolution of mind", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Evolution by natural selection", "monograph on the sub-class Cirripedia", "Volcanic Islands", "Beagle letters", "Darwin en Patagonia", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Charles Darwin's natural selection", "The education of Darwin", "The Power of Movement in Plants", "To the members of the Down Friendly Club", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Notes on the fertilization of orchids", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Del Plata a Tierra del Fuego", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "The Orgin of Species", "ontstaan der soorten door natuurlijke teeltkeus", "Darwin's journal", "The Variation of Animals and Plants under Domestication", "Darwin and Henslow", "The Formation of Vegetable Mould through the Action of Worms", "The foundations of the Origin of species", "Charles Darwin on the routes of male humble bees", "A student's introduction to Charles Darwin", "Human nature, Darwin's view", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "On Natural Selection", "Kleinere geologische Abhandlungen", "Les mouvements et les habitudes des plantes grimpantes", "More Letters of Charles Darwin", "From Darwin's unpublished notebooks", "Diario del Viaje de Un Naturalista Alrededor", "The Darwin Reader First Edition", "Darwin", "Questions about the breeding of animals", "Monographs of the fossil Lepadidae and the fossil Balanidae", "Het uitdrukken van emoties bij mens en dier", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Voyage d'un naturaliste autour du monde", "Insectivorous Plants", "The Correspondence of Charles Darwin, Volume 12: 1864", "Evolutionary Writings: Including the Autobiographies", "Die fundamente zur entstehung der arten", "Darwin's insects", "Darwin Compendium", "Charles Darwin's marginalia", "Evolution", "The Descent of Man, and Selection in Relation to Sex", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Resa kring jorden", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "The Correspondence of Charles Darwin, Volume 11: 1863", "Darwin's notebooks on transmutation of species", "The Essential Darwin", "El Origin De Las Especies", "Darwin Darwin", "La vie et la correspondance de Charles Darwin", "From so simple a beginning", "Works", "The living thoughts of Darwin", "Charles Darwin", "Geological Observations on the Volcanic Islands", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "H.M.S. Beagle in South America"], "ans_acc": 0.026143790849673203, "ans_hit": 1, "ans_f1": 0.012269938650306749, "ans_precission": 0.1, "ans_recall": 0.006535947712418301, "path_f1": 0.19999999999999998, "path_precision": 1.0, "path_recall": 0.1111111111111111, "path_ans_f1": 0.05095541401273886, "path_ans_precision": 1.0, "path_ans_recall": 0.026143790849673203}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> base.biblioness.bibs_location.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> common.image.appears_in_topic_gallery -> United States presidential election, 1972 -> time.event.locations -> United States of America\n# Answer:\nUnited States presidential election, 1972", "# Reasoning Path:\nRichard Nixon -> common.image.appears_in_topic_gallery -> United States presidential election, 1972 -> government.election.campaigns -> Richard Nixon presidential campaign, 1972\n# Answer:\nUnited States presidential election, 1972", "# Reasoning Path:\nRichard Nixon -> common.image.appears_in_topic_gallery -> United States presidential election, 1968 -> time.event.locations -> United States of America\n# Answer:\nUnited States presidential election, 1968", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> New York City -> location.administrative_division.country -> United States of America\n# Answer:\nNew York City", "# Reasoning Path:\nRichard Nixon -> common.image.appears_in_topic_gallery -> United States presidential election, 1972 -> government.election.office -> President of the United States\n# Answer:\nUnited States presidential election, 1972", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.010pgj7k -> film.personal_film_appearance.film -> End of the Road: How Money Became Worthless\n# Answer:\nm.010pgj7k", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.02shm15 -> film.personal_film_appearance.type_of_appearance -> Archive Footage\n# Answer:\nm.02shm15", "# Reasoning Path:\nRichard Nixon -> common.image.appears_in_topic_gallery -> United States presidential election, 1968 -> common.topic.image -> Nixon 30-0316a\n# Answer:\nUnited States presidential election, 1968", "# Reasoning Path:\nRichard Nixon -> film.person_or_entity_appearing_in_film.films -> m.0_0cs2_ -> film.personal_film_appearance.film -> Our Nixon\n# Answer:\nm.0_0cs2_"], "ground_truth": ["New York City"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_hints.included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.strict_included_types -> Topic\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> common.topic.notable_types -> Musical Artist -> freebase.type_profile.equivalent_topic -> Musician\n# Answer:\nMusical Artist", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.notable_types -> Profession\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.appears_in_topic_gallery -> Grammy Legend Award\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Music executive -> common.topic.notable_types -> Profession\n# Answer:\nMusic executive", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> A man in a light-colored suit sings into a microphone -> common.image.size -> m.0kjrkq\n# Answer:\nA man in a light-colored suit sings into a microphone", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.appears_in_topic_gallery -> Zubin Mehta\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees", "# Reasoning Path:\nSmokey Robinson -> people.person.profession -> Actor -> common.topic.subjects -> Bleona\n# Answer:\nActor", "# Reasoning Path:\nSmokey Robinson -> common.topic.image -> Zubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees -> common.image.size -> m.02cljr8\n# Answer:\nZubin Mehta laughs with singers Dolly Parton and William Smokey Robinson during a reception for the Kennedy Center honorees"], "ground_truth": ["When A Woman Cries", "I Can't Get Enough", "My Guy", "My World", "Shop Around", "You Are Forever", "Open", "My Girl", "Jesus Told Me To Love You", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "Shoe Soul", "I Like Your Face", "Easy", "I'm Glad There Is You", "Tracks of My Tears", "Ooh Baby Baby", "Wanna Know My Mind", "Take Me Through The Night", "I Am, I Am", "Tell Me Tomorrow, Part 1", "(It's The) Same Old Love", "More Than You Know", "Wedding Song", "You Really Got a Hold on Me", "You Are So Beautiful (feat. Dave Koz)", "Just Another Kiss", "Just Like You", "The Track of My Tears", "Love So Fine", "Fly Me to the Moon (In Other Words)", "Tell Me Tomorrow (12\\\" extended mix)", "A Silent Partner in a Three-Way Love Affair", "Ooo Baby Baby (live)", "I Love Your Face", "What's Too Much", "We\u2019ve Come Too Far to End It Now", "Going to a Go-Go", "The Agony And The Ecstasy", "Fulfill Your Need", "The Road to Damascus", "I Have Prayed On It", "The Tracks of My Tears", "The Tracks Of My Tears", "If You Can Want", "Pops, We Love You (disco)", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Why", "Be Careful What You Wish For (instrumental)", "Just Passing Through", "Noel", "Same Old Love", "I Second That Emotion", "Everything for Christmas", "I Can't Find", "Why Are You Running From My Love", "Time Flies", "It's Christmas Time", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "You've Really Go a Hold on Me", "Girl I'm Standing There", "Yester Love", "I\u2019ve Got You Under My Skin", "Driving Thru Life in the Fast Lane", "Quiet Storm", "Save Me", "Satisfy You", "The Way You Do (The Things You Do)", "Did You Know (Berry's Theme)", "Daylight & Darkness", "Gone Forever", "Rack Me Back", "Happy (Love Theme From Lady Sings the Blues)", "Be Kind to the Growing Mind", "I Want You Back", "Going to a Gogo", "Just to See Her", "The Tracks of My Heart", "Heavy On Pride (Light On Love)", "Hold on to Your Love", "Blame It On Love (Duet with Barbara Mitchell)", "Why Do Happy Memories Hurt So Bad", "I Hear The Children Singing", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Never My Love / Never Can Say Goodbye", "Come to Me Soon", "Rewind", "Night and Day", "One Heartbeat", "And I Don't Love You", "He Can Fix Anything", "Being With You", "Who's Sad", "You Don't Know What It's Like", "Ever Had A Dream", "There Will Come a Day (I'm Gonna Happen to You)", "Te Quiero Como Si No Hubiera Un Manana", "Our Love Is Here to Stay", "I Praise & Worship You Father", "Christmas Greeting", "Santa Claus is Coming to Town", "I Am I Am", "No Time to Stop Believing", "I've Got You Under My Skin", "Will You Still Love Me Tomorrow", "With Your Love Came", "It's Time to Stop Shoppin' Around", "Let Your Light Shine On Me", "Tracks Of My Tears (Live)", "The Hurt's On You", "Love Bath", "Unless You Do It Again", "Really Gonna Miss You", "Just To See Her Again", "Quiet Storm (Groove Boutique remix)", "Be Who You Are", "Be Kind To The Growing Mind (with The Temptations)", "Cruisin'", "Christmas Every Day", "Crusin'", "Let Me Be the Clock", "Winter Wonderland", "Just My Soul Responding", "Love Is The Light", "It's A Good Night", "Quiet Storm (Groove Boutique Chill Jazz mix)", "I Can\u2019t Stand to See You Cry (Commercial version)", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Mickey's Monkey", "There Will Come A Day ( I'm Gonna Happen To You )", "Standing On Jesus", "Quiet Storm (single version)", "Come by Here (Kum Ba Ya)", "A Child Is Waiting", "Love Brought Us Here", "Time After Time", "Speak Low", "And I Don't Love You (Larry Levan instrumental dub)", "Tea for Two", "Let Me Be The Clock", "We Are The Warriors", "I've Made Love to You a Thousand Times", "The Christmas Song", "Walk on By", "Love Don' Give No Reason (12 Inch Club Mix)", "Theme From the Big Time", "I Love The Nearness Of You", "Will You Love Me Tomorrow?", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Don't Play Another Love Song", "Hanging on by a Thread", "Be Careful What You Wish For", "Please Don't Take Your Love (feat. Carlos Santana)", "Melody Man", "It's Her Turn to Live", "Double Good Everything", "Tears Of A Clown", "Jasmin", "Coincidentally", "Wishful Thinking", "No\u00ebl", "Christmas Everyday", "Photograph in My Mind", "One Time", "God Rest Ye Merry Gentlemen", "Deck the Halls", "Fallin'", "The Family Song", "Blame It on Love", "The Agony and the Ecstasy", "Season's Greetings from Smokey Robinson", "The Tracks of My Tears (live)", "Sleepless Nights", "If You Wanna Make Love (Come 'round Here)", "Food For Thought", "If You Wanna Make Love", "Sweet Harmony", "Tell Me Tomorrow", "A Tattoo", "Close Encounters of the First Kind", "Bad Girl", "When Smokey Sings Tears Of A Clown", "Love Letters", "I Can't Give You Anything but Love", "Keep Me", "You've Really Got a Hold on Me", "Love' n Life", "Because of You It's the Best It's Ever Been", "You Take Me Away", "Ebony Eyes (Duet with Rick James)", "Some People Will Do Anything for Love", "Don't Wanna Be Just Physical", "Little Girl Little Girl", "More Love", "Yes It's You Lady", "Jingle Bells", "I Care About Detroit", "I've Made Love To You A Thousand Times", "Little Girl, Little Girl", "Holly", "Cruisin", "Love Don't Give No Reason", "Get Ready", "Don't Know Why", "We've Saved the Best for Last", "The Love Between Me and My Kids", "In My Corner", "Ooo Baby Baby", "It's a Good Feeling", "So Bad", "Please Come Home for Christmas", "Will You Love Me Tomorrow", "The Tears Of A Clown", "Tracks of my Tears", "Mother's Son", "Whatcha Gonna Do", "Tears of a Sweet Free Clown", "It's Fantastic", "And I Love Her", "Ebony Eyes", "Vitamin U", "You Made Me Feel Love", "Aqui Con Tigo (Being With You)", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "I Know You by Heart", "Share It", "Train of Thought", "You Cannot Laugh Alone", "I'm in the Mood for Love", "Skid Row", "Away in the Manger / Coventry Carol", "Asleep on My Love", "Just a Touch Away", "The Tears of a Clown", "Everything You Touch", "She's Only a Baby Herself", "As You Do", "If You Want My Love", "Baby That's Backatcha", "Tears of a Clown", "Girlfriend", "Virgin Man", "I'll Keep My Light In My Window", "That Place", "Ain't That Peculiar", "Going to a Go Go", "You're Just My Life (feat. India.Arie)", "Baby Come Close", "Medley: Never My Love / Never Can Say Goodbye", "Gang Bangin'", "Pops, We Love You", "I Second That Emotions", "You're the One for Me (feat. Joss Stone)", "You Go to My Head", "Crusin", "Nearness of You", "Can't Fight Love"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> visual_art.artwork.artist -> Elaine de Kooning\n# Answer:\nElaine de Kooning", "# Reasoning Path:\nJohn F. Kennedy -> influence.influence_node.influenced -> Felix Dodds -> common.topic.notable_types -> Author\n# Answer:\nFelix Dodds", "# Reasoning Path:\nJohn F. Kennedy -> influence.influence_node.influenced -> Felix Dodds -> influence.influence_node.influenced_by -> Martin Luther King, Jr.\n# Answer:\nFelix Dodds", "# Reasoning Path:\nJohn F. Kennedy -> influence.influence_node.influenced -> Felix Dodds -> people.person.education -> m.0_kw4fh\n# Answer:\nFelix Dodds"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Republic of Macedonia\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> common.topic.notable_types -> Human Language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.notable_types -> Multi-event tournament\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.countries_spoken_in -> Bosnia and Herzegovina\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Slavic languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> sports.multi_event_tournament.participating_countries -> United States of America\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language -> language.human_language.language_family -> Indo-European languages\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> sports.sport_country.multi_event_tournaments_participated_in -> 2013 FIBA Under-19 World Championship -> common.topic.article -> m.0l8p56r\n# Answer:\n2013 FIBA Under-19 World Championship", "# Reasoning Path:\nSerbia -> location.location.adjoin_s -> m.02wj9fq -> location.adjoining_relationship.adjoins -> Romania\n# Answer:\nm.02wj9fq"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.7499999999999999, "path_precision": 0.6, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
