{"id": "WebQTest-0", "prediction": ["# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Creole English Language\n# Answer:\nJamaican Creole English Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.official_language -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> finance.currency.countries_used -> Jamaican English\n# Answer:\nJamaican English", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Sign Language\n# Answer:\nJamaican Sign Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Cr English Language\n# Answer:\nJamaican Cr English Language", "# Reasoning Path:\nJamaica -> location.country.currency_used -> Jamaican dollar -> finance.currency.countries_used -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> Jamaican Hindi Language\n# Answer:\nJamaican Hindi Language", "# Reasoning Path:\nJamaica -> location.country.languages_spoken -> JamaicanEnglish\n# Answer:\nJamaican English"], "ground_truth": ["Jamaican English", "Jamaican Creole English Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-1", "prediction": ["# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kh -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> people.person.profession -> Politician -> base.onephylogeny.type_of_thing.things_of_this_type -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60kc -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.04j60k7 -> government.government_position_held.office_position_or_title -> United States Representative\n# Answer:\nUnited States Representative", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.0944j8_ -> government.government_position_held.office_position_or_title -> Speaker of the United States House of Representatives\n# Answer:\nSpeaker of the United States House of Representatives", "# Reasoning Path:\nJames K. Polk -> government.politician.government_positions_held -> m.0bqsx7p -> government.government_position_held.office_position_or_title -> Member of Illinois House of Representatives\n# Answer:\nMember of Illinois House of Representatives", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_bodies -> United States House of Representatives\n# Answer:\nUnited States House of Representatives", "# Reasoning Path:\nJames K. Polk -> common.topic.notable_types -> US President -> freebase.type_profile.equivalent_topic -> President of the United States\n# Answer:\nPresident of the United States", "# Reasoning Path:\nJames K. Polk -> people.person.nationality -> United States of America -> government.governmental_jurisdiction.government_positions -> Speaker of the United States House of Representatives\n# Answer:\nSpeaker of the United States House of Representatives"], "ground_truth": ["Speaker of the United States House of Representatives", "Governor of Tennessee", "United States Representative"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.6829268292682926, "ans_precission": 0.7, "ans_recall": 0.6666666666666666, "path_f1": 0.625, "path_precision": 0.5, "path_recall": 0.8333333333333334, "path_ans_f1": 0.6829268292682926, "path_ans_precision": 0.7, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-100", "prediction": ["# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.official_language -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> French\n# Answer:\nFrench", "# Reasoning Path:\nHaiti -> location.country.official_language -> Haitian Creole\n# Answer:\nHaitian Creole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Haitian Creole French\n# Answer:\nHaitian Creole French", "# Reasoning Path:\nHaiti -> location.country.currency_used -> Haitian gourde\n# Answer:\nHaitian gourde", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Creole Haitian\n# Answer:\nCreole Haitian", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Creole French\n# Answer:\nCreole French", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> Creole\n# Answer:\nCreole", "# Reasoning Path:\nHaiti -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language"], "ground_truth": ["Haitian Creole", "French"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-102", "prediction": ["# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0h2wryp -> tv.regular_tv_appearance.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0h2z6h2 -> tv.regular_tv_appearance.actor -> Alyssa Milano\n# Answer:\nAlyssa Milano", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_episodes -> m.0h2wryp -> tv.tv_guest_role.actor -> Zachary Gordon\n# Answer:\nZachary Gordon", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0h2z6h2 -> tv.regular_tv_appearance.actor -> Britt McKillip\n# Answer:\nBritt McKillip", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0h2z6h2 -> tv.regular_tv_appearance.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_episodes -> m.0h2z6h2 -> tv.tv_guest_role.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_episodes -> m.0h2wryp -> tv.tv_guest_role.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nBarbara Gordon -> film.film_character.portrayed_in_films -> m.0jv2kp -> film.performance.actor -> Hynden Walch\n# Answer:\nHynden Walch", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_episodes -> m.0h2z6h2 -> tv.tv_guest_role.actor -> Alyssa Milano\n# Answer:\nAlyssa Milano", "# Reasoning Path:\nBarbara Gordon -> tv.tv_character.appeared_in_tv_program -> m.0h2z6h2 -> tv.regular_tv_appearance.actor -> Wendy Carlos\n# Answer:\nWendy Carlos"], "ground_truth": ["Melinda McGraw", "Ilyssa Fradin", "Hannah Gunn"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-103", "prediction": ["# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjyzm -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qd4y1 -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qstvs -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> sports.drafted_athlete.drafted -> m.04fw77r -> sports.sports_league_draft_pick.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbchh -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qcyjt -> basketball.basketball_player_stats.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qck_w -> basketball.basketball_player_stats.team -> Miami Heat\n# Answer:\nMiami Heat", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qpksh -> basketball.basketball_player_stats.team -> Orlando Magic\n# Answer:\nOrlando Magic", "# Reasoning Path:\nShaquille O'Neal -> sports.sports_award_winner.awards -> m.02kbc58 -> sports.sports_award.team -> Los Angeles Lakers\n# Answer:\nLos Angeles Lakers", "# Reasoning Path:\nShaquille O'Neal -> basketball.basketball_player.player_statistics -> m.04qjnfl -> basketball.basketball_player_stats.team -> Miami Heat\n# Answer:\nMiami Heat"], "ground_truth": ["Miami Heat", "Boston Celtics", "LSU Tigers men's basketball", "Phoenix Suns", "Orlando Magic", "Cleveland Cavaliers", "Los Angeles Lakers"], "ans_acc": 0.42857142857142855, "ans_hit": 1, "ans_f1": 0.6, "ans_precission": 1.0, "ans_recall": 0.42857142857142855, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.6, "path_ans_precision": 1.0, "path_ans_recall": 0.42857142857142855}
{"id": "WebQTest-104", "prediction": ["# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Elmira\n# Answer:\nElmira", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Wilmette\n# Answer:\nWilmette", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> McComb\n# Answer:\nMcComb", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Washington Heights\n# Answer:\nWashington Heights", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Janesville\n# Answer:\nJanesville", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Chicago\n# Answer:\nChicago", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Leesburg\n# Answer:\nLeesburg", "# Reasoning Path:\nJay Leno -> people.person.place_of_birth -> Cleveland\n# Answer:\nCleveland"], "ground_truth": ["New Rochelle"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-105", "prediction": ["# Reasoning Path:\nThailand -> location.country.languages_spoken -> Malay, Pattani Language\n# Answer:\nMalay, Pattani Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Phu Thai language\n# Answer:\nPhu Thai language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Akha Language\n# Answer:\nAkha Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Lao Language\n# Answer:\nLao Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Cham language\n# Answer:\nCham language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Vietnamese Language\n# Answer:\nVietnamese Language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Khmer language\n# Answer:\nKhmer language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Hmong language\n# Answer:\nHmong language", "# Reasoning Path:\nThailand -> location.country.languages_spoken -> Nyaw Language\n# Answer:\nNyaw Language", "# Reasoning Path:\nThailand -> location.country.official_language -> Thai Language\n# Answer:\nThai Language"], "ground_truth": ["Vietnamese Language", "Mlabri Language", "Hmong language", "Malay, Pattani Language", "Mon Language", "Khmer language", "Nyaw Language", "Cham language", "Lao Language", "Saek language", "Thai Language", "Phu Thai language", "Akha Language"], "ans_acc": 0.7692307692307693, "ans_hit": 1, "ans_f1": 0.8695652173913044, "ans_precission": 1.0, "ans_recall": 0.7692307692307693, "path_f1": 0.7826086956521738, "path_precision": 0.9, "path_recall": 0.6923076923076923, "path_ans_f1": 0.8695652173913044, "path_ans_precision": 1.0, "path_ans_recall": 0.7692307692307693}
{"id": "WebQTest-107", "prediction": ["# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Architect\n# Answer:\nArchitect", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Farmer\n# Answer:\nFarmer", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Inventor\n# Answer:\nInventor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Archaeologist\n# Answer:\nArchaeologist", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nThomas Jefferson -> people.person.profession -> Teacher\n# Answer:\nTeacher"], "ground_truth": ["Inventor", "Philosopher", "Teacher", "Writer", "Archaeologist", "Farmer", "Architect", "Lawyer", "Statesman", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-108", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> people.person.quotations -> There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved. -> media_common.quotation.source -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> book.book_subject.works -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 6: 1856-1857\n# Answer:\nThe Correspondence of Charles Darwin, Volume 6: 1856-1857", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin, Volume 3: 1844-1846", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin, Volume 12: 1864", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 4: 1847-1850\n# Answer:\nThe Correspondence of Charles Darwin, Volume 4: 1847-1850", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1838-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1838-1843", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1881-1885\n# Answer:\nThe Correspondence of Charles Darwin, Volume 5: 1881-1885", "# Reasoning Path:\nCharles Darwin -> people.person.children -> Francis Darwin -> book.author.works_written -> The Autobiography of Charles Darwin\n# Answer:\nThe Autobiography of Charles Darwin"], "ground_truth": ["Kleinere geologische Abhandlungen", "A student's introduction to Charles Darwin", "Les mouvements et les habitudes des plantes grimpantes", "Monographs of the fossil Lepadidae and the fossil Balanidae", "The Correspondence of Charles Darwin, Volume 13: 1865", "The Essential Darwin", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Questions about the breeding of animals", "Rejse om jorden", "Insectivorous Plants", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 18: 1870", "Voyage of the Beagle (NG Adventure Classics)", "The Correspondence of Charles Darwin, Volume 9", "Resa kring jorden", "Darwin's Ornithological notes", "Evolution", "On the origin of species by means of natural selection", "From Darwin's unpublished notebooks", "Origin of Species (Harvard Classics, Part 11)", "The Autobiography of Charles Darwin [EasyRead Edition]", "Darwin's journal", "The Descent Of Man And Selection In Relation To Sex (Kessinger Publishing's Rare Reprints)", "Motsa ha-minim", "Darwin Compendium", "Origin of Species (Everyman's University Paperbacks)", "The Autobiography of Charles Darwin", "Voyage of the Beagle", "Darwin", "Darwinism stated by Darwin himself", "Darwin and Henslow", "The Descent of Man and Selection in Relation to Sex", "Darwin for Today", "The Expression of the Emotions in Man And Animals", "Diary of the voyage of H.M.S. Beagle", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Evolution and natural selection", "Tesakneri tsagume\u030c", "The Origin of Species (Enriched Classics)", "Charles Darwin's natural selection", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Descent of Man, and Selection in Relation to Sex", "El Origin De Las Especies", "The Expression of the Emotions in Man and Animals (Large Print Edition): The Expression of the Emotions in Man and Animals (Large Print Edition)", "THE ORIGIN OF SPECIES (Wordsworth Collection) (Wordsworth Collection)", "The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life", "More Letters of Charles Darwin", "Les moyens d'expression chez les animaux", "vari\u00eberen der huisdieren en cultuurplanten", "The Voyage of the Beagle (Classics of World Literature) (Classics of World Literature)", "The autobiography of Charles Darwin, 1809-1882", "The Autobiography of Charles Darwin (Large Print)", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Voyage of the Beagle (Great Minds Series)", "Voyage of the Beagle (Harvard Classics, Part 29)", "Volcanic Islands", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Origin of Species (Barnes & Noble Classics Series) (Barnes & Noble Classics)", "Voyage of the Beagle (Dover Value Editions)", "The Autobiography of Charles Darwin (Great Minds Series)", "The Voyage of the Beagle", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 3", "The Correspondence of Charles Darwin, Volume 9: 1861", "From so simple a beginning", "Leben und Briefe von Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Fertilisation of Orchids", "Darwin Darwin", "The Autobiography of Charles Darwin [EasyRead Large Edition]", "Darwin en Patagonia", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 10: 1862", "The Origin of Species", "To the members of the Down Friendly Club", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "The Correspondence of Charles Darwin, Volume 11", "Die geschlechtliche Zuchtwahl", "The descent of man and selection in relation to sex.", "The Origin of Species (Oxford World's Classics)", "Voyage Of The Beagle", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "Origin of Species", "The collected papers of Charles Darwin", "Origins", "The Voyage of the Beagle (Everyman Paperbacks)", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notebooks on transmutation of species", "Het uitdrukken van emoties bij mens en dier", "Reise eines Naturforschers um die Welt", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 13", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The descent of man, and selection in relation to sex", "The Voyage of the Beagle (Mentor)", "The expression of the emotions in man and animals", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Power of Movement in Plants", "The Correspondence of Charles Darwin, Volume 1", "The Correspondence of Charles Darwin, Volume 6", "The portable Darwin", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "The Origin of Species (Great Books : Learning Channel)", "On Natural Selection", "The Autobiography of Charles Darwin, and selected letters", "Opsht\u0323amung fun menshen", "The Formation of Vegetable Mould through the Action of Worms", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "The Voyage of the \\\"Beagle\\\" (Everyman's Classics)", "Wu zhong qi yuan", "The Correspondence of Charles Darwin, Volume 10", "The Different Forms of Flowers on Plants of the Same Species", "Beagle letters", "The structure and distribution of coral reefs.", "The voyage of Charles Darwin", "H.M.S. Beagle in South America", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The voyage of the Beagle.", "The Expression of the Emotions in Man and Animals", "The Origin of Species (Collector's Library)", "Die fundamente zur entstehung der arten", "The Origin Of Species", "The Correspondence of Charles Darwin, Volume 5", "The Origin of Species (Great Minds Series)", "The Darwin Reader Second Edition", "ontstaan der soorten door natuurlijke teeltkeus", "genese\u014ds t\u014dn eid\u014dn", "The autobiography of Charles Darwin", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The action of carbonate of ammonia on the roots of certain plants", "The Autobiography of Charles Darwin (Dodo Press)", "The education of Darwin", "Human nature, Darwin's view", "The Correspondence of Charles Darwin, Volume 15", "The Structure And Distribution of Coral Reefs", "The structure and distribution of coral reefs", "On a remarkable bar of sandstone off Pernambuco", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 2", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Orgin of Species", "Geological Observations on South America", "A Darwin Selection", "Works", "The Voyage of the Beagle (Unabridged Classics)", "The principal works", "Darwin's notebooks on transmutation of species", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "The Autobiography Of Charles Darwin", "On the tendency of species to form varieties", "The Origin of Species (World's Classics)", "The Origin of Species (Variorum Reprint)", "The Correspondence of Charles Darwin, Volume 12", "The Structure and Distribution of Coral Reefs", "Descent of Man and Selection in Relation to Sex (Barnes & Noble Library of Essential Reading)", "The Autobiography of Charles Darwin [EasyRead Comfort Edition]", "The descent of man, and selection in relation to sex.", "The living thoughts of Darwin", "Die Bewegungen und Lebensweise der kletternden Pflanzen", "La vie et la correspondance de Charles Darwin", "The Correspondence of Charles Darwin, Volume 8", "Memorias y epistolario i\u0301ntimo", "On evolution", "Cartas de Darwin 18251859", "The expression of the emotions in man and animals.", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Voyage of the Beagle (Adventure Classics)", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Metaphysics, Materialism, & the evolution of mind", "The origin of species", "Proiskhozhdenie vidov", "The Correspondence of Charles Darwin, Volume 14: 1866", "red notebook of Charles Darwin", "Darwin on humus and the earthworm", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "The Expression Of The Emotions In Man And Animals", "The Correspondence of Charles Darwin, Volume 4", "Autobiography of Charles Darwin", "monograph on the sub-class Cirripedia", "The\u0301orie de l'e\u0301volution", "The foundations of the Origin of species", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Charles Darwin's letters", "The geology of the voyage of H.M.S. Beagle", "The Origin of Species (Mentor)", "Evolution by natural selection", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Reise um die Welt 1831 - 36", "Del Plata a Tierra del Fuego", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 12: 1864", "Gesammelte kleinere Schriften", "Darwin's insects", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin's marginalia", "The origin of species : complete and fully illustrated", "From So Simple a Beginning", "On the Movements and Habits of Climbing Plants", "La facult\u00e9 motrice dans les plantes", "The Correspondence of Charles Darwin, Volume 7", "The Correspondence of Charles Darwin, Volume 14"], "ans_acc": 0.12149532710280374, "ans_hit": 1, "ans_f1": 0.19409282700421943, "ans_precission": 1.0, "ans_recall": 0.10747663551401869, "path_f1": 0.5499999999999999, "path_precision": 1.0, "path_recall": 0.3793103448275862, "path_ans_f1": 0.21666666666666667, "path_ans_precision": 1.0, "path_ans_recall": 0.12149532710280374}
{"id": "WebQTest-109", "prediction": ["# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> people.person.nationality -> United States of America -> sports.sports_team_location.teams -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.0n1blq0 -> education.education.institution -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nTim Tebow -> people.person.nationality -> United States of America -> location.country.administrative_divisions -> Florida -> sports.sports_team_location.teams -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6gc -> sports.sports_team_roster.team -> Florida State Seminoles football\n# Answer:\nFlorida State Seminoles football", "# Reasoning Path:\nTim Tebow -> sports.drafted_athlete.drafted -> m.05bj8w_ -> sports.sports_league_draft_pick.school -> University of Florida\n# Answer:\nUniversity of Florida", "# Reasoning Path:\nTim Tebow -> people.person.education -> m.0n1bwkc -> education.education.institution -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.drafted_athlete.drafted -> m.05bj8w_ -> sports.sports_league_draft_pick.school -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> sports.pro_athlete.teams -> m.0hpc6g6 -> sports.sports_team_roster.team -> Florida Gators football\n# Answer:\nFlorida Gators football", "# Reasoning Path:\nTim Tebow -> people.person.nationality -> United States of America -> location.location.primarily_containedby -> North America\n# Answer:\nNorth America"], "ground_truth": ["Florida Gators football"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-110", "prediction": ["# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0zs5mvy -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.drafted_athlete.drafted -> m.04vb3pp -> sports.sports_league_draft_pick.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0j2lth4 -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> base.schemastaging.athlete_extra.salary -> m.0jh9fcj -> base.schemastaging.athlete_salary.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d1kv -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0h_1w26 -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Denver Nuggets\n# Answer:\nDenver Nuggets", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0k6zsxj -> sports.sports_team_roster.team -> Denver Broncos\n# Answer:\nDenver Broncos", "# Reasoning Path:\nPeyton Manning -> sports.pro_athlete.teams -> m.0j5d2kv -> sports.sports_team_roster.team -> Colorado Broncos\n# Answer:\nColorado Broncos"], "ground_truth": ["Denver Broncos"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-111", "prediction": ["# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Eastern Europe\n# Answer:\nEastern Europe", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Central Europe\n# Answer:\nCentral Europe", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.partially_containedby -> Romania\n# Answer:\nRomania", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Western Europe\n# Answer:\nWestern Europe", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Hungary\n# Answer:\nHungary", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Alps\n# Answer:\nAlps", "# Reasoning Path:\nCarpathian Mountains -> location.location.containedby -> Asia\n# Answer:\nAsia"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-112", "prediction": ["# Reasoning Path:\nRobert Burns -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> common.topic.notable_types -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Author\n# Answer:\nAuthor", "# Reasoning Path:\nRobert Burns -> fictional_universe.fictional_character.occupation -> Poet\n# Answer:\nPoet", "# Reasoning Path:\nRobert Burns -> fictional_universe.fictional_character.occupation -> Bard\n# Answer:\nBard", "# Reasoning Path:\nRobert Burns -> fictional_universe.fictional_character.occupation -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Lyricist\n# Answer:\nLyricist", "# Reasoning Path:\nRobert Burns -> people.person.profession -> Songwriter\n# Answer:\nSongwriter"], "ground_truth": ["Bard", "Poet", "Writer", "Author"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-114", "prediction": ["# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5ry -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5s6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> James Earl Jones\n# Answer:\nJames Earl Jones", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.03jr0x9 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> tv.tv_character.appeared_in_tv_program -> m.0cgp8rd -> tv.regular_tv_appearance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.0j7zstf -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5rn -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.04m6bsf -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5r6 -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen", "# Reasoning Path:\nDarth Vader -> film.film_character.portrayed_in_films -> m.02sg5sy -> film.performance.actor -> Hayden Christensen\n# Answer:\nHayden Christensen"], "ground_truth": ["Hayden Christensen"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-115", "prediction": ["# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2001 NFL season\n# Answer:\n2001 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0_qxx6x -> sports.sports_award.season -> 2001 NFL season\n# Answer:\n2001 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2000 NFL season\n# Answer:\n2000 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0_qxv1m -> sports.sports_award.season -> 2001 NFL season\n# Answer:\n2001 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0_qxz6s -> sports.sports_award.season -> 2001 NFL season\n# Answer:\n2001 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2006 NFL season\n# Answer:\n2006 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2003 NFL season\n# Answer:\n2003 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2014 NFL season\n# Answer:\n2014 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2009 NFL season\n# Answer:\n2009 NFL season", "# Reasoning Path:\nPeyton Manning -> sports.sports_award_winner.awards -> m.0j5d2kv -> sports.sports_award.season -> 2013 NFL season\n# Answer:\n2013 NFL season"], "ground_truth": ["Super Bowl XLI"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-116", "prediction": ["# Reasoning Path:\nJustin Bieber -> people.person.nationality -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> London\n# Answer:\nLondon", "# Reasoning Path:\nJustin Bieber -> music.artist.origin -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> people.person.place_of_birth -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> celebrities.celebrity.celebrity_friends -> Jaxon Bieber -> people.person.place_of_birth -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> celebrities.celebrity.celebrity_friends -> Jaxon Bieber -> people.person.nationality -> Canada\n# Answer:\nCanada", "# Reasoning Path:\nJustin Bieber -> celebrities.celebrity.celebrity_friends -> m.012zfw1_ -> celebrities.friendship.friend -> Jaxon Bieber\n# Answer:\nJaxon Bieber", "# Reasoning Path:\nJustin Bieber -> celebrities.celebrity.celebrity_friends -> Selena Gomez -> people.person.nationality -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nJustin Bieber -> people.person.places_lived -> m.0gxnnwp -> people.place_lived.location -> Stratford\n# Answer:\nStratford", "# Reasoning Path:\nJustin Bieber -> celebrities.celebrity.celebrity_friends -> m.0gxnnwp -> celebrities.friendship.friend -> Jaxon Bieber\n# Answer:\nJaxon Bieber"], "ground_truth": ["Canada"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-118", "prediction": ["# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.03z96c5 -> military.military_combatant_group.combatants -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> military.military_conflict.combatants -> m.06vz4t9 -> military.military_combatant_group.combatants -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> base.culturalevent.event.entity_involved -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2 -> military.casualties.combatant -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wphl -> military.casualties.combatant -> Argentina\n# Answer:\nArgentina", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2 -> military.casualties.combatant -> France\n# Answer:\nFrance", "# Reasoning Path:\nGulf War -> time.event.includes_event -> Iraqi no-fly zones -> base.culturalevent.event.entity_involved -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> time.event.locations -> Saudi Arabia\n# Answer:\nSaudi Arabia", "# Reasoning Path:\nGulf War -> military.military_conflict.casualties -> m.043wpj2 -> military.casualties.combatant -> Australia\n# Answer:\nAustralia", "# Reasoning Path:\nGulf War -> film.film_subject.films -> Three Kings -> film.film.country -> United States of America\n# Answer:\nUnited States of America"], "ground_truth": ["United States of America", "Saudi Arabia", "Iraq", "United Kingdom", "Australia", "Argentina", "France"], "ans_acc": 0.8571428571428571, "ans_hit": 1, "ans_f1": 0.8333333333333333, "ans_precission": 1.0, "ans_recall": 0.7142857142857143, "path_f1": 0.208955223880597, "path_precision": 0.7, "path_recall": 0.12280701754385964, "path_ans_f1": 0.923076923076923, "path_ans_precision": 1.0, "path_ans_recall": 0.8571428571428571}
{"id": "WebQTest-119", "prediction": ["# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.040p0ym -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgkyn6 -> award.award_nomination.award_nominee -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sglqq_ -> award.award_nomination.award_nominee -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_winning_work.awards_won -> m.0sglslh -> award.award_honor.award_winner -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.03l6cs_ -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> award.award_nominated_work.award_nominations -> m.0sgltk9 -> award.award_nomination.award_nominee -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.04dgryb -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.02t8l7r -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0sglqq_ -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song", "# Reasoning Path:\nThe Suite Life on Deck -> tv.tv_program.regular_cast -> m.0sgkyn6 -> tv.regular_tv_appearance.actor -> Brenda Song\n# Answer:\nBrenda Song"], "ground_truth": ["Brenda Song"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.16666666666666669, "path_precision": 0.1, "path_recall": 0.5, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-12", "prediction": ["# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.04hfs25 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.04hfs25 -> government.government_position_held.office_holder -> Jodi Rell\n# Answer:\nJodi Rell", "# Reasoning Path:\nOhio -> government.political_district.representatives -> m.0gm5f32 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0jskpkj -> government.government_position_held.office_holder -> Jodi Rell\n# Answer:\nJodi Rell", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0hz7v_0 -> government.government_position_held.office_holder -> Rick Kasich\n# Answer:\nRick Kasich", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0hz7w93 -> government.government_position_held.office_holder -> Jodi Rell\n# Answer:\nJodi Rell", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0hz7v_0 -> government.government_position_held.office_holder -> Rob Portman\n# Answer:\nRob Portman", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0jsklnk -> government.government_position_held.office_holder -> Ted Strickland\n# Answer:\nTed Strickland", "# Reasoning Path:\nOhio -> government.governmental_jurisdiction.governing_officials -> m.0hz7w93 -> government.government_position_held.office_holder -> John Krasue\n# Answer:\nJohn Krasue"], "ground_truth": ["John Kasich", "Return J. Meigs, Jr.", "Ted Strickland"], "ans_acc": 0.3333333333333333, "ans_hit": 1, "ans_f1": 0.15384615384615383, "ans_precission": 0.1, "ans_recall": 0.3333333333333333, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.15384615384615383, "path_ans_precision": 0.1, "path_ans_recall": 0.3333333333333333}
{"id": "WebQTest-121", "prediction": ["# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9plqp -> soccer.football_player_stats.team -> Paris Saint-Germain F.C.\n# Answer:\nParis Saint-Germain F.C.", "# Reasoning Path:\nDavid Beckham -> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team -> m.07m4bqp -> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.current_club -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.02nr829 -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0j2zzj_ -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> sports.pro_athlete.teams -> m.0qd3y3c -> sports.sports_team_roster.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w8w79m -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.schemastaging.athlete_extra.salary -> m.0k4ytw5 -> base.schemastaging.athlete_salary.team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> base.x2010fifaworldcupsouthafrica.world_cup_participant.world_cup_team -> m.07m4bqp -> base.x2010fifaworldcupsouthafrica.current_world_cup_squad.national_team -> England national football team\n# Answer:\nEngland national football team", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.loans -> m.0bvdd3n -> soccer.football_player_loan.lending_team -> LA Galaxy\n# Answer:\nLA Galaxy", "# Reasoning Path:\nDavid Beckham -> soccer.football_player.statistics -> m.0w9pdz2 -> soccer.football_player_stats.team -> LA Galaxy\n# Answer:\nLA Galaxy"], "ground_truth": ["LA Galaxy"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.7058823529411764, "path_precision": 0.6, "path_recall": 0.8571428571428571, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-122", "prediction": ["# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.nationality -> Spain\n# Answer:\nSpain", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Velville\n# Answer:\nVelville", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Castilleja de la Cuesta\n# Answer:\nCastilleja de la Cuesta", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Guadalajara\n# Answer:\nGuadalajara", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Santo Domingo\n# Answer:\nSanto Domingo", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Guadiana de la Cuesta\n# Answer:\nGuadiana de la Cuesta", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> V\u00e1zquez de Coronado\n# Answer:\nV\u00e1zquez de Coronado", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> \u00c1gueda\n# Answer:\n\u00c1gueda", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Santo Domingo de la Cuesta\n# Answer:\nSanto Domingo de la Cuesta", "# Reasoning Path:\nFrancisco V\u00e1zquez de Coronado -> people.person.place_of_birth -> Madrid\n# Answer:\nMadrid"], "ground_truth": ["Salamanca"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-13", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Cuban Missile Crisis -> military.military_conflict.commanders -> m.0vmx69n -> military.military_command.military_commander -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> film.film_subject.films -> Thirteen Days -> film.film.subjects -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Cuban Missile Crisis -> military.military_conflict.commanders -> m.0vmx6nr -> military.military_command.military_commander -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Vietnam War -> base.culturalevent.event.entity_involved -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Cuban Missile Crisis -> military.military_conflict.commanders -> m.0vmx6jf -> military.military_command.military_commander -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-14", "prediction": ["# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.citytown -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> base.aareas.schema.administrative_area.administrative_children -> Fukushima Prefecture\n# Answer:\nFukushima Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Fukushima -> location.location.containedby -> Japan\n# Answer:\nJapan", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Fukushima -> location.location.containedby -> Fukushima Prefecture\n# Answer:\nFukushima Prefecture", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> architecture.structure.architect -> Tach\u016b Nait\u014d -> people.deceased_person.place_of_death -> Tokyo\n# Answer:\nTokyo", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Japan -> location.country.administrative_divisions -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.street_address -> m.0ggj3z2 -> location.mailing_address.state_province_region -> Fukushima\n# Answer:\nFukushima", "# Reasoning Path:\nFukushima Daiichi Nuclear Power Plant -> location.location.containedby -> Fukui Prefecture\n# Answer:\nFukui Prefecture"], "ground_truth": ["Japan", "Okuma"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.28571428571428575, "ans_precission": 0.2, "ans_recall": 0.5, "path_f1": 0.37499999999999994, "path_precision": 0.3, "path_recall": 0.5, "path_ans_f1": 0.4444444444444445, "path_ans_precision": 0.4, "path_ans_recall": 0.5}
{"id": "WebQTest-16", "prediction": ["# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> base.aareas.schema.administrative_area.administrative_children -> Northern Ireland\n# Answer:\nNorthern Ireland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> Scotland\n# Answer:\nScotland", "# Reasoning Path:\nUnited Kingdom -> location.country.first_level_divisions -> England\n# Answer:\nEngland", "# Reasoning Path:\nUnited Kingdom -> location.country.administrative_divisions -> Wales\n# Answer:\nWales", "# Reasoning Path:\nUnited Kingdom -> location.country.administrative_divisions -> England\n# Answer:\nEngland"], "ground_truth": ["England", "Northern Ireland", "Wales", "Scotland"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-19", "prediction": ["# Reasoning Path:\nLouisiana -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> UTC\u221205:00\n# Answer:\nUTC\u221205:00", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Gulf Time Zone\n# Answer:\nGulf Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Louisiana Time Zone\n# Answer:\nLouisiana Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones ->Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> Atlantic Time Zone\n# Answer:\nAtlantic Time Zone", "# Reasoning Path:\nLouisiana -> location.location.time_zones -> CDT\n# Answer:\nCDT"], "ground_truth": ["Central Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-20", "prediction": ["# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Philosopher\n# Answer:\nPhilosopher", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Writer\n# Answer:\nWriter", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.founding_figures -> Paul the Apostle\n# Answer:\nPaul the Apostle", "# Reasoning Path:\nAugustine of Hippo -> people.person.profession -> Theologian\n# Answer:\nTheologian", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.notable_figures -> Paul the Apostle\n# Answer:\nPaul the Apostle", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.notable_figures -> Thomas Aquinas\n# Answer:\nThomas Aquinas", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Catholicism -> religion.religion.deities -> Jesus Christ\n# Answer:\nJesus Christ", "# Reasoning Path:\nAugustine of Hippo -> common.topic.notable_types -> Physician\n# Answer:\nPhysician", "# Reasoning Path:\nAugustine of Hippo -> base.saints.saint.venerated_in -> Eastern Christianity -> religion.religion.is_part_of -> Christianity\n# Answer:\nChristianity"], "ground_truth": ["Philosopher", "Writer", "Physician"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-21", "prediction": ["# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francine Lons\n# Answer:\nFrancine Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leon Cole\n# Answer:\nLeon Cole", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Sal Gibson\n# Answer:\nSal Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francisca Lons\n# Answer:\nFrancisca Lons", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francis Gibson\n# Answer:\nFrancis Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Francisco Joseph Gibson\n# Answer:\nFrancisco Joseph Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Frank Gibson\n# Answer:\nFrank Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leonard Gibson\n# Answer:\nLeonard Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Leo Gibson\n# Answer:\nLeo Gibson", "# Reasoning Path:\nKeyshia Cole -> people.person.parents -> Franks Gibson\n# Answer:\nFranks Gibson"], "ground_truth": ["Sal Gibson", "Leon Cole", "Francine Lons"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-22", "prediction": ["# Reasoning Path:\nEgypt -> location.country.form_of_government -> Provisional government\n# Answer:\nProvisional government", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Semi-presidential system\n# Answer:\nSemi-presidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Unitary state\n# Answer:\nUnitary state", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Parliamentary republic\n# Answer:\nParliamentary republic", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Proletarian state\n# Answer:\nProletarian state", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Military dictatorship\n# Answer:\nMilitary dictatorship", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Presidential system\n# Answer:\nPresidential system", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Constitutional republic\n# Answer:\nConstitutional republic", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Socialist state\n# Answer:\nSocialist state", "# Reasoning Path:\nEgypt -> location.country.form_of_government -> Republic\n# Answer:\nRepublic"], "ground_truth": ["Provisional government", "Semi-presidential system"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-23", "prediction": ["# Reasoning Path:\nMartin Luther King, Jr. -> people.deceased_person.place_of_death -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song -> book.written_work.subjects -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activist_organizations -> Montgomery Improvement Association -> organization.organization.place_founded -> Montgomery\n# Answer:\nMontgomery", "# Reasoning Path:\nMartin Luther King, Jr. -> symbols.name_source.namesakes -> Martin Luther King, Jr. National Historic Site -> travel.tourist_attraction.near_travel_destination -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> book.book_subject.works -> Countdown of 10 Amazing Moments from the Year 2011: No. 9 Belafonte's New Song -> base.reviews.review.mentions -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activist_organizations -> Montgomery Improvement Association -> organization.organization.place_founded -> Birmingham\n# Answer:\nBirmingham", "# Reasoning Path:\nMartin Luther King, Jr. -> book.author.works_written -> The Martin Luther King, Jr. Companion -> book.written_work.subjects -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activist_organizations -> Southern Christian Leadership Conference -> organization.organization.place_founded -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> Anti-Apartheid -> base.activism.activism_issue.activist_organizations -> Southern Christian Leadership Conference -> organization.organization.place_founded -> Memphis\n# Answer:\nMemphis", "# Reasoning Path:\nMartin Luther King, Jr. -> base.activism.activist.area_of_activism -> African Americans' rights -> base.activism.activism_issue.activist_organizations -> Montgomery Improvement Association -> organization.organization.place_founded -> Montgomery\n# Answer:\nMontgomery"], "ground_truth": ["Memphis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-24", "prediction": ["# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimore\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimor\n# Answer:\nBaltimor", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_burial -> Westminster Hall and Burying Ground\n# Answer:\nWestminster Hall and Burying Ground", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimor\n# Answer:\nBaltimore", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Charlottesville\n# Answer:\nCharlottesville", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Virginia City\n# Answer:\nVirginia City", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Baltimora\n# Answer:\nBaltimora", "# Reasoning Path:\nEdgar Allan Poe -> people.deceased_person.place_of_death -> Chesapeake\n# Answer:\nChesapeake", "# Reasoning Path:\nEdgar Allan Poe -> people.death.place_of_death -> Baltimore\n# Answer:\nBaltimore"], "ground_truth": ["Baltimore"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5, "ans_precission": 0.3333333333333333, "ans_recall": 1.0, "path_f1": 0.19999999999999998, "path_precision": 0.1111111111111111, "path_recall": 1.0, "path_ans_f1": 0.3636363636363636, "path_ans_precision": 0.2222222222222222, "path_ans_recall": 1.0}
{"id": "WebQTest-26", "prediction": ["# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta Zoo\n# Answer:\nAtlanta Zoo", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia Aquarium\n# Answer:\nGeorgia Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Georgia State Capitol\n# Answer:\nGeorgia State Capitol", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Martin Luther King, Jr. National Historic Site\n# Answer:\nMartin Luther King, Jr. National Historic Site", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta Museum of Art\n# Answer:\nAtlanta Museum of Art", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Centennial Park (Atlanta)\n# Answer:\nCentennial Park (Atlanta)", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> State Mutual Stadium\n# Answer:\nState Mutual Stadium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Turner Field\n# Answer:\nTurner Field", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> Atlanta Aquarium\n# Answer:\nAtlanta Aquarium", "# Reasoning Path:\nAtlanta -> travel.travel_destination.tourist_attractions -> National Center for the Visual Arts\n# Answer:\nNational Center for the Visual Arts"], "ground_truth": ["Six Flags Over Georgia", "Atlanta Marriott Marquis", "Variety Playhouse", "Peachtree Road Race", "Jimmy Carter Library and Museum", "The Tabernacle", "Turner Field", "Omni Coliseum", "Georgia State Capitol", "Underground Atlanta", "Arbor Place Mall", "Cobb Energy Performing Arts Centre", "Hyatt Regency Atlanta", "Fernbank Museum of Natural History", "CNN Center", "Fox Theatre", "Martin Luther King, Jr. National Historic Site", "Atlanta Symphony Orchestra", "Four Seasons Hotel Atlanta", "Six Flags White Water", "Atlanta Cyclorama & Civil War Museum", "Atlanta History Center", "Georgia Aquarium", "Philips Arena", "Margaret Mitchell House & Museum", "Masquerade", "Fernbank Science Center", "Centennial Olympic Park", "Zoo Atlanta", "Atlanta Ballet", "Georgia Dome", "Center for Puppetry Arts", "World of Coca-Cola", "Woodruff Arts Center", "Georgia World Congress Center", "Atlanta Jewish Film Festival"], "ans_acc": 0.1111111111111111, "ans_hit": 1, "ans_f1": 0.1739130434782609, "ans_precission": 0.4, "ans_recall": 0.1111111111111111, "path_f1": 0.1739130434782609, "path_precision": 0.4, "path_recall": 0.1111111111111111, "path_ans_f1": 0.20618556701030927, "path_ans_precision": 0.4, "path_ans_recall": 0.1388888888888889}
{"id": "WebQTest-28", "prediction": ["# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Cootamundra -> location.location.containedby -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> location.country.administrative_divisions -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.political_appointer.appointees -> m.0zs80_z -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.05t7fc3 -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0h2vrxl -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.nationality -> Australia -> base.aareas.schema.administrative_area.administrative_children -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> people.person.place_of_birth -> Sydney -> location.location.containedby -> New South Wales\n# Answer:\nNew South Wales", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.05t7fc3 -> government.government_position_held.district_represented -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0h2vnxl -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland", "# Reasoning Path:\nAnna Bligh -> government.politician.government_positions_held -> m.0h2vnsx -> government.government_position_held.jurisdiction_of_office -> Queensland\n# Answer:\nQueensland"], "ground_truth": ["Electoral district of South Brisbane"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-33", "prediction": ["# Reasoning Path:\nSweden -> location.location.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> CEST (Central European Time Zone)\n# Answer:\nCEST (Central European Time Zone)", "# Reasoning Path:\nSweden -> location.location.time_zones -> Swedish Central Time Zone\n# Answer:\nSwedish Central Time Zone", "# Reasoning Path:\nSweden -> location.country.time_zones -> Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Greenwich Mean Time Zone\n# Answer:\nGreenwich Mean Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Stockholm Time Zone\n# Answer:\nStockholm Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Central Europe Time Zone\n# Answer:\nCentral Europe Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones ->Central European Time Zone\n# Answer:\nCentral European Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> Swedish Time Zone\n# Answer:\nSwedish Time Zone", "# Reasoning Path:\nSweden -> location.location.time_zones -> UTC+1\n# Answer:\nUTC+1"], "ground_truth": ["Central European Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-34", "prediction": ["# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.0j2lth4 -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.0j2l1lz -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.rushing -> m.09grpy_ -> american_football.player_rushing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> sports.pro_athlete.teams -> m.0h_1w26 -> sports.sports_team_roster.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.rushing -> m.0903tjg -> american_football.player_rushing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.passing -> m.07z2dyp -> american_football.player_passing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.rushing -> m.09tc6zn -> american_football.player_rushing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> base.schemastaging.athlete_extra.salary -> m.0j31qpc -> base.schemastaging.athlete_salary.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.rushing -> m.07vzrj9 -> american_football.player_rushing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers", "# Reasoning Path:\nCam Newton -> american_football.football_player.rushing -> m.0903fq9 -> american_football.player_rushing_statistics.team -> Carolina Panthers\n# Answer:\nCarolina Panthers"], "ground_truth": ["Carolina Panthers"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-35", "prediction": ["# Reasoning Path:\nFrederick -> location.location.containedby -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.hud_county_place.county -> Frederick County\n# Answer:\nFrederick County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Montgomery County\n# Answer:\nMontgomery County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Washington County\n# Answer:\nWashington County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Maryland\n# Answer:\nMaryland", "# Reasoning Path:\nFrederick -> location.location.containedby -> Anne Arundel County\n# Answer:\nAnne Arundel County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Franklin County\n# Answer:\nFranklin County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Carroll County\n# Answer:\nCarroll County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Caroline County\n# Answer:\nCaroline County", "# Reasoning Path:\nFrederick -> location.location.containedby -> Baltimore County\n# Answer:\nBaltimore County"], "ground_truth": ["Frederick County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-36", "prediction": ["# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy1 -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> Butler High School\n# Answer:\nButler High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> Monroe High School\n# Answer:\nMonroe High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> Butler High School\n# Answer:\nButler High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> Huntingdon College\n# Answer:\nHuntingdon College", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> University of Alabama School of Law\n# Answer:\nUniversity of Alabama School of Law", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0lwxmy9 -> education.education.institution -> Monroe County School District\n# Answer:\nMonroe County School District", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> Monroe County High School\n# Answer:\nMonroe County High School", "# Reasoning Path:\nHarper Lee -> people.person.education -> m.0n1l46h -> education.education.institution -> Monroe County School District\n# Answer:\nMonroe County School District"], "ground_truth": ["Monroe County High School"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-37", "prediction": ["# Reasoning Path:\nUtah -> location.location.time_zones -> Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Pacific Time Zone\n# Answer:\nPacific Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Utah Time Zone\n# Answer:\nUtah Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> UTC\u221207:00\n# Answer:\nUTC\u221207:00", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mtn Time Zone\n# Answer:\nMtn Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Central Time Zone\n# Answer:\nCentral Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountian Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones ->Mountain Time Zone\n# Answer:\nMountain Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Mountains Time Zone\n# Answer:\nMountains Time Zone", "# Reasoning Path:\nUtah -> location.location.time_zones -> Eastern Time Zone\n# Answer:\nEastern Time Zone"], "ground_truth": ["Mountain Time Zone"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-39", "prediction": ["# Reasoning Path:\nNiall Ferguson -> people.person.children -> Rory Ferguson -> people.person.parents -> Susan Ferguson\n# Answer:\nSusan Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Connor Ferguson -> people.person.parents -> Susan Ferguson\n# Answer:\nSusan Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Isabella Ferguson -> people.person.parents -> Christine Lagarde\n# Answer:\nChristine Lagarde", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j6b5x_ -> people.marriage.spouse -> Rachael Biester\n# Answer:\nRachael Biester", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j6b5x_ -> people.marriage.spouse -> Susan Ferguson\n# Answer:\nSusan Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Patrick Ferguson -> people.person.parents -> Christine Lagarde\n# Answer:\nChristine Lagarde", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.02kkmwz -> people.marriage.spouse -> Chrissy Ferguson\n# Answer:\nChrissy Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.02kkmwz -> people.marriage.spouse -> Susan Ferguson\n# Answer:\nSusan Ferguson", "# Reasoning Path:\nNiall Ferguson -> people.person.spouse_s -> m.0j6b5x_ -> people.marriage.spouse -> Christine Lagarde\n# Answer:\nChristine Lagarde", "# Reasoning Path:\nNiall Ferguson -> people.person.children -> Patrick Ferguson -> people.person.parents -> Christine Ferguson\n# Answer:\nChristine Ferguson"], "ground_truth": ["Ayaan Hirsi Ali"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-41", "prediction": ["# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Ecuador\n# Answer:\nEcuador", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Pacific Ocean\n# Answer:\nPacific Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Province of Gal\u00e1pagos\n# Answer:\nProvince of Gal\u00e1pagos", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island\n# Answer:\nSan Crist\u00f3bal Island", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Gal\u00e1pagos Province\n# Answer:\nGal\u00e1pagos Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Equatorial Province\n# Answer:\nEquatorial Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Santa Cruz Province\n# Answer:\nSanta Cruz Province", "# Reasoning Path:\nGal\u00e1pagos Islands -> location.location.containedby -> Atlantic Ocean\n# Answer:\nAtlantic Ocean", "# Reasoning Path:\nGal\u00e1pagos Islands -> geography.island_group.islands_in_group -> San Crist\u00f3bal Island -> location.location.containedby -> Americas\n# Answer:\nAmericas"], "ground_truth": ["Gal\u00e1pagos Province", "Ecuador", "Pacific Ocean"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-42", "prediction": ["# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Gonna Get Caught\n# Answer:\nGonna Get Caught", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> I Don't Care\n# Answer:\nI Don't Care", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> Don't Forget\n# Answer:\nDon't Forget", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> What's Wrong With Them\n# Answer:\nWhat's Wrong With Them", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> What's Wrong With Them\n# Answer:\nWhat's Wrong With Them", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> The Boys Are Back\n# Answer:\nThe Boys Are Back", "# Reasoning Path:\nJustin Bieber -> music.composer.compositions -> The Night Is Still Young\n# Answer:\nThe Night Is Still Young", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> I Thought I Lost You\n# Answer:\nI Thought I Lost You", "# Reasoning Path:\nJustin Bieber -> music.artist.album -> My World\n# Answer:\nMy World", "# Reasoning Path:\nJustin Bieber -> music.artist.track_contributions -> m.0rqp4h0 -> music.track_contribution.track -> Come On a Cone\n# Answer:\nCome On a Cone"], "ground_truth": ["All Bad", "Wait for a Minute", "Recovery", "PYD", "All That Matters", "Confident", "Change Me", "Right Here", "Eenie Meenie", "Lolly", "Pray", "Somebody to Love", "Die in Your Arms", "Hold Tight", "Turn to You (Mother's Day Dedication)", "Roller Coaster", "Bad Day", "Never Let You Go", "Heartbreaker", "As Long as You Love Me", "Beauty And A Beat", "Thought Of You", "#thatPower", "Baby", "Bigger", "All Around The World", "Live My Life", "Never Say Never", "First Dance", "Boyfriend", "Home to Mama"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-43", "prediction": ["# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Lawyer\n# Answer:\nLawyer", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> common.topic.notable_types -> Politician\n# Answer:\nPolitician", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Statesman\n# Answer:\nStatesman", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Jurist\n# Answer:\nJurist", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Military Officer\n# Answer:\nMilitary Officer", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Diplomat\n# Answer:\nDiplomat", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Soldier\n# Answer:\nSoldier", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Businessperson\n# Answer:\nBusinessperson", "# Reasoning Path:\nGeorges Cl\u00e9menceau -> people.person.profession -> Judge\n# Answer:\nJudge"], "ground_truth": ["Writer", "Publisher", "Physician", "Statesman", "Journalist"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.1, "ans_recall": 0.2, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.1, "path_ans_recall": 0.2}
{"id": "WebQTest-44", "prediction": ["# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> base.ecology.ecosystem.ecosystem_members -> Chihuahuan Desert -> location.location.partially_containedby -> New Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Yucca flower\n# Answer:\nYucca flower", "# Reasoning Path:\nArizona -> base.biblioness.bibs_location.country -> United States of America -> location.country.administrative_divisions -> New Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Yucca flower\n# Answer:\nYucca flower", "# Reasoning Path:\nArizona -> base.ecology.ecosystem.ecosystem_members -> Chihuahuan Desert -> location.location.partially_containedby -> New Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> location.us_state.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> base.biblioness.bibs_location.country -> United States of America -> location.country.administrative_divisions -> New Mexico -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hzbf17 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> symbols.flag_referent.flag -> m.0hz8v06 -> symbols.flag_use.flag -> Saguaro\n# Answer:\nSaguaro", "# Reasoning Path:\nArizona -> government.governmental_jurisdiction.official_symbols -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro cactus\n# Answer:\nSaguaro cactus", "# Reasoning Path:\nArizona -> location.offical_symbol_variety.symbols_of_this_kind -> m.0hz8v06 -> location.location_symbol_relationship.symbol -> Saguaro\n# Answer:\nSaguaro"], "ground_truth": ["Saguaro"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-45", "prediction": ["# Reasoning Path:\nRihanna -> music.artist.origin -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> St. Michael Parish\n# Answer:\nSt. Michael Parish", "# Reasoning Path:\nRihanna -> people.person.ethnicity -> Afro-Guyanese\n# Answer:\nAfro-Guyanese", "# Reasoning Path:\nRihanna -> people.person.nationality -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> people.person.place_of_birth -> Barbados\n# Answer:\nBarbados", "# Reasoning Path:\nRihanna -> music.artist.origin -> Caribbean special municipalities of Barbados\n# Answer:\nCaribbean special municipalities of Barbados", "# Reasoning Path:\nRihanna -> music.artist.origin ->Saint Michael Parish\n# Answer:\nSaint Michael Parish", "# Reasoning Path:\nRihanna -> music.artist.origin -> Rihanna Town\n# Answer:\nRihanna Town"], "ground_truth": ["Saint Michael Parish"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-47", "prediction": ["# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Mississauga\n# Answer:\nMississauga", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth ->Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Missin' Hills\n# Answer:\nMissin' Hills", "# Reasoning Path:\nGeorge Lopez -> music.artist.origin -> Mission Hills\n# Answer:\nMission Hills", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Missisquoi\n# Answer:\nMissisquoi", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Los Angeles\n# Answer:\nLos Angeles", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Missingleton\n# Answer:\nMissingleton", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Washington Heights\n# Answer:\nWashington Heights", "# Reasoning Path:\nGeorge Lopez -> people.person.place_of_birth -> Missler\n# Answer:\nMissler"], "ground_truth": ["Mission Hills"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-48", "prediction": ["# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.0ydz8h9 -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.02t77k0 -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.0ydz8h9 -> location.mailing_address.citytown -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.place_founded -> South Korea\n# Answer:\nSouth Korea", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.02_793g -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.02t7mw5 -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.02_77k6 -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.02_793g -> location.mailing_address.citytown -> Daegu\n# Answer:\nDaegu", "# Reasoning Path:\nSamsung Group -> organization.organization.headquarters -> m.0ydz8v9 -> location.mailing_address.citytown -> Seoul\n# Answer:\nSeoul"], "ground_truth": ["Suwon"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-49", "prediction": ["# Reasoning Path:\nIslam -> religion.religion.beliefs -> \u1e6c\u016bb\u0101\n# Answer:\n\u1e6c\u016bb\u0101", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Masih ad-Dajjal\n# Answer:\nMasih ad-Dajjal", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic view of angels\n# Answer:\nIslamic view of angels", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Prophets in Islam\n# Answer:\nProphets in Islam", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Entering Heaven alive\n# Answer:\nEntering Heaven alive", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Islamic holy books\n# Answer:\nIslamic holy books", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Sharia\n# Answer:\nSharia", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Monotheism\n# Answer:\nMonotheism", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Tawhid\n# Answer:\nTawhid", "# Reasoning Path:\nIslam -> religion.religion.beliefs -> Mahdi\n# Answer:\nMahdi"], "ground_truth": ["Tawhid", "Mahdi", "Predestination in Islam", "Entering Heaven alive", "Qiyamah", "Islamic holy books", "Islamic view of angels", "\u1e6c\u016bb\u0101", "God in Islam", "Sharia", "Prophets in Islam", "Masih ad-Dajjal", "Monotheism"], "ans_acc": 0.7692307692307693, "ans_hit": 1, "ans_f1": 0.8695652173913044, "ans_precission": 1.0, "ans_recall": 0.7692307692307693, "path_f1": 0.8695652173913044, "path_precision": 1.0, "path_recall": 0.7692307692307693, "path_ans_f1": 0.8695652173913044, "path_ans_precision": 1.0, "path_ans_recall": 0.7692307692307693}
{"id": "WebQTest-51", "prediction": ["# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz0r8 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.05tflhq -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.04m6l9_ -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.075wxc2 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0gbwhrn -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0b7bh45 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.05tflhw -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0b7b7v1 -> film.performance.actor -> Taylor Lautner\n# Answer:\nTaylor Lautner", "# Reasoning Path:\nChristian Grey -> film.film_character.portrayed_in_films -> m.0djz10s -> film.performance.actor -> Tyler Mane\n# Answer:\nTyler Mane"], "ground_truth": ["Jamie Dornan"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-52", "prediction": ["# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tuberculous cervical lymphadenitis\n# Answer:\nTuberculous cervical lymphadenitis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tobacco smoking\n# Answer:\nTobacco smoking", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Respiratory failure\n# Answer:\nRespiratory failure", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Respiratory disease\n# Answer:\nRespiratory disease", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Tubeerculosis\n# Answer:\nTuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> tuberculosis\n# Answer:\ntuberculosis", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> Stroke\n# Answer:\nStroke", "# Reasoning Path:\nGeorge Orwell -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack"], "ground_truth": ["Tuberculosis"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-54", "prediction": ["# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.0pz073c -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.politician.government_positions_held -> m.08_ksdj -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.04ls_tb -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> government.political_appointer.appointees -> m.07jrjrl -> government.government_position_held.jurisdiction_of_office -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Waffen-SS -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.02h7nns -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> military.military_commander.military_commands -> m.0dl81ld -> military.military_command.military_combatant -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Schutzstaffel -> organization.organization.place_founded -> Nazi Germany\n# Answer:\nNazi Germany", "# Reasoning Path:\nAdolf Hitler -> organization.organization_founder.organizations_founded -> Nazi Party -> organization.organization.geographic_scope -> Germany\n# Answer:\nGermany"], "ground_truth": ["Nazi Germany"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.761904761904762, "path_precision": 0.8, "path_recall": 0.7272727272727273, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-55", "prediction": ["# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Actor\n# Answer:\nActor", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Film Score Composer\n# Answer:\nFilm Score Composer", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Singer\n# Answer:\nSinger", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Songwriter\n# Answer:\nSongwriter", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Record producer\n# Answer:\nRecord producer", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Bandleader\n# Answer:\nBandleader", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Film Producer\n# Answer:\nFilm Producer", "# Reasoning Path:\nMichael Bubl\u00e9 -> people.person.profession -> Dancer\n# Answer:\nDancer"], "ground_truth": ["Songwriter", "Singer", "Actor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-56", "prediction": ["# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Platte County\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Platte County\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Platte County\n# Answer:\nPlatte County", "# Reasoning Path:\nKansas Speedway -> location.location.containedby -> Wyandotte County\n# Answer:\nWyandotte County", "# Reasoning Path:\nKansas City -> location.hud_county_place.county -> Wyand County\n# Answer:\nWyand County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas\n# Answer:\nKansas", "# Reasoning Path:\nKansas City -> location.location.containedby -> Kansas County\n# Answer:\nKansas County", "# Reasoning Path:\nKansas City -> location.location.containedby -> Johnson County\n# Answer:\nJohnson County"], "ground_truth": ["Wyandotte County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-58", "prediction": ["# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpz2 -> baseball.batting_statistics.team -> St. Louis Cardinals\n# Answer:\nSt. Louis Cardinals", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpwy -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpts -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpw6 -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpzt -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbp_k -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpyc -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbpxn -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> baseball.baseball_player.batting_stats -> m.06sbq0l -> baseball.batting_statistics.team -> Brooklyn Dodgers\n# Answer:\nBrooklyn Dodgers", "# Reasoning Path:\nJackie Robinson -> sports.pro_athlete.teams -> m.0hpgh_h -> sports.sports_team_roster.team -> Pasadena City College Lancers baseball\n# Answer:\nPasadena City College Lancers baseball"], "ground_truth": ["UCLA Bruins football"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-6", "prediction": ["# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Leesburg\n# Answer:\nLeesburg", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Decatur\n# Answer:\nDecatur", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Washington Heights\n# Answer:\nWashington Heights", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Mobile\n# Answer:\nMobile", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Washington, D.C.\n# Answer:\nWashington, D.C.", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> St. Louis\n# Answer:\nSt. Louis", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Marion County\n# Answer:\nMarion County", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Southern California\n# Answer:\nSouthern California", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Watts\n# Answer:\nWatts", "# Reasoning Path:\nJaMarcus Russell -> people.person.place_of_birth -> Atlanta\n# Answer:\nAtlanta"], "ground_truth": ["Mobile"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-60", "prediction": ["# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.biblioness.bibs_location.city -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> Manhattan\n# Answer:\nManhattan", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.location.containedby -> New York City\n# Answer:\nNew York City", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> East Side\n# Answer:\nEast Side", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> base.biblioness.bibs_location.state -> New York\n# Answer:\nNew York", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> Manhattan 101\n# Answer:\nManhattan 101", "# Reasoning Path:\nEleanor Roosevelt -> people.deceased_person.place_of_death -> Manhattan -> location.place_with_neighborhoods.neighborhoods -> 10021\n# Answer:\n10021"], "ground_truth": ["Manhattan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 1.0, "path_precision": 1.0, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-61", "prediction": ["# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.0tkwpxg -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrjs -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrjy -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrj_ -> location.religion_percentage.religion -> Catholicism\n# Answer:\nCatholicism", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrjs -> location.religion_percentage.religion -> Batak Language\n# Answer:\nBatak Language", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04hfs5l -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrjw -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrj_ -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.04dsrj3 -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam", "# Reasoning Path:\nIndonesia -> location.statistical_region.religions -> m.0wkc9d7 -> location.religion_percentage.religion -> Islam\n# Answer:\nIslam"], "ground_truth": ["Hinduism", "Islam", "Protestantism", "Catholicism"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6428571428571429, "ans_precission": 0.9, "ans_recall": 0.5, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.6428571428571429, "path_ans_precision": 0.9, "path_ans_recall": 0.5}
{"id": "WebQTest-62", "prediction": ["# Reasoning Path:\nJesse James -> people.deceased_person.cause_of_death -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> people.deceased_person.place_of_death -> Saint Joseph\n# Answer:\nSaint Joseph", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.fight.crime_type.crimes_of_this_type -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor\n# Answer:\nTraitor", "# Reasoning Path:\nJesse James -> base.culturalevent.event.entity_involved -> Robert E. Lee -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.honouriam.dishonor_type.dishonor_bestowed -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.fight.crime_type.crimes_of_this_type -> Fireball\n# Answer:\nFireball", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.honouriam.dishonor_bestowed.dishonor_bestowed_by -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.honouriam.dishonor_bestowed.type_of_dishonor -> Firearm\n# Answer:\nFirearm", "# Reasoning Path:\nJesse James -> base.honouriam.dishonoured_person.dishonor_bestowed -> Traitor -> base.honouriam.dishonor_bestowed.dishonor_bestowed -> Firearm\n# Answer:\nFirearm"], "ground_truth": ["Firearm"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-63", "prediction": ["# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andrew Johnson\n# Answer:\nAndrew Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hannibal Hamlin\n# Answer:\nHannibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> John C. Calhoun\n# Answer:\nJohn C. Calhoun", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hanibal Hamlin\n# Answer:\nHanibal Hamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Charles W. Fairbanks\n# Answer:\nCharles W. Fairbanks", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Henry Johnson\n# Answer:\nHenry Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Hamlin\n# Answer:\nHamlin", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Andy Johnson\n# Answer:\nAndy Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> William Johnson\n# Answer:\nWilliam Johnson", "# Reasoning Path:\nAbraham Lincoln -> government.us_president.vice_president -> Abraham Lincoln\n# Answer:\nAbraham Lincoln"], "ground_truth": ["Hannibal Hamlin", "Andrew Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-64", "prediction": ["# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Yet to Come -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Fezziwig -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> TV Ghost of Christmas Past -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Bob Cratchit -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Edward Murdstone -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nDavid Copperfield", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Wilkins Micawber -> book.book_character.appears_in_book -> David Copperfield\n# Answer:\nDavid Copperfield", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Ghost of Christmas Present -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> award.award_nominee.award_nominations -> m.011lncpm -> award.award_nomination.nominated_for -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Tiny Tim -> book.book_character.appears_in_book -> A Christmas Carol\n# Answer:\nA Christmas Carol", "# Reasoning Path:\nCharles Dickens -> fictional_universe.fictional_character_creator.fictional_characters_created -> Oliver Twist\n# Answer:\nOliver Twist"], "ground_truth": ["A Tale of Two Cities (Dramatized)", "Dombey and son", "A Christmas Carol (Acting Edition)", "Bleak House.", "A Tale of Two Cities (Puffin Classics)", "A Tale of Two Cities (Pacemaker Classics)", "A Christmas Carol (Webster's Portuguese Thesaurus Edition)", "The old curiosity shop.", "A Christmas Carol (Classics Illustrated)", "Little Dorrit", "Dombey and Son.", "A Tale of Two Cities (Penguin Classics)", "Bleak house", "A Tale of Two Cities (Bookcassette(r) Edition)", "Great Expectations", "A Christmas Carol (Classic, Picture, Ladybird)", "A Tale of Two Cities (Cover to Cover Classics)", "A Tale of Two Cities (Soundings)", "A Christmas Carol (Reissue)", "A Christmas Carol (Ladybird Children's Classics)", "A Tale of Two Cities (Cassette (1 Hr).)", "A Christmas Carol (Saddleback Classics)", "A Tale of Two Cities (Longman Classics, Stage 2)", "A Tale of Two Cities (Clear Print)", "A Tale of Two Cities (Ladybird Children's Classics)", "A Tale of Two Cities (Macmillan Students' Novels)", "The Old Curiosity Shop", "Great expectations.", "A Christmas Carol (Whole Story)", "A Christmas Carol (Dramascripts)", "A Tale of Two Cities (Ultimate Classics)", "A Tale of Two Cities (Oxford Bookworms Library)", "A Tale of Two Cities (Paperback Classics)", "A Christmas Carol (R)", "The Pickwick papers", "The mystery of Edwin Drood", "A Christmas Carol (Classics for Young Adults and Adults)", "A Tale of Two Cities (Cyber Classics)", "Dombey and Son", "A Tale of Two Cities (Amsco Literature Program - N 380 ALS)", "A Tale of Two Cities (40th Anniversary Edition)", "A Tale of Two Cities (Masterworks)", "A Christmas Carol (Children's Classics)", "A Christmas Carol (Dramascripts Classic Texts)", "A Christmas Carol (Nelson Graded Readers)", "A Christmas Carol (Children's Theatre Playscript)", "A Tale of Two Cities (Penguin Readers, Level 5)", "A Christmas Carol (Classic Collection)", "David Copperfield", "Our mutual friend.", "The life and adventures of Nicholas Nickleby", "A Christmas Carol (Bantam Classic)", "A Tale of Two Cities (BBC Audio Series)", "A Tale Of Two Cities (Adult Classics in Audio)", "A Tale of Two Cities (Unabridged Classics)", "A Christmas Carol (Enriched Classics)", "A Tale of Two Cities (Collector's Library)", "A Christmas Carol (Puffin Classics)", "A Tale of Two Cities (The Classic Collection)", "A Tale Of Two Cities (Adult Classics)", "A Tale of Two Cities (Webster's Chinese-Traditional Thesaurus Edition)", "A Tale of Two Cities (Silver Classics)", "A Christmas Carol (Tor Classics)", "A Tale of Two Cities (Signet Classics)", "A Tale of Two Cities (Compact English Classics)", "A Christmas Carol (Classic Books on Cassettes Collection)", "A Tale of Two Cities (Webster's Portuguese Thesaurus Edition)", "A Tale of Two Cities (Lake Illustrated Classics, Collection 2)", "A Christmas Carol (Through the Magic Window Series)", "A Christmas Carol (Limited Editions)", "A Christmas Carol (Aladdin Classics)", "A Tale of Two Cities (Barnes & Noble Classics Series)", "A Tale of Two Cities (Simple English)", "A Tale of Two Cities (Courage Literary Classics)", "A Christmas Carol (Soundings)", "A Tale of Two Cities (Longman Fiction)", "A Christmas Carol (Radio Theatre)", "A Tale of Two Cities (The Greatest Historical Novels)", "A Christmas Carol (Oxford Bookworms Library)", "A Christmas Carol (Cover to Cover)", "A Christmas Carol (Apple Classics)", "A Christmas Carol (Usborne Young Reading)", "A Christmas Carol (Penguin Readers, Level 2)", "A Tale of Two Cities (Adopted Classic)", "A Tale of Two Cities (New Oxford Illustrated Dickens)", "A Tale of Two Cities (Progressive English)", "A Tale of Two Cities (Wordsworth Classics)", "A Christmas Carol (Watermill Classic)", "A Tale of Two Cities (Penguin Popular Classics)", "A Tale of Two Cities (Everyman Paperbacks)", "A CHRISTMAS CAROL", "Our mutual friend", "A Tale of Two Cities (Illustrated Junior Library)", "A Tale of Two Cities (Illustrated Classics)", "A Tale of Two Cities (Everyman's Library Classics)", "A Christmas Carol (Puffin Choice)", "A Christmas Carol (Chrysalis Children's Classics Series)", "The Mystery of Edwin Drood", "A Christmas Carol (Wordsworth Collection) (Wordsworth Collection)", "A Christmas Carol (Large Print)", "A Tale Of Two Cities (Classic Books on Cassettes Collection)", "A Christmas Carol (Pacemaker Classics)", "The Pickwick Papers", "A Tale of Two Cities (Classic Retelling)", "Great Expectations.", "A Christmas Carol (Illustrated Classics)", "Great expectations", "A Christmas Carol (Take Part)", "A Tale of Two Cities (10 Cassettes)", "A Tale of Two Cities (Everyman's Library (Paper))", "A Christmas Carol (Value Books)", "A Tale of Two Cities (Isis Clear Type Classic)", "Martin Chuzzlewit", "A Tale of Two Cities (Collected Works of Charles Dickens)", "A Christmas Carol (Penguin Student Editions)", "A Christmas Carol (Illustrated Classics (Graphic Novels))", "A Christmas Carol (Scholastic Classics)", "A Tale of Two Cities (Large Print Edition)", "A Tale of Two Cities (Dover Thrift Editions)", "A Christmas Carol (Everyman's Library Children's Classics)", "A Tale of Two Cities (Enriched Classic)", "A Tale of Two Cities (Acting Edition)", "A Christmas Carol. (Lernmaterialien)", "A Christmas Carol (Audio Editions)", "The cricket on the hearth", "A Tale of Two Cities (Saddleback Classics)", "A Christmas Carol (New Longman Literature)", "A Tale of Two Cities (Dramascripts S.)", "A Christmas Carol (Thornes Classic Novels)", "David Copperfield.", "A Christmas Carol (Cp 1135)", "A Christmas Carol", "A Tale of Two Cities (Webster's Italian Thesaurus Edition)", "A Christmas Carol (Ladybird Classics)", "A Tale of Two Cities (Student's Novels)", "A Christmas Carol (The Kennett Library)", "A Tale of Two Cities (Prentice Hall Science)", "A Christmas Carol (Webster's Korean Thesaurus Edition)", "A Tale of Two Cities (Classic Literature with Classical Music)", "A Tale of Two Cities (Webster's Chinese-Simplified Thesaurus Edition)", "Hard times", "Oliver Twist", "A Christmas Carol (Gollancz Children's Classics)", "A Tale of Two Cities (Unabridged Classics for High School and Adults)", "A Tale of Two Cities (Webster's German Thesaurus Edition)", "A Tale of Two Cities (Dodo Press)", "A Tale of Two Cities (Classic Fiction)", "A Christmas Carol (Classic Fiction)", "A Christmas Carol (Watermill Classics)", "The old curiosity shop", "Sketches by Boz", "A Tale of Two Cities (Tor Classics)", "A Tale of Two Cities", "A Christmas Carol (Pacemaker Classic)", "A Tale of Two Cities (Classics Illustrated Notes)", "A Tale of Two Cities (Oxford Playscripts)", "A Tale of Two Cities (Konemann Classics)", "A Christmas Carol (Clear Print)", "A Christmas Carol (Young Reading Series 2)", "Bleak House", "A Tale of Two Cities (Naxos AudioBooks)", "A Christmas Carol (Family Classics)", "A Christmas Carol (Green Integer, 50)", "A TALE OF TWO CITIES", "A Tale of Two Cities (Classics Illustrated)", "A Tale of Two Cities (Bantam Classic)", "A Tale of Two Cities (Piccolo Books)", "A Christmas Carol (Great Stories)", "A Christmas Carol (Read & Listen Books)"], "ans_acc": 0.03550295857988166, "ans_hit": 1, "ans_f1": 0.05747126436781609, "ans_precission": 1.0, "ans_recall": 0.029585798816568046, "path_f1": 0.6852589641434264, "path_precision": 1.0, "path_recall": 0.5212121212121212, "path_ans_f1": 0.05747126436781609, "path_ans_precision": 1.0, "path_ans_recall": 0.029585798816568046}
{"id": "WebQTest-65", "prediction": ["# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0vmx6nr -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6zw -> government.government_position_held.office_holder -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.04fvhtr -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.0dl79fr -> military.military_command.military_commander -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> location.location.events -> Eastern Front -> base.culturalevent.event.entity_involved -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> military.military_combatant.armed_forces -> Red Army -> organization.organization.founders -> Joseph Stalin\n# Answer:\nJoseph Stalin", "# Reasoning Path:\nSoviet Union -> government.governmental_jurisdiction.governing_officials -> m.049x6_k -> government.government_position_held.office_holder -> Leonid Brezhnev\n# Answer:\nLeonid Brezhnev", "# Reasoning Path:\nSoviet Union -> military.military_combatant.military_commanders -> m.02h7nmf -> military.military_command.military_commander -> Nikita Khrushchev\n# Answer:\nNikita Khrushchev"], "ground_truth": ["Vladimir Lenin"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-66", "prediction": ["# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_parent -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejandro Garc\u00eda Padilla", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.containedby -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> meteorology.cyclone_affected_area.cyclones -> Hurricane Jeanne -> meteorology.tropical_cyclone.affected_areas -> Dominican Republic -> location.country.currency_used -> Dominican peso\n# Answer:\nDominican peso", "# Reasoning Path:\nPuerto Rico -> location.country.currency_used -> Puerto Rican col\u00f3n\n# Answer:\nPuerto Rican col\u00f3n", "# Reasoning Path:\nPuerto Rico -> base.locations.countries.states_provinces_within -> Coamo -> location.location.containedby -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.locations.countries.states_provinces_within -> Coamo -> location.location.people_born_here -> Alejandro Garc\u00eda Padilla\n# Answer:\nAlejandro Garc\u00eda Padilla", "# Reasoning Path:\nPuerto Rico -> base.aareas.schema.administrative_area.administrative_children -> Coamo -> location.location.containedby -> United States Dollar\n# Answer:\nUnited States Dollar", "# Reasoning Path:\nPuerto Rico -> base.locations.countries.states_provinces_within -> Coamo -> location.location.containedby -> United States of America -> location.country.currency_used -> United States Dollar\n# Answer:\nUnited States Dollar"], "ground_truth": ["United States Dollar"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-67", "prediction": ["# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Head and neck cancer\n# Answer:\nHead and neck cancer", "# Reasoning Path:\nCarl Wilson -> medicine.notable_person_with_medical_condition.condition -> Brain tumor\n# Answer:\nBrain tumor", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain tumour\n# Answer:\nBrain tumour", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Brain cancer\n# Answer:\nBrain cancer", "# Reasoning Path:\nCarl Wilson -> medicine.notable_person_with_medical_condition.condition -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Respiratory cancer\n# Answer:\nRespiratory cancer", "# Reasoning Path:\nCarl Wilson -> people.deceased_person.cause_of_death -> Tumor\n# Answer:\nTumor"], "ground_truth": ["Lung cancer", "Brain tumor"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.5714285714285715, "ans_precission": 0.4, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.5714285714285715, "path_ans_precision": 0.4, "path_ans_recall": 1.0}
{"id": "WebQTest-68", "prediction": ["# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Knight Rider Universe -> fictional_universe.fictional_universe.characters -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> film.film.starring -> m.0j7ph8m -> film.performance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.works_set_here -> Knight Rider: The Series -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.fictional_universe.works_set_here -> Knight Rider: The Original Series -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> award.award_nominated_work.award_nominations -> m.0n5xnkk -> award.award_nomination.award_nominee -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Knight Rider Universe -> fictional_universe.fictional_universe.characters -> Kitt\n# Answer:\nKitt", "# Reasoning Path:\nKnight Rider -> film.film.starring -> m.03lj4m5 -> film.performance.actor -> William Daniels\n# Answer:\nWilliam Daniels", "# Reasoning Path:\nKnight Rider -> tv.tv_program.regular_cast -> m.03lj4m5 -> tv.regular_tv_appearance.actor -> Will Daniels\n# Answer:\nWill Daniels", "# Reasoning Path:\nKnight Rider -> fictional_universe.work_of_fiction.part_of_these_fictional_universes -> Knight Rider Universe -> fictional_universe.fictional_universe.characters -> Knight Rider\n# Answer:\nKnight Rider"], "ground_truth": ["William Daniels"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-69", "prediction": ["# Reasoning Path:\nBrentwood -> location.location.containedby -> Tennessee\n# Answer:\nTennessee", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.hud_county_place.county -> Williamson County\n# Answer:\nWilliamson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Hamilton County\n# Answer:\nHamilton County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Madison County\n# Answer:\nMadison County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Nashville County\n# Answer:\nNashville County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Hillsborough County\n# Answer:\nHillsborough County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Davidson County\n# Answer:\nDavidson County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Montgomery County\n# Answer:\nMontgomery County", "# Reasoning Path:\nBrentwood -> location.location.containedby -> Washington County\n# Answer:\nWashington County"], "ground_truth": ["Williamson County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-7", "prediction": ["# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Sioux City\n# Answer:\nSioux City", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Simpsonville\n# Answer:\nSimpsonville", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Iowa\n# Answer:\nIowa", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Minneapolis\n# Answer:\nMinneapolis", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Simpson County\n# Answer:\nSimpson County", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Galveston\n# Answer:\nGalveston", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Simpson\n# Answer:\nSimpson", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Savannah\n# Answer:\nSavannah", "# Reasoning Path:\nGeorge Washington Carver -> people.person.place_of_birth -> Sioux City\n# Answer:\nSiox City"], "ground_truth": ["Diamond"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-71", "prediction": ["# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Fox -> people.person.parents -> Christine Taylor\n# Answer:\nChristine Taylor", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Michael Joseph Fox, Jr. -> people.person.parents -> Christine Taylor\n# Answer:\nChristine Taylor", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Michael Joseph Fox, Jr. -> people.person.parents -> Christy Oglevee\n# Answer:\nChristy Oglevee", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Michael Joseph Fox, Jr. -> people.person.parents -> Phyllis Piper\n# Answer:\nPhyllis Piper", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Sam Fox -> people.person.parents -> Christy Oglevee\n# Answer:\nChristy Oglevee", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q5w_ -> base.popstra.dated.participant -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Michael Joseph Fox, Jr. -> people.person.parents -> Tracy Pollan\n# Answer:\nTracy Pollan", "# Reasoning Path:\nMichael J. Fox -> base.popstra.celebrity.dated -> m.065q5w_ -> base.popstra.dated.participant -> Christine Taylor\n# Answer:\nChristine Taylor", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Molly Fox -> people.person.parents -> Christine Taylor\n# Answer:\nChristine Taylor", "# Reasoning Path:\nMichael J. Fox -> people.person.children -> Michael Joseph Fox, Jr. -> people.person.parents -> Christina Elfers\n# Answer:\nChristina Elfers"], "ground_truth": ["Tracy Pollan"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-72", "prediction": ["# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04yvq68 -> military.military_command.military_conflict -> How Few Remain\n# Answer:\nHow Few Remain", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9q6 -> military.military_command.military_conflict -> Battle of McDowell\n# Answer:\nBattle of McDowell", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04fv9nd -> military.military_command.military_conflict -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Chancellorsville\n# Answer:\nBattle of Chancellorsville", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Mexican\u2013American War\n# Answer:\nMexican\u2013American War", "# Reasoning Path:\nStonewall Jackson -> military.military_commander.military_commands -> m.04q5wcf -> military.military_command.military_conflict -> First Battle of Kernstown\n# Answer:\nFirst Battle of Kernstown", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> American Civil War\n# Answer:\nAmerican Civil War", "# Reasoning Path:\nStonewall Jackson -> military.military_person.participated_in_conflicts -> Battle of Cedar Mountain\n# Answer:\nBattle of Cedar Mountain", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> Battle of Port Republic\n# Answer:\nBattle of Port Republic", "# Reasoning Path:\nStonewall Jackson -> common.image.appears_in_topic_gallery -> Jackson's Valley Campaign -> time.event.includes_event -> First Battle of Winchester\n# Answer:\nFirst Battle of Winchester"], "ground_truth": ["Battle of Chancellorsville", "Manassas Station Operations", "Battle of White Oak Swamp", "Battle of Chantilly", "Battle of Cedar Mountain", "Battle of Hoke's Run", "How Few Remain", "First Battle of Winchester", "Battle of Front Royal", "Jackson's Valley Campaign", "First Battle of Kernstown", "First Battle of Rappahannock Station", "American Civil War", "Battle of Port Republic", "Battle of McDowell", "Battle of Hancock", "Romney Expedition", "Second Battle of Bull Run", "Battle of Harpers Ferry"], "ans_acc": 0.47368421052631576, "ans_hit": 1, "ans_f1": 0.5737051792828686, "ans_precission": 0.9, "ans_recall": 0.42105263157894735, "path_f1": 0.37837837837837834, "path_precision": 0.7, "path_recall": 0.25925925925925924, "path_ans_f1": 0.6206896551724138, "path_ans_precision": 0.9, "path_ans_recall": 0.47368421052631576}
{"id": "WebQTest-73", "prediction": ["# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> English Language\n# Answer:\nEnglish Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Swahili Language\n# Answer:\nSwahili Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maasai Language\n# Answer:\nMaasai Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maa Language\n# Answer:\nMaa Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Maa language\n# Answer:\nMaa language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Arabic Language\n# Answer:\nArabic Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Nobiin Language\n# Answer:\nNobiin Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Turkana Language\n# Answer:\nTurkana Language", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Kalamazoo English\n# Answer:\nKalamazoo English", "# Reasoning Path:\nMaasai people -> people.ethnicity.languages_spoken -> Masai Language\n# Answer:\nMasai Language"], "ground_truth": ["Maasai Language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.18181818181818182, "ans_precission": 0.1, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.18181818181818182, "path_ans_precision": 0.1, "path_ans_recall": 1.0}
{"id": "WebQTest-74", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin, Jr. -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> William Franklin -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Thomas Franklin -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin Jr. -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Josiah Franklin -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin, Jr. -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin II -> people.person.parents -> Deborah Hitchborn\n# Answer:\nDeborah Hitchborn", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin, Jr. -> people.person.parents -> Franklin's mother, Josiah Franklin\n# Answer:\nFranklin's mother, Josiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin II -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin", "# Reasoning Path:\nBenjamin Franklin -> people.person.children -> Benjamin Franklin Jr. -> people.person.parents -> Josiah Franklin\n# Answer:\nJosiah Franklin"], "ground_truth": ["Deborah Read"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-75", "prediction": ["# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> medicine.notable_person_with_medical_condition.condition -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Rhabdomyolysis\n# Answer:\nRhabdomyolysis", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Pneumonia\n# Answer:\nPneumonia", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Marfan syndrome\n# Answer:\nMarfan syndrome", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Complication\n# Answer:\nComplication", "# Reasoning Path:\nPatrick Swayze -> people.deceased_person.cause_of_death -> Kidney cancer\n# Answer:\nKidney cancer"], "ground_truth": ["Pancreatic cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-76", "prediction": ["# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Sculpture\n# Answer:\nSculpture", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Drawing\n# Answer:\nDrawing", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.art_forms -> Painting\n# Answer:\nPainting", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Diving suit\n# Answer:\nDiving suit", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Supper\n# Answer:\nThe Last Supper", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Armored car\n# Answer:\nArmored car", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artwork -> Diving suit\n# Answer:\nDiving suit", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Armored car\n# Answer:\nThe Armored car", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> Anemone\n# Answer:\nAnemone", "# Reasoning Path:\nLeonardo da Vinci -> visual_art.visual_artist.artworks -> The Last Dinosaur\n# Answer:\nThe Last Dinosaur"], "ground_truth": ["Madonna of the Carnation", "The Baptism of Christ", "Portrait of a man in red chalk", "Annunciation", "Leda and the Swan", "g.1219sb0g", "Drapery for a Seated Figure", "Leonardo's horse", "St. Jerome in the Wilderness", "g.12215rxg", "The Last Supper", "Madonna Litta", "g.12314dm1", "Bacchus", "Madonna and Child with St Joseph", "Mona Lisa", "Portrait of a Musician", "The Holy Infants Embracing", "g.120vt1gz", "Salvator Mundi", "Lady with an Ermine", "Ginevra de' Benci", "g.121wt37c", "Adoration of the Magi", "La belle ferronni\u00e8re", "Portrait of a Young Fianc\u00e9e", "Medusa", "Horse and Rider", "g.121yh91r", "g.1239jd9p", "The Battle of Anghiari", "Lucan portrait of Leonardo da Vinci", "St. John the Baptist", "Portrait of Isabella d'Este", "The Virgin and Child with St Anne and St John the Baptist", "Sala delle Asse", "Virgin of the Rocks", "Head of a Woman", "Madonna of Laroque", "The Virgin and Child with St. Anne", "Vitruvian Man", "Benois Madonna", "g.1224tf0c", "g.1213jb_b", "Madonna of the Yarnwinder"], "ans_acc": 0.022222222222222223, "ans_hit": 1, "ans_f1": 0.03636363636363636, "ans_precission": 0.1, "ans_recall": 0.022222222222222223, "path_f1": 0.037037037037037035, "path_precision": 0.1, "path_recall": 0.022727272727272728, "path_ans_f1": 0.03636363636363636, "path_ans_precision": 0.1, "path_ans_recall": 0.022222222222222223}
{"id": "WebQTest-77", "prediction": ["# Reasoning Path:\nAustria -> location.country.administrative_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.administrative_children -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.capital -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.second_level_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> periodicals.newspaper_circulation_area.newspapers -> Wiener Zeitung -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.country.first_level_divisions -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> periodicals.newspaper_circulation_area.newspapers -> Der Standard -> book.newspaper.circulation_areas -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> base.aareas.schema.administrative_area.subdividing_place -> Vienna\n# Answer:\nVienna", "# Reasoning Path:\nAustria -> location.us_state.capital -> Vienna\n# Answer:\nVienna"], "ground_truth": ["Vienna"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 1.0, "ans_precission": 1.0, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 1.0, "path_ans_precision": 1.0, "path_ans_recall": 1.0}
{"id": "WebQTest-78", "prediction": ["# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervical cancer\n# Answer:\nCervical cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nEva Per\u00f3n -> medicine.notable_person_with_medical_condition.condition -> Ovarian cancer\n# Answer:\nOvarian cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Pancreatic cancer\n# Answer:\nPancreatic cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Leukemia\n# Answer:\nLeukemia", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Cervinal cancer\n# Answer:\nCervinal cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Lung cancer\n# Answer:\nLung cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> Breast cancer\n# Answer:\nBreast cancer", "# Reasoning Path:\nEva Per\u00f3n -> people.deceased_person.cause_of_death -> cervical cancer\n# Answer:\ncervical cancer"], "ground_truth": ["Cervical cancer"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-79", "prediction": ["# Reasoning Path:\nGautama Buddha -> people.person.nationality -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.person.place_of_birth -> Kapilavastu\n# Answer:\nKapilavastu", "# Reasoning Path:\nGautama Buddha -> people.person.nationality -> India\n# Answer:\nIndia", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.geographic_distribution -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.geographical_distribution -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.geographic_distribution -> India\n# Answer:\nIndia", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.geographical_distribution -> India\n# Answer:\nIndia", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism -> religion.religion.place_of_worship -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> people.deceased_person.place_of_death -> Kathmandu -> location.location.containedby -> Nepal\n# Answer:\nNepal", "# Reasoning Path:\nGautama Buddha -> symbols.name_source.namesakes -> Buddhism\n# Answer:\nBuddhism"], "ground_truth": ["Nepal"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 0.5, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 0.5, "path_ans_recall": 1.0}
{"id": "WebQTest-8", "prediction": ["# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Franklin stove\n# Answer:\nFranklin stove", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightning rod\n# Answer:\nLightning rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Liberty before liberalism\n# Answer:\nLiberty before liberalism", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Glass harmonica\n# Answer:\nGlass harmonica", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.inventions -> Lightening rod\n# Answer:\nLightening rod", "# Reasoning Path:\nBenjamin Franklin -> law.inventor.us_patents -> Bifocals\n# Answer:\nBifocals", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Liberty\n# Answer:\nLiberty", "# Reasoning Path:\nBenjamin Franklin -> base.argumentmaps.innovator.original_ideas -> Franklin's idea of liberty\n# Answer:\nFranklin's idea of liberty"], "ground_truth": ["Glass harmonica", "Lightning rod", "Franklin stove", "Bifocals"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.5714285714285715, "path_precision": 0.4, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-80", "prediction": ["# Reasoning Path:\nGreeley -> location.hud_county_place.county -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Adams County\n# Answer:\nAdams County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Jefferson County\n# Answer:\nJefferson County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Lincoln County\n# Answer:\nLincoln County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Arapahoe County\n# Answer:\nArapahoe County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Windsor County\n# Answer:\nWindsor County", "# Reasoning Path:\nGreeley -> location.location.containedby -> United States of America\n# Answer:\nUnited States of America", "# Reasoning Path:\nGreeley -> location.location.containedby -> Weld County\n# Answer:\nWeld County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Greenwood County\n# Answer:\nGreenwood County", "# Reasoning Path:\nGreeley -> location.location.containedby -> Colorado\n# Answer:\nColorado"], "ground_truth": ["Weld County"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-82", "prediction": ["# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Pianist\n# Answer:\nPianist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Musician\n# Answer:\nMusician", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> common.topic.notable_types -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Lyricist\n# Answer:\nLyricist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Violist\n# Answer:\nViolist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> fictional_universe.fictional_character.occupation -> Composer\n# Answer:\nComposer", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Artist\n# Answer:\nArtist", "# Reasoning Path:\nPyotr Ilyich Tchaikovsky -> people.person.profession -> Teacher\n# Answer:\nTeacher"], "ground_truth": ["Composer", "Librettist", "Musician"], "ans_acc": 0.6666666666666666, "ans_hit": 1, "ans_f1": 0.5333333333333333, "ans_precission": 0.4444444444444444, "ans_recall": 0.6666666666666666, "path_f1": 0.3333333333333333, "path_precision": 0.2222222222222222, "path_recall": 0.6666666666666666, "path_ans_f1": 0.5333333333333333, "path_ans_precision": 0.4444444444444444, "path_ans_recall": 0.6666666666666666}
{"id": "WebQTest-83", "prediction": ["# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Belgium\n# Answer:\nBelgium", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Switzerland\n# Answer:\nSwitzerland", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Liechtenstein\n# Answer:\nLiechtenstein", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Austria\n# Answer:\nAustria", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Luxembourg\n# Answer:\nLuxembourg", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> East Germany\n# Answer:\nEast Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> West Germany\n# Answer:\nWest Germany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Czech Republic\n# Answer:\nCzech Republic", "# Reasoning Path:\nGerman Language -> language.human_language.main_country -> Germany\n# Answer:\nGermany", "# Reasoning Path:\nGerman Language -> language.human_language.countries_spoken_in -> Vatican City\n# Answer:\nVatican City"], "ground_truth": ["Luxembourg", "Switzerland", "Liechtenstein", "East Germany", "Germany", "Belgium", "Austria"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.8235294117647058, "path_precision": 0.7, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-84", "prediction": ["# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Soft rock\n# Answer:\nSoft rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental music\n# Answer:\nExperimental music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Psychedelic rock\n# Answer:\nPsychedelic rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Experimental rock\n# Answer:\nExperimental rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop rock\n# Answer:\nPop rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Rock music\n# Answer:\nRock music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Pop music\n# Answer:\nPop music", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Art rock\n# Answer:\nArt rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Blues rock\n# Answer:\nBlues rock", "# Reasoning Path:\nJohn Lennon -> music.artist.genre -> Psychedelia\n# Answer:\nPsychedelia"], "ground_truth": ["Pop rock", "Pop music", "Experimental rock", "Rock music", "Experimental music", "Psychedelic rock", "Soft rock", "Art rock", "Blues rock"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.9473684210526316, "ans_precission": 0.9, "ans_recall": 1.0, "path_f1": 0.9473684210526316, "path_precision": 0.9, "path_recall": 1.0, "path_ans_f1": 0.9473684210526316, "path_ans_precision": 0.9, "path_ans_recall": 1.0}
{"id": "WebQTest-85", "prediction": ["# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kh0nn -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.05kfbwl -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> common.topic.image -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.elections -> United States Senate election in Colorado, 2008 -> government.election.winner -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.0gm5f70 -> government.government_position_held.office_holder -> Mark Udall\n# Answer:\nMark Udall", "# Reasoning Path:\nColorado -> government.political_district.representatives -> m.0nbbt_3 -> government.government_position_held.office_holder -> Michael Bennet\n# Answer:\nMichael Bennet", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcwb -> government.government_position_held.office_holder -> Bill Ritter\n# Answer:\nBill Ritter", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.011dfp8j -> government.government_position_held.office_holder -> Mary Ruwart\n# Answer:\nMary Ruwart", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcv6 -> government.government_position_held.office_holder -> Bill Ritter\n# Answer:\nBill Ritter", "# Reasoning Path:\nColorado -> government.governmental_jurisdiction.governing_officials -> m.04krcwb -> government.government_position_held.office_holder -> Bill R. Clements\n# Answer:\nBill R. Clements"], "ground_truth": ["Michael Bennet", "Mark Udall"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0.4615384615384615, "path_precision": 0.3, "path_recall": 1.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-86", "prediction": ["# Reasoning Path:\nGreenland -> location.administrative_division.country -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> base.aareas.schema.administrative_area.administrative_parent -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.government -> m.010r2fjz -> government.government_position_held.administrative_division -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.government -> m.010r2fjz -> government.government_position_held.office_holder -> Margrethe II of Denmark\n# Answer:\nMargrethe II of Denmark", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.government -> m.010r2fjz -> government.government_position_held.administrative_parent -> Denmark\n# Answer:\nDenmark", "# Reasoning Path:\nGreenland -> location.administrative_division.country -> Kingdom of Denmark\n# Answer:\nKingdom of Denmark", "# Reasoning Path:\nGreenland -> location.location.containedby -> Nordic countries\n# Answer:\nNordic countries", "# Reasoning Path:\nGreenland -> government.governmental_jurisdiction.government -> m.010r2fjz -> government.government_position_held.governmental_body -> Ministry of Food Processing Industries\n# Answer:\nMinistry of Food Processing Industries"], "ground_truth": ["Denmark"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.888888888888889, "ans_precission": 0.8, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.888888888888889, "path_ans_precision": 0.8, "path_ans_recall": 1.0}
{"id": "WebQTest-87", "prediction": ["# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98040\n# Answer:\n98040", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98043\n# Answer:\n98043", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98045\n# Answer:\n98045", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98044\n# Answer:\n98044", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98041\n# Answer:\n98041", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98046\n# Answer:\n98046", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98001\n# Answer:\n98001", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98004\n# Answer:\n98004", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98002\n# Answer:\n98002", "# Reasoning Path:\nSeattle -> location.citytown.postal_codes -> 98020\n# Answer:\n98020"], "ground_truth": ["98185", "98177", "98129", "98127", "98115", "98132", "98160", "98161", "98136", "98121", "98171", "98175", "98166", "98146", "98108", "98104", "98199", "98133", "98103", "98148", "98124", "98191", "98116", "98139", "98170", "98134", "98106", "98102", "98184", "98144", "98174", "98154", "98141", "98138", "98164", "98107", "98198", "98119", "98125", "98119-4114", "98114", "98158", "98165", "98111", "98105", "98181", "98109", "98155", "98190", "98101", "98117", "98178", "98195", "98118", "98112", "98145", "98168", "98122", "98194", "98188", "98131", "98113", "98126"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-88", "prediction": ["# Reasoning Path:\nWillow Smith -> people.person.parents -> Will Smith\n# Answer:\nWill Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith-Monaghan\n# Answer:\nWillow Smith-Monaghan", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith-Monroe\n# Answer:\nWillow Smith-Monroe", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith-McCombs\n# Answer:\nWillow Smith-McCombs", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith, Sr.\n# Answer:\nWillow Smith, Sr.", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Jada Smith\n# Answer:\nJada Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith-Monae\n# Answer:\nWillow Smith-Monae", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Smith\n# Answer:\nWillow Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Joseph Smith\n# Answer:\nWillow Joseph Smith", "# Reasoning Path:\nWillow Smith -> people.person.parents -> Willow Rosenberg\n# Answer:\nWillow Rosenberg"], "ground_truth": ["Jada Pinkett Smith"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-89", "prediction": ["# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Standard Chinese\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese characters\n# Answer:\nChinese characters", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese character\n# Answer:\nChinese character", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese alphabet\n# Answer:\nChinese alphabet", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Hanzi\n# Answer:\nHanzi", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Chinese orthography\n# Answer:\nChinese orthography", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Traditional Chinese\n# Answer:\nTraditional Chinese", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Taiwan -> location.country.official_language -> Standard Chinese\n# Answer:\nStandard Chinese", "# Reasoning Path:\nChinese language -> language.human_language.writing_system -> Standard Chinese orthography\n# Answer:\nStandard Chinese orthography", "# Reasoning Path:\nChinese language -> language.human_language.countries_spoken_in -> Hong Kong -> location.country.languages_spoken -> English Language\n# Answer:\nEnglish Language"], "ground_truth": ["Traditional Chinese characters", "'Phags-pa script", "N\u00fcshu script", "Chinese characters", "Simplified Chinese character"], "ans_acc": 0.2, "ans_hit": 1, "ans_f1": 0.13333333333333333, "ans_precission": 0.1, "ans_recall": 0.2, "path_f1": 0.13333333333333333, "path_precision": 0.1, "path_recall": 0.2, "path_ans_f1": 0.13333333333333333, "path_ans_precision": 0.1, "path_ans_recall": 0.2}
{"id": "WebQTest-9", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.03lb5yj -> people.marriage.spouse -> Gerald Ford\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.03lb5yj -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Patrick Nixon -> people.person.parents -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.03fb9sw -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Patricia Nixon -> people.person.parents -> Gerald Ford\n# Answer:\nGerald Ford", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Richard Milhous Nixon, Jr. -> people.person.parents -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.0j4k3r7 -> people.marriage.spouse -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.03fb9sw -> people.marriage.spouse -> Patricia Nixon\n# Answer:\nPatricia Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.children -> Pat Nixon -> people.person.parents -> Pat Nixon\n# Answer:\nPat Nixon", "# Reasoning Path:\nRichard Nixon -> people.person.spouse_s -> m.03lb5yj -> people.marriage.spouse -> Patricia Nixon\n# Answer:\nPatricia Nixon"], "ground_truth": ["Pat Nixon"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.7499999999999999, "ans_precission": 0.6, "ans_recall": 1.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0.7499999999999999, "path_ans_precision": 0.6, "path_ans_recall": 1.0}
{"id": "WebQTest-90", "prediction": ["# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_y5jrl -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_k3 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> tv.tv_program.regular_cast -> m.03lkkdh -> tv.regular_tv_appearance.actor -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0zbmbwz -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_tlrg2 -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.0_yc_3q -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_winning_work.awards_won -> m.0_tlrff -> award.award_honor.award_winner -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.091nx8n -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_nq -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs", "# Reasoning Path:\nThe Jeffersons -> award.award_nominated_work.award_nominations -> m.07t6_mc -> award.award_nomination.award_nominee -> Marla Gibbs\n# Answer:\nMarla Gibbs"], "ground_truth": ["Marla Gibbs", "Berlinda Tolbert", "Roxie Roker", "Franklin Cover", "Mike Evans", "Jay Hammer", "Isabel Sanford", "Damon Evans", "Paul Benedict", "Zara Cully", "Sherman Hemsley"], "ans_acc": 0.09090909090909091, "ans_hit": 1, "ans_f1": 0.16666666666666669, "ans_precission": 1.0, "ans_recall": 0.09090909090909091, "path_f1": 0.2898550724637681, "path_precision": 1.0, "path_recall": 0.1694915254237288, "path_ans_f1": 0.16666666666666669, "path_ans_precision": 1.0, "path_ans_recall": 0.09090909090909091}
{"id": "WebQTest-91", "prediction": ["# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Guardian\n# Answer:\nSan Francisco Bay Guardian", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Business Times\n# Answer:\nSan Francisco Business Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Foghorn\n# Answer:\nSan Francisco Foghorn", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay Times\n# Answer:\nSan Francisco Bay Times", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Bay View\n# Answer:\nSan Francisco Bay View", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> San Francisco Daily\n# Answer:\nSan Francisco Daily", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> The Golden Era\n# Answer:\nThe Golden Era", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Sing Tao Daily\n# Answer:\nSing Tao Daily", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Synapse\n# Answer:\nSynapse", "# Reasoning Path:\nSan Francisco -> periodicals.newspaper_circulation_area.newspapers -> Street Sheet\n# Answer:\nStreet Sheet"], "ground_truth": ["Free Society", "Dock of the Bay", "San Francisco Bay Guardian", "San Francisco Business Times", "The Golden Era", "Bay Area Reporter", "San Francisco News-Call Bulletin Newspaper", "San Francisco Foghorn", "San Francisco Chronicle", "San Francisco Bay Times", "Synapse", "San Francisco Daily", "Street Sheet", "AsianWeek", "San Francisco Bay View", "California Star", "Sing Tao Daily", "San Francisco Call", "The San Francisco Examiner", "The Daily Alta California"], "ans_acc": 0.5, "ans_hit": 1, "ans_f1": 0.6666666666666666, "ans_precission": 1.0, "ans_recall": 0.5, "path_f1": 0.6666666666666666, "path_precision": 1.0, "path_recall": 0.5, "path_ans_f1": 0.6666666666666666, "path_ans_precision": 1.0, "path_ans_recall": 0.5}
{"id": "WebQTest-93", "prediction": ["# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> base.locations.countries.continent -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Asia\n# Answer:\nAsia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Eurasia\n# Answer:\nEurasia", "# Reasoning Path:\nArmenia -> location.location.containedby -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2014 Winter Olympics -> olympics.olympic_games.participating_countries -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2010 Winter Olympics -> olympics.olympic_games.participating_countries -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> olympics.olympic_participating_country.olympics_participated_in -> 2002 Winter Olympics -> olympics.olympic_games.participating_countries -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> biology.breed_origin.breeds_originating_here -> Caucasian Shepherd Dog -> biology.animal_breed.place_of_origin -> Europe\n# Answer:\nEurope", "# Reasoning Path:\nArmenia -> location.location.partially_containedby -> Eurasia\n# Answer:\nEurasia"], "ground_truth": ["Europe"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.8235294117647058, "ans_precission": 0.7, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.8235294117647058, "path_ans_precision": 0.7, "path_ans_recall": 1.0}
{"id": "WebQTest-94", "prediction": ["# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> heart attack\n# Answer:\nheart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Cancer\n# Answer:\nCancer", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Cardiovascular disease\n# Answer:\nCardiovascular disease", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Heart failure\n# Answer:\nHeart failure", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Complication\n# Answer:\nComplication", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Aortic dissection\n# Answer:\nAortic dissection", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Cardiac arrest\n# Answer:\nCardiac arrest", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Heart attack\n# Answer:\nHeart attack", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Prostate cancer\n# Answer:\nProstate cancer", "# Reasoning Path:\nRandy Savage -> people.deceased_person.cause_of_death -> Head and neck cancer\n# Answer:\nHead and neck cancer"], "ground_truth": ["heart attack"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.33333333333333337, "path_precision": 0.2, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
{"id": "WebQTest-95", "prediction": ["# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 1: 1821-1836\n# Answer:\nThe Correspondence of Charles Darwin, Volume 1: 1821-1836", "# Reasoning Path:\nCharles Darwin -> people.person.quotations -> There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one; and that, whilst this planet has gone cycling on according to the fixed law of gravity, from so simple a beginning endless forms most beautiful and most wonderful have been, and are being, evolved. -> media_common.quotation.source -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> base.concepts.concept_developer.concepts_developed -> Natural selection -> book.book_subject.works -> On the origin of species by means of natural selection\n# Answer:\nOn the origin of species by means of natural selection", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 6: 1856-1857\n# Answer:\nThe Correspondence of Charles Darwin, Volume 6: 1856-1857", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 12: 1864\n# Answer:\nThe Correspondence of Charles Darwin, Volume 12: 1864", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 3: 1844-1846\n# Answer:\nThe Correspondence of Charles Darwin, Volume 3: 1844-1846", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 7: 1858-1859\n# Answer:\nThe Correspondence of Charles Darwin, Volume 7: 1858-1859", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 4: 1847-1850\n# Answer:\nThe Correspondence of Charles Darwin, Volume 4: 1847-1850", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 2: 1838-1843\n# Answer:\nThe Correspondence of Charles Darwin, Volume 2: 1838-1843", "# Reasoning Path:\nCharles Darwin -> book.author.series_written_or_contributed_to -> The Correspondence of Charles Darwin -> book.literary_series.works_in_this_series -> The Correspondence of Charles Darwin, Volume 5: 1881-1885\n# Answer:\nThe Correspondence of Charles Darwin, Volume 5: 1881-1885"], "ground_truth": ["Kleinere geologische Abhandlungen", "A student's introduction to Charles Darwin", "Les mouvements et les habitudes des plantes grimpantes", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Lepadidae; or, Pedunculated Cirripedes.", "The Correspondence of Charles Darwin, Volume 13: 1865", "Monographs of the fossil Lepadidae and the fossil Balanidae", "A Monograph on the Fossil Balanid\u00e6 and Verrucid\u00e6 of Great Britain", "The Essential Darwin", "The Darwin Reader First Edition", "The Correspondence of Charles Darwin, Volume 1: 1821-1836", "Questions about the breeding of animals", "Rejse om jorden", "Insectivorous Plants", "The Variation of Animals and Plants under Domestication", "The Correspondence of Charles Darwin, Volume 18: 1870", "Resa kring jorden", "Darwin's Ornithological notes", "Evolution", "On the origin of species by means of natural selection", "From Darwin's unpublished notebooks", "Darwin's journal", "Motsa ha-minim", "Darwin Compendium", "Geology from A Manual of scientific enquiry; prepared for the use of Her Majesty's Navy: and adapted for travellers in general", "The Autobiography of Charles Darwin", "Darwin", "Darwinism stated by Darwin himself", "Darwin and Henslow", "Darwin for Today", "A Monograph of the Sub-class Cirripedia, with Figures of all the Species. The Balanidae (or Sessile Cirripedes); the Verrucidae, etc.", "Diary of the voyage of H.M.S. Beagle", "Charles Darwin's zoology notes & specimen lists from H.M.S. Beagle", "Evolution and natural selection", "Tesakneri tsagume\u030c", "Charles Darwin's natural selection", "Geological observations on the volcanic islands and parts of South America visited during the voyage of H.M.S. 'Beagle", "The Descent of Man, and Selection in Relation to Sex", "El Origin De Las Especies", "A Monograph on the Fossil Lepadidae, or, Pedunculated Cirripedes of Great Britain", "More Letters of Charles Darwin", "Les moyens d'expression chez les animaux", "vari\u00eberen der huisdieren en cultuurplanten", "Darwin from Insectivorous Plants to Worms", "Part I: Contributions to the Theory of Natural Selection / Part II", "The Correspondence of Charles Darwin, Volume 17: 1869", "The Life and Letters of Charles Darwin Volume 2", "Volcanic Islands", "The Effects of Cross and Self Fertilisation in the Vegetable Kingdom", "Vospominanii\ufe20a\ufe21 o razvitii moego uma i kharaktera", "The Voyage of the Beagle", "Charles Darwin on the routes of male humble bees", "The Correspondence of Charles Darwin, Volume 4: 1847-1850", "Diario del Viaje de Un Naturalista Alrededor", "The Correspondence of Charles Darwin, Volume 9: 1861", "From so simple a beginning", "Leben und Briefe von Charles Darwin", "\u00dcber die Wege der Hummel-M\u00e4nnchen", "Fertilisation of Orchids", "Darwin Darwin", "Darwin en Patagonia", "Voyage d'un naturaliste autour du monde", "The Correspondence of Charles Darwin, Volume 10: 1862", "To the members of the Down Friendly Club", "\u00dcber den Bau und die Verbreitung der Corallen-Riffe", "Die geschlechtliche Zuchtwahl", "The Life and Letters of Charles Darwin Volume 1", "Origins", "The Correspondence of Charles Darwin, Volume 2: 1837-1843", "The collected papers of Charles Darwin", "Die Entstehung der Arten durch nat\u00fcrliche Zuchtwahl", "Notebooks on transmutation of species", "Reise eines Naturforschers um die Welt", "Het uitdrukken van emoties bij mens en dier", "Charles Darwin, 1809-1882--Anton Dohrn, 1840-1909", "Viaje de Un Naturalista Alrededor del Mundo 2 Vol", "The Correspondence of Charles Darwin, Volume 6: 1856-1857", "The zoology of the voyage of H.M.S. Beagle during the years 1832-1836", "The Power of Movement in Plants", "Geological Observations on the Volcanic Islands", "The portable Darwin", "Die verschiedenen Bl\u00fctenformen an Pflanzen der n\u00e4mlichen Art", "On Natural Selection", "Opsht\u0323amung fun menshen", "The Formation of Vegetable Mould through the Action of Worms", "Der Ausdruck der Gem\u00fcthsbewegungen bei dem Menschen und den Thieren", "Wu zhong qi yuan", "The Different Forms of Flowers on Plants of the Same Species", "Beagle letters", "The voyage of Charles Darwin", "H.M.S. Beagle in South America", "Seul celui qui change reste fid\u00e8le \u00e0 lui-m\u00eame", "The Expression of the Emotions in Man and Animals", "Die fundamente zur entstehung der arten", "The Darwin Reader Second Edition", "ontstaan der soorten door natuurlijke teeltkeus", "genese\u014ds t\u014dn eid\u014dn", "Letters from C. Darwin, Esq., to A. Hancock, Esq", "The action of carbonate of ammonia on the roots of certain plants", "The education of Darwin", "Human nature, Darwin's view", "On a remarkable bar of sandstone off Pernambuco", "Das Variiren der Thiere und Pflanzen im Zustande der Domestication", "Darwin-Wallace", "The Correspondence of Charles Darwin, Volume 8: 1860", "Les r\u00e9cifs de corail, leur structure et leur distribution", "The Orgin of Species", "Geological Observations on South America", "A Darwin Selection", "Works", "South American Geology", "The principal works", "Darwin's notebooks on transmutation of species", "Die Wirkungen der Kreuz- und Selbst-Befruchtung im Pflanzenreich", "The Correspondence of Charles Darwin, Volume 15: 1867", "The Correspondence of Charles Darwin, Volume 7: 1858-1859", "On the tendency of species to form varieties", "The Structure and Distribution of Coral Reefs", "The living thoughts of Darwin", "La vie et la correspondance de Charles Darwin", "Memorias y epistolario i\u0301ntimo", "On evolution", "Cartas de Darwin 18251859", "The Correspondence of Charles Darwin, Volume 3: 1844-1846", "The Correspondence of Charles Darwin, Volume 16: 1868", "The Correspondence of Charles Darwin, Volume 5: 1851-1855", "Metaphysics, Materialism, & the evolution of mind", "The Correspondence of Charles Darwin, Volume 14: 1866", "Proiskhozhdenie vidov", "red notebook of Charles Darwin", "Darwin on humus and the earthworm", "Un m\u00e9moire in\u00e9dit de Charles Darwin sur l'instinct", "monograph on the sub-class Cirripedia", "The\u0301orie de l'e\u0301volution", "The foundations of the Origin of species", "Journal of researches into the natural history and geology of the countries visited during the voyage round the world of the H. M. S. 'Beagle' under the command of Captain Fitz Roy, R. N", "Charles Darwin's letters", "The geology of the voyage of H.M.S. Beagle", "Evolutionary Writings: Including the Autobiographies", "Evolution by natural selection", "Charles Darwin", "La descendance de l'homme et la s\u00a9\u00d8election sexuelle", "Reise um die Welt 1831 - 36", "Del Plata a Tierra del Fuego", "The Life of Erasmus Darwin", "The Correspondence of Charles Darwin, Volume 12: 1864", "Darwin's insects", "Gesammelte kleinere Schriften", "Notes on the fertilization of orchids", "The Correspondence of Charles Darwin, Volume 11: 1863", "Charles Darwin's marginalia", "On the Movements and Habits of Climbing Plants", "La facult\u00e9 motrice dans les plantes", "Die Bewegungen und Lebensweise der kletternden Pflanzen"], "ans_acc": 0.0718954248366013, "ans_hit": 1, "ans_f1": 0.1111111111111111, "ans_precission": 1.0, "ans_recall": 0.058823529411764705, "path_f1": 0.5, "path_precision": 1.0, "path_recall": 0.3333333333333333, "path_ans_f1": 0.13414634146341461, "path_ans_precision": 1.0, "path_ans_recall": 0.0718954248366013}
{"id": "WebQTest-96", "prediction": ["# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> Yorba Linda\n# Answer:\nYorba Linda", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> La Ca\u00f1ada de la Cuesta\n# Answer:\nLa Ca\u00f1ada de la Cuesta", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> La Paloma\n# Answer:\nLa Paloma", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> Plano\n# Answer:\nPlano", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> Stonewall\n# Answer:\nStonewall", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> La Mirada\n# Answer:\nLa Mirada", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> Bel-Air\n# Answer:\nBel-Air", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> La Mesa\n# Answer:\nLa Mesa", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> La Habana\n# Answer:\nLa Habana", "# Reasoning Path:\nRichard Nixon -> people.deceased_person.place_of_death -> California\n# Answer:\nCalifornia"], "ground_truth": ["New York City"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-97", "prediction": ["# Reasoning Path:\nSmokey Robinson -> music.artist.track -> Your Nose Is Gonna Grow\n# Answer:\nYour Nose Is Gonna Grow", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> I Don't Need You\n# Answer:\nI Don't Need You", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> Cry on My Shoulder\n# Answer:\nCry on My Shoulder", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> It's Not My Cross to Bear\n# Answer:\nIt's Not My Cross to Bear", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> Little Martha\n# Answer:\nLittle Martha", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> It Takes Time\n# Answer:\nIt Takes Time", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> No One Really Loves\n# Answer:\nNo One Really Loves", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> Don't Want You No More\n# Answer:\nDon't Want You No More", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> One More Night\n# Answer:\nOne More Night", "# Reasoning Path:\nSmokey Robinson -> music.artist.track -> Don't Be a Fool\n# Answer:\nDon't Be a Fool"], "ground_truth": ["Tell Me Tomorrow (12\\\" extended mix)", "The Tracks of My Heart", "Did You Know (Berry's Theme)", "Crusin'", "Really Gonna Miss You", "Christmas Greeting", "Love Letters", "A Tattoo", "Time Flies", "You Made Me Feel Love", "You've Really Go a Hold on Me", "Love Is The Light", "Will You Still Love Me Tomorrow", "The Family Song", "Ooo Baby Baby", "Ebony Eyes (Duet with Rick James)", "Ebony Eyes", "Unless You Do It Again", "Some People Will Do Anything for Love", "We've Saved The Best For Last (Kenny G with Smokey Robinson)", "I Can\u2019t Stand to See You Cry (Commercial version)", "Season's Greetings from Smokey Robinson", "Girlfriend", "Food For Thought", "A Child Is Waiting", "Jasmin", "It's a Good Feeling", "And I Love Her", "Theme From the Big Time", "You're the One for Me (feat. Joss Stone)", "Will You Love Me Tomorrow", "Santa Claus is Coming to Town", "Never My Love / Never Can Say Goodbye", "Take Me Through The Night", "Baby Come Close", "Love' n Life", "Save Me", "I Know You by Heart", "You Go to My Head", "There Will Come a Day (I'm Gonna Happen to You)", "When A Woman Cries", "Jingle Bells", "He Can Fix Anything", "And I Don't Love You (Larry Levan instrumental dub)", "Gang Bangin'", "Let Me Be The Clock", "The Tears Of A Clown", "Pops, We Love You", "Yes It's You Lady", "Hanging on by a Thread", "If You Wanna Make Love (Come 'round Here)", "First Time on a Ferris Wheel (Love Theme From \\\"Berry Gordy's The Last Dragon\\\")", "Skid Row", "Easy", "Be Kind To The Growing Mind (with The Temptations)", "Sleepless Nights", "Tears of a Clown", "Quiet Storm", "There Will Come A Day ( I'm Gonna Happen To You )", "We Are The Warriors", "Whole Lot of Shakin\u2019 in My Heart (Since I Met You)", "In My Corner", "What's Too Much", "Hold on to Your Love", "Satisfy You", "I Love Your Face", "You Really Got a Hold on Me", "I Like Your Face", "Be Careful What You Wish For (instrumental)", "Ever Had A Dream", "Tracks of My Tears", "Quiet Storm (Groove Boutique Chill Jazz mix feat. Ray Ayers)", "Just to See Her", "Rewind", "The Tracks of My Tears (live)", "Melody Man", "Quiet Storm (Groove Boutique Chill Jazz mix)", "I Can\u2019t Stand to See You Cry (Stereo Promo version)", "Keep Me", "Get Ready", "I Can't Give You Anything but Love", "Share It", "Can't Fight Love", "I Want You Back", "Quiet Storm (Groove Boutique remix)", "Time After Time", "The Love Between Me and My Kids", "My World", "It's A Good Night", "(It's The) Same Old Love", "The Tracks of My Tears", "Love Don't Give No Reason", "Photograph in My Mind", "I Have Prayed On It", "You Don't Know What It's Like", "Aqui Con Tigo (Being With You)", "If You Can Want", "Close Encounters of the First Kind", "Bad Girl", "It's Fantastic", "Be Who You Are", "Fallin'", "Ooh Baby Baby", "Fulfill Your Need", "My Guy", "Ooo Baby Baby (live)", "Come to Me Soon", "Mother's Son", "Blame It on Love", "If You Want My Love", "Tracks Of My Tears (Live)", "You Take Me Away", "Going to a Go-Go", "I've Made Love To You A Thousand Times", "We\u2019ve Come Too Far to End It Now", "The Tears of a Clown", "Tea for Two", "You Are So Beautiful (feat. Dave Koz)", "We've Saved the Best for Last", "Te Quiero Como Si No Hubiera Un Manana", "Noel", "That Place", "Pops, We Love You (disco)", "Daylight & Darkness", "Asleep on My Love", "More Than You Know", "Coincidentally", "The Agony And The Ecstasy", "Just Passing Through", "Medley: Never My Love / Never Can Say Goodbye", "Holly", "Crusin", "Heavy On Pride (Light On Love)", "I Can't Get Enough", "Love Brought Us Here", "Night and Day", "Be Kind to the Growing Mind", "More Love", "Tears of a Sweet Free Clown", "Cruisin", "Jesus Told Me To Love You", "The Road to Damascus", "Driving Thru Life in the Fast Lane", "I Second That Emotion", "You've Really Got a Hold on Me", "Be Careful What You Wish For", "Sweet Harmony", "The Christmas Song", "And I Don't Love You", "Little Girl Little Girl", "Tears Of A Clown", "Going to a Go Go", "Away in the Manger / Coventry Carol", "I've Got You Under My Skin", "Please Don't Take Your Love (feat. Carlos Santana)", "Speak Low", "When Smokey Sings Tears Of A Clown", "I Love The Nearness Of You", "So Bad", "No\u00ebl", "Tell Me Tomorrow", "Wedding Song", "Aqui Contigo (Being With You) (Eric Bodi Rivera Mix)", "Shop Around", "I Care About Detroit", "Don't Know Why", "Rack Me Back", "It's Time to Stop Shoppin' Around", "Blame It On Love (Duet with Barbara Mitchell)", "I Can't Find", "Love Bath", "I've Made Love to You a Thousand Times", "A Silent Partner in a Three-Way Love Affair", "I Am I Am", "With Your Love Came", "Mickey's Monkey", "Just My Soul Responding", "The Way You Do (The Things You Do)", "Vitamin U", "Open", "Just To See Her Again", "The Agony and the Ecstasy", "Just Another Kiss", "I Praise & Worship You Father", "Christmas Every Day", "Everything for Christmas", "The Christmas Song (Chestnuts Roasting on an Open Fire) (feat. The Temptations)", "Why", "Who's Sad", "Virgin Man", "Little Girl, Little Girl", "Our Love Is Here to Stay", "Girl I'm Standing There", "God Rest Ye Merry Gentlemen", "Just a Touch Away", "Tell Me Tomorrow, Part 1", "Quiet Storm (single version)", "Will You Love Me Tomorrow?", "My Girl", "Cruisin'", "Why Do Happy Memories Hurt So Bad", "She's Only a Baby Herself", "I\u2019ve Got You Under My Skin", "You Cannot Laugh Alone", "Standing On Jesus", "I'm in the Mood for Love", "I'm Glad There Is You", "Train of Thought", "The Tracks Of My Tears", "It's Her Turn to Live", "Gone Forever", "One Time", "As You Do", "The Hurt's On You", "I Second That Emotions", "You're Just My Life (feat. India.Arie)", "Christmas Everyday", "Don't Play Another Love Song", "Because of You It's the Best It's Ever Been", "Everything You Touch", "If You Wanna Make Love", "Walk on By", "Shoe Soul", "Going to a Gogo", "Ain't That Peculiar", "Wishful Thinking", "I Am, I Am", "Fly Me to the Moon (In Other Words)", "I'll Keep My Light In My Window", "Why Are You Running From My Love", "Being With You", "No Time to Stop Believing", "Just Like You", "Let Me Be the Clock", "Please Come Home for Christmas", "Tracks of my Tears", "Baby That's Backatcha", "Nearness of You", "Come by Here (Kum Ba Ya)", "I Hear The Children Singing", "Double Good Everything", "Love Don' Give No Reason (12 Inch Club Mix)", "One Heartbeat", "Let Your Light Shine On Me", "Wanna Know My Mind", "Same Old Love", "Winter Wonderland", "You Are Forever", "Happy (Love Theme From Lady Sings the Blues)", "Don't Wanna Be Just Physical", "Yester Love", "Deck the Halls", "It's Christmas Time", "Love So Fine", "Whatcha Gonna Do", "The Track of My Tears"], "ans_acc": 0.0, "ans_hit": 0, "ans_f1": 0, "ans_precission": 0.0, "ans_recall": 0.0, "path_f1": 0, "path_precision": 0.0, "path_recall": 0.0, "path_ans_f1": 0, "path_ans_precision": 0.0, "path_ans_recall": 0.0}
{"id": "WebQTest-98", "prediction": ["# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_to -> lee harvey oswald allegedly assassinated john fitzgerald kennedy -> base.kwebbase.kwconnection.subject -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy worked with lee harvey oswald -> base.kwebbase.kwconnection.other -> Lee Harvey Oswald\n# Answer:\nLee Harvey Oswald", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Cuban Missile Crisis -> military.military_conflict.military_personnel_involved -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> military.military_person.participated_in_conflicts -> Cuban Missile Crisis -> military.military_conflict.commanders -> m.0vmx69n -> military.military_command.military_commander -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> film.film_subject.films -> Thirteen Days -> film.film.subjects -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> base.kwebbase.kwtopic.connections_from -> john fitzgerald kennedy allegedly assassinated by lee harvey oswald -> base.kwebbase.kwconnection.other -> Lyndon B. Johnson\n# Answer:\nLyndon B. Johnson", "# Reasoning Path:\nJohn F. Kennedy -> government.us_president.vice_president -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy", "# Reasoning Path:\nJohn F. Kennedy -> book.book_subject.works -> Thirteen Days -> book.written_work.subjects -> Robert F. Kennedy\n# Answer:\nRobert F. Kennedy"], "ground_truth": ["Lyndon B. Johnson"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.4615384615384615, "ans_precission": 0.3, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.4615384615384615, "path_ans_precision": 0.3, "path_ans_recall": 1.0}
{"id": "WebQTest-99", "prediction": ["# Reasoning Path:\nSerbia -> location.country.official_language -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Serbian language\n# Answer:\nSerbian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Serbo-Croatian Language\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nSerbia -> location.country.official_language -> Serbo-Croatian Language\n# Answer:\nSerbo-Croatian Language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Romanian, Macedo Language\n# Answer:\nRomanian, Macedo Language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Croatian language\n# Answer:\nCroatian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Hungarian language\n# Answer:\nHungarian language", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Albanian language\n# Answer:\nAlbanian language", "# Reasoning Path:\nSerbia -> location.country.currency_used -> Serbian dinar\n# Answer:\nSerbian dinar", "# Reasoning Path:\nSerbia -> location.country.languages_spoken -> Russian Language\n# Answer:\nRussian Language"], "ground_truth": ["Serbian language"], "ans_acc": 1.0, "ans_hit": 1, "ans_f1": 0.33333333333333337, "ans_precission": 0.2, "ans_recall": 1.0, "path_f1": 0.18181818181818182, "path_precision": 0.1, "path_recall": 1.0, "path_ans_f1": 0.33333333333333337, "path_ans_precision": 0.2, "path_ans_recall": 1.0}
