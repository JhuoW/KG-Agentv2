#!/bin/bash

# Finetuning script for AGC-Agent
# This script trains the LLM to perform step-by-step KG navigation
# using special tokens: <REL>, </REL>, <ENT>, </ENT>, <PATH>, </PATH>

# Default settings
MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct"
OUTPUT_DIR="save_models/AGC-Meta-Llama-3.1-8B-Instruct"

# Training data paths (generated by build_gt_path_gcr.sh)
DATA_PATH="data/shortest_path_index/RoG-webqsp/train data/shortest_path_index/RoG-cwq/train"

# Training hyperparameters
NUM_EPOCHS=3
BATCH_SIZE=2
GRAD_ACCUM=8
LR=2e-5
MAX_SEQ_LEN=4096

# Run training
python finetune_agc.py \
    --model_name_or_path ${MODEL_NAME} \
    --output_dir ${OUTPUT_DIR} \
    --data_path_list ${DATA_PATH} \
    --num_train_epochs ${NUM_EPOCHS} \
    --per_device_train_batch_size ${BATCH_SIZE} \
    --gradient_accumulation_steps ${GRAD_ACCUM} \
    --learning_rate ${LR} \
    --max_seq_length ${MAX_SEQ_LEN} \
    --logging_steps 10 \
    --save_steps 500 \
    --bf16 \
    --max_neg_candidates 10 \
    --include_stop_samples \
    --include_continue_samples \
    --overwrite_output_dir
